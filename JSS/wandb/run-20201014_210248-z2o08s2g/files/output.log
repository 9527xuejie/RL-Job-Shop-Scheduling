2020-10-14 21:02:52,176	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_a052f_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=422)[0m 2020-10-14 21:02:54,925	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=336)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=336)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=301)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=301)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=321)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=321)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=367)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=367)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=404)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=404)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=412)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=412)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=326)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=326)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=407)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=407)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=308)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=308)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=384)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=384)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=388)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=388)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=333)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=333)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=414)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=414)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=383)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=383)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=364)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=364)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=373)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=373)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=379)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=379)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=411)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=411)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=417)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=417)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=392)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=392)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=390)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=390)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=419)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=419)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=374)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=374)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=362)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=362)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=315)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=315)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=305)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=305)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=408)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=408)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=413)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=413)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=423)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=423)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=316)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=316)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=429)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=429)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=313)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=313)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=376)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=376)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=389)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=389)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=370)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=370)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=409)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=409)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=300)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=300)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=369)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=369)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=297)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=297)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=381)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=381)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=378)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=378)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=401)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=401)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=426)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=426)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=399)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=399)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=433)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=433)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=380)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=380)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=396)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=396)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=400)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=400)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=318)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=318)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=309)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=309)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=314)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=314)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=395)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=395)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=319)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=319)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=307)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=307)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=361)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=361)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=328)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=328)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=317)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=317)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=375)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=375)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=330)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=330)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=386)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=386)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=356)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=356)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=310)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=310)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=320)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=320)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=359)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=359)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=377)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=377)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=357)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=357)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=299)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=299)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=304)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=304)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=298)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=298)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=302)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=302)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=368)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=368)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=329)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=329)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=312)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=312)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=397)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=397)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=365)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=365)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=303)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=303)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=394)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=394)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=363)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=363)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=324)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=324)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-14_21-03-28
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1851047078768413
        entropy_coeff: 0.0005000000000000001
        kl: 0.004071502441850801
        model: {}
        policy_loss: -0.00785889983914482
        total_loss: 507.07567087809247
        vf_explained_var: 0.540532648563385
        vf_loss: 507.0832926432292
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.05757575757576
    gpu_util_percent0: 0.36818181818181817
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5575757575757576
    vram_util_percent0: 0.08736346740610434
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16797264772787918
    mean_env_wait_ms: 1.1681326777525027
    mean_inference_ms: 5.3567412135850585
    mean_raw_obs_processing_ms: 0.43960491202796964
  time_since_restore: 28.13359045982361
  time_this_iter_s: 28.13359045982361
  time_total_s: 28.13359045982361
  timers:
    learn_throughput: 8285.934
    learn_time_ms: 19526.102
    sample_throughput: 18950.001
    sample_time_ms: 8537.836
    update_time_ms: 44.19
  timestamp: 1602709408
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      1 |          28.1336 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3614.4305555555557
    time_step_min: 3250
  date: 2020-10-14_21-03-56
  done: false
  episode_len_mean: 890.8607594936709
  episode_reward_max: 273.5959595959592
  episode_reward_mean: 217.6365234624726
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1561074058214824
        entropy_coeff: 0.0005000000000000001
        kl: 0.007923512797181806
        model: {}
        policy_loss: -0.010965243893830726
        total_loss: 127.46906661987305
        vf_explained_var: 0.8076093792915344
        vf_loss: 127.47981770833333
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.90625
    gpu_util_percent0: 0.28125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7593750000000004
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1644697490894654
    mean_env_wait_ms: 1.1643447442552444
    mean_inference_ms: 5.274918357821516
    mean_raw_obs_processing_ms: 0.43312069147187726
  time_since_restore: 55.80924940109253
  time_this_iter_s: 27.67565894126892
  time_total_s: 55.80924940109253
  timers:
    learn_throughput: 8278.375
    learn_time_ms: 19543.932
    sample_throughput: 19526.756
    sample_time_ms: 8285.657
    update_time_ms: 39.034
  timestamp: 1602709436
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      2 |          55.8092 | 323584 |  217.637 |              273.596 |              145.717 |            890.861 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3601.8677130044844
    time_step_min: 3250
  date: 2020-10-14_21-04-23
  done: false
  episode_len_mean: 885.132911392405
  episode_reward_max: 273.5959595959592
  episode_reward_mean: 219.87009333844756
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1456398169199626
        entropy_coeff: 0.0005000000000000001
        kl: 0.008224547879459957
        model: {}
        policy_loss: -0.013529085864623388
        total_loss: 61.275455474853516
        vf_explained_var: 0.8916645646095276
        vf_loss: 61.28873507181803
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.758064516129032
    gpu_util_percent0: 0.2945161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16202246416545443
    mean_env_wait_ms: 1.1639664955356654
    mean_inference_ms: 5.163724414780632
    mean_raw_obs_processing_ms: 0.4268547752872556
  time_since_restore: 82.59170770645142
  time_this_iter_s: 26.782458305358887
  time_total_s: 82.59170770645142
  timers:
    learn_throughput: 8286.405
    learn_time_ms: 19524.994
    sample_throughput: 20463.34
    sample_time_ms: 7906.432
    update_time_ms: 40.76
  timestamp: 1602709463
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      3 |          82.5917 | 485376 |   219.87 |              273.596 |              145.717 |            885.133 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3596.0099337748343
    time_step_min: 3231
  date: 2020-10-14_21-04-49
  done: false
  episode_len_mean: 878.7689873417721
  episode_reward_max: 276.47474747474763
  episode_reward_mean: 220.6047340493541
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1263898611068726
        entropy_coeff: 0.0005000000000000001
        kl: 0.008100568510902425
        model: {}
        policy_loss: -0.013406771836647144
        total_loss: 47.16934140523275
        vf_explained_var: 0.9198758602142334
        vf_loss: 47.18250052134196
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72666666666667
    gpu_util_percent0: 0.3456666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16012064049546954
    mean_env_wait_ms: 1.164612439662183
    mean_inference_ms: 5.066839245558316
    mean_raw_obs_processing_ms: 0.4213159869942824
  time_since_restore: 109.37071919441223
  time_this_iter_s: 26.779011487960815
  time_total_s: 109.37071919441223
  timers:
    learn_throughput: 8258.447
    learn_time_ms: 19591.094
    sample_throughput: 21127.332
    sample_time_ms: 7657.947
    update_time_ms: 36.581
  timestamp: 1602709489
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      4 |          109.371 | 647168 |  220.605 |              276.475 |              145.717 |            878.769 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3581.6985583224114
    time_step_min: 3204
  date: 2020-10-14_21-05-16
  done: false
  episode_len_mean: 872.4867256637168
  episode_reward_max: 280.5656565656565
  episode_reward_mean: 222.48133675567283
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 159
  episodes_total: 791
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0873714486757915
        entropy_coeff: 0.0005000000000000001
        kl: 0.006956188706681132
        model: {}
        policy_loss: -0.011262792395427823
        total_loss: 34.19948164621989
        vf_explained_var: 0.9459590911865234
        vf_loss: 34.2105925877889
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203225806451613
    gpu_util_percent0: 0.2932258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15862506856835393
    mean_env_wait_ms: 1.1666529609326048
    mean_inference_ms: 4.987482402804091
    mean_raw_obs_processing_ms: 0.4166214436489198
  time_since_restore: 135.77992701530457
  time_this_iter_s: 26.409207820892334
  time_total_s: 135.77992701530457
  timers:
    learn_throughput: 8267.911
    learn_time_ms: 19568.667
    sample_throughput: 21578.792
    sample_time_ms: 7497.732
    update_time_ms: 35.062
  timestamp: 1602709516
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      5 |           135.78 | 808960 |  222.481 |              280.566 |              145.717 |            872.487 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3564.887755102041
    time_step_min: 3204
  date: 2020-10-14_21-05-43
  done: false
  episode_len_mean: 860.6943942133815
  episode_reward_max: 280.5656565656565
  episode_reward_mean: 225.7112809834327
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 315
  episodes_total: 1106
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0886386533578236
        entropy_coeff: 0.0005000000000000001
        kl: 0.007588425030310948
        model: {}
        policy_loss: -0.01092883839737624
        total_loss: 30.730765342712402
        vf_explained_var: 0.9611188769340515
        vf_loss: 30.741480032602947
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.506666666666664
    gpu_util_percent0: 0.31133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15665935939519685
    mean_env_wait_ms: 1.1717385763775916
    mean_inference_ms: 4.878261275270092
    mean_raw_obs_processing_ms: 0.41058148949868417
  time_since_restore: 162.52765536308289
  time_this_iter_s: 26.74772834777832
  time_total_s: 162.52765536308289
  timers:
    learn_throughput: 8262.231
    learn_time_ms: 19582.119
    sample_throughput: 21815.562
    sample_time_ms: 7416.357
    update_time_ms: 35.692
  timestamp: 1602709543
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      6 |          162.528 | 970752 |  225.711 |              280.566 |              145.717 |            860.694 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3552.704692556634
    time_step_min: 3179
  date: 2020-10-14_21-06-09
  done: false
  episode_len_mean: 855.0387658227849
  episode_reward_max: 284.35353535353545
  episode_reward_mean: 227.46978487405684
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0706470410029094
        entropy_coeff: 0.0005000000000000001
        kl: 0.007387861027382314
        model: {}
        policy_loss: -0.013462736414415607
        total_loss: 19.742233912150066
        vf_explained_var: 0.9632093906402588
        vf_loss: 19.75549300511678
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490322580645167
    gpu_util_percent0: 0.2838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1559362762181637
    mean_env_wait_ms: 1.1739296740367058
    mean_inference_ms: 4.838086770027385
    mean_raw_obs_processing_ms: 0.4083166221477109
  time_since_restore: 189.12698912620544
  time_this_iter_s: 26.59933376312256
  time_total_s: 189.12698912620544
  timers:
    learn_throughput: 8266.464
    learn_time_ms: 19572.093
    sample_throughput: 21985.221
    sample_time_ms: 7359.125
    update_time_ms: 33.432
  timestamp: 1602709569
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      7 |          189.127 | 1132544 |   227.47 |              284.354 |              145.717 |            855.039 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3540.6398852223815
    time_step_min: 3179
  date: 2020-10-14_21-06-36
  done: false
  episode_len_mean: 850.7552742616034
  episode_reward_max: 284.35353535353545
  episode_reward_mean: 229.23441162681647
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0475670397281647
        entropy_coeff: 0.0005000000000000001
        kl: 0.007399068369219701
        model: {}
        policy_loss: -0.009183195322596779
        total_loss: 16.652963479359943
        vf_explained_var: 0.9667003154754639
        vf_loss: 16.661930561065674
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.71666666666666
    gpu_util_percent0: 0.318
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15529763017321097
    mean_env_wait_ms: 1.1758429577289389
    mean_inference_ms: 4.802703090940908
    mean_raw_obs_processing_ms: 0.40625843821594676
  time_since_restore: 215.48828315734863
  time_this_iter_s: 26.36129403114319
  time_total_s: 215.48828315734863
  timers:
    learn_throughput: 8278.422
    learn_time_ms: 19543.82
    sample_throughput: 22141.326
    sample_time_ms: 7307.241
    update_time_ms: 31.549
  timestamp: 1602709596
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      8 |          215.488 | 1294336 |  229.234 |              284.354 |              145.717 |            850.755 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3526.8721374045804
    time_step_min: 3179
  date: 2020-10-14_21-07-03
  done: false
  episode_len_mean: 845.9875
  episode_reward_max: 285.7171717171716
  episode_reward_mean: 231.28724747474732
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 178
  episodes_total: 1600
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9999773452679316
        entropy_coeff: 0.0005000000000000001
        kl: 0.007928823702968657
        model: {}
        policy_loss: -0.011458211791856835
        total_loss: 16.58501172065735
        vf_explained_var: 0.9729644656181335
        vf_loss: 16.596176783243816
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993548387096773
    gpu_util_percent0: 0.29387096774193544
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15464999594543705
    mean_env_wait_ms: 1.1780557616549023
    mean_inference_ms: 4.76792634372993
    mean_raw_obs_processing_ms: 0.40415644333563394
  time_since_restore: 242.3112816810608
  time_this_iter_s: 26.822998523712158
  time_total_s: 242.3112816810608
  timers:
    learn_throughput: 8272.764
    learn_time_ms: 19557.188
    sample_throughput: 22220.441
    sample_time_ms: 7281.224
    update_time_ms: 31.962
  timestamp: 1602709623
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      9 |          242.311 | 1456128 |  231.287 |              285.717 |              145.717 |            845.987 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3508.0337259100643
    time_step_min: 3173
  date: 2020-10-14_21-07-29
  done: false
  episode_len_mean: 839.0052742616034
  episode_reward_max: 285.7171717171716
  episode_reward_mean: 234.27066551591852
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 296
  episodes_total: 1896
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9948871235052744
        entropy_coeff: 0.0005000000000000001
        kl: 0.006681857941051324
        model: {}
        policy_loss: -0.011002168253374597
        total_loss: 15.828110535939535
        vf_explained_var: 0.9757750630378723
        vf_loss: 15.838941733042398
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62666666666667
    gpu_util_percent0: 0.243
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15383279552979012
    mean_env_wait_ms: 1.1816130020394002
    mean_inference_ms: 4.721096556773406
    mean_raw_obs_processing_ms: 0.401484041778798
  time_since_restore: 268.6884000301361
  time_this_iter_s: 26.377118349075317
  time_total_s: 268.6884000301361
  timers:
    learn_throughput: 8283.974
    learn_time_ms: 19530.724
    sample_throughput: 22303.452
    sample_time_ms: 7254.124
    update_time_ms: 30.675
  timestamp: 1602709649
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     10 |          268.688 | 1617920 |  234.271 |              285.717 |              145.717 |            839.005 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3499.19842053307
    time_step_min: 3171
  date: 2020-10-14_21-07-56
  done: false
  episode_len_mean: 835.7263875365142
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 235.87525203347977
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.975288137793541
        entropy_coeff: 0.0005000000000000001
        kl: 0.007294710182274382
        model: {}
        policy_loss: -0.012727556153549813
        total_loss: 11.962000767389933
        vf_explained_var: 0.9750909805297852
        vf_loss: 11.974486589431763
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49032258064517
    gpu_util_percent0: 0.2429032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15346055964066002
    mean_env_wait_ms: 1.1832618180585688
    mean_inference_ms: 4.700248117692935
    mean_raw_obs_processing_ms: 0.4002798984804907
  time_since_restore: 295.3983256816864
  time_this_iter_s: 26.709925651550293
  time_total_s: 295.3983256816864
  timers:
    learn_throughput: 8281.307
    learn_time_ms: 19537.012
    sample_throughput: 22772.438
    sample_time_ms: 7104.729
    update_time_ms: 28.523
  timestamp: 1602709676
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     11 |          295.398 | 1779712 |  235.875 |              294.202 |              145.717 |            835.726 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3490.923534798535
    time_step_min: 3159
  date: 2020-10-14_21-08-23
  done: false
  episode_len_mean: 832.6595840867993
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 237.1319752680511
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9521185209353765
        entropy_coeff: 0.0005000000000000001
        kl: 0.006661186693236232
        model: {}
        policy_loss: -0.013089668517447231
        total_loss: 12.603836615880331
        vf_explained_var: 0.9737562537193298
        vf_loss: 12.61673672993978
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.477419354838712
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15312252956972325
    mean_env_wait_ms: 1.1848090383105752
    mean_inference_ms: 4.681388267276605
    mean_raw_obs_processing_ms: 0.399154195717825
  time_since_restore: 322.18752455711365
  time_this_iter_s: 26.789198875427246
  time_total_s: 322.18752455711365
  timers:
    learn_throughput: 8286.795
    learn_time_ms: 19524.074
    sample_throughput: 23017.742
    sample_time_ms: 7029.013
    update_time_ms: 27.778
  timestamp: 1602709703
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     12 |          322.188 | 1941504 |  237.132 |              294.202 |              145.717 |             832.66 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3477.030998389694
    time_step_min: 3151
  date: 2020-10-14_21-08-50
  done: false
  episode_len_mean: 827.7671178343949
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 239.24816235604442
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 300
  episodes_total: 2512
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9211943199237188
        entropy_coeff: 0.0005000000000000001
        kl: 0.0069003046955913305
        model: {}
        policy_loss: -0.011187698284629732
        total_loss: 15.527917702992758
        vf_explained_var: 0.9792836308479309
        vf_loss: 15.538876056671143
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.738709677419354
    gpu_util_percent0: 0.22483870967741937
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15257686397002923
    mean_env_wait_ms: 1.1876912397652413
    mean_inference_ms: 4.650923491552926
    mean_raw_obs_processing_ms: 0.39736320906724354
  time_since_restore: 349.16273403167725
  time_this_iter_s: 26.9752094745636
  time_total_s: 349.16273403167725
  timers:
    learn_throughput: 8272.827
    learn_time_ms: 19557.038
    sample_throughput: 23048.074
    sample_time_ms: 7019.762
    update_time_ms: 28.051
  timestamp: 1602709730
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     13 |          349.163 | 2103296 |  239.248 |              294.202 |              145.717 |            827.767 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3468.7953348382243
    time_step_min: 3151
  date: 2020-10-14_21-09-17
  done: false
  episode_len_mean: 825.1623231571109
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 240.4743300465563
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 174
  episodes_total: 2686
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9143994450569153
        entropy_coeff: 0.0005000000000000001
        kl: 0.0060155229875817895
        model: {}
        policy_loss: -0.012139652118397256
        total_loss: 10.54153060913086
        vf_explained_var: 0.979185163974762
        vf_loss: 10.553526004155477
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.141935483870967
    gpu_util_percent0: 0.3683870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.152289456661776
    mean_env_wait_ms: 1.1891260627894187
    mean_inference_ms: 4.635358873544114
    mean_raw_obs_processing_ms: 0.3964728387235993
  time_since_restore: 375.75564908981323
  time_this_iter_s: 26.592915058135986
  time_total_s: 375.75564908981323
  timers:
    learn_throughput: 8285.665
    learn_time_ms: 19526.737
    sample_throughput: 23035.609
    sample_time_ms: 7023.561
    update_time_ms: 28.254
  timestamp: 1602709757
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     14 |          375.756 | 2265088 |  240.474 |              294.202 |              145.717 |            825.162 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3462.4602272727275
    time_step_min: 3131
  date: 2020-10-14_21-09-44
  done: false
  episode_len_mean: 822.9542897327707
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 241.45540851553497
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9019962549209595
        entropy_coeff: 0.0005000000000000001
        kl: 0.006133316123547654
        model: {}
        policy_loss: -0.012229806568939239
        total_loss: 9.555021127065023
        vf_explained_var: 0.9795403480529785
        vf_loss: 9.567088762919107
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84333333333333
    gpu_util_percent0: 0.38433333333333325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520571188276099
    mean_env_wait_ms: 1.1903605198268268
    mean_inference_ms: 4.622489399949343
    mean_raw_obs_processing_ms: 0.39571469986441193
  time_since_restore: 402.6329152584076
  time_this_iter_s: 26.87726616859436
  time_total_s: 402.6329152584076
  timers:
    learn_throughput: 8271.367
    learn_time_ms: 19560.491
    sample_throughput: 22997.017
    sample_time_ms: 7035.347
    update_time_ms: 27.999
  timestamp: 1602709784
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     15 |          402.633 | 2426880 |  241.455 |              294.202 |              145.717 |            822.954 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3453.8821989528797
    time_step_min: 3083
  date: 2020-10-14_21-10-10
  done: false
  episode_len_mean: 819.9792477302204
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 242.75903981448718
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 240
  episodes_total: 3084
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8659086326758066
        entropy_coeff: 0.0005000000000000001
        kl: 0.00634678197093308
        model: {}
        policy_loss: -0.012134946203635385
        total_loss: 13.195513248443604
        vf_explained_var: 0.980156421661377
        vf_loss: 13.207446098327637
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.309677419354838
    gpu_util_percent0: 0.36096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1517332658779976
    mean_env_wait_ms: 1.192259437186598
    mean_inference_ms: 4.604579073734175
    mean_raw_obs_processing_ms: 0.3946603100703554
  time_since_restore: 429.12290596961975
  time_this_iter_s: 26.489990711212158
  time_total_s: 429.12290596961975
  timers:
    learn_throughput: 8279.016
    learn_time_ms: 19542.42
    sample_throughput: 23020.64
    sample_time_ms: 7028.128
    update_time_ms: 26.01
  timestamp: 1602709810
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     16 |          429.123 | 2588672 |  242.759 |              298.899 |              145.717 |            819.979 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3445.377811550152
    time_step_min: 3083
  date: 2020-10-14_21-10-37
  done: false
  episode_len_mean: 817.4445449065702
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 243.9846110289147
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 234
  episodes_total: 3318
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8606445689996084
        entropy_coeff: 0.0005000000000000001
        kl: 0.005582816433161497
        model: {}
        policy_loss: -0.011729711521184072
        total_loss: 9.780861934026083
        vf_explained_var: 0.9827695488929749
        vf_loss: 9.792463779449463
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.576666666666664
    gpu_util_percent0: 0.37900000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1514577798692161
    mean_env_wait_ms: 1.1938746610135567
    mean_inference_ms: 4.589150214465355
    mean_raw_obs_processing_ms: 0.3937904501711353
  time_since_restore: 455.6629276275635
  time_this_iter_s: 26.540021657943726
  time_total_s: 455.6629276275635
  timers:
    learn_throughput: 8274.015
    learn_time_ms: 19554.23
    sample_throughput: 23082.272
    sample_time_ms: 7009.362
    update_time_ms: 25.933
  timestamp: 1602709837
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     17 |          455.663 | 2750464 |  243.985 |              298.899 |              145.717 |            817.445 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3440.181554524362
    time_step_min: 3083
  date: 2020-10-14_21-11-04
  done: false
  episode_len_mean: 815.873417721519
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 244.73267194383405
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8570553759733835
        entropy_coeff: 0.0005000000000000001
        kl: 0.00693794801676025
        model: {}
        policy_loss: -0.012595690621916825
        total_loss: 9.302302360534668
        vf_explained_var: 0.9800246357917786
        vf_loss: 9.314632733662924
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28064516129032
    gpu_util_percent0: 0.28935483870967743
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1512869388842221
    mean_env_wait_ms: 1.1948860891963016
    mean_inference_ms: 4.579551596598977
    mean_raw_obs_processing_ms: 0.39323684175907914
  time_since_restore: 482.1069633960724
  time_this_iter_s: 26.44403576850891
  time_total_s: 482.1069633960724
  timers:
    learn_throughput: 8269.598
    learn_time_ms: 19564.676
    sample_throughput: 23100.114
    sample_time_ms: 7003.948
    update_time_ms: 28.338
  timestamp: 1602709864
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     18 |          482.107 | 2912256 |  244.733 |              298.899 |              145.717 |            815.873 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3435.160891089109
    time_step_min: 3083
  date: 2020-10-14_21-11-30
  done: false
  episode_len_mean: 814.1853165938865
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 245.55176216311577
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 3664
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8260929683844248
        entropy_coeff: 0.0005000000000000001
        kl: 0.00627721450291574
        model: {}
        policy_loss: -0.010709817167177485
        total_loss: 11.524338483810425
        vf_explained_var: 0.9801642894744873
        vf_loss: 11.534833749135336
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583870967741937
    gpu_util_percent0: 0.4041935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15108964699814104
    mean_env_wait_ms: 1.1960450147791832
    mean_inference_ms: 4.568663786078855
    mean_raw_obs_processing_ms: 0.39260767936611063
  time_since_restore: 508.7718939781189
  time_this_iter_s: 26.66493058204651
  time_total_s: 508.7718939781189
  timers:
    learn_throughput: 8268.954
    learn_time_ms: 19566.198
    sample_throughput: 23189.124
    sample_time_ms: 6977.064
    update_time_ms: 35.073
  timestamp: 1602709890
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     19 |          508.772 | 3074048 |  245.552 |              298.899 |              145.717 |            814.185 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3428.4441611422744
    time_step_min: 3083
  date: 2020-10-14_21-11-57
  done: false
  episode_len_mean: 812.0630379746835
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 246.60976857179378
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 286
  episodes_total: 3950
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8063790599505106
        entropy_coeff: 0.0005000000000000001
        kl: 0.00581474454763035
        model: {}
        policy_loss: -0.010059737542178482
        total_loss: 10.378987232844034
        vf_explained_var: 0.9842923283576965
        vf_loss: 10.388868490854898
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.36666666666667
    gpu_util_percent0: 0.31966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15083686241602978
    mean_env_wait_ms: 1.1977366106159508
    mean_inference_ms: 4.554172739039668
    mean_raw_obs_processing_ms: 0.3917865708011254
  time_since_restore: 535.195830821991
  time_this_iter_s: 26.42393684387207
  time_total_s: 535.195830821991
  timers:
    learn_throughput: 8263.14
    learn_time_ms: 19579.966
    sample_throughput: 23222.566
    sample_time_ms: 6967.016
    update_time_ms: 35.303
  timestamp: 1602709917
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     20 |          535.196 | 3235840 |   246.61 |              298.899 |              145.717 |            812.063 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3424.633333333333
    time_step_min: 3083
  date: 2020-10-14_21-12-24
  done: false
  episode_len_mean: 811.1747809152872
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 247.171761431255
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 4108
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8050975054502487
        entropy_coeff: 0.0005000000000000001
        kl: 0.006411496355819206
        model: {}
        policy_loss: -0.0109325938198405
        total_loss: 8.397321462631226
        vf_explained_var: 0.9822394847869873
        vf_loss: 8.408015330632528
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.345161290322583
    gpu_util_percent0: 0.33387096774193553
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1507044803921526
    mean_env_wait_ms: 1.1985632518212732
    mean_inference_ms: 4.546739803666223
    mean_raw_obs_processing_ms: 0.39136140122967195
  time_since_restore: 561.7221903800964
  time_this_iter_s: 26.52635955810547
  time_total_s: 561.7221903800964
  timers:
    learn_throughput: 8265.424
    learn_time_ms: 19574.556
    sample_throughput: 23297.173
    sample_time_ms: 6944.705
    update_time_ms: 43.065
  timestamp: 1602709944
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     21 |          561.722 | 3397632 |  247.172 |              298.899 |              145.717 |            811.175 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3421.5082547169814
    time_step_min: 3083
  date: 2020-10-14_21-12-50
  done: false
  episode_len_mean: 810.3659793814433
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 247.6299262541061
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 160
  episodes_total: 4268
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7943403224150339
        entropy_coeff: 0.0005000000000000001
        kl: 0.006092905105712513
        model: {}
        policy_loss: -0.012508889408006022
        total_loss: 10.069480101267496
        vf_explained_var: 0.9799533486366272
        vf_loss: 10.08177661895752
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.81666666666667
    gpu_util_percent0: 0.253
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15057738067312423
    mean_env_wait_ms: 1.1993669989655864
    mean_inference_ms: 4.539606631295978
    mean_raw_obs_processing_ms: 0.390950346310237
  time_since_restore: 588.3143429756165
  time_this_iter_s: 26.59215259552002
  time_total_s: 588.3143429756165
  timers:
    learn_throughput: 8259.281
    learn_time_ms: 19589.114
    sample_throughput: 23421.01
    sample_time_ms: 6907.986
    update_time_ms: 44.326
  timestamp: 1602709970
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     22 |          588.314 | 3559424 |   247.63 |              298.899 |              145.717 |            810.366 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3415.407570422535
    time_step_min: 3083
  date: 2020-10-14_21-13-17
  done: false
  episode_len_mean: 808.5879265091863
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 248.62850287653427
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 304
  episodes_total: 4572
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7644506990909576
        entropy_coeff: 0.0005000000000000001
        kl: 0.005771325357879202
        model: {}
        policy_loss: -0.009549629636846172
        total_loss: 10.615382512410482
        vf_explained_var: 0.9846494197845459
        vf_loss: 10.624737024307251
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08709677419355
    gpu_util_percent0: 0.3470967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15036060576180685
    mean_env_wait_ms: 1.200837895461041
    mean_inference_ms: 4.527163047815564
    mean_raw_obs_processing_ms: 0.3902497259773404
  time_since_restore: 614.8627808094025
  time_this_iter_s: 26.54843783378601
  time_total_s: 614.8627808094025
  timers:
    learn_throughput: 8272.154
    learn_time_ms: 19558.63
    sample_throughput: 23487.2
    sample_time_ms: 6888.518
    update_time_ms: 42.454
  timestamp: 1602709997
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     23 |          614.863 | 3721216 |  248.629 |              298.899 |              145.717 |            808.588 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3412.3495331069607
    time_step_min: 3083
  date: 2020-10-14_21-13-44
  done: false
  episode_len_mean: 807.6881856540084
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 249.11699271192933
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 168
  episodes_total: 4740
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7512289037307104
        entropy_coeff: 0.0005000000000000001
        kl: 0.006618439607943098
        model: {}
        policy_loss: -0.011942399355272451
        total_loss: 7.106495062510173
        vf_explained_var: 0.9852812886238098
        vf_loss: 7.118151148160298
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.763333333333332
    gpu_util_percent0: 0.272
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1502473661939675
    mean_env_wait_ms: 1.2015818529322149
    mean_inference_ms: 4.520789877874915
    mean_raw_obs_processing_ms: 0.38989796231221446
  time_since_restore: 641.3350374698639
  time_this_iter_s: 26.472256660461426
  time_total_s: 641.3350374698639
  timers:
    learn_throughput: 8271.35
    learn_time_ms: 19560.532
    sample_throughput: 23514.635
    sample_time_ms: 6880.481
    update_time_ms: 42.61
  timestamp: 1602710024
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     24 |          641.335 | 3883008 |  249.117 |              298.899 |              145.717 |            807.688 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3409.523613963039
    time_step_min: 3083
  date: 2020-10-14_21-14-10
  done: false
  episode_len_mean: 806.9183340138832
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 249.55621548271597
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 4898
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7577964961528778
        entropy_coeff: 0.0005000000000000001
        kl: 0.005755245918408036
        model: {}
        policy_loss: -0.00993577616463881
        total_loss: 7.848556240399678
        vf_explained_var: 0.9825068116188049
        vf_loss: 7.858295281728108
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06774193548387
    gpu_util_percent0: 0.2922580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1501490253393215
    mean_env_wait_ms: 1.2022478349453032
    mean_inference_ms: 4.515108016272416
    mean_raw_obs_processing_ms: 0.3895752935542475
  time_since_restore: 668.0025424957275
  time_this_iter_s: 26.667505025863647
  time_total_s: 668.0025424957275
  timers:
    learn_throughput: 8278.009
    learn_time_ms: 19544.795
    sample_throughput: 23536.559
    sample_time_ms: 6874.072
    update_time_ms: 43.337
  timestamp: 1602710050
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     25 |          668.003 | 4044800 |  249.556 |              298.899 |              145.717 |            806.918 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3405.1021997274675
    time_step_min: 3083
  date: 2020-10-14_21-14-37
  done: false
  episode_len_mean: 805.7744433688287
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 250.1943931082362
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 267
  episodes_total: 5165
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.734145000576973
        entropy_coeff: 0.0005000000000000001
        kl: 0.00586831111771365
        model: {}
        policy_loss: -0.009772113577734368
        total_loss: 11.755430380503336
        vf_explained_var: 0.9823409914970398
        vf_loss: 11.76498262087504
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.22666666666667
    gpu_util_percent0: 0.29566666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14999721678332353
    mean_env_wait_ms: 1.2033614261477694
    mean_inference_ms: 4.506158258902504
    mean_raw_obs_processing_ms: 0.3890744939361603
  time_since_restore: 694.2319819927216
  time_this_iter_s: 26.22943949699402
  time_total_s: 694.2319819927216
  timers:
    learn_throughput: 8283.812
    learn_time_ms: 19531.104
    sample_throughput: 23581.891
    sample_time_ms: 6860.858
    update_time_ms: 43.287
  timestamp: 1602710077
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     26 |          694.232 | 4206592 |  250.194 |              298.899 |              145.717 |            805.774 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3401.4015718562873
    time_step_min: 3083
  date: 2020-10-14_21-15-03
  done: false
  episode_len_mean: 804.8773268801191
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 250.75784276119336
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 207
  episodes_total: 5372
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7132567266623179
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055438703469311195
        model: {}
        policy_loss: -0.011437343899160624
        total_loss: 6.541075627009074
        vf_explained_var: 0.9872210621833801
        vf_loss: 6.5523152351379395
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.353333333333335
    gpu_util_percent0: 0.35100000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14988135384306817
    mean_env_wait_ms: 1.2041428049451586
    mean_inference_ms: 4.499582938127375
    mean_raw_obs_processing_ms: 0.38871711747737686
  time_since_restore: 720.3953409194946
  time_this_iter_s: 26.16335892677307
  time_total_s: 720.3953409194946
  timers:
    learn_throughput: 8297.771
    learn_time_ms: 19498.248
    sample_throughput: 23599.897
    sample_time_ms: 6855.623
    update_time_ms: 43.091
  timestamp: 1602710103
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     27 |          720.395 | 4368384 |  250.758 |              298.899 |              145.717 |            804.877 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3398.81697564522
    time_step_min: 3083
  date: 2020-10-14_21-15-30
  done: false
  episode_len_mean: 804.1676311030741
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 251.131851973624
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 5530
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7216055691242218
        entropy_coeff: 0.0005000000000000001
        kl: 0.005685570455777149
        model: {}
        policy_loss: -0.011318465374642983
        total_loss: 6.673397620519002
        vf_explained_var: 0.9849072098731995
        vf_loss: 6.684508363405864
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.886666666666667
    gpu_util_percent0: 0.2823333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14980043216178882
    mean_env_wait_ms: 1.2047185869915957
    mean_inference_ms: 4.494854897407615
    mean_raw_obs_processing_ms: 0.38845720586260357
  time_since_restore: 746.6311824321747
  time_this_iter_s: 26.235841512680054
  time_total_s: 746.6311824321747
  timers:
    learn_throughput: 8303.173
    learn_time_ms: 19485.562
    sample_throughput: 23620.984
    sample_time_ms: 6849.503
    update_time_ms: 40.457
  timestamp: 1602710130
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     28 |          746.631 | 4530176 |  251.132 |              298.899 |              145.717 |            804.168 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3395.749869178441
    time_step_min: 3083
  date: 2020-10-14_21-15-56
  done: false
  episode_len_mean: 803.1786148238153
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 251.58607600041373
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 231
  episodes_total: 5761
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6937208970387777
        entropy_coeff: 0.0005000000000000001
        kl: 0.005959675957759221
        model: {}
        policy_loss: -0.011802961448362717
        total_loss: 10.309924920399984
        vf_explained_var: 0.983536958694458
        vf_loss: 10.321478684743246
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.92258064516129
    gpu_util_percent0: 0.417741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496903041319913
    mean_env_wait_ms: 1.205558648276445
    mean_inference_ms: 4.488274076300299
    mean_raw_obs_processing_ms: 0.38809971432498575
  time_since_restore: 773.0392935276031
  time_this_iter_s: 26.408111095428467
  time_total_s: 773.0392935276031
  timers:
    learn_throughput: 8317.418
    learn_time_ms: 19452.189
    sample_throughput: 23590.607
    sample_time_ms: 6858.323
    update_time_ms: 32.563
  timestamp: 1602710156
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     29 |          773.039 | 4691968 |  251.586 |              298.899 |              145.717 |            803.179 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3392.510374832664
    time_step_min: 3083
  date: 2020-10-14_21-16-23
  done: false
  episode_len_mean: 802.1863757495004
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 252.1170717837939
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 243
  episodes_total: 6004
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6723145395517349
        entropy_coeff: 0.0005000000000000001
        kl: 0.005279912458111842
        model: {}
        policy_loss: -0.011053321950991327
        total_loss: 7.6361691157023115
        vf_explained_var: 0.9863631129264832
        vf_loss: 7.647030512491862
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.860000000000003
    gpu_util_percent0: 0.3133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14957739547666185
    mean_env_wait_ms: 1.2063717178260402
    mean_inference_ms: 4.481825393409482
    mean_raw_obs_processing_ms: 0.3877499422841646
  time_since_restore: 799.4389925003052
  time_this_iter_s: 26.399698972702026
  time_total_s: 799.4389925003052
  timers:
    learn_throughput: 8320.988
    learn_time_ms: 19443.845
    sample_throughput: 23585.905
    sample_time_ms: 6859.69
    update_time_ms: 34.74
  timestamp: 1602710183
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     30 |          799.439 | 4853760 |  252.117 |              298.899 |              145.717 |            802.186 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3390.042386697098
    time_step_min: 3083
  date: 2020-10-14_21-16-49
  done: false
  episode_len_mean: 801.4599156118144
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 252.45022769073393
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 6162
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6794097969929377
        entropy_coeff: 0.0005000000000000001
        kl: 0.005776377705236276
        model: {}
        policy_loss: -0.011071474878311468
        total_loss: 6.609892050425212
        vf_explained_var: 0.9848344326019287
        vf_loss: 6.620725552241008
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.277419354838713
    gpu_util_percent0: 0.25258064516129036
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1495093060246751
    mean_env_wait_ms: 1.2068785550500694
    mean_inference_ms: 4.477842010054414
    mean_raw_obs_processing_ms: 0.3875324536178687
  time_since_restore: 826.0708198547363
  time_this_iter_s: 26.631827354431152
  time_total_s: 826.0708198547363
  timers:
    learn_throughput: 8316.229
    learn_time_ms: 19454.972
    sample_throughput: 23563.199
    sample_time_ms: 6866.3
    update_time_ms: 27.804
  timestamp: 1602710209
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     31 |          826.071 | 5015552 |   252.45 |              298.899 |              145.717 |             801.46 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3386.696788413098
    time_step_min: 3083
  date: 2020-10-14_21-17-16
  done: false
  episode_len_mean: 800.385893416928
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 252.93216649251121
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 218
  episodes_total: 6380
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6597986618677775
        entropy_coeff: 0.0005000000000000001
        kl: 0.005507051052215199
        model: {}
        policy_loss: -0.01063559478886115
        total_loss: 8.718894402186075
        vf_explained_var: 0.9843838214874268
        vf_loss: 8.729309240976969
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65666666666667
    gpu_util_percent0: 0.31466666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14942336945631843
    mean_env_wait_ms: 1.2075933530118947
    mean_inference_ms: 4.472619260274858
    mean_raw_obs_processing_ms: 0.3872484802290064
  time_since_restore: 852.6489214897156
  time_this_iter_s: 26.578101634979248
  time_total_s: 852.6489214897156
  timers:
    learn_throughput: 8318.686
    learn_time_ms: 19449.227
    sample_throughput: 23545.491
    sample_time_ms: 6871.464
    update_time_ms: 25.913
  timestamp: 1602710236
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     32 |          852.649 | 5177344 |  252.932 |              298.899 |              145.717 |            800.386 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3383.374754048736
    time_step_min: 3083
  date: 2020-10-14_21-17-43
  done: false
  episode_len_mean: 799.2391861341372
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 253.41443827879388
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 255
  episodes_total: 6635
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6365775416294733
        entropy_coeff: 0.0005000000000000001
        kl: 0.005616956856101751
        model: {}
        policy_loss: -0.012718923583937189
        total_loss: 7.9217236042022705
        vf_explained_var: 0.9864194393157959
        vf_loss: 7.934199412663777
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.948387096774198
    gpu_util_percent0: 0.27838709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14932189346086894
    mean_env_wait_ms: 1.2083754344986544
    mean_inference_ms: 4.466813879680601
    mean_raw_obs_processing_ms: 0.38693984548896015
  time_since_restore: 879.1936430931091
  time_this_iter_s: 26.544721603393555
  time_total_s: 879.1936430931091
  timers:
    learn_throughput: 8314.788
    learn_time_ms: 19458.343
    sample_throughput: 23551.139
    sample_time_ms: 6869.816
    update_time_ms: 25.193
  timestamp: 1602710263
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     33 |          879.194 | 5339136 |  253.414 |              298.899 |              145.717 |            799.239 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3381.266627253917
    time_step_min: 3083
  date: 2020-10-14_21-18-10
  done: false
  episode_len_mean: 798.6435089785105
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 253.7262884957909
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 159
  episodes_total: 6794
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6548023074865341
        entropy_coeff: 0.0005000000000000001
        kl: 0.005482170420388381
        model: {}
        policy_loss: -0.012428849945232892
        total_loss: 6.324912707010905
        vf_explained_var: 0.9852306842803955
        vf_loss: 6.337120532989502
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380645161290325
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492620448104757
    mean_env_wait_ms: 1.2088387513992318
    mean_inference_ms: 4.463358913349285
    mean_raw_obs_processing_ms: 0.38675634808095327
  time_since_restore: 905.8512279987335
  time_this_iter_s: 26.65758490562439
  time_total_s: 905.8512279987335
  timers:
    learn_throughput: 8309.566
    learn_time_ms: 19470.571
    sample_throughput: 23555.989
    sample_time_ms: 6868.402
    update_time_ms: 31.554
  timestamp: 1602710290
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     34 |          905.851 | 5500928 |  253.726 |              298.899 |              145.717 |            798.644 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3378.6727925340992
    time_step_min: 3083
  date: 2020-10-14_21-18-37
  done: false
  episode_len_mean: 797.975975975976
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 254.1101534434867
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 6993
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6353265345096588
        entropy_coeff: 0.0005000000000000001
        kl: 0.005509571443932752
        model: {}
        policy_loss: -0.009562680769401291
        total_loss: 8.754625161488852
        vf_explained_var: 0.9838367104530334
        vf_loss: 8.763954242070517
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95
    gpu_util_percent0: 0.31233333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1491908616652621
    mean_env_wait_ms: 1.209401559495888
    mean_inference_ms: 4.459184196972034
    mean_raw_obs_processing_ms: 0.3865304885380238
  time_since_restore: 932.4499733448029
  time_this_iter_s: 26.598745346069336
  time_total_s: 932.4499733448029
  timers:
    learn_throughput: 8310.884
    learn_time_ms: 19467.484
    sample_throughput: 23569.578
    sample_time_ms: 6864.442
    update_time_ms: 30.441
  timestamp: 1602710317
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     35 |           932.45 | 5662720 |   254.11 |              298.899 |              145.717 |            797.976 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3375.4936446532192
    time_step_min: 3083
  date: 2020-10-14_21-19-03
  done: false
  episode_len_mean: 797.1249655931737
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 254.59083819199418
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 273
  episodes_total: 7266
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6047424674034119
        entropy_coeff: 0.0005000000000000001
        kl: 0.005252860602922738
        model: {}
        policy_loss: -0.011149611789733171
        total_loss: 9.254522800445557
        vf_explained_var: 0.9846187233924866
        vf_loss: 9.265449444452921
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.483870967741936
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1490986760045902
    mean_env_wait_ms: 1.2101597359740133
    mean_inference_ms: 4.4538385955731545
    mean_raw_obs_processing_ms: 0.3862470824192933
  time_since_restore: 959.1641254425049
  time_this_iter_s: 26.714152097702026
  time_total_s: 959.1641254425049
  timers:
    learn_throughput: 8296.583
    learn_time_ms: 19501.04
    sample_throughput: 23521.053
    sample_time_ms: 6878.604
    update_time_ms: 30.952
  timestamp: 1602710343
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     36 |          959.164 | 5824512 |  254.591 |              298.899 |              145.717 |            797.125 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3373.9128142741283
    time_step_min: 3083
  date: 2020-10-14_21-19-30
  done: false
  episode_len_mean: 796.6110961486669
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 254.86798499402855
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 160
  episodes_total: 7426
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.623786672949791
        entropy_coeff: 0.0005000000000000001
        kl: 0.005110279773361981
        model: {}
        policy_loss: -0.010506908385044275
        total_loss: 6.253582954406738
        vf_explained_var: 0.9853312373161316
        vf_loss: 6.263890822728475
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.670967741935485
    gpu_util_percent0: 0.4116129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14904566679121448
    mean_env_wait_ms: 1.2105702014679869
    mean_inference_ms: 4.450825568659911
    mean_raw_obs_processing_ms: 0.3860872100491004
  time_since_restore: 985.8022673130035
  time_this_iter_s: 26.638141870498657
  time_total_s: 985.8022673130035
  timers:
    learn_throughput: 8288.409
    learn_time_ms: 19520.271
    sample_throughput: 23434.803
    sample_time_ms: 6903.92
    update_time_ms: 33.079
  timestamp: 1602710370
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     37 |          985.802 | 5986304 |  254.868 |              298.899 |              145.717 |            796.611 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3371.3291606272237
    time_step_min: 3083
  date: 2020-10-14_21-19-57
  done: false
  episode_len_mean: 796.0093212550873
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 255.2320527050735
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 191
  episodes_total: 7617
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6084752033154169
        entropy_coeff: 0.0005000000000000001
        kl: 0.005453399266116321
        model: {}
        policy_loss: -0.012583952009057006
        total_loss: 7.687996904055278
        vf_explained_var: 0.9848602414131165
        vf_loss: 7.7003395557403564
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34193548387097
    gpu_util_percent0: 0.26677419354838716
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14898384279232157
    mean_env_wait_ms: 1.211044487723327
    mean_inference_ms: 4.447284709553837
    mean_raw_obs_processing_ms: 0.38589802963953507
  time_since_restore: 1012.4797642230988
  time_this_iter_s: 26.677496910095215
  time_total_s: 1012.4797642230988
  timers:
    learn_throughput: 8270.575
    learn_time_ms: 19562.364
    sample_throughput: 23438.367
    sample_time_ms: 6902.87
    update_time_ms: 34.854
  timestamp: 1602710397
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     38 |          1012.48 | 6148096 |  255.232 |              298.899 |              145.717 |            796.009 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3367.1486142893464
    time_step_min: 3079
  date: 2020-10-14_21-20-24
  done: false
  episode_len_mean: 795.201418799088
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 255.79623061115328
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 277
  episodes_total: 7894
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5781072477499644
        entropy_coeff: 0.0005000000000000001
        kl: 0.004660504714896281
        model: {}
        policy_loss: -0.009350795717182336
        total_loss: 7.616626938184102
        vf_explained_var: 0.9870114326477051
        vf_loss: 7.625800848007202
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.24193548387097
    gpu_util_percent0: 0.33258064516129027
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14890157515624422
    mean_env_wait_ms: 1.211739870260709
    mean_inference_ms: 4.442542860612138
    mean_raw_obs_processing_ms: 0.38565094768259334
  time_since_restore: 1038.9783704280853
  time_this_iter_s: 26.498606204986572
  time_total_s: 1038.9783704280853
  timers:
    learn_throughput: 8259.856
    learn_time_ms: 19587.75
    sample_throughput: 23475.349
    sample_time_ms: 6891.996
    update_time_ms: 36.057
  timestamp: 1602710424
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     39 |          1038.98 | 6309888 |  255.796 |              299.505 |              145.717 |            795.201 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3365.2104607721044
    time_step_min: 3079
  date: 2020-10-14_21-20-51
  done: false
  episode_len_mean: 794.7603623727972
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 256.0866896816263
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 8058
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5902702659368515
        entropy_coeff: 0.0005000000000000001
        kl: 0.005777042941190302
        model: {}
        policy_loss: -0.01098569861399786
        total_loss: 5.731792251269023
        vf_explained_var: 0.9865006804466248
        vf_loss: 5.742784023284912
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.450000000000003
    gpu_util_percent0: 0.3176666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14885388410218328
    mean_env_wait_ms: 1.2121164159753581
    mean_inference_ms: 4.439828475757026
    mean_raw_obs_processing_ms: 0.38550896828653286
  time_since_restore: 1065.4780147075653
  time_this_iter_s: 26.49964427947998
  time_total_s: 1065.4780147075653
  timers:
    learn_throughput: 8251.34
    learn_time_ms: 19607.968
    sample_throughput: 23508.224
    sample_time_ms: 6882.357
    update_time_ms: 35.768
  timestamp: 1602710451
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     40 |          1065.48 | 6471680 |  256.087 |              299.505 |              145.717 |             794.76 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3363.1631136557435
    time_step_min: 3079
  date: 2020-10-14_21-21-17
  done: false
  episode_len_mean: 794.3325239771762
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 256.39864960151465
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 8237
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.582388644417127
        entropy_coeff: 0.0005000000000000001
        kl: 0.005592259811237454
        model: {}
        policy_loss: -0.008816406540669655
        total_loss: 6.511826952298482
        vf_explained_var: 0.9866585731506348
        vf_loss: 6.520654956499736
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9516129032258
    gpu_util_percent0: 0.3506451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14880213040415996
    mean_env_wait_ms: 1.2125166192106014
    mean_inference_ms: 4.436882798358669
    mean_raw_obs_processing_ms: 0.38535330734419243
  time_since_restore: 1091.992312669754
  time_this_iter_s: 26.51429796218872
  time_total_s: 1091.992312669754
  timers:
    learn_throughput: 8251.704
    learn_time_ms: 19607.102
    sample_throughput: 23556.385
    sample_time_ms: 6868.287
    update_time_ms: 36.913
  timestamp: 1602710477
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     41 |          1091.99 | 6633472 |  256.399 |              299.505 |              145.717 |            794.333 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3359.921030952101
    time_step_min: 3079
  date: 2020-10-14_21-21-44
  done: false
  episode_len_mean: 793.6757771260997
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 256.8849847448087
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 288
  episodes_total: 8525
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5509877155224482
        entropy_coeff: 0.0005000000000000001
        kl: 0.00548382840740184
        model: {}
        policy_loss: -0.011438940273365006
        total_loss: 7.636974573135376
        vf_explained_var: 0.9875645637512207
        vf_loss: 7.648415048917134
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.083333333333336
    gpu_util_percent0: 0.38133333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14872585136465738
    mean_env_wait_ms: 1.2131497550292742
    mean_inference_ms: 4.432481682185068
    mean_raw_obs_processing_ms: 0.3851282181474167
  time_since_restore: 1118.3758387565613
  time_this_iter_s: 26.38352608680725
  time_total_s: 1118.3758387565613
  timers:
    learn_throughput: 8253.483
    learn_time_ms: 19602.876
    sample_throughput: 23620.369
    sample_time_ms: 6849.681
    update_time_ms: 39.287
  timestamp: 1602710504
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     42 |          1118.38 | 6795264 |  256.885 |              299.505 |              145.717 |            793.676 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3358.1212191179866
    time_step_min: 3079
  date: 2020-10-14_21-22-11
  done: false
  episode_len_mean: 793.313003452244
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 257.1631737396984
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 165
  episodes_total: 8690
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5553108304738998
        entropy_coeff: 0.0005000000000000001
        kl: 0.005491969912933807
        model: {}
        policy_loss: -0.011925454396987334
        total_loss: 5.948849519093831
        vf_explained_var: 0.9862935543060303
        vf_loss: 5.960778037707011
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29677419354839
    gpu_util_percent0: 0.35870967741935483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486825051742952
    mean_env_wait_ms: 1.2134859565254763
    mean_inference_ms: 4.4300261346729375
    mean_raw_obs_processing_ms: 0.38500153789518043
  time_since_restore: 1145.093224287033
  time_this_iter_s: 26.7173855304718
  time_total_s: 1145.093224287033
  timers:
    learn_throughput: 8244.585
    learn_time_ms: 19624.031
    sample_throughput: 23644.131
    sample_time_ms: 6842.797
    update_time_ms: 41.224
  timestamp: 1602710531
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     43 |          1145.09 | 6957056 |  257.163 |              299.505 |              145.717 |            793.313 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3356.3764292992187
    time_step_min: 3079
  date: 2020-10-14_21-22-37
  done: false
  episode_len_mean: 792.9971786480081
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 257.43694819769746
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 171
  episodes_total: 8861
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5548702428738276
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055360287660732865
        model: {}
        policy_loss: -0.009155239444226027
        total_loss: 6.339707454045613
        vf_explained_var: 0.9862532615661621
        vf_loss: 6.3488632043202715
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.456666666666667
    gpu_util_percent0: 0.314
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14863801267512117
    mean_env_wait_ms: 1.2138293343946893
    mean_inference_ms: 4.427510629286129
    mean_raw_obs_processing_ms: 0.38487201902743895
  time_since_restore: 1171.4629249572754
  time_this_iter_s: 26.36970067024231
  time_total_s: 1171.4629249572754
  timers:
    learn_throughput: 8251.999
    learn_time_ms: 19606.401
    sample_throughput: 23660.205
    sample_time_ms: 6838.149
    update_time_ms: 34.468
  timestamp: 1602710557
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     44 |          1171.46 | 7118848 |  257.437 |              299.505 |              145.717 |            792.997 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3353.178888523512
    time_step_min: 3079
  date: 2020-10-14_21-23-04
  done: false
  episode_len_mean: 792.5362255491203
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 257.9257231919235
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 290
  episodes_total: 9151
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5242825845877329
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052440856816247106
        model: {}
        policy_loss: -0.008842121120930338
        total_loss: 7.925516088803609
        vf_explained_var: 0.9871301651000977
        vf_loss: 7.9343580802281695
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.290322580645167
    gpu_util_percent0: 0.32387096774193547
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14856982992753753
    mean_env_wait_ms: 1.2143821059520652
    mean_inference_ms: 4.423520718371641
    mean_raw_obs_processing_ms: 0.3846649944931702
  time_since_restore: 1198.2076036930084
  time_this_iter_s: 26.744678735733032
  time_total_s: 1198.2076036930084
  timers:
    learn_throughput: 8251.095
    learn_time_ms: 19608.549
    sample_throughput: 23623.475
    sample_time_ms: 6848.781
    update_time_ms: 34.698
  timestamp: 1602710584
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     45 |          1198.21 | 7280640 |  257.926 |              299.505 |              145.717 |            792.536 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3351.434366257801
    time_step_min: 3079
  date: 2020-10-14_21-23-31
  done: false
  episode_len_mean: 792.2524136451406
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 258.21556262041133
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 171
  episodes_total: 9322
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5219554007053375
        entropy_coeff: 0.0005000000000000001
        kl: 0.005586102333230277
        model: {}
        policy_loss: -0.012437824868053818
        total_loss: 5.265158772468567
        vf_explained_var: 0.9879205822944641
        vf_loss: 5.277578234672546
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.074193548387097
    gpu_util_percent0: 0.32967741935483874
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14852860122587302
    mean_env_wait_ms: 1.2146896012471777
    mean_inference_ms: 4.421253721128335
    mean_raw_obs_processing_ms: 0.3845505713670322
  time_since_restore: 1224.8877756595612
  time_this_iter_s: 26.680171966552734
  time_total_s: 1224.8877756595612
  timers:
    learn_throughput: 8256.34
    learn_time_ms: 19596.093
    sample_throughput: 23599.786
    sample_time_ms: 6855.655
    update_time_ms: 35.786
  timestamp: 1602710611
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     46 |          1224.89 | 7442432 |  258.216 |              299.505 |              145.717 |            792.252 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3349.8470078240643
    time_step_min: 3079
  date: 2020-10-14_21-23-58
  done: false
  episode_len_mean: 791.9538266919672
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 258.45257444783056
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 9486
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5329876442750295
        entropy_coeff: 0.0005000000000000001
        kl: 0.005586201014618079
        model: {}
        policy_loss: -0.011357899541811397
        total_loss: 6.645129680633545
        vf_explained_var: 0.9848142266273499
        vf_loss: 6.656474828720093
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970000000000002
    gpu_util_percent0: 0.2813333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14849062452985368
    mean_env_wait_ms: 1.2149768754329622
    mean_inference_ms: 4.419098479277539
    mean_raw_obs_processing_ms: 0.3844401457940519
  time_since_restore: 1251.1344583034515
  time_this_iter_s: 26.24668264389038
  time_total_s: 1251.1344583034515
  timers:
    learn_throughput: 8263.988
    learn_time_ms: 19577.957
    sample_throughput: 23673.368
    sample_time_ms: 6834.346
    update_time_ms: 34.268
  timestamp: 1602710638
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     47 |          1251.13 | 7604224 |  258.453 |              299.505 |              145.717 |            791.954 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3346.939020634432
    time_step_min: 3079
  date: 2020-10-14_21-24-24
  done: false
  episode_len_mean: 791.4166240147405
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 258.9031775426493
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 283
  episodes_total: 9769
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5053925216197968
        entropy_coeff: 0.0005000000000000001
        kl: 0.005304940083685021
        model: {}
        policy_loss: -0.008086977589603824
        total_loss: 8.415432214736938
        vf_explained_var: 0.9861140847206116
        vf_loss: 8.423506577809652
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93870967741936
    gpu_util_percent0: 0.35354838709677416
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14843049830506916
    mean_env_wait_ms: 1.2154625444428422
    mean_inference_ms: 4.415577100397063
    mean_raw_obs_processing_ms: 0.38426102532863704
  time_since_restore: 1277.5222396850586
  time_this_iter_s: 26.387781381607056
  time_total_s: 1277.5222396850586
  timers:
    learn_throughput: 8279.636
    learn_time_ms: 19540.956
    sample_throughput: 23647.487
    sample_time_ms: 6841.826
    update_time_ms: 33.665
  timestamp: 1602710664
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     48 |          1277.52 | 7766016 |  258.903 |              299.505 |              145.717 |            791.417 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3345.2163006246224
    time_step_min: 3079
  date: 2020-10-14_21-24-51
  done: false
  episode_len_mean: 791.089813140446
  episode_reward_max: 299.8080808080811
  episode_reward_mean: 259.18265841050646
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 185
  episodes_total: 9954
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49424417813618976
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056748971886311965
        model: {}
        policy_loss: -0.011210046359337866
        total_loss: 5.939767042795817
        vf_explained_var: 0.9866703152656555
        vf_loss: 5.950940489768982
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53225806451613
    gpu_util_percent0: 0.2429032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7903225806451615
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483902241086303
    mean_env_wait_ms: 1.2157640504437783
    mean_inference_ms: 4.413376289258957
    mean_raw_obs_processing_ms: 0.38415024420521526
  time_since_restore: 1304.2675821781158
  time_this_iter_s: 26.74534249305725
  time_total_s: 1304.2675821781158
  timers:
    learn_throughput: 8277.637
    learn_time_ms: 19545.675
    sample_throughput: 23581.287
    sample_time_ms: 6861.034
    update_time_ms: 33.144
  timestamp: 1602710691
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     49 |          1304.27 | 7927808 |  259.183 |              299.808 |              145.717 |             791.09 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3343.438398255526
    time_step_min: 3079
  date: 2020-10-14_21-25-18
  done: false
  episode_len_mean: 790.8114065434418
  episode_reward_max: 299.8080808080811
  episode_reward_mean: 259.45167499847736
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 163
  episodes_total: 10117
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5094549457232157
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052692491638784604
        model: {}
        policy_loss: -0.011700866136076607
        total_loss: 5.405965685844421
        vf_explained_var: 0.9866686463356018
        vf_loss: 5.417657653490703
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.90322580645162
    gpu_util_percent0: 0.3296774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14835558068008722
    mean_env_wait_ms: 1.216020668613672
    mean_inference_ms: 4.411423575489114
    mean_raw_obs_processing_ms: 0.38405139892195345
  time_since_restore: 1330.9151751995087
  time_this_iter_s: 26.647593021392822
  time_total_s: 1330.9151751995087
  timers:
    learn_throughput: 8269.667
    learn_time_ms: 19564.513
    sample_throughput: 23593.072
    sample_time_ms: 6857.606
    update_time_ms: 31.58
  timestamp: 1602710718
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     50 |          1330.92 | 8089600 |  259.452 |              299.808 |              145.717 |            790.811 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3340.86744545279
    time_step_min: 3054
  date: 2020-10-14_21-25-45
  done: false
  episode_len_mean: 790.3745426535721
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 259.8196931767122
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 269
  episodes_total: 10386
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4893743023276329
        entropy_coeff: 0.0005000000000000001
        kl: 0.005739350298730035
        model: {}
        policy_loss: -0.009993007969266424
        total_loss: 9.506619215011597
        vf_explained_var: 0.9842750430107117
        vf_loss: 9.51656993230184
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.277419354838713
    gpu_util_percent0: 0.4093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14830399125015475
    mean_env_wait_ms: 1.2164398147438829
    mean_inference_ms: 4.408384117786884
    mean_raw_obs_processing_ms: 0.38390040400627307
  time_since_restore: 1357.7133193016052
  time_this_iter_s: 26.798144102096558
  time_total_s: 1357.7133193016052
  timers:
    learn_throughput: 8258.289
    learn_time_ms: 19591.468
    sample_throughput: 23586.405
    sample_time_ms: 6859.545
    update_time_ms: 30.735
  timestamp: 1602710745
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     51 |          1357.71 | 8251392 |   259.82 |              303.293 |              145.717 |            790.375 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3339.231315714692
    time_step_min: 3054
  date: 2020-10-14_21-26-12
  done: false
  episode_len_mean: 790.126594237128
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 260.0842148456697
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 10585
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47421347349882126
        entropy_coeff: 0.0005000000000000001
        kl: 0.004842397950900097
        model: {}
        policy_loss: -0.00872566036802406
        total_loss: 5.855418086051941
        vf_explained_var: 0.9877085089683533
        vf_loss: 5.864138642946879
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06451612903226
    gpu_util_percent0: 0.32516129032258073
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826426632240464
    mean_env_wait_ms: 1.2167301495521732
    mean_inference_ms: 4.40622411388077
    mean_raw_obs_processing_ms: 0.38378873770024996
  time_since_restore: 1384.352088212967
  time_this_iter_s: 26.638768911361694
  time_total_s: 1384.352088212967
  timers:
    learn_throughput: 8246.827
    learn_time_ms: 19618.696
    sample_throughput: 23589.936
    sample_time_ms: 6858.518
    update_time_ms: 29.019
  timestamp: 1602710772
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     52 |          1384.35 | 8413184 |  260.084 |              303.293 |              145.717 |            790.127 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3337.8276891501073
    time_step_min: 3054
  date: 2020-10-14_21-26-39
  done: false
  episode_len_mean: 789.9024844142551
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 260.2794174178746
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 162
  episodes_total: 10747
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4887526035308838
        entropy_coeff: 0.0005000000000000001
        kl: 0.005664059310220182
        model: {}
        policy_loss: -0.01032462390139699
        total_loss: 6.339387933413188
        vf_explained_var: 0.9850199818611145
        vf_loss: 6.349815567334493
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.97096774193549
    gpu_util_percent0: 0.29032258064516125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482340157169599
    mean_env_wait_ms: 1.2169590182543433
    mean_inference_ms: 4.404484109421736
    mean_raw_obs_processing_ms: 0.3837008279766649
  time_since_restore: 1410.92511677742
  time_this_iter_s: 26.573028564453125
  time_total_s: 1410.92511677742
  timers:
    learn_throughput: 8253.095
    learn_time_ms: 19603.798
    sample_throughput: 23590.153
    sample_time_ms: 6858.455
    update_time_ms: 29.38
  timestamp: 1602710799
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     53 |          1410.93 | 8574976 |  260.279 |              303.293 |              145.717 |            789.902 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3335.6289749430525
    time_step_min: 3054
  date: 2020-10-14_21-27-06
  done: false
  episode_len_mean: 789.6325547577933
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 260.61444491263626
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 256
  episodes_total: 11003
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4678051024675369
        entropy_coeff: 0.0005000000000000001
        kl: 0.005607184759962062
        model: {}
        policy_loss: -0.00997994407225633
        total_loss: 7.096481243769328
        vf_explained_var: 0.9879934191703796
        vf_loss: 7.106554945309957
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.136666666666667
    gpu_util_percent0: 0.3783333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481893421503586
    mean_env_wait_ms: 1.2173164053200967
    mean_inference_ms: 4.401842760352221
    mean_raw_obs_processing_ms: 0.3835707240649742
  time_since_restore: 1437.389036655426
  time_this_iter_s: 26.46391987800598
  time_total_s: 1437.389036655426
  timers:
    learn_throughput: 8250.197
    learn_time_ms: 19610.683
    sample_throughput: 23583.38
    sample_time_ms: 6860.425
    update_time_ms: 29.167
  timestamp: 1602710826
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     54 |          1437.39 | 8736768 |  260.614 |              303.293 |              145.717 |            789.633 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3333.8342867357883
    time_step_min: 3054
  date: 2020-10-14_21-27-33
  done: false
  episode_len_mean: 789.420203281027
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 260.87382022795714
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 11216
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4456804369886716
        entropy_coeff: 0.0005000000000000001
        kl: 0.005640700381870071
        model: {}
        policy_loss: -0.009555026073940098
        total_loss: 6.3164375225702925
        vf_explained_var: 0.9872992038726807
        vf_loss: 6.326074282328288
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.667741935483875
    gpu_util_percent0: 0.27741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148150428883912
    mean_env_wait_ms: 1.2175935708835073
    mean_inference_ms: 4.399705251535145
    mean_raw_obs_processing_ms: 0.3834603782849835
  time_since_restore: 1463.9117136001587
  time_this_iter_s: 26.522676944732666
  time_total_s: 1463.9117136001587
  timers:
    learn_throughput: 8253.073
    learn_time_ms: 19603.849
    sample_throughput: 23633.928
    sample_time_ms: 6845.752
    update_time_ms: 28.618
  timestamp: 1602710853
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     55 |          1463.91 | 8898560 |  260.874 |              303.293 |              145.717 |             789.42 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3332.7957349312655
    time_step_min: 3054
  date: 2020-10-14_21-27-59
  done: false
  episode_len_mean: 789.2950070323488
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 261.0446820525934
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 160
  episodes_total: 11376
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.461775283018748
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052875694042692585
        model: {}
        policy_loss: -0.009517594526793497
        total_loss: 5.4199016491572065
        vf_explained_var: 0.9872874617576599
        vf_loss: 5.42951778570811
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.835483870967746
    gpu_util_percent0: 0.26290322580645165
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481230181145526
    mean_env_wait_ms: 1.2177934427698083
    mean_inference_ms: 4.3981339830101485
    mean_raw_obs_processing_ms: 0.3833821968505351
  time_since_restore: 1490.2626390457153
  time_this_iter_s: 26.35092544555664
  time_total_s: 1490.2626390457153
  timers:
    learn_throughput: 8259.05
    learn_time_ms: 19589.662
    sample_throughput: 23699.33
    sample_time_ms: 6826.86
    update_time_ms: 27.907
  timestamp: 1602710879
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     56 |          1490.26 | 9060352 |  261.045 |              303.293 |              145.717 |            789.295 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3330.9388548233874
    time_step_min: 3054
  date: 2020-10-14_21-28-26
  done: false
  episode_len_mean: 789.062720771948
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 261.33558119316706
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 231
  episodes_total: 11607
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.44272469977537793
        entropy_coeff: 0.0005000000000000001
        kl: 0.005777890793979168
        model: {}
        policy_loss: -0.0090979779100356
        total_loss: 7.434427936871846
        vf_explained_var: 0.9864635467529297
        vf_loss: 7.443602800369263
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.86451612903226
    gpu_util_percent0: 0.262258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480864661455434
    mean_env_wait_ms: 1.2180805443476657
    mean_inference_ms: 4.395943897867924
    mean_raw_obs_processing_ms: 0.38327163873565595
  time_since_restore: 1516.941904783249
  time_this_iter_s: 26.67926573753357
  time_total_s: 1516.941904783249
  timers:
    learn_throughput: 8242.512
    learn_time_ms: 19628.967
    sample_throughput: 23690.03
    sample_time_ms: 6829.54
    update_time_ms: 27.76
  timestamp: 1602710906
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     57 |          1516.94 | 9222144 |  261.336 |              303.293 |              145.717 |            789.063 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3328.9515992553734
    time_step_min: 3054
  date: 2020-10-14_21-28-53
  done: false
  episode_len_mean: 788.7513084585514
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 261.6369298250101
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 239
  episodes_total: 11846
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4169960568348567
        entropy_coeff: 0.0005000000000000001
        kl: 0.004996339984548588
        model: {}
        policy_loss: -0.00955103572535639
        total_loss: 5.380350391070048
        vf_explained_var: 0.9896233677864075
        vf_loss: 5.389984925587972
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.403225806451612
    gpu_util_percent0: 0.20741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14804587352171267
    mean_env_wait_ms: 1.2183642120045028
    mean_inference_ms: 4.393720208147801
    mean_raw_obs_processing_ms: 0.3831618947398373
  time_since_restore: 1543.5758926868439
  time_this_iter_s: 26.63398790359497
  time_total_s: 1543.5758926868439
  timers:
    learn_throughput: 8228.315
    learn_time_ms: 19662.835
    sample_throughput: 23720.829
    sample_time_ms: 6820.672
    update_time_ms: 26.949
  timestamp: 1602710933
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     58 |          1543.58 | 9383936 |  261.637 |              303.293 |              145.717 |            788.751 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3327.7068447412353
    time_step_min: 3054
  date: 2020-10-14_21-29-20
  done: false
  episode_len_mean: 788.5756162558295
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 261.83277898909137
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 162
  episodes_total: 12008
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4326689839363098
        entropy_coeff: 0.0005000000000000001
        kl: 0.005560785842438539
        model: {}
        policy_loss: -0.01230035779735772
        total_loss: 4.9922739664713545
        vf_explained_var: 0.9877254366874695
        vf_loss: 5.00472108523051
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.85806451612903
    gpu_util_percent0: 0.32419354838709674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14802011603707202
    mean_env_wait_ms: 1.2185471345762913
    mean_inference_ms: 4.392252333946096
    mean_raw_obs_processing_ms: 0.383089095621222
  time_since_restore: 1570.0169990062714
  time_this_iter_s: 26.44110631942749
  time_total_s: 1570.0169990062714
  timers:
    learn_throughput: 8230.139
    learn_time_ms: 19658.477
    sample_throughput: 23811.258
    sample_time_ms: 6794.769
    update_time_ms: 25.577
  timestamp: 1602710960
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     59 |          1570.02 | 9545728 |  261.833 |              303.293 |              145.717 |            788.576 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3326.1090894189
    time_step_min: 3054
  date: 2020-10-14_21-29-46
  done: false
  episode_len_mean: 788.3061574944803
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 262.05879136445816
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 221
  episodes_total: 12229
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4189060578743617
        entropy_coeff: 0.0005000000000000001
        kl: 0.006021614070050418
        model: {}
        policy_loss: -0.010093243676237762
        total_loss: 6.481432318687439
        vf_explained_var: 0.9878926277160645
        vf_loss: 6.491659641265869
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.973333333333333
    gpu_util_percent0: 0.2863333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14798859523927888
    mean_env_wait_ms: 1.2187971358272611
    mean_inference_ms: 4.3903264940377404
    mean_raw_obs_processing_ms: 0.3829919949168549
  time_since_restore: 1596.373233795166
  time_this_iter_s: 26.356234788894653
  time_total_s: 1596.373233795166
  timers:
    learn_throughput: 8244.323
    learn_time_ms: 19624.655
    sample_throughput: 23794.638
    sample_time_ms: 6799.515
    update_time_ms: 25.037
  timestamp: 1602710986
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     60 |          1596.37 | 9707520 |  262.059 |              303.293 |              145.717 |            788.306 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3324.5120501285346
    time_step_min: 3054
  date: 2020-10-14_21-30-13
  done: false
  episode_len_mean: 788.0955434434113
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 262.3158411625067
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 247
  episodes_total: 12476
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3973720247546832
        entropy_coeff: 0.0005000000000000001
        kl: 0.005185622295054297
        model: {}
        policy_loss: -0.010166727685524771
        total_loss: 5.6593455870946245
        vf_explained_var: 0.9898659586906433
        vf_loss: 5.669646143913269
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.819354838709682
    gpu_util_percent0: 0.37258064516129025
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479488285181702
    mean_env_wait_ms: 1.2190623514181977
    mean_inference_ms: 4.388196989454011
    mean_raw_obs_processing_ms: 0.3828884357850992
  time_since_restore: 1622.8782396316528
  time_this_iter_s: 26.505005836486816
  time_total_s: 1622.8782396316528
  timers:
    learn_throughput: 8260.419
    learn_time_ms: 19586.415
    sample_throughput: 23796.896
    sample_time_ms: 6798.87
    update_time_ms: 24.904
  timestamp: 1602711013
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     61 |          1622.88 | 9869312 |  262.316 |              303.293 |              145.717 |            788.096 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3323.4093720266414
    time_step_min: 3054
  date: 2020-10-14_21-30-40
  done: false
  episode_len_mean: 787.9549050632911
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 262.4966836082342
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 12640
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4058798501888911
        entropy_coeff: 0.0005000000000000001
        kl: 0.005482674071875711
        model: {}
        policy_loss: -0.008212998790744072
        total_loss: 4.714298248291016
        vf_explained_var: 0.9888290762901306
        vf_loss: 4.7226459582646685
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.340000000000007
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14792476320855866
    mean_env_wait_ms: 1.219232135322461
    mean_inference_ms: 4.386821256626507
    mean_raw_obs_processing_ms: 0.3828216433299465
  time_since_restore: 1649.508445262909
  time_this_iter_s: 26.630205631256104
  time_total_s: 1649.508445262909
  timers:
    learn_throughput: 8270.832
    learn_time_ms: 19561.756
    sample_throughput: 23723.098
    sample_time_ms: 6820.02
    update_time_ms: 25.72
  timestamp: 1602711040
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     62 |          1649.51 | 10031104 |  262.497 |              303.293 |              145.717 |            787.955 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3322.009992193599
    time_step_min: 3054
  date: 2020-10-14_21-31-07
  done: false
  episode_len_mean: 787.7519083969465
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 262.70189273951536
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 12838
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.40383800864219666
        entropy_coeff: 0.0005000000000000001
        kl: 0.005184352203893165
        model: {}
        policy_loss: -0.011298387316249622
        total_loss: 6.758020639419556
        vf_explained_var: 0.9866301417350769
        vf_loss: 6.769456267356873
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.132258064516133
    gpu_util_percent0: 0.26064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14789715488696753
    mean_env_wait_ms: 1.2194345978442338
    mean_inference_ms: 4.385197726146957
    mean_raw_obs_processing_ms: 0.38274170812990216
  time_since_restore: 1676.3861701488495
  time_this_iter_s: 26.87772488594055
  time_total_s: 1676.3861701488495
  timers:
    learn_throughput: 8264.879
    learn_time_ms: 19575.847
    sample_throughput: 23664.011
    sample_time_ms: 6837.049
    update_time_ms: 23.802
  timestamp: 1602711067
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     63 |          1676.39 | 10192896 |  262.702 |              303.293 |              145.717 |            787.752 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3319.940114722753
    time_step_min: 3054
  date: 2020-10-14_21-31-34
  done: false
  episode_len_mean: 787.527283828131
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 263.0203569696815
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 265
  episodes_total: 13103
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.37833695362011593
        entropy_coeff: 0.0005000000000000001
        kl: 0.00497432976650695
        model: {}
        policy_loss: -0.00904030004555049
        total_loss: 6.6286492347717285
        vf_explained_var: 0.9883511066436768
        vf_loss: 6.6378166278203325
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.570967741935494
    gpu_util_percent0: 0.25709677419354837
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14785898137777184
    mean_env_wait_ms: 1.2196917701579972
    mean_inference_ms: 4.38310464542362
    mean_raw_obs_processing_ms: 0.38263814579319405
  time_since_restore: 1702.8904285430908
  time_this_iter_s: 26.504258394241333
  time_total_s: 1702.8904285430908
  timers:
    learn_throughput: 8262.637
    learn_time_ms: 19581.158
    sample_throughput: 23671.734
    sample_time_ms: 6834.818
    update_time_ms: 23.809
  timestamp: 1602711094
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     64 |          1702.89 | 10354688 |   263.02 |              303.293 |              145.717 |            787.527 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3318.8529145273333
    time_step_min: 3054
  date: 2020-10-14_21-32-01
  done: false
  episode_len_mean: 787.4322634116938
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 263.1863709427
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 13272
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.38818367818991345
        entropy_coeff: 0.0005000000000000001
        kl: 0.005403678476189573
        model: {}
        policy_loss: -0.008791962182537342
        total_loss: 4.96499502658844
        vf_explained_var: 0.9885714650154114
        vf_loss: 4.973947246869405
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.683870967741928
    gpu_util_percent0: 0.3325806451612902
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478361876535312
    mean_env_wait_ms: 1.2198497856614656
    mean_inference_ms: 4.381800650508263
    mean_raw_obs_processing_ms: 0.3825748817401946
  time_since_restore: 1729.5645966529846
  time_this_iter_s: 26.6741681098938
  time_total_s: 1729.5645966529846
  timers:
    learn_throughput: 8258.166
    learn_time_ms: 19591.76
    sample_throughput: 23660.913
    sample_time_ms: 6837.944
    update_time_ms: 23.859
  timestamp: 1602711121
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     65 |          1729.56 | 10516480 |  263.186 |              303.293 |              145.717 |            787.432 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3317.604422604423
    time_step_min: 3054
  date: 2020-10-14_21-32-28
  done: false
  episode_len_mean: 787.2763206776135
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 263.3700096289441
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 187
  episodes_total: 13459
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3901598701874415
        entropy_coeff: 0.0005000000000000001
        kl: 0.005447537211390833
        model: {}
        policy_loss: -0.00999179832676115
        total_loss: 5.5021640459696455
        vf_explained_var: 0.9887394309043884
        vf_loss: 5.512316902478536
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.054838709677416
    gpu_util_percent0: 0.34387096774193554
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14781182740827503
    mean_env_wait_ms: 1.2200195347633245
    mean_inference_ms: 4.380375590500778
    mean_raw_obs_processing_ms: 0.3825039536548175
  time_since_restore: 1756.3174815177917
  time_this_iter_s: 26.75288486480713
  time_total_s: 1756.3174815177917
  timers:
    learn_throughput: 8242.141
    learn_time_ms: 19629.85
    sample_throughput: 23654.599
    sample_time_ms: 6839.769
    update_time_ms: 23.207
  timestamp: 1602711148
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     66 |          1756.32 | 10678272 |   263.37 |              303.293 |              145.717 |            787.276 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3315.7638686131386
    time_step_min: 3054
  date: 2020-10-14_21-32-55
  done: false
  episode_len_mean: 787.0347465034965
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 263.6431866744366
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 269
  episodes_total: 13728
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.36908528457085293
        entropy_coeff: 0.0005000000000000001
        kl: 0.005574712258142729
        model: {}
        policy_loss: -0.0077352875377982855
        total_loss: 7.273245175679524
        vf_explained_var: 0.9879145622253418
        vf_loss: 7.281130115191142
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.006451612903227
    gpu_util_percent0: 0.2948387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147775822972931
    mean_env_wait_ms: 1.2202540253941094
    mean_inference_ms: 4.378395437655779
    mean_raw_obs_processing_ms: 0.3824070877027523
  time_since_restore: 1782.9382164478302
  time_this_iter_s: 26.620734930038452
  time_total_s: 1782.9382164478302
  timers:
    learn_throughput: 8244.662
    learn_time_ms: 19623.849
    sample_throughput: 23655.662
    sample_time_ms: 6839.462
    update_time_ms: 23.353
  timestamp: 1602711175
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     67 |          1782.94 | 10840064 |  263.643 |              303.293 |              145.717 |            787.035 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3314.3908468468467
    time_step_min: 3050
  date: 2020-10-14_21-33-22
  done: false
  episode_len_mean: 786.9310220815652
  episode_reward_max: 303.89898989898995
  episode_reward_mean: 263.82819637066916
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 13903
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3753599176804225
        entropy_coeff: 0.0005000000000000001
        kl: 0.00516426598187536
        model: {}
        policy_loss: -0.011990850054038068
        total_loss: 5.293622771898906
        vf_explained_var: 0.9880210757255554
        vf_loss: 5.305769085884094
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.235483870967744
    gpu_util_percent0: 0.32774193548387104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147753670823053
    mean_env_wait_ms: 1.2204066990669495
    mean_inference_ms: 4.377160446091333
    mean_raw_obs_processing_ms: 0.38234675592493583
  time_since_restore: 1809.3420903682709
  time_this_iter_s: 26.403873920440674
  time_total_s: 1809.3420903682709
  timers:
    learn_throughput: 8257.807
    learn_time_ms: 19592.612
    sample_throughput: 23633.311
    sample_time_ms: 6845.93
    update_time_ms: 24.074
  timestamp: 1602711202
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     68 |          1809.34 | 11001856 |  263.828 |              303.899 |              145.717 |            786.931 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3313.058383766465
    time_step_min: 3050
  date: 2020-10-14_21-33-48
  done: false
  episode_len_mean: 786.8681162509771
  episode_reward_max: 304.6565656565655
  episode_reward_mean: 264.01149848517144
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 14073
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3813230370481809
        entropy_coeff: 0.0005000000000000001
        kl: 0.005576765242343147
        model: {}
        policy_loss: -0.009249821014236659
        total_loss: 5.994030078252156
        vf_explained_var: 0.9869537353515625
        vf_loss: 6.003435492515564
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.833333333333332
    gpu_util_percent0: 0.35733333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477331158304998
    mean_env_wait_ms: 1.2205406514506938
    mean_inference_ms: 4.375951431787532
    mean_raw_obs_processing_ms: 0.38228644001086715
  time_since_restore: 1835.9444494247437
  time_this_iter_s: 26.60235905647278
  time_total_s: 1835.9444494247437
  timers:
    learn_throughput: 8255.091
    learn_time_ms: 19599.057
    sample_throughput: 23608.501
    sample_time_ms: 6853.125
    update_time_ms: 25.354
  timestamp: 1602711228
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     69 |          1835.94 | 11163648 |  264.011 |              304.657 |              145.717 |            786.868 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3311.080061478273
    time_step_min: 3050
  date: 2020-10-14_21-34-15
  done: false
  episode_len_mean: 786.7468274996513
  episode_reward_max: 305.11111111111137
  episode_reward_mean: 264.31309468975064
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 269
  episodes_total: 14342
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3640394906202952
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049436708601812525
        model: {}
        policy_loss: -0.006588246421112369
        total_loss: 6.8674137989679975
        vf_explained_var: 0.9885039329528809
        vf_loss: 6.874153256416321
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.212903225806453
    gpu_util_percent0: 0.3364516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.803225806451613
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14770083862480862
    mean_env_wait_ms: 1.220752664135294
    mean_inference_ms: 4.374152751262027
    mean_raw_obs_processing_ms: 0.3821976157845324
  time_since_restore: 1862.525829076767
  time_this_iter_s: 26.581379652023315
  time_total_s: 1862.525829076767
  timers:
    learn_throughput: 8246.803
    learn_time_ms: 19618.754
    sample_throughput: 23604.484
    sample_time_ms: 6854.291
    update_time_ms: 25.736
  timestamp: 1602711255
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     70 |          1862.53 | 11325440 |  264.313 |              305.111 |              145.717 |            786.747 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3309.8613677099133
    time_step_min: 3050
  date: 2020-10-14_21-34-42
  done: false
  episode_len_mean: 786.6852208614283
  episode_reward_max: 305.11111111111137
  episode_reward_mean: 264.4968044279314
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 192
  episodes_total: 14534
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3534674321611722
        entropy_coeff: 0.0005000000000000001
        kl: 0.005419445573352277
        model: {}
        policy_loss: -0.010775467390582586
        total_loss: 5.534964203834534
        vf_explained_var: 0.9884769916534424
        vf_loss: 5.545899470647176
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.112903225806456
    gpu_util_percent0: 0.30612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.825806451612903
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14767750687244366
    mean_env_wait_ms: 1.2208970984599274
    mean_inference_ms: 4.372879008452877
    mean_raw_obs_processing_ms: 0.3821319582558965
  time_since_restore: 1889.0181050300598
  time_this_iter_s: 26.492275953292847
  time_total_s: 1889.0181050300598
  timers:
    learn_throughput: 8245.674
    learn_time_ms: 19621.44
    sample_throughput: 23586.578
    sample_time_ms: 6859.495
    update_time_ms: 24.47
  timestamp: 1602711282
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     71 |          1889.02 | 11487232 |  264.497 |              305.111 |              145.717 |            786.685 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3308.722922772817
    time_step_min: 3050
  date: 2020-10-14_21-35-09
  done: false
  episode_len_mean: 786.687869923124
  episode_reward_max: 305.26262626262655
  episode_reward_mean: 264.6774906009547
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 165
  episodes_total: 14699
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3651345248023669
        entropy_coeff: 0.0005000000000000001
        kl: 0.005380182294175029
        model: {}
        policy_loss: -0.011825502306843797
        total_loss: 4.975037892659505
        vf_explained_var: 0.9884076118469238
        vf_loss: 4.987029155095418
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91935483870968
    gpu_util_percent0: 0.2396774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14765827875033607
    mean_env_wait_ms: 1.2210108297748468
    mean_inference_ms: 4.371776342452649
    mean_raw_obs_processing_ms: 0.38207656554722647
  time_since_restore: 1915.6963443756104
  time_this_iter_s: 26.678239345550537
  time_total_s: 1915.6963443756104
  timers:
    learn_throughput: 8234.827
    learn_time_ms: 19647.285
    sample_throughput: 23655.565
    sample_time_ms: 6839.49
    update_time_ms: 24.104
  timestamp: 1602711309
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     72 |           1915.7 | 11649024 |  264.677 |              305.263 |              145.717 |            786.688 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3307.004625905068
    time_step_min: 3036
  date: 2020-10-14_21-35-36
  done: false
  episode_len_mean: 786.6783993576017
  episode_reward_max: 306.02020202020213
  episode_reward_mean: 264.92590452166195
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 245
  episodes_total: 14944
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3530029207468033
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052069373584042
        model: {}
        policy_loss: -0.011354393825361816
        total_loss: 7.1340252955754595
        vf_explained_var: 0.9878211617469788
        vf_loss: 7.145540157953898
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.619354838709675
    gpu_util_percent0: 0.2912903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14763134994836477
    mean_env_wait_ms: 1.2211809182437448
    mean_inference_ms: 4.3702391931210185
    mean_raw_obs_processing_ms: 0.38199801812804945
  time_since_restore: 1942.4890699386597
  time_this_iter_s: 26.792725563049316
  time_total_s: 1942.4890699386597
  timers:
    learn_throughput: 8232.672
    learn_time_ms: 19652.428
    sample_throughput: 23710.988
    sample_time_ms: 6823.503
    update_time_ms: 25.568
  timestamp: 1602711336
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     73 |          1942.49 | 11810816 |  264.926 |               306.02 |              145.717 |            786.678 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3305.694945490585
    time_step_min: 3036
  date: 2020-10-14_21-36-03
  done: false
  episode_len_mean: 786.695706654356
  episode_reward_max: 306.02020202020213
  episode_reward_mean: 265.13021662912837
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 219
  episodes_total: 15163
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3319677660862605
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056212386504436536
        model: {}
        policy_loss: -0.011522360475889096
        total_loss: 5.482268452644348
        vf_explained_var: 0.9896093010902405
        vf_loss: 5.493939399719238
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.76666666666667
    gpu_util_percent0: 0.2983333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760613887018423
    mean_env_wait_ms: 1.2213171173495705
    mean_inference_ms: 4.368884781075094
    mean_raw_obs_processing_ms: 0.3819273937254984
  time_since_restore: 1968.8719534873962
  time_this_iter_s: 26.382883548736572
  time_total_s: 1968.8719534873962
  timers:
    learn_throughput: 8236.123
    learn_time_ms: 19644.195
    sample_throughput: 23730.665
    sample_time_ms: 6817.845
    update_time_ms: 26.174
  timestamp: 1602711363
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     74 |          1968.87 | 11972608 |   265.13 |               306.02 |              145.717 |            786.696 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3304.437181330893
    time_step_min: 3036
  date: 2020-10-14_21-36-30
  done: false
  episode_len_mean: 786.724977162991
  episode_reward_max: 306.02020202020213
  episode_reward_mean: 265.3161004538402
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 163
  episodes_total: 15326
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3481028328339259
        entropy_coeff: 0.0005000000000000001
        kl: 0.005596096394583583
        model: {}
        policy_loss: -0.011133084670291282
        total_loss: 3.747858941555023
        vf_explained_var: 0.9911004900932312
        vf_loss: 3.7591486970583596
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.309677419354838
    gpu_util_percent0: 0.3461290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475883618349286
    mean_env_wait_ms: 1.2214142725626
    mean_inference_ms: 4.367894635039916
    mean_raw_obs_processing_ms: 0.3818762580196532
  time_since_restore: 1995.5325574874878
  time_this_iter_s: 26.660604000091553
  time_total_s: 1995.5325574874878
  timers:
    learn_throughput: 8240.315
    learn_time_ms: 19634.201
    sample_throughput: 23712.601
    sample_time_ms: 6823.039
    update_time_ms: 28.575
  timestamp: 1602711390
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     75 |          1995.53 | 12134400 |  265.316 |               306.02 |              145.717 |            786.725 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3303.129627241068
    time_step_min: 3036
  date: 2020-10-14_21-36-57
  done: false
  episode_len_mean: 786.7769409038239
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 265.5173070995782
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 208
  episodes_total: 15534
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.344746895134449
        entropy_coeff: 0.0005000000000000001
        kl: 0.005574784707278013
        model: {}
        policy_loss: -0.011940839467570186
        total_loss: 5.638649384180705
        vf_explained_var: 0.9892699122428894
        vf_loss: 5.650745153427124
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.045161290322582
    gpu_util_percent0: 0.34838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756755073915154
    mean_env_wait_ms: 1.2215357862106515
    mean_inference_ms: 4.366647145518273
    mean_raw_obs_processing_ms: 0.38181245611233283
  time_since_restore: 2022.1858370304108
  time_this_iter_s: 26.653279542922974
  time_total_s: 2022.1858370304108
  timers:
    learn_throughput: 8245.019
    learn_time_ms: 19622.998
    sample_throughput: 23716.365
    sample_time_ms: 6821.956
    update_time_ms: 29.372
  timestamp: 1602711417
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     76 |          2022.19 | 12296192 |  265.517 |              307.838 |              145.717 |            786.777 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3301.4182371977918
    time_step_min: 3036
  date: 2020-10-14_21-37-24
  done: false
  episode_len_mean: 786.8494330778489
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 265.7781539983351
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 253
  episodes_total: 15787
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3148241142431895
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051173852213347954
        model: {}
        policy_loss: -0.009240640594119517
        total_loss: 4.566282232602437
        vf_explained_var: 0.9920072555541992
        vf_loss: 4.575664361317952
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.799999999999997
    gpu_util_percent0: 0.30322580645161296
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754029216685838
    mean_env_wait_ms: 1.221668996027946
    mean_inference_ms: 4.3651791652823215
    mean_raw_obs_processing_ms: 0.3817350423443251
  time_since_restore: 2048.803506374359
  time_this_iter_s: 26.617669343948364
  time_total_s: 2048.803506374359
  timers:
    learn_throughput: 8243.292
    learn_time_ms: 19627.111
    sample_throughput: 23735.59
    sample_time_ms: 6816.431
    update_time_ms: 29.885
  timestamp: 1602711444
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     77 |           2048.8 | 12457984 |  265.778 |              307.838 |              145.717 |            786.849 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3300.309812292046
    time_step_min: 3036
  date: 2020-10-14_21-37-51
  done: false
  episode_len_mean: 786.868270978254
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 265.9498234839464
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 15957
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3213893920183182
        entropy_coeff: 0.0005000000000000001
        kl: 0.004714472258153061
        model: {}
        policy_loss: -0.011277195008005947
        total_loss: 4.23752232392629
        vf_explained_var: 0.9905653595924377
        vf_loss: 4.248945474624634
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.351612903225806
    gpu_util_percent0: 0.41322580645161305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752306404242416
    mean_env_wait_ms: 1.2217526252657314
    mean_inference_ms: 4.3642198214782075
    mean_raw_obs_processing_ms: 0.3816837063353247
  time_since_restore: 2075.6177308559418
  time_this_iter_s: 26.81422448158264
  time_total_s: 2075.6177308559418
  timers:
    learn_throughput: 8223.653
    learn_time_ms: 19673.981
    sample_throughput: 23764.364
    sample_time_ms: 6808.177
    update_time_ms: 31.246
  timestamp: 1602711471
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     78 |          2075.62 | 12619776 |   265.95 |              307.838 |              145.717 |            786.868 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3299.1434960864703
    time_step_min: 3036
  date: 2020-10-14_21-38-18
  done: false
  episode_len_mean: 786.9035718715119
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 266.1219387224596
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 16126
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3279392917950948
        entropy_coeff: 0.0005000000000000001
        kl: 0.004747464826020102
        model: {}
        policy_loss: -0.009505098908751583
        total_loss: 4.234664003054301
        vf_explained_var: 0.990907609462738
        vf_loss: 4.244325558344523
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.380645161290325
    gpu_util_percent0: 0.43225806451612914
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475060240117405
    mean_env_wait_ms: 1.2218277411013263
    mean_inference_ms: 4.363238959013707
    mean_raw_obs_processing_ms: 0.3816322807498394
  time_since_restore: 2102.3469364643097
  time_this_iter_s: 26.72920560836792
  time_total_s: 2102.3469364643097
  timers:
    learn_throughput: 8220.578
    learn_time_ms: 19681.342
    sample_throughput: 23776.384
    sample_time_ms: 6804.735
    update_time_ms: 32.046
  timestamp: 1602711498
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     79 |          2102.35 | 12781568 |  266.122 |              307.838 |              145.717 |            786.904 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3297.2708282433423
    time_step_min: 3036
  date: 2020-10-14_21-38-45
  done: false
  episode_len_mean: 786.8728658536585
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 266.42263180586343
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 274
  episodes_total: 16400
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.30738384028275806
        entropy_coeff: 0.0005000000000000001
        kl: 0.004842223133891821
        model: {}
        policy_loss: -0.009525117831799434
        total_loss: 4.971691926320394
        vf_explained_var: 0.99152010679245
        vf_loss: 4.981367111206055
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.710000000000004
    gpu_util_percent0: 0.35500000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747963930497024
    mean_env_wait_ms: 1.2219546251468356
    mean_inference_ms: 4.3617740650134476
    mean_raw_obs_processing_ms: 0.38155473975347604
  time_since_restore: 2128.902690410614
  time_this_iter_s: 26.55575394630432
  time_total_s: 2128.902690410614
  timers:
    learn_throughput: 8218.456
    learn_time_ms: 19686.422
    sample_throughput: 23807.097
    sample_time_ms: 6795.957
    update_time_ms: 32.221
  timestamp: 1602711525
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     80 |           2128.9 | 12943360 |  266.423 |              307.838 |              145.717 |            786.873 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3295.956101684681
    time_step_min: 3028
  date: 2020-10-14_21-39-12
  done: false
  episode_len_mean: 786.8584001446742
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 266.62450899981786
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 189
  episodes_total: 16589
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29351264983415604
        entropy_coeff: 0.0005000000000000001
        kl: 0.00474738422781229
        model: {}
        policy_loss: -0.00894037855323404
        total_loss: 3.6123553117116294
        vf_explained_var: 0.9922553896903992
        vf_loss: 3.6214405298233032
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.187096774193545
    gpu_util_percent0: 0.302258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746149679884374
    mean_env_wait_ms: 1.2220326880623036
    mean_inference_ms: 4.360760913138791
    mean_raw_obs_processing_ms: 0.38150103693148446
  time_since_restore: 2155.514250278473
  time_this_iter_s: 26.611559867858887
  time_total_s: 2155.514250278473
  timers:
    learn_throughput: 8211.056
    learn_time_ms: 19704.165
    sample_throughput: 23833.197
    sample_time_ms: 6788.514
    update_time_ms: 32.737
  timestamp: 1602711552
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     81 |          2155.51 | 13105152 |  266.625 |              307.838 |              145.717 |            786.858 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3294.880657698057
    time_step_min: 3028
  date: 2020-10-14_21-39-39
  done: false
  episode_len_mean: 786.8624127022026
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 266.79466303939523
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 16753
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.30822569131851196
        entropy_coeff: 0.0005000000000000001
        kl: 0.005055491502086322
        model: {}
        policy_loss: -0.010468179389135912
        total_loss: 4.045590460300446
        vf_explained_var: 0.9906083941459656
        vf_loss: 4.056211709976196
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.012903225806454
    gpu_util_percent0: 0.37999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14744559368762994
    mean_env_wait_ms: 1.2220937251839483
    mean_inference_ms: 4.359867539364266
    mean_raw_obs_processing_ms: 0.3814546998826047
  time_since_restore: 2182.0587606430054
  time_this_iter_s: 26.54451036453247
  time_total_s: 2182.0587606430054
  timers:
    learn_throughput: 8217.604
    learn_time_ms: 19688.463
    sample_throughput: 23825.586
    sample_time_ms: 6790.683
    update_time_ms: 31.794
  timestamp: 1602711579
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     82 |          2182.06 | 13266944 |  266.795 |              307.838 |              145.717 |            786.862 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3293.0890552248484
    time_step_min: 3024
  date: 2020-10-14_21-40-05
  done: false
  episode_len_mean: 786.8927331568108
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.04934606435046
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 242
  episodes_total: 16995
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.30028821031252545
        entropy_coeff: 0.0005000000000000001
        kl: 0.00516949799687912
        model: {}
        policy_loss: -0.011151680955663323
        total_loss: 5.393274426460266
        vf_explained_var: 0.9906125664710999
        vf_loss: 5.404575149218242
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73548387096774
    gpu_util_percent0: 0.2974193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14742490407647033
    mean_env_wait_ms: 1.2221859874682885
    mean_inference_ms: 4.358670023058495
    mean_raw_obs_processing_ms: 0.38139001125036304
  time_since_restore: 2208.3918821811676
  time_this_iter_s: 26.33312153816223
  time_total_s: 2208.3918821811676
  timers:
    learn_throughput: 8235.062
    learn_time_ms: 19646.726
    sample_throughput: 23842.479
    sample_time_ms: 6785.872
    update_time_ms: 31.545
  timestamp: 1602711605
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     83 |          2208.39 | 13428736 |  267.049 |              307.838 |              145.717 |            786.893 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3291.564107038976
    time_step_min: 3024
  date: 2020-10-14_21-40-32
  done: false
  episode_len_mean: 786.925078406319
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.26899204614386
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 17218
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.2772934337457021
        entropy_coeff: 0.0005000000000000001
        kl: 0.005035444589642187
        model: {}
        policy_loss: -0.010432839084387524
        total_loss: 4.142730514208476
        vf_explained_var: 0.9920956492424011
        vf_loss: 4.153300980726878
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809677419354845
    gpu_util_percent0: 0.41000000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740320713135768
    mean_env_wait_ms: 1.2222553638599216
    mean_inference_ms: 4.357520601129855
    mean_raw_obs_processing_ms: 0.38132897619173173
  time_since_restore: 2234.945456981659
  time_this_iter_s: 26.553574800491333
  time_total_s: 2234.945456981659
  timers:
    learn_throughput: 8229.538
    learn_time_ms: 19659.913
    sample_throughput: 23836.085
    sample_time_ms: 6787.692
    update_time_ms: 32.856
  timestamp: 1602711632
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     84 |          2234.95 | 13590528 |  267.269 |              307.838 |              145.717 |            786.925 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3290.478303463378
    time_step_min: 3024
  date: 2020-10-14_21-41-00
  done: false
  episode_len_mean: 786.9530521834187
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.42807861132457
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 163
  episodes_total: 17381
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.29011810074249905
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049016874593993025
        model: {}
        policy_loss: -0.012001630871964153
        total_loss: 3.5467106898625693
        vf_explained_var: 0.9918622970581055
        vf_loss: 3.5588563283284507
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.68064516129032
    gpu_util_percent0: 0.3558064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473889991311359
    mean_env_wait_ms: 1.222303683869779
    mean_inference_ms: 4.356704015721401
    mean_raw_obs_processing_ms: 0.3812853002478482
  time_since_restore: 2261.5811443328857
  time_this_iter_s: 26.635687351226807
  time_total_s: 2261.5811443328857
  timers:
    learn_throughput: 8224.215
    learn_time_ms: 19672.637
    sample_throughput: 23885.542
    sample_time_ms: 6773.637
    update_time_ms: 31.114
  timestamp: 1602711660
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     85 |          2261.58 | 13752320 |  267.428 |              307.838 |              145.717 |            786.953 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3288.9958983707415
    time_step_min: 3024
  date: 2020-10-14_21-41-26
  done: false
  episode_len_mean: 786.9745193948356
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.64620209603714
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 201
  episodes_total: 17582
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2905880734324455
        entropy_coeff: 0.0005000000000000001
        kl: 0.005135899099210898
        model: {}
        policy_loss: -0.010168266812494645
        total_loss: 3.7239726185798645
        vf_explained_var: 0.9926049709320068
        vf_loss: 3.7342856923739114
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.396666666666665
    gpu_util_percent0: 0.28900000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737249900912702
    mean_env_wait_ms: 1.2223630345149628
    mean_inference_ms: 4.355706025085994
    mean_raw_obs_processing_ms: 0.38123397352420385
  time_since_restore: 2288.0330748558044
  time_this_iter_s: 26.4519305229187
  time_total_s: 2288.0330748558044
  timers:
    learn_throughput: 8229.105
    learn_time_ms: 19660.948
    sample_throughput: 23916.776
    sample_time_ms: 6764.791
    update_time_ms: 31.17
  timestamp: 1602711686
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     86 |          2288.03 | 13914112 |  267.646 |              307.838 |              145.717 |            786.975 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3287.35663354107
    time_step_min: 3024
  date: 2020-10-14_21-41-54
  done: false
  episode_len_mean: 787.0316721789338
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.89563837262693
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 257
  episodes_total: 17839
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2631889382998149
        entropy_coeff: 0.0005000000000000001
        kl: 0.004604808132474621
        model: {}
        policy_loss: -0.009357839425016815
        total_loss: 3.8989155888557434
        vf_explained_var: 0.9933080673217773
        vf_loss: 3.908404588699341
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96451612903226
    gpu_util_percent0: 0.2841935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735013594749458
    mean_env_wait_ms: 1.2224240980725907
    mean_inference_ms: 4.354506634755918
    mean_raw_obs_processing_ms: 0.3811659595933394
  time_since_restore: 2314.7521579265594
  time_this_iter_s: 26.719083070755005
  time_total_s: 2314.7521579265594
  timers:
    learn_throughput: 8230.668
    learn_time_ms: 19657.213
    sample_throughput: 23874.629
    sample_time_ms: 6776.734
    update_time_ms: 32.681
  timestamp: 1602711714
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     87 |          2314.75 | 14075904 |  267.896 |              307.838 |              145.717 |            787.032 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3286.345159317133
    time_step_min: 3024
  date: 2020-10-14_21-42-20
  done: false
  episode_len_mean: 787.0565210149354
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.0581653523744
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 172
  episodes_total: 18011
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.26712100207805634
        entropy_coeff: 0.0005000000000000001
        kl: 0.004592248937115073
        model: {}
        policy_loss: -0.009840659738983959
        total_loss: 4.159258266290029
        vf_explained_var: 0.990882396697998
        vf_loss: 4.169232288996379
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.890322580645165
    gpu_util_percent0: 0.3325806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733553008257375
    mean_env_wait_ms: 1.2224646247104172
    mean_inference_ms: 4.353705028256507
    mean_raw_obs_processing_ms: 0.3811241258606705
  time_since_restore: 2341.2809410095215
  time_this_iter_s: 26.528783082962036
  time_total_s: 2341.2809410095215
  timers:
    learn_throughput: 8246.276
    learn_time_ms: 19620.007
    sample_throughput: 23841.517
    sample_time_ms: 6786.145
    update_time_ms: 31.178
  timestamp: 1602711740
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     88 |          2341.28 | 14237696 |  268.058 |              311.323 |              145.717 |            787.057 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3285.2285446733504
    time_step_min: 3024
  date: 2020-10-14_21-42-48
  done: false
  episode_len_mean: 787.0586844131559
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.2335965529233
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 171
  episodes_total: 18182
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.2779242917895317
        entropy_coeff: 0.0005000000000000001
        kl: 0.005256622641657789
        model: {}
        policy_loss: -0.009451207355596125
        total_loss: 3.4253863096237183
        vf_explained_var: 0.9924994111061096
        vf_loss: 3.434976279735565
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.29032258064516
    gpu_util_percent0: 0.3393548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147321288519311
    mean_env_wait_ms: 1.2225008349278939
    mean_inference_ms: 4.35288667705278
    mean_raw_obs_processing_ms: 0.3810801970085438
  time_since_restore: 2368.049071073532
  time_this_iter_s: 26.76813006401062
  time_total_s: 2368.049071073532
  timers:
    learn_throughput: 8247.624
    learn_time_ms: 19616.8
    sample_throughput: 23787.987
    sample_time_ms: 6801.416
    update_time_ms: 30.114
  timestamp: 1602711768
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     89 |          2368.05 | 14399488 |  268.234 |              311.323 |              145.717 |            787.059 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3283.477835723598
    time_step_min: 3024
  date: 2020-10-14_21-43-15
  done: false
  episode_len_mean: 787.0879800390541
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.5053595183775
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 254
  episodes_total: 18436
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.26092210908730823
        entropy_coeff: 0.0005000000000000001
        kl: 0.005213543151815732
        model: {}
        policy_loss: -0.008479621440831883
        total_loss: 4.420211752255757
        vf_explained_var: 0.992413341999054
        vf_loss: 4.428821722666423
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.125806451612902
    gpu_util_percent0: 0.29709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473014829866963
    mean_env_wait_ms: 1.2225468447641787
    mean_inference_ms: 4.351786745936452
    mean_raw_obs_processing_ms: 0.3810183029677754
  time_since_restore: 2395.052999019623
  time_this_iter_s: 27.0039279460907
  time_total_s: 2395.052999019623
  timers:
    learn_throughput: 8240.551
    learn_time_ms: 19633.639
    sample_throughput: 23696.004
    sample_time_ms: 6827.818
    update_time_ms: 30.644
  timestamp: 1602711795
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     90 |          2395.05 | 14561280 |  268.505 |              311.323 |              145.717 |            787.088 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3282.0970237455676
    time_step_min: 3024
  date: 2020-10-14_21-43-42
  done: false
  episode_len_mean: 787.1156528269499
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.70999827694385
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 18642
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.24939925596117973
        entropy_coeff: 0.0005000000000000001
        kl: 0.004798348527401686
        model: {}
        policy_loss: -0.006871319356529663
        total_loss: 3.8024387756983438
        vf_explained_var: 0.9924138188362122
        vf_loss: 3.809434731801351
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.421875
    gpu_util_percent0: 0.3265625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728457767623976
    mean_env_wait_ms: 1.2225789639434597
    mean_inference_ms: 4.350879563161335
    mean_raw_obs_processing_ms: 0.38096849609160766
  time_since_restore: 2421.8803718090057
  time_this_iter_s: 26.827372789382935
  time_total_s: 2421.8803718090057
  timers:
    learn_throughput: 8239.209
    learn_time_ms: 19636.836
    sample_throughput: 23635.593
    sample_time_ms: 6845.269
    update_time_ms: 30.703
  timestamp: 1602711822
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     91 |          2421.88 | 14723072 |   268.71 |              311.323 |              145.717 |            787.116 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3281.048516802471
    time_step_min: 3024
  date: 2020-10-14_21-44-09
  done: false
  episode_len_mean: 787.1419303376762
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.8674863498048
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 163
  episodes_total: 18805
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.26461029549439746
        entropy_coeff: 0.0005000000000000001
        kl: 0.004730994813144207
        model: {}
        policy_loss: -0.010463534825248644
        total_loss: 3.69461061557134
        vf_explained_var: 0.9918532371520996
        vf_loss: 3.7052063941955566
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.803225806451614
    gpu_util_percent0: 0.4509677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727235563982546
    mean_env_wait_ms: 1.2226003147576727
    mean_inference_ms: 4.350170709025106
    mean_raw_obs_processing_ms: 0.3809293338018115
  time_since_restore: 2448.39555478096
  time_this_iter_s: 26.515182971954346
  time_total_s: 2448.39555478096
  timers:
    learn_throughput: 8246.0
    learn_time_ms: 19620.665
    sample_throughput: 23594.602
    sample_time_ms: 6857.162
    update_time_ms: 30.524
  timestamp: 1602711849
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     92 |           2448.4 | 14884864 |  268.867 |              311.323 |              145.717 |            787.142 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3279.4267452259455
    time_step_min: 3013
  date: 2020-10-14_21-44-37
  done: false
  episode_len_mean: 787.1345274990807
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.12081470268157
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 232
  episodes_total: 19037
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.26356350630521774
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048559077161674695
        model: {}
        policy_loss: -0.008704963935694346
        total_loss: 4.425322691599528
        vf_explained_var: 0.9918349385261536
        vf_loss: 4.434159358342488
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.78064516129033
    gpu_util_percent0: 0.3583870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14725621313575524
    mean_env_wait_ms: 1.2226317747112556
    mean_inference_ms: 4.349222432690657
    mean_raw_obs_processing_ms: 0.38087608548298424
  time_since_restore: 2475.3068203926086
  time_this_iter_s: 26.91126561164856
  time_total_s: 2475.3068203926086
  timers:
    learn_throughput: 8230.888
    learn_time_ms: 19656.687
    sample_throughput: 23531.3
    sample_time_ms: 6875.608
    update_time_ms: 30.362
  timestamp: 1602711877
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     93 |          2475.31 | 15046656 |  269.121 |              311.323 |              145.717 |            787.135 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3277.8198451062945
    time_step_min: 3007
  date: 2020-10-14_21-45-04
  done: false
  episode_len_mean: 787.1460528364561
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.36334959078505
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 230
  episodes_total: 19267
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.24006237337986627
        entropy_coeff: 0.0005000000000000001
        kl: 0.00443189680421104
        model: {}
        policy_loss: -0.009926851313745525
        total_loss: 3.5811255971590676
        vf_explained_var: 0.9931641221046448
        vf_loss: 3.591172436873118
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.922580645161293
    gpu_util_percent0: 0.2648387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147237535307467
    mean_env_wait_ms: 1.2226524974477424
    mean_inference_ms: 4.348265891876617
    mean_raw_obs_processing_ms: 0.3808211546698452
  time_since_restore: 2502.052219390869
  time_this_iter_s: 26.745398998260498
  time_total_s: 2502.052219390869
  timers:
    learn_throughput: 8229.042
    learn_time_ms: 19661.099
    sample_throughput: 23478.631
    sample_time_ms: 6891.032
    update_time_ms: 28.462
  timestamp: 1602711904
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     94 |          2502.05 | 15208448 |  269.363 |              311.323 |              145.717 |            787.146 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3276.7372462125118
    time_step_min: 3007
  date: 2020-10-14_21-45-31
  done: false
  episode_len_mean: 787.1644540496038
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.53681302060426
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 19434
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2535593919456005
        entropy_coeff: 0.0005000000000000001
        kl: 0.004572930202508966
        model: {}
        policy_loss: -0.0108901071944274
        total_loss: 2.831205983956655
        vf_explained_var: 0.9933860301971436
        vf_loss: 2.8422229290008545
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.351612903225814
    gpu_util_percent0: 0.30451612903225805
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722525631145267
    mean_env_wait_ms: 1.2226655450445298
    mean_inference_ms: 4.347594069609549
    mean_raw_obs_processing_ms: 0.3807836836843628
  time_since_restore: 2529.032241821289
  time_this_iter_s: 26.980022430419922
  time_total_s: 2529.032241821289
  timers:
    learn_throughput: 8221.855
    learn_time_ms: 19678.285
    sample_throughput: 23428.81
    sample_time_ms: 6905.686
    update_time_ms: 28.606
  timestamp: 1602711931
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     95 |          2529.03 | 15370240 |  269.537 |              311.323 |              145.717 |            787.164 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3275.3305789339456
    time_step_min: 3007
  date: 2020-10-14_21-45-58
  done: false
  episode_len_mean: 787.1740437019304
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.74527684011707
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 19633
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.2574249133467674
        entropy_coeff: 0.0005000000000000001
        kl: 0.00437789751837651
        model: {}
        policy_loss: -0.008738268283195794
        total_loss: 3.3785081704457602
        vf_explained_var: 0.9932436347007751
        vf_loss: 3.3873751759529114
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89677419354839
    gpu_util_percent0: 0.27838709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472121494546638
    mean_env_wait_ms: 1.2226804394982738
    mean_inference_ms: 4.346788862870428
    mean_raw_obs_processing_ms: 0.3807393173677255
  time_since_restore: 2555.5665895938873
  time_this_iter_s: 26.534347772598267
  time_total_s: 2555.5665895938873
  timers:
    learn_throughput: 8217.357
    learn_time_ms: 19689.055
    sample_throughput: 23441.122
    sample_time_ms: 6902.059
    update_time_ms: 27.757
  timestamp: 1602711958
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     96 |          2555.57 | 15532032 |  269.745 |              311.323 |              145.717 |            787.174 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3273.708595598973
    time_step_min: 3007
  date: 2020-10-14_21-46-25
  done: false
  episode_len_mean: 787.2295972243174
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.99387295797
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 254
  episodes_total: 19887
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2361191933353742
        entropy_coeff: 0.0005000000000000001
        kl: 0.004461579645673434
        model: {}
        policy_loss: -0.007947136803219715
        total_loss: 3.9202739000320435
        vf_explained_var: 0.9932805895805359
        vf_loss: 3.928339143594106
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.70322580645162
    gpu_util_percent0: 0.3345161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471939877193906
    mean_env_wait_ms: 1.2226910705634182
    mean_inference_ms: 4.3458321193028215
    mean_raw_obs_processing_ms: 0.38068053260136225
  time_since_restore: 2582.2500932216644
  time_this_iter_s: 26.6835036277771
  time_total_s: 2582.2500932216644
  timers:
    learn_throughput: 8213.017
    learn_time_ms: 19699.461
    sample_throughput: 23487.143
    sample_time_ms: 6888.535
    update_time_ms: 26.445
  timestamp: 1602711985
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     97 |          2582.25 | 15693824 |  269.994 |              311.323 |              145.717 |             787.23 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3272.6159604731247
    time_step_min: 3007
  date: 2020-10-14_21-46-52
  done: false
  episode_len_mean: 787.2852728631946
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.1595597137584
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 178
  episodes_total: 20065
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.23126362388332686
        entropy_coeff: 0.0005000000000000001
        kl: 0.004464010902059575
        model: {}
        policy_loss: -0.009416341490577906
        total_loss: 2.99352898200353
        vf_explained_var: 0.9935998320579529
        vf_loss: 3.0030609170595803
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.17741935483871
    gpu_util_percent0: 0.33032258064516123
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718054696576985
    mean_env_wait_ms: 1.222694409187115
    mean_inference_ms: 4.345144563349734
    mean_raw_obs_processing_ms: 0.38064350128866803
  time_since_restore: 2608.9015679359436
  time_this_iter_s: 26.651474714279175
  time_total_s: 2608.9015679359436
  timers:
    learn_throughput: 8206.524
    learn_time_ms: 19715.046
    sample_throughput: 23512.426
    sample_time_ms: 6881.127
    update_time_ms: 27.98
  timestamp: 1602712012
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     98 |           2608.9 | 15855616 |   270.16 |              311.323 |              145.717 |            787.285 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3271.5971094832707
    time_step_min: 3007
  date: 2020-10-14_21-47-19
  done: false
  episode_len_mean: 787.2972024515619
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.32549546473024
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 20232
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.24772328014175096
        entropy_coeff: 0.0005000000000000001
        kl: 0.004681173246353865
        model: {}
        policy_loss: -0.009907095324403295
        total_loss: 3.0358158548672995
        vf_explained_var: 0.993319571018219
        vf_loss: 3.0458468397458396
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.049999999999997
    gpu_util_percent0: 0.30433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471692454595795
    mean_env_wait_ms: 1.2226936439970344
    mean_inference_ms: 4.34448970490729
    mean_raw_obs_processing_ms: 0.38060664464864263
  time_since_restore: 2635.458029270172
  time_this_iter_s: 26.556461334228516
  time_total_s: 2635.458029270172
  timers:
    learn_throughput: 8209.745
    learn_time_ms: 19707.312
    sample_throughput: 23564.508
    sample_time_ms: 6865.919
    update_time_ms: 27.931
  timestamp: 1602712039
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     99 |          2635.46 | 16017408 |  270.325 |              311.323 |              145.717 |            787.297 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3270.012954634337
    time_step_min: 3007
  date: 2020-10-14_21-47-46
  done: false
  episode_len_mean: 787.3193223979691
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.5659376423875
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 252
  episodes_total: 20484
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.23757333805163702
        entropy_coeff: 0.0005000000000000001
        kl: 0.004610497465667625
        model: {}
        policy_loss: -0.008927385647742389
        total_loss: 4.04001663128535
        vf_explained_var: 0.99312424659729
        vf_loss: 4.049062748750051
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.70967741935484
    gpu_util_percent0: 0.2603225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715361613569167
    mean_env_wait_ms: 1.2226949952895687
    mean_inference_ms: 4.34361659610733
    mean_raw_obs_processing_ms: 0.3805538353020298
  time_since_restore: 2662.1150550842285
  time_this_iter_s: 26.657025814056396
  time_total_s: 2662.1150550842285
  timers:
    learn_throughput: 8220.678
    learn_time_ms: 19681.102
    sample_throughput: 23599.045
    sample_time_ms: 6855.871
    update_time_ms: 28.504
  timestamp: 1602712066
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    100 |          2662.12 | 16179200 |  270.566 |              311.323 |              145.717 |            787.319 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3268.748113024966
    time_step_min: 3007
  date: 2020-10-14_21-48-13
  done: false
  episode_len_mean: 787.356928875145
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.7558924185808
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 212
  episodes_total: 20696
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.21814849972724915
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042378978493313
        model: {}
        policy_loss: -0.008028034179005772
        total_loss: 3.258223215738932
        vf_explained_var: 0.9937694668769836
        vf_loss: 3.266360282897949
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.977419354838712
    gpu_util_percent0: 0.34387096774193543
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713867471880768
    mean_env_wait_ms: 1.222689928864633
    mean_inference_ms: 4.342837880492267
    mean_raw_obs_processing_ms: 0.38050967743801056
  time_since_restore: 2688.9822533130646
  time_this_iter_s: 26.86719822883606
  time_total_s: 2688.9822533130646
  timers:
    learn_throughput: 8222.918
    learn_time_ms: 19675.741
    sample_throughput: 23568.806
    sample_time_ms: 6864.667
    update_time_ms: 28.059
  timestamp: 1602712093
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    101 |          2688.98 | 16340992 |  270.756 |              311.323 |              145.717 |            787.357 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3267.7349848768545
    time_step_min: 3007
  date: 2020-10-14_21-48-41
  done: false
  episode_len_mean: 787.3735915999424
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.90694885761286
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 161
  episodes_total: 20857
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.23141959185401598
        entropy_coeff: 0.0005000000000000001
        kl: 0.004781549524826308
        model: {}
        policy_loss: -0.008303154177156102
        total_loss: 2.709630091985067
        vf_explained_var: 0.9937422275543213
        vf_loss: 2.718048930168152
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.074193548387097
    gpu_util_percent0: 0.32677419354838705
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712880428848316
    mean_env_wait_ms: 1.2226839229747812
    mean_inference_ms: 4.342266198418744
    mean_raw_obs_processing_ms: 0.38047694904173684
  time_since_restore: 2715.7698924541473
  time_this_iter_s: 26.787639141082764
  time_total_s: 2715.7698924541473
  timers:
    learn_throughput: 8210.55
    learn_time_ms: 19705.379
    sample_throughput: 23585.575
    sample_time_ms: 6859.786
    update_time_ms: 30.406
  timestamp: 1602712121
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    102 |          2715.77 | 16502784 |  270.907 |              311.323 |              145.717 |            787.374 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3266.3458778807317
    time_step_min: 2985
  date: 2020-10-14_21-49-08
  done: false
  episode_len_mean: 787.3928249418688
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.10886926494567
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 216
  episodes_total: 21073
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.23476263011495271
        entropy_coeff: 0.0005000000000000001
        kl: 0.004458297664920489
        model: {}
        policy_loss: -0.010898580202289546
        total_loss: 3.2126270731290183
        vf_explained_var: 0.9940955638885498
        vf_loss: 3.2236429850260415
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06129032258065
    gpu_util_percent0: 0.3109677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471163926255774
    mean_env_wait_ms: 1.222676516689104
    mean_inference_ms: 4.341509409134686
    mean_raw_obs_processing_ms: 0.3804343104214227
  time_since_restore: 2742.3438205718994
  time_this_iter_s: 26.573928117752075
  time_total_s: 2742.3438205718994
  timers:
    learn_throughput: 8219.446
    learn_time_ms: 19684.052
    sample_throughput: 23622.405
    sample_time_ms: 6849.091
    update_time_ms: 29.115
  timestamp: 1602712148
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    103 |          2742.34 | 16664576 |  271.109 |              313.747 |              145.717 |            787.393 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3264.953454511296
    time_step_min: 2985
  date: 2020-10-14_21-49-35
  done: false
  episode_len_mean: 787.4012852385197
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.31441247694346
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 246
  episodes_total: 21319
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.2095079111556212
        entropy_coeff: 0.0005000000000000001
        kl: 0.004064000347473969
        model: {}
        policy_loss: -0.007974418074203035
        total_loss: 3.6233737667401633
        vf_explained_var: 0.993736982345581
        vf_loss: 3.6314529180526733
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.26875
    gpu_util_percent0: 0.338125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709957415784317
    mean_env_wait_ms: 1.2226608795127623
    mean_inference_ms: 4.3406844524693105
    mean_raw_obs_processing_ms: 0.3803821819854836
  time_since_restore: 2769.2847611904144
  time_this_iter_s: 26.940940618515015
  time_total_s: 2769.2847611904144
  timers:
    learn_throughput: 8215.373
    learn_time_ms: 19693.811
    sample_throughput: 23618.899
    sample_time_ms: 6850.108
    update_time_ms: 30.803
  timestamp: 1602712175
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    104 |          2769.28 | 16826368 |  271.314 |              313.747 |              145.717 |            787.401 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3264.003261882572
    time_step_min: 2985
  date: 2020-10-14_21-50-02
  done: false
  episode_len_mean: 787.4226079672376
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.4605676083244
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 21488
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.21215776354074478
        entropy_coeff: 0.0005000000000000001
        kl: 0.004357238727000852
        model: {}
        policy_loss: -0.009719787897968976
        total_loss: 2.438874820868174
        vf_explained_var: 0.9945716857910156
        vf_loss: 2.4487006862958274
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.009677419354844
    gpu_util_percent0: 0.2429032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470892897843261
    mean_env_wait_ms: 1.2226480644450273
    mean_inference_ms: 4.340120156769623
    mean_raw_obs_processing_ms: 0.3803508232331379
  time_since_restore: 2796.2824625968933
  time_this_iter_s: 26.997701406478882
  time_total_s: 2796.2824625968933
  timers:
    learn_throughput: 8214.196
    learn_time_ms: 19696.632
    sample_throughput: 23625.236
    sample_time_ms: 6848.27
    update_time_ms: 31.677
  timestamp: 1602712202
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    105 |          2796.28 | 16988160 |  271.461 |              313.747 |              145.717 |            787.423 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3262.879203621582
    time_step_min: 2985
  date: 2020-10-14_21-50-29
  done: false
  episode_len_mean: 787.444916036169
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.61888818056923
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 21676
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.22695259129007658
        entropy_coeff: 0.0005000000000000001
        kl: 0.004886048574311038
        model: {}
        policy_loss: -0.010865791712906988
        total_loss: 2.7513832449913025
        vf_explained_var: 0.9945626854896545
        vf_loss: 2.762362480163574
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.112903225806452
    gpu_util_percent0: 0.2825806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707868871729182
    mean_env_wait_ms: 1.2226350953249057
    mean_inference_ms: 4.339482583221035
    mean_raw_obs_processing_ms: 0.3803147365621617
  time_since_restore: 2822.7481787204742
  time_this_iter_s: 26.465716123580933
  time_total_s: 2822.7481787204742
  timers:
    learn_throughput: 8222.514
    learn_time_ms: 19676.706
    sample_throughput: 23587.252
    sample_time_ms: 6859.298
    update_time_ms: 33.636
  timestamp: 1602712229
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    106 |          2822.75 | 17149952 |  271.619 |              313.747 |              145.717 |            787.445 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3261.406272826881
    time_step_min: 2985
  date: 2020-10-14_21-50-56
  done: false
  episode_len_mean: 787.4860477840598
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.8472279792268
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 256
  episodes_total: 21932
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.20683047796289125
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040669710336563485
        model: {}
        policy_loss: -0.009566397726302966
        total_loss: 2.7036399642626443
        vf_explained_var: 0.9954362511634827
        vf_loss: 2.7133097449938455
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72258064516129
    gpu_util_percent0: 0.27709677419354845
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706391046818196
    mean_env_wait_ms: 1.2226107018628973
    mean_inference_ms: 4.338703137582242
    mean_raw_obs_processing_ms: 0.3802655795528472
  time_since_restore: 2849.311582326889
  time_this_iter_s: 26.563403606414795
  time_total_s: 2849.311582326889
  timers:
    learn_throughput: 8230.308
    learn_time_ms: 19658.073
    sample_throughput: 23565.192
    sample_time_ms: 6865.72
    update_time_ms: 32.465
  timestamp: 1602712256
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    107 |          2849.31 | 17311744 |  271.847 |              313.747 |              145.717 |            787.486 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3260.364175456068
    time_step_min: 2985
  date: 2020-10-14_21-51-23
  done: false
  episode_len_mean: 787.5276911252769
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.0044648300446
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 187
  episodes_total: 22119
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.19546745344996452
        entropy_coeff: 0.0005000000000000001
        kl: 0.00398314530805995
        model: {}
        policy_loss: -0.007873045280575752
        total_loss: 2.0725629329681396
        vf_explained_var: 0.9957442879676819
        vf_loss: 2.080533673365911
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.74193548387097
    gpu_util_percent0: 0.2435483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705188881334083
    mean_env_wait_ms: 1.222589024497927
    mean_inference_ms: 4.338084506002126
    mean_raw_obs_processing_ms: 0.38023115262430085
  time_since_restore: 2876.000137090683
  time_this_iter_s: 26.688554763793945
  time_total_s: 2876.000137090683
  timers:
    learn_throughput: 8230.034
    learn_time_ms: 19658.728
    sample_throughput: 23549.64
    sample_time_ms: 6870.254
    update_time_ms: 31.79
  timestamp: 1602712283
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    108 |             2876 | 17473536 |  272.004 |              313.747 |              145.717 |            787.528 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3259.476432262413
    time_step_min: 2985
  date: 2020-10-14_21-51-50
  done: false
  episode_len_mean: 787.5573307005341
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.14390142958996
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 22283
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.21093723302086195
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045021023058022065
        model: {}
        policy_loss: -0.01046574228287985
        total_loss: 2.077916463216146
        vf_explained_var: 0.995455265045166
        vf_loss: 2.08848774433136
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.04193548387097
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704295270573675
    mean_env_wait_ms: 1.2225683118810309
    mean_inference_ms: 4.337574360465659
    mean_raw_obs_processing_ms: 0.3802015013712212
  time_since_restore: 2902.563551425934
  time_this_iter_s: 26.563414335250854
  time_total_s: 2902.563551425934
  timers:
    learn_throughput: 8232.628
    learn_time_ms: 19652.533
    sample_throughput: 23524.953
    sample_time_ms: 6877.463
    update_time_ms: 31.611
  timestamp: 1602712310
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    109 |          2902.56 | 17635328 |  272.144 |              313.747 |              145.717 |            787.557 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3258.254001422728
    time_step_min: 2985
  date: 2020-10-14_21-52-17
  done: false
  episode_len_mean: 787.6022202486679
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.33629142221497
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 237
  episodes_total: 22520
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.20596953481435776
        entropy_coeff: 0.0005000000000000001
        kl: 0.004496099737783273
        model: {}
        policy_loss: -0.009117586785578169
        total_loss: 2.7136199871699014
        vf_explained_var: 0.9953751564025879
        vf_loss: 2.722840507825216
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.60967741935484
    gpu_util_percent0: 0.3090322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703049596102444
    mean_env_wait_ms: 1.2225411172478857
    mean_inference_ms: 4.336848515335811
    mean_raw_obs_processing_ms: 0.38015906871665867
  time_since_restore: 2929.122708797455
  time_this_iter_s: 26.559157371520996
  time_total_s: 2929.122708797455
  timers:
    learn_throughput: 8230.119
    learn_time_ms: 19658.525
    sample_throughput: 23577.931
    sample_time_ms: 6862.01
    update_time_ms: 30.808
  timestamp: 1602712337
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    110 |          2929.12 | 17797120 |  272.336 |              313.747 |              145.717 |            787.602 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3257.0302443319392
    time_step_min: 2985
  date: 2020-10-14_21-52-44
  done: false
  episode_len_mean: 787.6562458778525
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.52058375604076
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 22743
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.18245642135540643
        entropy_coeff: 0.0005000000000000001
        kl: 0.004177523893304169
        model: {}
        policy_loss: -0.006182894188289841
        total_loss: 2.2897426883379617
        vf_explained_var: 0.9958382248878479
        vf_loss: 2.2960168520609536
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32903225806452
    gpu_util_percent0: 0.3458064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701716965814704
    mean_env_wait_ms: 1.2225084958648815
    mean_inference_ms: 4.336187748017109
    mean_raw_obs_processing_ms: 0.38011866452121934
  time_since_restore: 2955.4546155929565
  time_this_iter_s: 26.33190679550171
  time_total_s: 2955.4546155929565
  timers:
    learn_throughput: 8239.188
    learn_time_ms: 19636.887
    sample_throughput: 23696.357
    sample_time_ms: 6827.716
    update_time_ms: 32.786
  timestamp: 1602712364
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    111 |          2955.45 | 17958912 |  272.521 |              313.747 |              145.717 |            787.656 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3256.166368046148
    time_step_min: 2985
  date: 2020-10-14_21-53-11
  done: false
  episode_len_mean: 787.6954737898826
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.6533463481216
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 168
  episodes_total: 22911
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.19364663834373155
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046233854567011195
        model: {}
        policy_loss: -0.009124417009237126
        total_loss: 2.153838813304901
        vf_explained_var: 0.995291531085968
        vf_loss: 2.1630599896113076
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.166666666666668
    gpu_util_percent0: 0.322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700824325122
    mean_env_wait_ms: 1.2224808371985434
    mean_inference_ms: 4.335678107103493
    mean_raw_obs_processing_ms: 0.3800898393293273
  time_since_restore: 2981.8721601963043
  time_this_iter_s: 26.41754460334778
  time_total_s: 2981.8721601963043
  timers:
    learn_throughput: 8249.824
    learn_time_ms: 19611.57
    sample_throughput: 23738.575
    sample_time_ms: 6815.573
    update_time_ms: 32.423
  timestamp: 1602712391
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    112 |          2981.87 | 18120704 |  272.653 |              313.747 |              145.717 |            787.695 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3255.1842994541203
    time_step_min: 2985
  date: 2020-10-14_21-53-38
  done: false
  episode_len_mean: 787.7565988749459
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.80740551337686
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 23110
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.20393426095445952
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041561375837773085
        model: {}
        policy_loss: -0.007814861898320183
        total_loss: 2.656420866648356
        vf_explained_var: 0.9950793385505676
        vf_loss: 2.664337714513143
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.822580645161292
    gpu_util_percent0: 0.30096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699829088059763
    mean_env_wait_ms: 1.222453187011124
    mean_inference_ms: 4.335106077800435
    mean_raw_obs_processing_ms: 0.38005671119123063
  time_since_restore: 3008.433802127838
  time_this_iter_s: 26.561641931533813
  time_total_s: 3008.433802127838
  timers:
    learn_throughput: 8249.627
    learn_time_ms: 19612.037
    sample_throughput: 23748.976
    sample_time_ms: 6812.589
    update_time_ms: 33.431
  timestamp: 1602712418
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    113 |          3008.43 | 18282496 |  272.807 |              313.747 |              145.717 |            787.757 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3253.889388874604
    time_step_min: 2985
  date: 2020-10-14_21-54-05
  done: false
  episode_len_mean: 787.8023285677596
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.0050492079427
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 252
  episodes_total: 23362
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.18517948811252913
        entropy_coeff: 0.0005000000000000001
        kl: 0.005169444329415758
        model: {}
        policy_loss: -0.007178351321878533
        total_loss: 2.651616891225179
        vf_explained_var: 0.99568110704422
        vf_loss: 2.6588878631591797
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.728125
    gpu_util_percent0: 0.38
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469847220820147
    mean_env_wait_ms: 1.222404693061969
    mean_inference_ms: 4.334392676473266
    mean_raw_obs_processing_ms: 0.3800125661284136
  time_since_restore: 3035.3678319454193
  time_this_iter_s: 26.934029817581177
  time_total_s: 3035.3678319454193
  timers:
    learn_throughput: 8246.908
    learn_time_ms: 19618.505
    sample_throughput: 23751.149
    sample_time_ms: 6811.965
    update_time_ms: 31.627
  timestamp: 1602712445
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    114 |          3035.37 | 18444288 |  273.005 |              313.747 |              145.717 |            787.802 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3253.006294390337
    time_step_min: 2985
  date: 2020-10-14_21-54-33
  done: false
  episode_len_mean: 787.8456310267194
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.14702652882835
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 23541
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.17693759376804033
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520641989074647
        model: {}
        policy_loss: -0.008549871447030455
        total_loss: 2.3380528887112937
        vf_explained_var: 0.9951727986335754
        vf_loss: 2.346691310405731
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.400000000000002
    gpu_util_percent0: 0.39774193548387093
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697447919976178
    mean_env_wait_ms: 1.2223720316269442
    mean_inference_ms: 4.333869661581909
    mean_raw_obs_processing_ms: 0.3799832275517544
  time_since_restore: 3061.967455625534
  time_this_iter_s: 26.599623680114746
  time_total_s: 3061.967455625534
  timers:
    learn_throughput: 8258.45
    learn_time_ms: 19591.085
    sample_throughput: 23794.481
    sample_time_ms: 6799.56
    update_time_ms: 30.431
  timestamp: 1602712473
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    115 |          3061.97 | 18606080 |  273.147 |              313.747 |              145.717 |            787.846 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3252.040369916811
    time_step_min: 2985
  date: 2020-10-14_21-55-00
  done: false
  episode_len_mean: 787.8978447003248
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.2932296519541
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 168
  episodes_total: 23709
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.19318794459104538
        entropy_coeff: 0.0005000000000000001
        kl: 0.005103734554722905
        model: {}
        policy_loss: -0.009572173541528173
        total_loss: 2.062616149584452
        vf_explained_var: 0.9956663250923157
        vf_loss: 2.0722849369049072
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.506451612903227
    gpu_util_percent0: 0.32548387096774195
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696635239945793
    mean_env_wait_ms: 1.22233942830394
    mean_inference_ms: 4.333398939736894
    mean_raw_obs_processing_ms: 0.37995542595572424
  time_since_restore: 3088.836566209793
  time_this_iter_s: 26.869110584259033
  time_total_s: 3088.836566209793
  timers:
    learn_throughput: 8239.808
    learn_time_ms: 19635.408
    sample_throughput: 23807.931
    sample_time_ms: 6795.718
    update_time_ms: 28.818
  timestamp: 1602712500
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    116 |          3088.84 | 18767872 |  273.293 |              313.747 |              145.717 |            787.898 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3250.701211867948
    time_step_min: 2985
  date: 2020-10-14_21-55-27
  done: false
  episode_len_mean: 787.9596794390183
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.4973843114338
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 249
  episodes_total: 23958
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.1883237063884735
        entropy_coeff: 0.0005000000000000001
        kl: 0.005285117503566046
        model: {}
        policy_loss: -0.008873280491873933
        total_loss: 2.5894548892974854
        vf_explained_var: 0.995692253112793
        vf_loss: 2.5984223087628684
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.932258064516123
    gpu_util_percent0: 0.31548387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695423836044533
    mean_env_wait_ms: 1.2222894507942368
    mean_inference_ms: 4.332721598937966
    mean_raw_obs_processing_ms: 0.3799137816501735
  time_since_restore: 3115.454750776291
  time_this_iter_s: 26.618184566497803
  time_total_s: 3115.454750776291
  timers:
    learn_throughput: 8240.483
    learn_time_ms: 19633.801
    sample_throughput: 23823.202
    sample_time_ms: 6791.362
    update_time_ms: 38.837
  timestamp: 1602712527
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    117 |          3115.45 | 18929664 |  273.497 |              313.747 |              145.717 |             787.96 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3249.568108376833
    time_step_min: 2985
  date: 2020-10-14_21-55-54
  done: false
  episode_len_mean: 788.0351320036415
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.6633533046261
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 208
  episodes_total: 24166
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.16562138870358467
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037574616338436804
        model: {}
        policy_loss: -0.009835591190494597
        total_loss: 2.84849351644516
        vf_explained_var: 0.9947153925895691
        vf_loss: 2.858411947886149
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.193548387096772
    gpu_util_percent0: 0.2851612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694326067957314
    mean_env_wait_ms: 1.2222445579142092
    mean_inference_ms: 4.3321702422043575
    mean_raw_obs_processing_ms: 0.3798819170617715
  time_since_restore: 3142.1360278129578
  time_this_iter_s: 26.68127703666687
  time_total_s: 3142.1360278129578
  timers:
    learn_throughput: 8237.548
    learn_time_ms: 19640.797
    sample_throughput: 23854.968
    sample_time_ms: 6782.319
    update_time_ms: 38.113
  timestamp: 1602712554
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    118 |          3142.14 | 19091456 |  273.663 |              313.747 |              145.717 |            788.035 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3248.696634298881
    time_step_min: 2985
  date: 2020-10-14_21-56-22
  done: false
  episode_len_mean: 788.0884432023672
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.7988661064034
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 166
  episodes_total: 24332
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.1790087049206098
        entropy_coeff: 0.0005000000000000001
        kl: 0.004484872003862013
        model: {}
        policy_loss: -0.007627869034574057
        total_loss: 1.894111563762029
        vf_explained_var: 0.9958425164222717
        vf_loss: 1.901828944683075
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.509677419354837
    gpu_util_percent0: 0.26129032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693517889004373
    mean_env_wait_ms: 1.2222067443837727
    mean_inference_ms: 4.331726846533893
    mean_raw_obs_processing_ms: 0.37985635825204195
  time_since_restore: 3168.8149740695953
  time_this_iter_s: 26.678946256637573
  time_total_s: 3168.8149740695953
  timers:
    learn_throughput: 8230.362
    learn_time_ms: 19657.943
    sample_throughput: 23876.494
    sample_time_ms: 6776.204
    update_time_ms: 38.022
  timestamp: 1602712582
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    119 |          3168.81 | 19253248 |  273.799 |              313.747 |              145.717 |            788.088 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3247.578212974296
    time_step_min: 2985
  date: 2020-10-14_21-56-49
  done: false
  episode_len_mean: 788.1217295623115
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.9706412894121
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 24538
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.18523199607928595
        entropy_coeff: 0.0005000000000000001
        kl: 0.004502383215973775
        model: {}
        policy_loss: -0.010654812766006216
        total_loss: 2.2246050238609314
        vf_explained_var: 0.9959731698036194
        vf_loss: 2.235352416833242
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.451612903225808
    gpu_util_percent0: 0.3574193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692605172456882
    mean_env_wait_ms: 1.2221627193782518
    mean_inference_ms: 4.331182917600608
    mean_raw_obs_processing_ms: 0.379824683605861
  time_since_restore: 3195.450960636139
  time_this_iter_s: 26.63598656654358
  time_total_s: 3195.450960636139
  timers:
    learn_throughput: 8230.285
    learn_time_ms: 19658.128
    sample_throughput: 23851.147
    sample_time_ms: 6783.405
    update_time_ms: 37.613
  timestamp: 1602712609
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    120 |          3195.45 | 19415040 |  273.971 |              313.747 |              145.717 |            788.122 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3246.2695880452343
    time_step_min: 2985
  date: 2020-10-14_21-57-16
  done: false
  episode_len_mean: 788.1631837986122
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 274.17512546800907
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 250
  episodes_total: 24788
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.16276827827095985
        entropy_coeff: 0.0005000000000000001
        kl: 0.00412072289812689
        model: {}
        policy_loss: -0.009454146483525014
        total_loss: 2.279143989086151
        vf_explained_var: 0.9961078763008118
        vf_loss: 2.288679520289103
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.712903225806457
    gpu_util_percent0: 0.29193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691395330608997
    mean_env_wait_ms: 1.2220987406135744
    mean_inference_ms: 4.330558513157163
    mean_raw_obs_processing_ms: 0.3797854103351339
  time_since_restore: 3222.1494965553284
  time_this_iter_s: 26.698535919189453
  time_total_s: 3222.1494965553284
  timers:
    learn_throughput: 8225.719
    learn_time_ms: 19669.042
    sample_throughput: 23766.175
    sample_time_ms: 6807.659
    update_time_ms: 37.677
  timestamp: 1602712636
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    121 |          3222.15 | 19576832 |  274.175 |              313.747 |              145.717 |            788.163 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3245.258913174253
    time_step_min: 2985
  date: 2020-10-14_21-57-43
  done: false
  episode_len_mean: 788.1857549172776
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 274.3315148844531
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 24963
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.1596402848760287
        entropy_coeff: 0.0005000000000000001
        kl: 0.005416504534271856
        model: {}
        policy_loss: -0.00870276420027949
        total_loss: 1.4710561831792195
        vf_explained_var: 0.9967753887176514
        vf_loss: 1.479838788509369
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.03125
    gpu_util_percent0: 0.3575
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690504579431587
    mean_env_wait_ms: 1.222054786083814
    mean_inference_ms: 4.330111059841767
    mean_raw_obs_processing_ms: 0.37975963910118726
  time_since_restore: 3249.149427652359
  time_this_iter_s: 26.99993109703064
  time_total_s: 3249.149427652359
  timers:
    learn_throughput: 8211.934
    learn_time_ms: 19702.058
    sample_throughput: 23706.842
    sample_time_ms: 6824.696
    update_time_ms: 35.645
  timestamp: 1602712663
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    122 |          3249.15 | 19738624 |  274.332 |              313.747 |              145.717 |            788.186 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3244.2511844567425
    time_step_min: 2985
  date: 2020-10-14_21-58-10
  done: false
  episode_len_mean: 788.2116126466494
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 274.487660458231
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 182
  episodes_total: 25145
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.180524201442798
        entropy_coeff: 0.0005000000000000001
        kl: 0.004025873844511807
        model: {}
        policy_loss: -0.008737184204316387
        total_loss: 2.357350011666616
        vf_explained_var: 0.9952574372291565
        vf_loss: 2.366177499294281
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.780645161290327
    gpu_util_percent0: 0.3396774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689715861787356
    mean_env_wait_ms: 1.2220093013753637
    mean_inference_ms: 4.3296432230599
    mean_raw_obs_processing_ms: 0.3797326021799113
  time_since_restore: 3275.551288843155
  time_this_iter_s: 26.4018611907959
  time_total_s: 3275.551288843155
  timers:
    learn_throughput: 8215.149
    learn_time_ms: 19694.349
    sample_throughput: 23734.291
    sample_time_ms: 6816.804
    update_time_ms: 34.24
  timestamp: 1602712690
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    123 |          3275.55 | 19900416 |  274.488 |              313.747 |              145.717 |            788.212 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3242.902815013405
    time_step_min: 2983
  date: 2020-10-14_21-58-37
  done: false
  episode_len_mean: 788.2408238815375
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 274.69141915373
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 247
  episodes_total: 25392
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.16817189256350198
        entropy_coeff: 0.0005000000000000001
        kl: 0.003952971640198181
        model: {}
        policy_loss: -0.008649292712410292
        total_loss: 2.81756599744161
        vf_explained_var: 0.9952892661094666
        vf_loss: 2.8262993494669595
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.373333333333335
    gpu_util_percent0: 0.3496666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468867584911421
    mean_env_wait_ms: 1.2219450087193173
    mean_inference_ms: 4.329069771586382
    mean_raw_obs_processing_ms: 0.37969496216336956
  time_since_restore: 3301.9769790172577
  time_this_iter_s: 26.425690174102783
  time_total_s: 3301.9769790172577
  timers:
    learn_throughput: 8227.546
    learn_time_ms: 19664.673
    sample_throughput: 23809.058
    sample_time_ms: 6795.397
    update_time_ms: 34.262
  timestamp: 1602712717
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    124 |          3301.98 | 20062208 |  274.691 |              314.051 |              145.717 |            788.241 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3241.7715749941317
    time_step_min: 2983
  date: 2020-10-14_21-59-04
  done: false
  episode_len_mean: 788.261899179367
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 274.85554845050734
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 25590
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.15114022915561995
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037504432257264853
        model: {}
        policy_loss: -0.008024289476452395
        total_loss: 2.2723763585090637
        vf_explained_var: 0.9954743385314941
        vf_loss: 2.2804762721061707
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.340625000000003
    gpu_util_percent0: 0.3521875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687689153435077
    mean_env_wait_ms: 1.2218896995550659
    mean_inference_ms: 4.328587331390635
    mean_raw_obs_processing_ms: 0.3796666291415128
  time_since_restore: 3328.5426602363586
  time_this_iter_s: 26.565681219100952
  time_total_s: 3328.5426602363586
  timers:
    learn_throughput: 8230.856
    learn_time_ms: 19656.764
    sample_throughput: 23792.087
    sample_time_ms: 6800.244
    update_time_ms: 33.936
  timestamp: 1602712744
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    125 |          3328.54 | 20224000 |  274.856 |              314.051 |              145.717 |            788.262 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3240.8137510202496
    time_step_min: 2983
  date: 2020-10-14_21-59-32
  done: false
  episode_len_mean: 788.2614046666926
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.0015674860182
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 25757
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.16752837101618448
        entropy_coeff: 0.0005000000000000001
        kl: 0.004407510122594734
        model: {}
        policy_loss: -0.007346496883352908
        total_loss: 2.0575188398361206
        vf_explained_var: 0.9953958988189697
        vf_loss: 2.0649491449197135
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.258064516129032
    gpu_util_percent0: 0.31516129032258067
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686987014498562
    mean_env_wait_ms: 1.2218428066000897
    mean_inference_ms: 4.328190910264672
    mean_raw_obs_processing_ms: 0.37964332726773314
  time_since_restore: 3355.275861978531
  time_this_iter_s: 26.73320174217224
  time_total_s: 3355.275861978531
  timers:
    learn_throughput: 8238.218
    learn_time_ms: 19639.2
    sample_throughput: 23806.796
    sample_time_ms: 6796.043
    update_time_ms: 33.632
  timestamp: 1602712772
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    126 |          3355.28 | 20385792 |  275.002 |              314.051 |              145.717 |            788.261 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3239.4418461301384
    time_step_min: 2983
  date: 2020-10-14_21-59-59
  done: false
  episode_len_mean: 788.2647296517222
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.2016761807024
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 228
  episodes_total: 25985
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.176772673924764
        entropy_coeff: 0.0005000000000000001
        kl: 0.005184893767970304
        model: {}
        policy_loss: -0.006322067997340734
        total_loss: 2.315654675165812
        vf_explained_var: 0.9958818554878235
        vf_loss: 2.3220651348431907
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.48387096774194
    gpu_util_percent0: 0.2887096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686079329113325
    mean_env_wait_ms: 1.2217838851963236
    mean_inference_ms: 4.327678873898756
    mean_raw_obs_processing_ms: 0.37961186223991417
  time_since_restore: 3381.8737349510193
  time_this_iter_s: 26.597872972488403
  time_total_s: 3381.8737349510193
  timers:
    learn_throughput: 8240.256
    learn_time_ms: 19634.341
    sample_throughput: 23766.837
    sample_time_ms: 6807.469
    update_time_ms: 25.104
  timestamp: 1602712799
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    127 |          3381.87 | 20547584 |  275.202 |              314.051 |              145.717 |            788.265 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3238.1139704478637
    time_step_min: 2983
  date: 2020-10-14_22-00-26
  done: false
  episode_len_mean: 788.2727411419199
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.39961921360896
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 234
  episodes_total: 26219
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.15511388828357062
        entropy_coeff: 0.0005000000000000001
        kl: 0.004329611896537244
        model: {}
        policy_loss: -0.008789342808692405
        total_loss: 2.1119957665602365
        vf_explained_var: 0.9960920214653015
        vf_loss: 2.120862672726313
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.664516129032254
    gpu_util_percent0: 0.43000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684961444677772
    mean_env_wait_ms: 1.2217119959976164
    mean_inference_ms: 4.327134080112943
    mean_raw_obs_processing_ms: 0.3795778836235071
  time_since_restore: 3408.5677347183228
  time_this_iter_s: 26.693999767303467
  time_total_s: 3408.5677347183228
  timers:
    learn_throughput: 8239.91
    learn_time_ms: 19635.166
    sample_throughput: 23778.291
    sample_time_ms: 6804.19
    update_time_ms: 26.015
  timestamp: 1602712826
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    128 |          3408.57 | 20709376 |    275.4 |              314.051 |              145.717 |            788.273 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3237.162765109838
    time_step_min: 2983
  date: 2020-10-14_22-00-53
  done: false
  episode_len_mean: 788.2795906765208
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.54598859544836
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 166
  episodes_total: 26385
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.15931311622262
        entropy_coeff: 0.0005000000000000001
        kl: 0.004361555601159732
        model: {}
        policy_loss: -0.010182505958558371
        total_loss: 1.4885950684547424
        vf_explained_var: 0.9966419339179993
        vf_loss: 1.4988572200139363
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.732258064516135
    gpu_util_percent0: 0.27129032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468423038471931
    mean_env_wait_ms: 1.2216616500857445
    mean_inference_ms: 4.326749606372286
    mean_raw_obs_processing_ms: 0.37955515653901656
  time_since_restore: 3435.2815778255463
  time_this_iter_s: 26.71384310722351
  time_total_s: 3435.2815778255463
  timers:
    learn_throughput: 8237.815
    learn_time_ms: 19640.16
    sample_throughput: 23788.688
    sample_time_ms: 6801.216
    update_time_ms: 25.549
  timestamp: 1602712853
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    129 |          3435.28 | 20871168 |  275.546 |              314.051 |              145.717 |             788.28 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3236.0583694219545
    time_step_min: 2983
  date: 2020-10-14_22-01-21
  done: false
  episode_len_mean: 788.281570928789
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.7120305868753
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 26583
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17179154232144356
        entropy_coeff: 0.0005000000000000001
        kl: 0.004558554695298274
        model: {}
        policy_loss: -0.006725228300638264
        total_loss: 1.883975436290105
        vf_explained_var: 0.9963571429252625
        vf_loss: 1.8907865385214488
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.715625000000003
    gpu_util_percent0: 0.4453125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468349583931652
    mean_env_wait_ms: 1.2216033421672037
    mean_inference_ms: 4.326296459233892
    mean_raw_obs_processing_ms: 0.379528995683971
  time_since_restore: 3462.2451581954956
  time_this_iter_s: 26.96358036994934
  time_total_s: 3462.2451581954956
  timers:
    learn_throughput: 8228.746
    learn_time_ms: 19661.806
    sample_throughput: 23784.402
    sample_time_ms: 6802.441
    update_time_ms: 25.315
  timestamp: 1602712881
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    130 |          3462.25 | 21032960 |  275.712 |              314.051 |              145.717 |            788.282 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3234.639588228712
    time_step_min: 2983
  date: 2020-10-14_22-01-48
  done: false
  episode_len_mean: 788.2970304407764
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.9265797811942
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 256
  episodes_total: 26839
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.15825197969873747
        entropy_coeff: 0.0005000000000000001
        kl: 0.003911467889944713
        model: {}
        policy_loss: -0.008784900870523416
        total_loss: 2.381384531656901
        vf_explained_var: 0.9959767460823059
        vf_loss: 2.3902485966682434
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.309677419354834
    gpu_util_percent0: 0.4325806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468237326686998
    mean_env_wait_ms: 1.22152273841787
    mean_inference_ms: 4.325758573108254
    mean_raw_obs_processing_ms: 0.37949304567464204
  time_since_restore: 3489.1912281513214
  time_this_iter_s: 26.946069955825806
  time_total_s: 3489.1912281513214
  timers:
    learn_throughput: 8215.364
    learn_time_ms: 19693.833
    sample_throughput: 23811.357
    sample_time_ms: 6794.741
    update_time_ms: 24.992
  timestamp: 1602712908
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    131 |          3489.19 | 21194752 |  275.927 |              314.051 |              145.717 |            788.297 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3233.6164079001
    time_step_min: 2978
  date: 2020-10-14_22-02-15
  done: false
  episode_len_mean: 788.3138626688876
  episode_reward_max: 314.80808080808055
  episode_reward_mean: 276.0745264976247
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 176
  episodes_total: 27015
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.14602292825778326
        entropy_coeff: 0.0005000000000000001
        kl: 0.004378903075121343
        model: {}
        policy_loss: -0.008366142290469725
        total_loss: 1.7862057387828827
        vf_explained_var: 0.9961981773376465
        vf_loss: 1.7946448524792988
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.961290322580645
    gpu_util_percent0: 0.27419354838709675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681632919947016
    mean_env_wait_ms: 1.2214688320884948
    mean_inference_ms: 4.32536735876815
    mean_raw_obs_processing_ms: 0.37946994136740936
  time_since_restore: 3515.857789516449
  time_this_iter_s: 26.666561365127563
  time_total_s: 3515.857789516449
  timers:
    learn_throughput: 8218.577
    learn_time_ms: 19686.134
    sample_throughput: 23875.534
    sample_time_ms: 6776.477
    update_time_ms: 25.048
  timestamp: 1602712935
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    132 |          3515.86 | 21356544 |  276.075 |              314.808 |              145.717 |            788.314 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3232.6280739213666
    time_step_min: 2977
  date: 2020-10-14_22-02-43
  done: false
  episode_len_mean: 788.342453662842
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.21974266049716
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 27192
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.16214740027983984
        entropy_coeff: 0.0005000000000000001
        kl: 0.004044433900465568
        model: {}
        policy_loss: -0.007778106645370523
        total_loss: 2.16769078373909
        vf_explained_var: 0.9955258369445801
        vf_loss: 2.1755499641100564
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.070967741935483
    gpu_util_percent0: 0.2603225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680946888451413
    mean_env_wait_ms: 1.2214132382622591
    mean_inference_ms: 4.324996399705925
    mean_raw_obs_processing_ms: 0.3794474435365348
  time_since_restore: 3542.487686395645
  time_this_iter_s: 26.629896879196167
  time_total_s: 3542.487686395645
  timers:
    learn_throughput: 8213.066
    learn_time_ms: 19699.342
    sample_throughput: 23846.606
    sample_time_ms: 6784.697
    update_time_ms: 25.777
  timestamp: 1602712963
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    133 |          3542.49 | 21518336 |   276.22 |               314.96 |              145.717 |            788.342 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3231.324647835924
    time_step_min: 2977
  date: 2020-10-14_22-03-10
  done: false
  episode_len_mean: 788.3818082391542
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.4198750170313
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 238
  episodes_total: 27430
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.1610215405623118
        entropy_coeff: 0.0005000000000000001
        kl: 0.004177411707739036
        model: {}
        policy_loss: -0.009487569427922912
        total_loss: 2.4388925631841025
        vf_explained_var: 0.9958627223968506
        vf_loss: 2.4484606782595315
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.803225806451614
    gpu_util_percent0: 0.2551612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468005387600175
    mean_env_wait_ms: 1.2213396407974169
    mean_inference_ms: 4.324503154826005
    mean_raw_obs_processing_ms: 0.37941540726892586
  time_since_restore: 3569.093435525894
  time_this_iter_s: 26.605749130249023
  time_total_s: 3569.093435525894
  timers:
    learn_throughput: 8210.097
    learn_time_ms: 19706.465
    sample_throughput: 23818.381
    sample_time_ms: 6792.737
    update_time_ms: 27.536
  timestamp: 1602712990
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    134 |          3569.09 | 21680128 |   276.42 |               314.96 |              145.717 |            788.382 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3230.0831794314686
    time_step_min: 2977
  date: 2020-10-14_22-03-37
  done: false
  episode_len_mean: 788.4385196975726
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.60860787449786
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 27643
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.1374009164671103
        entropy_coeff: 0.0005000000000000001
        kl: 0.003439911912816266
        model: {}
        policy_loss: -0.009646366641391069
        total_loss: 1.6807946264743805
        vf_explained_var: 0.9967214465141296
        vf_loss: 1.6905097166697185
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81612903225807
    gpu_util_percent0: 0.24516129032258063
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679150128133814
    mean_env_wait_ms: 1.2212645108804558
    mean_inference_ms: 4.324055436291136
    mean_raw_obs_processing_ms: 0.3793889073447351
  time_since_restore: 3595.5855836868286
  time_this_iter_s: 26.49214816093445
  time_total_s: 3595.5855836868286
  timers:
    learn_throughput: 8213.186
    learn_time_ms: 19699.054
    sample_throughput: 23828.645
    sample_time_ms: 6789.811
    update_time_ms: 28.326
  timestamp: 1602713017
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    135 |          3595.59 | 21841920 |  276.609 |               314.96 |              145.717 |            788.439 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3229.0721382289416
    time_step_min: 2977
  date: 2020-10-14_22-04-04
  done: false
  episode_len_mean: 788.4840333716916
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.75267599760537
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 165
  episodes_total: 27808
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.14746239284674326
        entropy_coeff: 0.0005000000000000001
        kl: 0.004442893531328688
        model: {}
        policy_loss: -0.008263095058888817
        total_loss: 1.513249506553014
        vf_explained_var: 0.9966320991516113
        vf_loss: 1.521586388349533
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9741935483871
    gpu_util_percent0: 0.28354838709677416
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678502071578864
    mean_env_wait_ms: 1.2212071209042408
    mean_inference_ms: 4.323716254961139
    mean_raw_obs_processing_ms: 0.3793680245470699
  time_since_restore: 3621.960104703903
  time_this_iter_s: 26.374521017074585
  time_total_s: 3621.960104703903
  timers:
    learn_throughput: 8225.671
    learn_time_ms: 19669.154
    sample_throughput: 23819.346
    sample_time_ms: 6792.462
    update_time_ms: 28.233
  timestamp: 1602713044
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    136 |          3621.96 | 22003712 |  276.753 |               314.96 |              145.717 |            788.484 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3227.871663867948
    time_step_min: 2977
  date: 2020-10-14_22-04-31
  done: false
  episode_len_mean: 788.5376378627262
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.92742717895294
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 209
  episodes_total: 28017
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.162939532349507
        entropy_coeff: 0.0005000000000000001
        kl: 0.004543364746496081
        model: {}
        policy_loss: -0.008048403319359446
        total_loss: 1.9196917812029521
        vf_explained_var: 0.996406614780426
        vf_loss: 1.9278217256069183
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.765625
    gpu_util_percent0: 0.389375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467780829432538
    mean_env_wait_ms: 1.2211382015179773
    mean_inference_ms: 4.323288457731263
    mean_raw_obs_processing_ms: 0.3793431610782495
  time_since_restore: 3648.7413232326508
  time_this_iter_s: 26.78121852874756
  time_total_s: 3648.7413232326508
  timers:
    learn_throughput: 8212.843
    learn_time_ms: 19699.877
    sample_throughput: 23866.906
    sample_time_ms: 6778.926
    update_time_ms: 27.86
  timestamp: 1602713071
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    137 |          3648.74 | 22165504 |  276.927 |               314.96 |              145.717 |            788.538 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3226.4393638875117
    time_step_min: 2977
  date: 2020-10-14_22-04-58
  done: false
  episode_len_mean: 788.5592668600948
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 277.13715850744353
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 245
  episodes_total: 28262
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.1433006003499031
        entropy_coeff: 0.0005000000000000001
        kl: 0.003983418709443261
        model: {}
        policy_loss: -0.010830612683397097
        total_loss: 1.635247270266215
        vf_explained_var: 0.9971110820770264
        vf_loss: 1.646149565776189
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.445161290322584
    gpu_util_percent0: 0.4067741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676816423943645
    mean_env_wait_ms: 1.2210495095077953
    mean_inference_ms: 4.32282637650454
    mean_raw_obs_processing_ms: 0.37931139278786397
  time_since_restore: 3675.2304146289825
  time_this_iter_s: 26.489091396331787
  time_total_s: 3675.2304146289825
  timers:
    learn_throughput: 8221.149
    learn_time_ms: 19679.975
    sample_throughput: 23859.778
    sample_time_ms: 6780.952
    update_time_ms: 27.321
  timestamp: 1602713098
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    138 |          3675.23 | 22327296 |  277.137 |               314.96 |              145.717 |            788.559 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3225.426378964413
    time_step_min: 2977
  date: 2020-10-14_22-05-25
  done: false
  episode_len_mean: 788.5693990224004
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 277.2912679916582
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 28437
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.13673369710644087
        entropy_coeff: 0.0005000000000000001
        kl: 0.004674826787474255
        model: {}
        policy_loss: -0.01004581570305163
        total_loss: 1.6409256358941395
        vf_explained_var: 0.9963629841804504
        vf_loss: 1.651039759318034
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.43870967741936
    gpu_util_percent0: 0.3619354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676133011804599
    mean_env_wait_ms: 1.2209851484073917
    mean_inference_ms: 4.322471444171859
    mean_raw_obs_processing_ms: 0.3792901569692155
  time_since_restore: 3701.7334649562836
  time_this_iter_s: 26.503050327301025
  time_total_s: 3701.7334649562836
  timers:
    learn_throughput: 8229.479
    learn_time_ms: 19660.055
    sample_throughput: 23867.568
    sample_time_ms: 6778.738
    update_time_ms: 27.556
  timestamp: 1602713125
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    139 |          3701.73 | 22489088 |  277.291 |               314.96 |              145.717 |            788.569 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3224.360272774961
    time_step_min: 2977
  date: 2020-10-14_22-05-53
  done: false
  episode_len_mean: 788.5837962477727
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 277.4581429005492
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 186
  episodes_total: 28623
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.15852307404081026
        entropy_coeff: 0.0005000000000000001
        kl: 0.006202052386167149
        model: {}
        policy_loss: -0.010324364644475281
        total_loss: 1.5840671956539154
        vf_explained_var: 0.9966803193092346
        vf_loss: 1.5944707691669464
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.28709677419355
    gpu_util_percent0: 0.34322580645161294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467553997141158
    mean_env_wait_ms: 1.2209165732071883
    mean_inference_ms: 4.32211689573195
    mean_raw_obs_processing_ms: 0.3792678622794397
  time_since_restore: 3728.4802691936493
  time_this_iter_s: 26.746804237365723
  time_total_s: 3728.4802691936493
  timers:
    learn_throughput: 8231.868
    learn_time_ms: 19654.347
    sample_throughput: 23905.494
    sample_time_ms: 6767.984
    update_time_ms: 28.142
  timestamp: 1602713153
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    140 |          3728.48 | 22650880 |  277.458 |               314.96 |              145.717 |            788.584 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3223.0193814575964
    time_step_min: 2977
  date: 2020-10-14_22-06-20
  done: false
  episode_len_mean: 788.6248354693454
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 277.64970627648137
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 247
  episodes_total: 28870
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.16830350582798323
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009521870427609732
        total_loss: .inf
        vf_explained_var: 0.9946596622467041
        vf_loss: 3.124215324719747
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.622580645161293
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674660740822057
    mean_env_wait_ms: 1.2208286835301723
    mean_inference_ms: 4.3216648757148395
    mean_raw_obs_processing_ms: 0.37923698790579463
  time_since_restore: 3755.0510790348053
  time_this_iter_s: 26.570809841156006
  time_total_s: 3755.0510790348053
  timers:
    learn_throughput: 8250.582
    learn_time_ms: 19609.768
    sample_throughput: 23909.992
    sample_time_ms: 6766.711
    update_time_ms: 33.964
  timestamp: 1602713180
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    141 |          3755.05 | 22812672 |   277.65 |               314.96 |              145.717 |            788.625 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3222.0412907225013
    time_step_min: 2977
  date: 2020-10-14_22-06-47
  done: false
  episode_len_mean: 788.6589141952797
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 277.8012478740476
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 196
  episodes_total: 29066
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0816681711721684e-18
        cur_lr: 5.0e-05
        entropy: 0.14376207565267882
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00921428571261155
        total_loss: .inf
        vf_explained_var: 0.9967970848083496
        vf_loss: 1.564143568277359
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.850000000000005
    gpu_util_percent0: 0.35100000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467385187188361
    mean_env_wait_ms: 1.2207525638959482
    mean_inference_ms: 4.321278703838757
    mean_raw_obs_processing_ms: 0.37921404783379176
  time_since_restore: 3781.2625522613525
  time_this_iter_s: 26.21147322654724
  time_total_s: 3781.2625522613525
  timers:
    learn_throughput: 8265.308
    learn_time_ms: 19574.829
    sample_throughput: 23948.698
    sample_time_ms: 6755.774
    update_time_ms: 34.427
  timestamp: 1602713207
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    142 |          3781.26 | 22974464 |  277.801 |               314.96 |              145.717 |            788.659 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3221.186487689621
    time_step_min: 2977
  date: 2020-10-14_22-07-13
  done: false
  episode_len_mean: 788.6914576990182
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 277.92805099332395
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 165
  episodes_total: 29231
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1225022567582536e-18
        cur_lr: 5.0e-05
        entropy: 0.14912444849809012
        entropy_coeff: 0.0005000000000000001
        kl: 0.004260889099289973
        model: {}
        policy_loss: -0.010115234045467028
        total_loss: 1.8075700600941975
        vf_explained_var: 0.9959540367126465
        vf_loss: 1.8177598416805267
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.290322580645157
    gpu_util_percent0: 0.3374193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467327088324031
    mean_env_wait_ms: 1.2206885371544
    mean_inference_ms: 4.320970331470218
    mean_raw_obs_processing_ms: 0.37919464402713277
  time_since_restore: 3807.6524941921234
  time_this_iter_s: 26.389941930770874
  time_total_s: 3807.6524941921234
  timers:
    learn_throughput: 8276.271
    learn_time_ms: 19548.9
    sample_throughput: 23969.676
    sample_time_ms: 6749.862
    update_time_ms: 40.565
  timestamp: 1602713233
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    143 |          3807.65 | 23136256 |  277.928 |               314.96 |              145.717 |            788.691 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3219.983244400639
    time_step_min: 2977
  date: 2020-10-14_22-07-40
  done: false
  episode_len_mean: 788.7232352042375
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 278.10812824177384
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 220
  episodes_total: 29451
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5612511283791268e-18
        cur_lr: 5.0e-05
        entropy: 0.1581003020207087
        entropy_coeff: 0.0005000000000000001
        kl: 0.003872800152748823
        model: {}
        policy_loss: -0.010787528386572376
        total_loss: 2.3225021163622537
        vf_explained_var: 0.9957163333892822
        vf_loss: 2.3333686987559
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.019354838709678
    gpu_util_percent0: 0.36322580645161284
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146726298032298
    mean_env_wait_ms: 1.2206072606529474
    mean_inference_ms: 4.3205766456246275
    mean_raw_obs_processing_ms: 0.37916947220300856
  time_since_restore: 3834.1043787002563
  time_this_iter_s: 26.451884508132935
  time_total_s: 3834.1043787002563
  timers:
    learn_throughput: 8285.589
    learn_time_ms: 19526.915
    sample_throughput: 23977.372
    sample_time_ms: 6747.695
    update_time_ms: 38.962
  timestamp: 1602713260
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    144 |           3834.1 | 23298048 |  278.108 |               314.96 |              145.717 |            788.723 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3218.720695816337
    time_step_min: 2977
  date: 2020-10-14_22-08-07
  done: false
  episode_len_mean: 788.7681789094338
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 278.2995275580906
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 240
  episodes_total: 29691
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.806255641895634e-19
        cur_lr: 5.0e-05
        entropy: 0.1400565207004547
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009861079827411837
        total_loss: .inf
        vf_explained_var: 0.9960361123085022
        vf_loss: 2.200152317682902
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809677419354838
    gpu_util_percent0: 0.39
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671667413781403
    mean_env_wait_ms: 1.220513206147692
    mean_inference_ms: 4.32014974989469
    mean_raw_obs_processing_ms: 0.3791414339381879
  time_since_restore: 3860.544247150421
  time_this_iter_s: 26.439868450164795
  time_total_s: 3860.544247150421
  timers:
    learn_throughput: 8289.666
    learn_time_ms: 19517.312
    sample_throughput: 23990.53
    sample_time_ms: 6743.994
    update_time_ms: 46.024
  timestamp: 1602713287
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    145 |          3860.54 | 23459840 |    278.3 |               314.96 |              145.717 |            788.768 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3217.780612073878
    time_step_min: 2977
  date: 2020-10-14_22-08-35
  done: false
  episode_len_mean: 788.7996048357389
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 278.445555653653
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 29861
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1709383462843447e-18
        cur_lr: 5.0e-05
        entropy: 0.1313407632211844
        entropy_coeff: 0.0005000000000000001
        kl: 0.004263762248835216
        model: {}
        policy_loss: -0.008506376393294582
        total_loss: 1.552038957675298
        vf_explained_var: 0.9964461922645569
        vf_loss: 1.5606110294659932
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.8
    gpu_util_percent0: 0.33032258064516123
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671087273808014
    mean_env_wait_ms: 1.2204449498000767
    mean_inference_ms: 4.31984052160572
    mean_raw_obs_processing_ms: 0.3791221862544339
  time_since_restore: 3887.0745553970337
  time_this_iter_s: 26.53030824661255
  time_total_s: 3887.0745553970337
  timers:
    learn_throughput: 8285.275
    learn_time_ms: 19527.657
    sample_throughput: 24008.653
    sample_time_ms: 6738.904
    update_time_ms: 53.125
  timestamp: 1602713315
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    146 |          3887.07 | 23621632 |  278.446 |               314.96 |              145.717 |              788.8 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3216.7197295226674
    time_step_min: 2977
  date: 2020-10-14_22-09-02
  done: false
  episode_len_mean: 788.8183633398781
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 278.6072845329059
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 30049
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.854691731421723e-19
        cur_lr: 5.0e-05
        entropy: 0.14911921819051108
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008715584854750583
        total_loss: .inf
        vf_explained_var: 0.9964790344238281
        vf_loss: 1.7178310453891754
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.035483870967745
    gpu_util_percent0: 0.3458064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467056384269816
    mean_env_wait_ms: 1.220370064767366
    mean_inference_ms: 4.319506789914819
    mean_raw_obs_processing_ms: 0.37910103926922273
  time_since_restore: 3913.671360015869
  time_this_iter_s: 26.59680461883545
  time_total_s: 3913.671360015869
  timers:
    learn_throughput: 8293.044
    learn_time_ms: 19509.362
    sample_throughput: 24014.913
    sample_time_ms: 6737.147
    update_time_ms: 53.954
  timestamp: 1602713342
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    147 |          3913.67 | 23783424 |  278.607 |               314.96 |              145.717 |            788.818 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3215.3069732104514
    time_step_min: 2977
  date: 2020-10-14_22-09-29
  done: false
  episode_len_mean: 788.8366720570278
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 278.8260000086671
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 252
  episodes_total: 30301
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.782037597132586e-19
        cur_lr: 5.0e-05
        entropy: 0.1474688226977984
        entropy_coeff: 0.0005000000000000001
        kl: 0.005272291911145051
        model: {}
        policy_loss: -0.008478500409789072
        total_loss: 1.800652265548706
        vf_explained_var: 0.9967991709709167
        vf_loss: 1.809204528729121
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.980645161290326
    gpu_util_percent0: 0.30870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669671388400285
    mean_env_wait_ms: 1.2202673054689648
    mean_inference_ms: 4.319088198014683
    mean_raw_obs_processing_ms: 0.37907246071840506
  time_since_restore: 3940.0601439476013
  time_this_iter_s: 26.388783931732178
  time_total_s: 3940.0601439476013
  timers:
    learn_throughput: 8300.515
    learn_time_ms: 19491.803
    sample_throughput: 24020.274
    sample_time_ms: 6735.643
    update_time_ms: 53.038
  timestamp: 1602713369
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    148 |          3940.06 | 23945216 |  278.826 |               314.96 |              145.717 |            788.837 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3214.239552214307
    time_step_min: 2977
  date: 2020-10-14_22-09-56
  done: false
  episode_len_mean: 788.8644101151235
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 278.9938924818388
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 30489
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.782037597132586e-19
        cur_lr: 5.0e-05
        entropy: 0.12451807595789433
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009789073355629322
        total_loss: .inf
        vf_explained_var: 0.9971298575401306
        vf_loss: 1.3217582404613495
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.958064516129035
    gpu_util_percent0: 0.29516129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669016408400745
    mean_env_wait_ms: 1.2201926888030215
    mean_inference_ms: 4.3187591295307675
    mean_raw_obs_processing_ms: 0.37905263591446775
  time_since_restore: 3966.639204978943
  time_this_iter_s: 26.579061031341553
  time_total_s: 3966.639204978943
  timers:
    learn_throughput: 8305.274
    learn_time_ms: 19480.634
    sample_throughput: 23964.853
    sample_time_ms: 6751.22
    update_time_ms: 55.149
  timestamp: 1602713396
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    149 |          3966.64 | 24107008 |  278.994 |               314.96 |              145.717 |            788.864 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3213.25314224152
    time_step_min: 2977
  date: 2020-10-14_22-10-23
  done: false
  episode_len_mean: 788.8841775661307
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 279.1432308011125
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 30659
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3173056395698883e-18
        cur_lr: 5.0e-05
        entropy: 0.13329482078552246
        entropy_coeff: 0.0005000000000000001
        kl: 0.003539993253070861
        model: {}
        policy_loss: -0.006602743407711387
        total_loss: 2.5157413283983865
        vf_explained_var: 0.9945247173309326
        vf_loss: 2.5224107106526694
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.783870967741937
    gpu_util_percent0: 0.2887096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668483180320466
    mean_env_wait_ms: 1.2201214148974038
    mean_inference_ms: 4.318469056696311
    mean_raw_obs_processing_ms: 0.379033809346038
  time_since_restore: 3992.923749446869
  time_this_iter_s: 26.284544467926025
  time_total_s: 3992.923749446869
  timers:
    learn_throughput: 8334.235
    learn_time_ms: 19412.94
    sample_throughput: 23884.056
    sample_time_ms: 6774.059
    update_time_ms: 54.421
  timestamp: 1602713423
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    150 |          3992.92 | 24268800 |  279.143 |               314.96 |              145.717 |            788.884 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3211.938909774436
    time_step_min: 2977
  date: 2020-10-14_22-10-50
  done: false
  episode_len_mean: 788.9210918274835
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 279.3422811197062
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 225
  episodes_total: 30884
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.586528197849442e-19
        cur_lr: 5.0e-05
        entropy: 0.1416296251118183
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042360400936255855
        model: {}
        policy_loss: -0.008614240045668945
        total_loss: 2.003620704015096
        vf_explained_var: 0.996284544467926
        vf_loss: 2.0123057762781777
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.75
    gpu_util_percent0: 0.3386666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667862758603506
    mean_env_wait_ms: 1.2200293177728259
    mean_inference_ms: 4.3181161896317715
    mean_raw_obs_processing_ms: 0.3790098245825065
  time_since_restore: 4019.306576728821
  time_this_iter_s: 26.382827281951904
  time_total_s: 4019.306576728821
  timers:
    learn_throughput: 8337.021
    learn_time_ms: 19406.453
    sample_throughput: 23900.205
    sample_time_ms: 6769.482
    update_time_ms: 46.878
  timestamp: 1602713450
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    151 |          4019.31 | 24430592 |  279.342 |               314.96 |              145.717 |            788.921 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3210.6438589718846
    time_step_min: 2977
  date: 2020-10-14_22-11-17
  done: false
  episode_len_mean: 788.9499582181654
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 279.5352668550906
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 230
  episodes_total: 31114
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.293264098924721e-19
        cur_lr: 5.0e-05
        entropy: 0.12199474312365055
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009122772695263848
        total_loss: .inf
        vf_explained_var: 0.9959666728973389
        vf_loss: 2.1170467833677926
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.88387096774193
    gpu_util_percent0: 0.2948387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667029330895726
    mean_env_wait_ms: 1.2199306517471726
    mean_inference_ms: 4.317727461537145
    mean_raw_obs_processing_ms: 0.37898537583669384
  time_since_restore: 4045.7274465560913
  time_this_iter_s: 26.420869827270508
  time_total_s: 4045.7274465560913
  timers:
    learn_throughput: 8336.122
    learn_time_ms: 19408.546
    sample_throughput: 23847.734
    sample_time_ms: 6784.376
    update_time_ms: 48.757
  timestamp: 1602713477
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    152 |          4045.73 | 24592384 |  279.535 |               314.96 |              145.717 |             788.95 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3209.670900655895
    time_step_min: 2977
  date: 2020-10-14_22-11-44
  done: false
  episode_len_mean: 788.9638781446793
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 279.6886287676172
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 31283
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.939896148387079e-19
        cur_lr: 5.0e-05
        entropy: 0.12038154527544975
        entropy_coeff: 0.0005000000000000001
        kl: 0.004832732685220738
        model: {}
        policy_loss: -0.00787825760320023
        total_loss: 1.0501419355471928
        vf_explained_var: 0.9975199699401855
        vf_loss: 1.0580803950627644
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.538709677419355
    gpu_util_percent0: 0.28161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666491977745316
    mean_env_wait_ms: 1.219860187192946
    mean_inference_ms: 4.317450216215717
    mean_raw_obs_processing_ms: 0.37896777621306593
  time_since_restore: 4072.3355321884155
  time_this_iter_s: 26.60808563232422
  time_total_s: 4072.3355321884155
  timers:
    learn_throughput: 8326.932
    learn_time_ms: 19429.966
    sample_throughput: 23829.484
    sample_time_ms: 6789.572
    update_time_ms: 42.518
  timestamp: 1602713504
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    153 |          4072.34 | 24754176 |  279.689 |               314.96 |              145.717 |            788.964 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3208.558600953895
    time_step_min: 2977
  date: 2020-10-14_22-12-11
  done: false
  episode_len_mean: 788.9978715293221
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 279.8578365778631
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 195
  episodes_total: 31478
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4699480741935395e-19
        cur_lr: 5.0e-05
        entropy: 0.1367199644446373
        entropy_coeff: 0.0005000000000000001
        kl: 0.006012260137746732
        model: {}
        policy_loss: -0.007332293961856824
        total_loss: 1.4409786363442738
        vf_explained_var: 0.9970514178276062
        vf_loss: 1.448379288117091
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.132258064516133
    gpu_util_percent0: 0.31354838709677424
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666060195350697
    mean_env_wait_ms: 1.2197785380635664
    mean_inference_ms: 4.3171365179561505
    mean_raw_obs_processing_ms: 0.3789482431750991
  time_since_restore: 4098.692469596863
  time_this_iter_s: 26.356937408447266
  time_total_s: 4098.692469596863
  timers:
    learn_throughput: 8330.196
    learn_time_ms: 19422.353
    sample_throughput: 23812.644
    sample_time_ms: 6794.374
    update_time_ms: 42.276
  timestamp: 1602713531
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    154 |          4098.69 | 24915968 |  279.858 |               314.96 |              145.717 |            788.998 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3207.1330241948203
    time_step_min: 2977
  date: 2020-10-14_22-12-37
  done: false
  episode_len_mean: 789.0392385514829
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 280.07707284958366
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 251
  episodes_total: 31729
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4699480741935395e-19
        cur_lr: 5.0e-05
        entropy: 0.13429266214370728
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00826829919121034
        total_loss: .inf
        vf_explained_var: 0.9979971051216125
        vf_loss: 1.1656927863756816
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.493333333333336
    gpu_util_percent0: 0.29366666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665156770998905
    mean_env_wait_ms: 1.2196647697260266
    mean_inference_ms: 4.316753087763206
    mean_raw_obs_processing_ms: 0.3789206195068426
  time_since_restore: 4124.516813993454
  time_this_iter_s: 25.824344396591187
  time_total_s: 4124.516813993454
  timers:
    learn_throughput: 8358.087
    learn_time_ms: 19357.54
    sample_throughput: 23774.825
    sample_time_ms: 6805.181
    update_time_ms: 34.207
  timestamp: 1602713557
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    155 |          4124.52 | 25077760 |  280.077 |               314.96 |              145.717 |            789.039 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3206.0883828879687
    time_step_min: 2977
  date: 2020-10-14_22-13-04
  done: false
  episode_len_mean: 789.0732639759339
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 280.2379798866072
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 183
  episodes_total: 31912
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.704922111290309e-19
        cur_lr: 5.0e-05
        entropy: 0.11535091946522395
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036586441759330532
        model: {}
        policy_loss: -0.009221723385659667
        total_loss: 0.8797199577093124
        vf_explained_var: 0.9980486035346985
        vf_loss: 0.8889993677536646
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.387096774193548
    gpu_util_percent0: 0.27903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664597756730313
    mean_env_wait_ms: 1.2195867788197057
    mean_inference_ms: 4.316462543192577
    mean_raw_obs_processing_ms: 0.3789029355825305
  time_since_restore: 4151.023004293442
  time_this_iter_s: 26.506190299987793
  time_total_s: 4151.023004293442
  timers:
    learn_throughput: 8356.021
    learn_time_ms: 19362.325
    sample_throughput: 23776.938
    sample_time_ms: 6804.577
    update_time_ms: 27.545
  timestamp: 1602713584
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    156 |          4151.02 | 25239552 |  280.238 |               314.96 |              145.717 |            789.073 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3205.047256620606
    time_step_min: 2977
  date: 2020-10-14_22-13-31
  done: false
  episode_len_mean: 789.1060554118491
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 280.3929018108279
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 32087
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8524610556451545e-19
        cur_lr: 5.0e-05
        entropy: 0.1237692895034949
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036459218439025185
        model: {}
        policy_loss: -0.008938320524369677
        total_loss: 0.8965774029493332
        vf_explained_var: 0.9979607462882996
        vf_loss: 0.9055776198705038
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.706451612903223
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664104401905423
    mean_env_wait_ms: 1.21950845921945
    mean_inference_ms: 4.316194501034713
    mean_raw_obs_processing_ms: 0.37888531172818213
  time_since_restore: 4177.365289449692
  time_this_iter_s: 26.34228515625
  time_total_s: 4177.365289449692
  timers:
    learn_throughput: 8366.403
    learn_time_ms: 19338.299
    sample_throughput: 23782.339
    sample_time_ms: 6803.031
    update_time_ms: 27.183
  timestamp: 1602713611
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    157 |          4177.37 | 25401344 |  280.393 |               314.96 |              145.717 |            789.106 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3203.7183164023786
    time_step_min: 2977
  date: 2020-10-14_22-13-59
  done: false
  episode_len_mean: 789.1454387919297
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 280.5936425150125
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 229
  episodes_total: 32316
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.262305278225773e-20
        cur_lr: 5.0e-05
        entropy: 0.1296235223611196
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009528563744728066
        total_loss: .inf
        vf_explained_var: 0.9976949691772461
        vf_loss: 1.2485245962937672
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.832258064516132
    gpu_util_percent0: 0.3716129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663526876367677
    mean_env_wait_ms: 1.2194080531129425
    mean_inference_ms: 4.315864745977273
    mean_raw_obs_processing_ms: 0.3788627550054175
  time_since_restore: 4203.900372266769
  time_this_iter_s: 26.535082817077637
  time_total_s: 4203.900372266769
  timers:
    learn_throughput: 8362.981
    learn_time_ms: 19346.211
    sample_throughput: 23764.956
    sample_time_ms: 6808.008
    update_time_ms: 35.566
  timestamp: 1602713639
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    158 |           4203.9 | 25563136 |  280.594 |               314.96 |              145.717 |            789.145 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3202.45630709606
    time_step_min: 2977
  date: 2020-10-14_22-14-26
  done: false
  episode_len_mean: 789.1876824733397
  episode_reward_max: 315.4141414141407
  episode_reward_mean: 280.78439175243
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 32539
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3893457917338664e-19
        cur_lr: 5.0e-05
        entropy: 0.11193696906169255
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035469140081355968
        model: {}
        policy_loss: -0.008465598075417802
        total_loss: 0.9968501478433609
        vf_explained_var: 0.9979853630065918
        vf_loss: 1.0053716997305553
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.790322580645164
    gpu_util_percent0: 0.27741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662752523355216
    mean_env_wait_ms: 1.2193032317866666
    mean_inference_ms: 4.315515215483651
    mean_raw_obs_processing_ms: 0.3788405757544616
  time_since_restore: 4230.2415652275085
  time_this_iter_s: 26.341192960739136
  time_total_s: 4230.2415652275085
  timers:
    learn_throughput: 8366.545
    learn_time_ms: 19337.971
    sample_throughput: 23813.451
    sample_time_ms: 6794.143
    update_time_ms: 33.797
  timestamp: 1602713666
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    159 |          4230.24 | 25724928 |  280.784 |              315.414 |              145.717 |            789.188 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3201.5061664167456
    time_step_min: 2977
  date: 2020-10-14_22-14-53
  done: false
  episode_len_mean: 789.213453600367
  episode_reward_max: 315.4141414141407
  episode_reward_mean: 280.9262816206707
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 166
  episodes_total: 32705
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.946728958669332e-20
        cur_lr: 5.0e-05
        entropy: 0.1108698242654403
        entropy_coeff: 0.0005000000000000001
        kl: 0.004437216479952137
        model: {}
        policy_loss: -0.008399140904657543
        total_loss: 0.7503145684798559
        vf_explained_var: 0.9982514381408691
        vf_loss: 0.7587691247463226
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28666666666667
    gpu_util_percent0: 0.38200000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466229577130303
    mean_env_wait_ms: 1.2192291282359076
    mean_inference_ms: 4.315271549501003
    mean_raw_obs_processing_ms: 0.37882488446273793
  time_since_restore: 4256.463173389435
  time_this_iter_s: 26.22160816192627
  time_total_s: 4256.463173389435
  timers:
    learn_throughput: 8358.887
    learn_time_ms: 19355.687
    sample_throughput: 23900.216
    sample_time_ms: 6769.478
    update_time_ms: 34.141
  timestamp: 1602713693
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    160 |          4256.46 | 25886720 |  280.926 |              315.414 |              145.717 |            789.213 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3200.396472019465
    time_step_min: 2977
  date: 2020-10-14_22-15-19
  done: false
  episode_len_mean: 789.2310380454601
  episode_reward_max: 315.4141414141407
  episode_reward_mean: 281.09134618336003
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 203
  episodes_total: 32908
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.473364479334666e-20
        cur_lr: 5.0e-05
        entropy: 0.13626275459925333
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008822207886017472
        total_loss: .inf
        vf_explained_var: 0.9974467158317566
        vf_loss: 1.284106453259786
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.080645161290327
    gpu_util_percent0: 0.3587096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661897093340295
    mean_env_wait_ms: 1.2191382413898726
    mean_inference_ms: 4.31498182910499
    mean_raw_obs_processing_ms: 0.3788065089008765
  time_since_restore: 4282.678260564804
  time_this_iter_s: 26.215087175369263
  time_total_s: 4282.678260564804
  timers:
    learn_throughput: 8365.702
    learn_time_ms: 19339.919
    sample_throughput: 23907.031
    sample_time_ms: 6767.549
    update_time_ms: 34.392
  timestamp: 1602713719
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    161 |          4282.68 | 26048512 |  281.091 |              315.414 |              145.717 |            789.231 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3199.194095629075
    time_step_min: 2977
  date: 2020-10-14_22-15-46
  done: false
  episode_len_mean: 789.266377126312
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 281.27058405261425
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 248
  episodes_total: 33156
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.210046719001998e-20
        cur_lr: 5.0e-05
        entropy: 0.12233523838222027
        entropy_coeff: 0.0005000000000000001
        kl: 0.004806389915756881
        model: {}
        policy_loss: -0.01026619664238145
        total_loss: 1.2340365846951802
        vf_explained_var: 0.9978044629096985
        vf_loss: 1.2443639437357585
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17096774193548
    gpu_util_percent0: 0.3287096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661065422464697
    mean_env_wait_ms: 1.2190196583102826
    mean_inference_ms: 4.314627504687946
    mean_raw_obs_processing_ms: 0.37878140365806395
  time_since_restore: 4308.963577747345
  time_this_iter_s: 26.285317182540894
  time_total_s: 4308.963577747345
  timers:
    learn_throughput: 8373.097
    learn_time_ms: 19322.839
    sample_throughput: 23918.814
    sample_time_ms: 6764.215
    update_time_ms: 32.083
  timestamp: 1602713746
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    162 |          4308.96 | 26210304 |  281.271 |              316.172 |              145.717 |            789.266 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3198.266678676515
    time_step_min: 2977
  date: 2020-10-14_22-16-14
  done: false
  episode_len_mean: 789.2882942341154
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 281.41206236481315
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 178
  episodes_total: 33334
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.605023359500999e-20
        cur_lr: 5.0e-05
        entropy: 0.10636095268030961
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007887531052498767
        total_loss: .inf
        vf_explained_var: 0.9983896613121033
        vf_loss: 0.722336952884992
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.406451612903226
    gpu_util_percent0: 0.3580645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466056570614998
    mean_env_wait_ms: 1.218937934188076
    mean_inference_ms: 4.314368926943604
    mean_raw_obs_processing_ms: 0.3787656968716787
  time_since_restore: 4335.571231842041
  time_this_iter_s: 26.607654094696045
  time_total_s: 4335.571231842041
  timers:
    learn_throughput: 8377.146
    learn_time_ms: 19313.5
    sample_throughput: 23922.067
    sample_time_ms: 6763.295
    update_time_ms: 33.904
  timestamp: 1602713774
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    163 |          4335.57 | 26372096 |  281.412 |              316.172 |              145.717 |            789.288 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3197.2965329829485
    time_step_min: 2977
  date: 2020-10-14_22-16-41
  done: false
  episode_len_mean: 789.2928241086081
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 281.5567189122312
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 181
  episodes_total: 33515
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.907535039251499e-20
        cur_lr: 5.0e-05
        entropy: 0.11600011462966602
        entropy_coeff: 0.0005000000000000001
        kl: 0.003600479511078447
        model: {}
        policy_loss: -0.010532642714679241
        total_loss: 1.0136735041936238
        vf_explained_var: 0.9977905750274658
        vf_loss: 1.0242641617854435
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.158064516129027
    gpu_util_percent0: 0.2896774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660120202407315
    mean_env_wait_ms: 1.218852793130226
    mean_inference_ms: 4.3141086359792205
    mean_raw_obs_processing_ms: 0.37874896357425103
  time_since_restore: 4362.068249940872
  time_this_iter_s: 26.497018098831177
  time_total_s: 4362.068249940872
  timers:
    learn_throughput: 8370.773
    learn_time_ms: 19328.202
    sample_throughput: 23926.997
    sample_time_ms: 6761.902
    update_time_ms: 35.856
  timestamp: 1602713801
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    164 |          4362.07 | 26533888 |  281.557 |              316.172 |              145.717 |            789.293 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3196.028520604803
    time_step_min: 2977
  date: 2020-10-14_22-17-08
  done: false
  episode_len_mean: 789.3023579595948
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 281.75096931756076
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 243
  episodes_total: 33758
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9537675196257494e-20
        cur_lr: 5.0e-05
        entropy: 0.11984730946520965
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00834720927135398
        total_loss: .inf
        vf_explained_var: 0.9980988502502441
        vf_loss: 1.0662032465140026
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.332258064516136
    gpu_util_percent0: 0.2796774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659492389797513
    mean_env_wait_ms: 1.218738742282473
    mean_inference_ms: 4.313787329058732
    mean_raw_obs_processing_ms: 0.37872671420954207
  time_since_restore: 4388.664645910263
  time_this_iter_s: 26.59639596939087
  time_total_s: 4388.664645910263
  timers:
    learn_throughput: 8333.668
    learn_time_ms: 19414.26
    sample_throughput: 23959.048
    sample_time_ms: 6752.856
    update_time_ms: 36.01
  timestamp: 1602713828
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    165 |          4388.66 | 26695680 |  281.751 |              316.172 |              145.717 |            789.302 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3194.9322548326263
    time_step_min: 2977
  date: 2020-10-14_22-17-35
  done: false
  episode_len_mean: 789.3066776586975
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 281.9188763741523
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 33964
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9306512794386246e-20
        cur_lr: 5.0e-05
        entropy: 0.10357615165412426
        entropy_coeff: 0.0005000000000000001
        kl: 0.004793360596522689
        model: {}
        policy_loss: -0.008841459993467046
        total_loss: 0.8595468650261561
        vf_explained_var: 0.9982029795646667
        vf_loss: 0.8684401015440623
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64838709677419
    gpu_util_percent0: 0.3383870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465887641402004
    mean_env_wait_ms: 1.2186413222230763
    mean_inference_ms: 4.313497076699435
    mean_raw_obs_processing_ms: 0.3787087506476081
  time_since_restore: 4415.141546487808
  time_this_iter_s: 26.476900577545166
  time_total_s: 4415.141546487808
  timers:
    learn_throughput: 8339.082
    learn_time_ms: 19401.656
    sample_throughput: 23925.534
    sample_time_ms: 6762.315
    update_time_ms: 35.695
  timestamp: 1602713855
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    166 |          4415.14 | 26857472 |  281.919 |              316.172 |              145.717 |            789.307 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3194.0483241943525
    time_step_min: 2977
  date: 2020-10-14_22-18-02
  done: false
  episode_len_mean: 789.2952154932466
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 282.0479223100299
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 34131
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4653256397193123e-20
        cur_lr: 5.0e-05
        entropy: 0.10760966315865517
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01013824592034022
        total_loss: .inf
        vf_explained_var: 0.9971725344657898
        vf_loss: 1.2592540433009465
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95483870967742
    gpu_util_percent0: 0.29161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465847703908514
    mean_env_wait_ms: 1.2185623491183366
    mean_inference_ms: 4.313271341242591
    mean_raw_obs_processing_ms: 0.3786941523521306
  time_since_restore: 4441.8384692668915
  time_this_iter_s: 26.696922779083252
  time_total_s: 4441.8384692668915
  timers:
    learn_throughput: 8329.561
    learn_time_ms: 19423.832
    sample_throughput: 23878.812
    sample_time_ms: 6775.547
    update_time_ms: 34.276
  timestamp: 1602713882
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:18:03,534	WARNING util.py:136 -- The `process_trial` operation took 0.5072624683380127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    167 |          4441.84 | 27019264 |  282.048 |              316.172 |              145.717 |            789.295 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3192.877672901008
    time_step_min: 2977
  date: 2020-10-14_22-18-29
  done: false
  episode_len_mean: 789.2875647668394
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 282.22752470857483
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 34354
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1979884595789678e-20
        cur_lr: 5.0e-05
        entropy: 0.12110500720640023
        entropy_coeff: 0.0005000000000000001
        kl: 0.006616584025323391
        model: {}
        policy_loss: -0.00615591459791176
        total_loss: 0.8147114167610804
        vf_explained_var: 0.9984524846076965
        vf_loss: 0.8209278732538223
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10967741935484
    gpu_util_percent0: 0.31645161290322577
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658008006986914
    mean_env_wait_ms: 1.2184579685249133
    mean_inference_ms: 4.312975744326101
    mean_raw_obs_processing_ms: 0.3786747451063069
  time_since_restore: 4468.122192144394
  time_this_iter_s: 26.28372287750244
  time_total_s: 4468.122192144394
  timers:
    learn_throughput: 8335.106
    learn_time_ms: 19410.911
    sample_throughput: 23891.058
    sample_time_ms: 6772.073
    update_time_ms: 25.803
  timestamp: 1602713909
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    168 |          4468.12 | 27181056 |  282.228 |              316.172 |              145.717 |            789.288 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3191.662403426025
    time_step_min: 2977
  date: 2020-10-14_22-18-57
  done: false
  episode_len_mean: 789.2723855783966
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 282.4052424671731
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 233
  episodes_total: 34587
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1979884595789678e-20
        cur_lr: 5.0e-05
        entropy: 0.1120890894283851
        entropy_coeff: 0.0005000000000000001
        kl: 0.003654043423011899
        model: {}
        policy_loss: -0.008505095436703414
        total_loss: 0.8483370890220007
        vf_explained_var: 0.9983482360839844
        vf_loss: 0.8568982233603796
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.390322580645165
    gpu_util_percent0: 0.3470967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657261782050202
    mean_env_wait_ms: 1.2183441369327426
    mean_inference_ms: 4.312670825450992
    mean_raw_obs_processing_ms: 0.37865432554139816
  time_since_restore: 4494.701295614243
  time_this_iter_s: 26.579103469848633
  time_total_s: 4494.701295614243
  timers:
    learn_throughput: 8329.537
    learn_time_ms: 19423.889
    sample_throughput: 23858.165
    sample_time_ms: 6781.41
    update_time_ms: 25.606
  timestamp: 1602713937
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    169 |           4494.7 | 27342848 |  282.405 |              316.172 |              145.717 |            789.272 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3190.840545909994
    time_step_min: 2977
  date: 2020-10-14_22-19-24
  done: false
  episode_len_mean: 789.2563940274462
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 282.5315437524935
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 172
  episodes_total: 34759
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0989942297894839e-20
        cur_lr: 5.0e-05
        entropy: 0.10570909331242244
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039060571774219475
        model: {}
        policy_loss: -0.008138513163430616
        total_loss: 0.7541509916385015
        vf_explained_var: 0.9982590079307556
        vf_loss: 0.7623423536618551
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.564516129032263
    gpu_util_percent0: 0.3174193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656829724018217
    mean_env_wait_ms: 1.2182627901572691
    mean_inference_ms: 4.31244228842455
    mean_raw_obs_processing_ms: 0.37864017035295633
  time_since_restore: 4521.192013263702
  time_this_iter_s: 26.49071764945984
  time_total_s: 4521.192013263702
  timers:
    learn_throughput: 8322.315
    learn_time_ms: 19440.744
    sample_throughput: 23826.806
    sample_time_ms: 6790.335
    update_time_ms: 25.668
  timestamp: 1602713964
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: a052f_00000
  
2020-10-14 22:19:24,906	WARNING util.py:136 -- The `process_trial` operation took 0.5091309547424316 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    170 |          4521.19 | 27504640 |  282.532 |              316.172 |              145.717 |            789.256 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3189.8831916172808
    time_step_min: 2977
  date: 2020-10-14_22-19-51
  done: false
  episode_len_mean: 789.2191549618102
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 282.6787244819969
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 34957
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.4949711489474196e-21
        cur_lr: 5.0e-05
        entropy: 0.11461860500276089
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008541499812660428
        total_loss: .inf
        vf_explained_var: 0.9980844855308533
        vf_loss: 0.9372938424348831
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.496774193548386
    gpu_util_percent0: 0.337741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465647229157379
    mean_env_wait_ms: 1.2181713206881648
    mean_inference_ms: 4.312192459298146
    mean_raw_obs_processing_ms: 0.37862454931835665
  time_since_restore: 4547.757585525513
  time_this_iter_s: 26.565572261810303
  time_total_s: 4547.757585525513
  timers:
    learn_throughput: 8311.235
    learn_time_ms: 19466.662
    sample_throughput: 23798.282
    sample_time_ms: 6798.474
    update_time_ms: 25.527
  timestamp: 1602713991
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    171 |          4547.76 | 27666432 |  282.679 |              316.172 |              145.717 |            789.219 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3188.6177197919333
    time_step_min: 2977
  date: 2020-10-14_22-20-18
  done: false
  episode_len_mean: 789.1895822090943
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 282.8718162338542
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 252
  episodes_total: 35209
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.24245672342113e-21
        cur_lr: 5.0e-05
        entropy: 0.10944850742816925
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036818081474242112
        model: {}
        policy_loss: -0.006735124601012406
        total_loss: 0.8543484757343928
        vf_explained_var: 0.9984748959541321
        vf_loss: 0.8611383239428202
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.44193548387097
    gpu_util_percent0: 0.2841935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655773216180562
    mean_env_wait_ms: 1.2180482882437036
    mean_inference_ms: 4.311880707632889
    mean_raw_obs_processing_ms: 0.37860219717248683
  time_since_restore: 4574.375772714615
  time_this_iter_s: 26.618187189102173
  time_total_s: 4574.375772714615
  timers:
    learn_throughput: 8296.727
    learn_time_ms: 19500.701
    sample_throughput: 23778.106
    sample_time_ms: 6804.242
    update_time_ms: 26.157
  timestamp: 1602714018
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    172 |          4574.38 | 27828224 |  282.872 |              316.172 |              145.717 |             789.19 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3187.7029411764706
    time_step_min: 2977
  date: 2020-10-14_22-20-45
  done: false
  episode_len_mean: 789.1712727478241
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 283.00964545420277
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 35388
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.121228361710565e-21
        cur_lr: 5.0e-05
        entropy: 0.09176758738855521
        entropy_coeff: 0.0005000000000000001
        kl: 0.003262273909058422
        model: {}
        policy_loss: -0.009252404716486732
        total_loss: 0.5286606550216675
        vf_explained_var: 0.9988150596618652
        vf_loss: 0.5379589423537254
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.187096774193552
    gpu_util_percent0: 0.3074193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655282999253802
    mean_env_wait_ms: 1.2179614566693369
    mean_inference_ms: 4.311641900801873
    mean_raw_obs_processing_ms: 0.3785875061540244
  time_since_restore: 4600.909541606903
  time_this_iter_s: 26.533768892288208
  time_total_s: 4600.909541606903
  timers:
    learn_throughput: 8299.786
    learn_time_ms: 19493.515
    sample_throughput: 23744.617
    sample_time_ms: 6813.839
    update_time_ms: 24.325
  timestamp: 1602714045
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    173 |          4600.91 | 27990016 |   283.01 |              316.172 |              145.717 |            789.171 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3186.772351319417
    time_step_min: 2977
  date: 2020-10-14_22-21-13
  done: false
  episode_len_mean: 789.1596390622365
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 283.1536166181973
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 186
  episodes_total: 35574
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0606141808552826e-21
        cur_lr: 5.0e-05
        entropy: 0.10149838402867317
        entropy_coeff: 0.0005000000000000001
        kl: 0.003697650875740995
        model: {}
        policy_loss: -0.0068799197518577175
        total_loss: 0.550728847583135
        vf_explained_var: 0.9988054633140564
        vf_loss: 0.5576595216989517
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.438709677419357
    gpu_util_percent0: 0.3158064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465491980260475
    mean_env_wait_ms: 1.2178728141144823
    mean_inference_ms: 4.31141193646075
    mean_raw_obs_processing_ms: 0.37857280309533203
  time_since_restore: 4627.416066169739
  time_this_iter_s: 26.506524562835693
  time_total_s: 4627.416066169739
  timers:
    learn_throughput: 8296.406
    learn_time_ms: 19501.456
    sample_throughput: 23764.967
    sample_time_ms: 6808.004
    update_time_ms: 22.36
  timestamp: 1602714073
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    174 |          4627.42 | 28151808 |  283.154 |              316.172 |              145.717 |             789.16 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3185.5586262784327
    time_step_min: 2977
  date: 2020-10-14_22-21-40
  done: false
  episode_len_mean: 789.1558329145028
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 283.33900066166757
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 240
  episodes_total: 35814
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0303070904276413e-21
        cur_lr: 5.0e-05
        entropy: 0.10694639074305694
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00786138538387604
        total_loss: .inf
        vf_explained_var: 0.9984866976737976
        vf_loss: 0.8571861237287521
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.009677419354844
    gpu_util_percent0: 0.357741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654323074103545
    mean_env_wait_ms: 1.217757056918528
    mean_inference_ms: 4.311127603667091
    mean_raw_obs_processing_ms: 0.37855323402318913
  time_since_restore: 4653.7782871723175
  time_this_iter_s: 26.362221002578735
  time_total_s: 4653.7782871723175
  timers:
    learn_throughput: 8305.403
    learn_time_ms: 19480.332
    sample_throughput: 23777.731
    sample_time_ms: 6804.35
    update_time_ms: 22.5
  timestamp: 1602714100
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:21:40,775	WARNING util.py:136 -- The `process_trial` operation took 0.526639461517334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    175 |          4653.78 | 28313600 |  283.339 |              316.172 |              145.717 |            789.156 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3184.519199777716
    time_step_min: 2977
  date: 2020-10-14_22-22-07
  done: false
  episode_len_mean: 789.1558942750847
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 283.49257694385113
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 204
  episodes_total: 36018
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5454606356414623e-21
        cur_lr: 5.0e-05
        entropy: 0.08881916043659051
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007484312761031712
        total_loss: .inf
        vf_explained_var: 0.9990243315696716
        vf_loss: 0.46316610525051755
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.003225806451614
    gpu_util_percent0: 0.32161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653770210728062
    mean_env_wait_ms: 1.2176571069665456
    mean_inference_ms: 4.310866913347631
    mean_raw_obs_processing_ms: 0.37853624118915796
  time_since_restore: 4680.01081776619
  time_this_iter_s: 26.23253059387207
  time_total_s: 4680.01081776619
  timers:
    learn_throughput: 8314.182
    learn_time_ms: 19459.761
    sample_throughput: 23803.14
    sample_time_ms: 6797.087
    update_time_ms: 23.44
  timestamp: 1602714127
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    176 |          4680.01 | 28475392 |  283.493 |              316.172 |              145.717 |            789.156 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3183.6925353320244
    time_step_min: 2977
  date: 2020-10-14_22-22-34
  done: false
  episode_len_mean: 789.1548984385795
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 283.61767879150756
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 36185
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3181909534621926e-21
        cur_lr: 5.0e-05
        entropy: 0.08789520896971226
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008363416670666387
        total_loss: .inf
        vf_explained_var: 0.9990481734275818
        vf_loss: 0.43173837165037793
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19354838709678
    gpu_util_percent0: 0.2709677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465338397809694
    mean_env_wait_ms: 1.2175761534677454
    mean_inference_ms: 4.310664359982423
    mean_raw_obs_processing_ms: 0.37852307123976814
  time_since_restore: 4706.654474020004
  time_this_iter_s: 26.643656253814697
  time_total_s: 4706.654474020004
  timers:
    learn_throughput: 8312.247
    learn_time_ms: 19464.291
    sample_throughput: 23843.426
    sample_time_ms: 6785.602
    update_time_ms: 23.78
  timestamp: 1602714154
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    177 |          4706.65 | 28637184 |  283.618 |              316.172 |              145.717 |            789.155 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3182.612445714914
    time_step_min: 2977
  date: 2020-10-14_22-23-01
  done: false
  episode_len_mean: 789.1589398516891
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 283.781523002616
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 225
  episodes_total: 36410
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4772864301932895e-21
        cur_lr: 5.0e-05
        entropy: 0.09677726527055104
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008536736965955546
        total_loss: .inf
        vf_explained_var: 0.9989333748817444
        vf_loss: 0.591393788655599
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69677419354839
    gpu_util_percent0: 0.3641935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653003811665805
    mean_env_wait_ms: 1.2174681272109213
    mean_inference_ms: 4.310403857873897
    mean_raw_obs_processing_ms: 0.37850547727087996
  time_since_restore: 4732.990352869034
  time_this_iter_s: 26.33587884902954
  time_total_s: 4732.990352869034
  timers:
    learn_throughput: 8313.134
    learn_time_ms: 19462.214
    sample_throughput: 23829.257
    sample_time_ms: 6789.637
    update_time_ms: 25.563
  timestamp: 1602714181
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:23:02,027	WARNING util.py:136 -- The `process_trial` operation took 0.5254552364349365 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    178 |          4732.99 | 28798976 |  283.782 |              316.172 |              145.717 |            789.159 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3181.5320915496804
    time_step_min: 2977
  date: 2020-10-14_22-23-28
  done: false
  episode_len_mean: 789.155968560668
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 283.9464391747835
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 232
  episodes_total: 36642
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.215929645289935e-21
        cur_lr: 5.0e-05
        entropy: 0.08810342724124591
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034540000876101353
        model: {}
        policy_loss: -0.007886913003555188
        total_loss: 0.45509646584590274
        vf_explained_var: 0.9991416335105896
        vf_loss: 0.46302743007739383
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.848387096774193
    gpu_util_percent0: 0.3141935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652317057708614
    mean_env_wait_ms: 1.217350692317594
    mean_inference_ms: 4.310128656089643
    mean_raw_obs_processing_ms: 0.3784866223425206
  time_since_restore: 4759.394148826599
  time_this_iter_s: 26.403795957565308
  time_total_s: 4759.394148826599
  timers:
    learn_throughput: 8320.799
    learn_time_ms: 19444.287
    sample_throughput: 23830.619
    sample_time_ms: 6789.249
    update_time_ms: 25.306
  timestamp: 1602714208
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: a052f_00000
  
2020-10-14 22:23:29,117	WARNING util.py:136 -- The `process_trial` operation took 0.5184285640716553 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    179 |          4759.39 | 28960768 |  283.946 |              316.172 |              145.717 |            789.156 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3180.7305768498886
    time_step_min: 2977
  date: 2020-10-14_22-23-55
  done: false
  episode_len_mean: 789.1586624653664
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 284.0673670479992
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 172
  episodes_total: 36814
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6079648226449673e-21
        cur_lr: 5.0e-05
        entropy: 0.0827391439427932
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008674756099935621
        total_loss: .inf
        vf_explained_var: 0.998986005783081
        vf_loss: 0.45394839346408844
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.703225806451616
    gpu_util_percent0: 0.29225806451612907
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651931821098177
    mean_env_wait_ms: 1.2172672697201958
    mean_inference_ms: 4.309925974003076
    mean_raw_obs_processing_ms: 0.3784735415007051
  time_since_restore: 4785.807953596115
  time_this_iter_s: 26.41380476951599
  time_total_s: 4785.807953596115
  timers:
    learn_throughput: 8326.317
    learn_time_ms: 19431.401
    sample_throughput: 23842.23
    sample_time_ms: 6785.942
    update_time_ms: 32.404
  timestamp: 1602714235
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    180 |          4785.81 | 29122560 |  284.067 |              316.172 |              145.717 |            789.159 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3179.8183637051857
    time_step_min: 2977
  date: 2020-10-14_22-24-22
  done: false
  episode_len_mean: 789.1583184740908
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 284.20785337570857
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 200
  episodes_total: 37014
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.91194723396745e-21
        cur_lr: 5.0e-05
        entropy: 0.09013648393253486
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040988395145783825
        model: {}
        policy_loss: -0.006616673005434374
        total_loss: 0.5569185142715772
        vf_explained_var: 0.9989748001098633
        vf_loss: 0.5635802671313286
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.574193548387093
    gpu_util_percent0: 0.2896774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651651657482975
    mean_env_wait_ms: 1.2171712932903038
    mean_inference_ms: 4.309705028971849
    mean_raw_obs_processing_ms: 0.3784593344476342
  time_since_restore: 4812.403325080872
  time_this_iter_s: 26.59537148475647
  time_total_s: 4812.403325080872
  timers:
    learn_throughput: 8323.104
    learn_time_ms: 19438.901
    sample_throughput: 23891.678
    sample_time_ms: 6771.898
    update_time_ms: 32.345
  timestamp: 1602714262
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: a052f_00000
  
2020-10-14 22:24:23,499	WARNING util.py:136 -- The `process_trial` operation took 0.5322270393371582 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    181 |           4812.4 | 29284352 |  284.208 |              316.172 |              145.717 |            789.158 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3178.7001423544893
    time_step_min: 2977
  date: 2020-10-14_22-24-49
  done: false
  episode_len_mean: 789.1515606967444
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 284.3754214628095
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 245
  episodes_total: 37259
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.955973616983725e-21
        cur_lr: 5.0e-05
        entropy: 0.09492196887731552
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007524763054486054
        total_loss: .inf
        vf_explained_var: 0.9988541603088379
        vf_loss: 0.6669893066088358
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.090322580645168
    gpu_util_percent0: 0.2919354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465096521756101
    mean_env_wait_ms: 1.2170473932330013
    mean_inference_ms: 4.309427695713072
    mean_raw_obs_processing_ms: 0.37843915538898726
  time_since_restore: 4838.647848367691
  time_this_iter_s: 26.244523286819458
  time_total_s: 4838.647848367691
  timers:
    learn_throughput: 8340.65
    learn_time_ms: 19398.01
    sample_throughput: 23880.645
    sample_time_ms: 6775.026
    update_time_ms: 33.014
  timestamp: 1602714289
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:24:50,765	WARNING util.py:136 -- The `process_trial` operation took 0.5004942417144775 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    182 |          4838.65 | 29446144 |  284.375 |              316.172 |              145.717 |            789.152 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3177.8588725843956
    time_step_min: 2977
  date: 2020-10-14_22-25-17
  done: false
  episode_len_mean: 789.1662882935819
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 284.502444384552
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 182
  episodes_total: 37441
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9339604254755885e-21
        cur_lr: 5.0e-05
        entropy: 0.07814933483799298
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007031382973461102
        total_loss: .inf
        vf_explained_var: 0.9992370009422302
        vf_loss: 0.35673628250757855
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.929032258064517
    gpu_util_percent0: 0.30032258064516126
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650550325126274
    mean_env_wait_ms: 1.21695620944699
    mean_inference_ms: 4.309215563685714
    mean_raw_obs_processing_ms: 0.37842579328174236
  time_since_restore: 4865.158463478088
  time_this_iter_s: 26.51061511039734
  time_total_s: 4865.158463478088
  timers:
    learn_throughput: 8342.381
    learn_time_ms: 19393.983
    sample_throughput: 23907.918
    sample_time_ms: 6767.298
    update_time_ms: 40.824
  timestamp: 1602714317
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    183 |          4865.16 | 29607936 |  284.502 |              316.172 |              145.717 |            789.166 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3177.03458182592
    time_step_min: 2977
  date: 2020-10-14_22-25-44
  done: false
  episode_len_mean: 789.1895534290271
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 284.62658885505755
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 37620
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.400940638213381e-21
        cur_lr: 5.0e-05
        entropy: 0.08312734899421532
        entropy_coeff: 0.0005000000000000001
        kl: 0.004286324760566155
        model: {}
        policy_loss: -0.010739777567020306
        total_loss: 0.35133766134579975
        vf_explained_var: 0.9992580413818359
        vf_loss: 0.36211900413036346
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.735483870967744
    gpu_util_percent0: 0.27999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650227112712155
    mean_env_wait_ms: 1.216866654888792
    mean_inference_ms: 4.309020091830666
    mean_raw_obs_processing_ms: 0.3784128122324609
  time_since_restore: 4891.842568159103
  time_this_iter_s: 26.684104681015015
  time_total_s: 4891.842568159103
  timers:
    learn_throughput: 8343.012
    learn_time_ms: 19392.517
    sample_throughput: 23846.281
    sample_time_ms: 6784.79
    update_time_ms: 41.016
  timestamp: 1602714344
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    184 |          4891.84 | 29769728 |  284.627 |              316.172 |              145.717 |             789.19 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3176.021388536379
    time_step_min: 2977
  date: 2020-10-14_22-26-11
  done: false
  episode_len_mean: 789.2166860403678
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 284.78076949351896
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 232
  episodes_total: 37852
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2004703191066905e-21
        cur_lr: 5.0e-05
        entropy: 0.08989276054004829
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007093626967010398
        total_loss: .inf
        vf_explained_var: 0.9989271759986877
        vf_loss: 0.6087484061717987
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15483870967742
    gpu_util_percent0: 0.3377419354838708
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649750286796795
    mean_env_wait_ms: 1.21674981677155
    mean_inference_ms: 4.308775813941868
    mean_raw_obs_processing_ms: 0.3783952252832749
  time_since_restore: 4918.251931190491
  time_this_iter_s: 26.40936303138733
  time_total_s: 4918.251931190491
  timers:
    learn_throughput: 8345.657
    learn_time_ms: 19386.37
    sample_throughput: 23810.732
    sample_time_ms: 6794.919
    update_time_ms: 41.298
  timestamp: 1602714371
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:26:12,404	WARNING util.py:136 -- The `process_trial` operation took 0.518258810043335 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    185 |          4918.25 | 29931520 |  284.781 |              316.172 |              145.717 |            789.217 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3175.06945320715
    time_step_min: 2977
  date: 2020-10-14_22-26-38
  done: false
  episode_len_mean: 789.2445098245246
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 284.9261987851615
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 216
  episodes_total: 38068
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3007054786600356e-21
        cur_lr: 5.0e-05
        entropy: 0.08078863471746445
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006942692671145778
        total_loss: .inf
        vf_explained_var: 0.9993320107460022
        vf_loss: 0.34289467583100003
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.967741935483875
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464917113269372
    mean_env_wait_ms: 1.2166409976876347
    mean_inference_ms: 4.308529717572895
    mean_raw_obs_processing_ms: 0.37837909786607127
  time_since_restore: 4944.654675722122
  time_this_iter_s: 26.40274453163147
  time_total_s: 4944.654675722122
  timers:
    learn_throughput: 8343.124
    learn_time_ms: 19392.256
    sample_throughput: 23773.209
    sample_time_ms: 6805.644
    update_time_ms: 41.879
  timestamp: 1602714398
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:26:39,503	WARNING util.py:136 -- The `process_trial` operation took 0.5144104957580566 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    186 |          4944.65 | 30093312 |  284.926 |              316.172 |              145.717 |            789.245 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3174.322245485475
    time_step_min: 2977
  date: 2020-10-14_22-27-05
  done: false
  episode_len_mean: 789.2692870966055
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 285.0370256780893
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 38238
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.951058217990054e-21
        cur_lr: 5.0e-05
        entropy: 0.0798715129494667
        entropy_coeff: 0.0005000000000000001
        kl: 0.003925071466558923
        model: {}
        policy_loss: -0.00869428199075628
        total_loss: 0.6001824289560318
        vf_explained_var: 0.9987242817878723
        vf_loss: 0.608916645248731
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08387096774194
    gpu_util_percent0: 0.3003225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648855223367024
    mean_env_wait_ms: 1.2165554436862414
    mean_inference_ms: 4.30835211512702
    mean_raw_obs_processing_ms: 0.37836730962653303
  time_since_restore: 4971.040429830551
  time_this_iter_s: 26.385754108428955
  time_total_s: 4971.040429830551
  timers:
    learn_throughput: 8356.14
    learn_time_ms: 19362.051
    sample_throughput: 23760.76
    sample_time_ms: 6809.21
    update_time_ms: 41.856
  timestamp: 1602714425
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: a052f_00000
  
2020-10-14 22:27:06,583	WARNING util.py:136 -- The `process_trial` operation took 0.5143845081329346 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    187 |          4971.04 | 30255104 |  285.037 |              316.172 |              145.717 |            789.269 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3173.467461474386
    time_step_min: 2977
  date: 2020-10-14_22-27-32
  done: false
  episode_len_mean: 789.2962230777234
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 285.16328854038227
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 38444
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.475529108995027e-21
        cur_lr: 5.0e-05
        entropy: 0.08393717619280021
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034277924181272588
        model: {}
        policy_loss: -0.009742526764360568
        total_loss: 0.6591404924790064
        vf_explained_var: 0.9987935423851013
        vf_loss: 0.6689249823490778
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.035483870967738
    gpu_util_percent0: 0.3654838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146485308097245
    mean_env_wait_ms: 1.2164512155777782
    mean_inference_ms: 4.308133178747842
    mean_raw_obs_processing_ms: 0.37835291230774565
  time_since_restore: 4997.230667591095
  time_this_iter_s: 26.190237760543823
  time_total_s: 4997.230667591095
  timers:
    learn_throughput: 8364.713
    learn_time_ms: 19342.205
    sample_throughput: 23764.58
    sample_time_ms: 6808.115
    update_time_ms: 39.756
  timestamp: 1602714452
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: a052f_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    188 |          4997.23 | 30416896 |  285.163 |              316.172 |              145.717 |            789.296 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3172.4381612644543
    time_step_min: 2977
  date: 2020-10-14_22-27-59
  done: false
  episode_len_mean: 789.3229417086726
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 285.3186694396464
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 241
  episodes_total: 38685
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2377645544975136e-21
        cur_lr: 5.0e-05
        entropy: 0.08340649120509624
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034477754573648176
        model: {}
        policy_loss: -0.005828554897258679
        total_loss: 0.5767933751145998
        vf_explained_var: 0.998996913433075
        vf_loss: 0.5826636304457983
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.148387096774194
    gpu_util_percent0: 0.33419354838709675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647912988421874
    mean_env_wait_ms: 1.2163265907590053
    mean_inference_ms: 4.307885339565213
    mean_raw_obs_processing_ms: 0.378334732176832
  time_since_restore: 5023.634807348251
  time_this_iter_s: 26.404139757156372
  time_total_s: 5023.634807348251
  timers:
    learn_throughput: 8368.949
    learn_time_ms: 19332.415
    sample_throughput: 23773.399
    sample_time_ms: 6805.59
    update_time_ms: 40.185
  timestamp: 1602714479
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: a052f_00000
  
2020-10-14 22:28:00,575	WARNING util.py:136 -- The `process_trial` operation took 0.5328316688537598 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    189 |          5023.63 | 30578688 |  285.319 |              316.172 |              145.717 |            789.323 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3171.674168297456
    time_step_min: 2977
  date: 2020-10-14_22-28-27
  done: false
  episode_len_mean: 789.3462330177027
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 285.4354745478663
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 38864
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.188822772487568e-22
        cur_lr: 5.0e-05
        entropy: 0.07350333717962106
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008622682929853909
        total_loss: .inf
        vf_explained_var: 0.9992330074310303
        vf_loss: 0.3560858691732089
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.00967741935484
    gpu_util_percent0: 0.3412903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647547593546933
    mean_env_wait_ms: 1.2162351950137875
    mean_inference_ms: 4.307698873208376
    mean_raw_obs_processing_ms: 0.3783229278314517
  time_since_restore: 5050.342933416367
  time_this_iter_s: 26.708126068115234
  time_total_s: 5050.342933416367
  timers:
    learn_throughput: 8356.853
    learn_time_ms: 19360.398
    sample_throughput: 23752.373
    sample_time_ms: 6811.614
    update_time_ms: 34.064
  timestamp: 1602714507
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:28:28,007	WARNING util.py:136 -- The `process_trial` operation took 0.5383214950561523 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    190 |          5050.34 | 30740480 |  285.435 |              316.172 |              145.717 |            789.346 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3170.8869297796
    time_step_min: 2977
  date: 2020-10-14_22-28-54
  done: false
  episode_len_mean: 789.3736426961689
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 285.55482684869577
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 184
  episodes_total: 39048
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.28323415873135e-22
        cur_lr: 5.0e-05
        entropy: 0.07272957141200702
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037143592295857766
        model: {}
        policy_loss: -0.007097428218306352
        total_loss: 0.37821681549151737
        vf_explained_var: 0.999241292476654
        vf_loss: 0.38535060981909436
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0
    gpu_util_percent0: 0.3551612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464725055889848
    mean_env_wait_ms: 1.2161404148044301
    mean_inference_ms: 4.307512191303056
    mean_raw_obs_processing_ms: 0.3783104112748328
  time_since_restore: 5076.732562303543
  time_this_iter_s: 26.389628887176514
  time_total_s: 5076.732562303543
  timers:
    learn_throughput: 8368.705
    learn_time_ms: 19332.98
    sample_throughput: 23700.638
    sample_time_ms: 6826.483
    update_time_ms: 34.167
  timestamp: 1602714534
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: a052f_00000
  
2020-10-14 22:28:55,116	WARNING util.py:136 -- The `process_trial` operation took 0.540529727935791 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    191 |          5076.73 | 30902272 |  285.555 |              316.172 |              145.717 |            789.374 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3169.8832730422378
    time_step_min: 2977
  date: 2020-10-14_22-29-21
  done: false
  episode_len_mean: 789.4060129321317
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 285.7058665160847
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 234
  episodes_total: 39282
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.641617079365675e-22
        cur_lr: 5.0e-05
        entropy: 0.07255261950194836
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005816230669249005
        total_loss: .inf
        vf_explained_var: 0.9995751976966858
        vf_loss: 0.2486284983654817
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.774193548387093
    gpu_util_percent0: 0.3396774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646800117353181
    mean_env_wait_ms: 1.216020637446992
    mean_inference_ms: 4.307288879268912
    mean_raw_obs_processing_ms: 0.37829386113539726
  time_since_restore: 5102.965971231461
  time_this_iter_s: 26.23340892791748
  time_total_s: 5102.965971231461
  timers:
    learn_throughput: 8364.869
    learn_time_ms: 19341.844
    sample_throughput: 23731.613
    sample_time_ms: 6817.573
    update_time_ms: 33.485
  timestamp: 1602714561
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:29:22,071	WARNING util.py:136 -- The `process_trial` operation took 0.5424308776855469 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    192 |          5102.97 | 31064064 |  285.706 |              316.172 |              145.717 |            789.406 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3168.986874113116
    time_step_min: 2977
  date: 2020-10-14_22-29-48
  done: false
  episode_len_mean: 789.4361389648536
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 285.83947496846287
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 210
  episodes_total: 39492
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.962425619048512e-22
        cur_lr: 5.0e-05
        entropy: 0.0704190315057834
        entropy_coeff: 0.0005000000000000001
        kl: 0.004805178051659216
        model: {}
        policy_loss: -0.005262976403173525
        total_loss: 0.365373636285464
        vf_explained_var: 0.9992805123329163
        vf_loss: 0.3706718335549037
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.483870967741932
    gpu_util_percent0: 0.3151612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464625716837957
    mean_env_wait_ms: 1.2159105625140563
    mean_inference_ms: 4.307068162608529
    mean_raw_obs_processing_ms: 0.3782796560717501
  time_since_restore: 5129.365455150604
  time_this_iter_s: 26.399483919143677
  time_total_s: 5129.365455150604
  timers:
    learn_throughput: 8370.403
    learn_time_ms: 19329.058
    sample_throughput: 23698.068
    sample_time_ms: 6827.223
    update_time_ms: 25.21
  timestamp: 1602714588
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: a052f_00000
  
2020-10-14 22:29:49,219	WARNING util.py:136 -- The `process_trial` operation took 0.5661823749542236 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    193 |          5129.37 | 31225856 |  285.839 |              316.172 |              145.717 |            789.436 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3168.3062753904774
    time_step_min: 2977
  date: 2020-10-14_22-30-15
  done: false
  episode_len_mean: 789.4559620767039
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 285.9438847487964
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 39659
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.481212809524256e-22
        cur_lr: 5.0e-05
        entropy: 0.06774980574846268
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005485486549635728
        total_loss: .inf
        vf_explained_var: 0.9992914199829102
        vf_loss: 0.3571974113583565
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.658064516129034
    gpu_util_percent0: 0.32064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464595071456494
    mean_env_wait_ms: 1.2158243900884722
    mean_inference_ms: 4.306906632895106
    mean_raw_obs_processing_ms: 0.3782685959371252
  time_since_restore: 5155.709764242172
  time_this_iter_s: 26.344309091567993
  time_total_s: 5155.709764242172
  timers:
    learn_throughput: 8384.504
    learn_time_ms: 19296.551
    sample_throughput: 23730.808
    sample_time_ms: 6817.804
    update_time_ms: 24.667
  timestamp: 1602714615
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:30:16,362	WARNING util.py:136 -- The `process_trial` operation took 0.5489702224731445 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    194 |          5155.71 | 31387648 |  285.944 |              316.172 |              145.717 |            789.456 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3167.435961147504
    time_step_min: 2977
  date: 2020-10-14_22-30-42
  done: false
  episode_len_mean: 789.4818790599684
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 286.07560037687176
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 212
  episodes_total: 39871
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.221819214286386e-22
        cur_lr: 5.0e-05
        entropy: 0.07301179816325505
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038767790344233313
        model: {}
        policy_loss: -0.005751242914508718
        total_loss: 0.3461113820473353
        vf_explained_var: 0.999345064163208
        vf_loss: 0.3518991321325302
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15806451612903
    gpu_util_percent0: 0.2977419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645664420668775
    mean_env_wait_ms: 1.215714567370082
    mean_inference_ms: 4.306700727238129
    mean_raw_obs_processing_ms: 0.378255105986374
  time_since_restore: 5181.958292245865
  time_this_iter_s: 26.248528003692627
  time_total_s: 5181.958292245865
  timers:
    learn_throughput: 8390.35
    learn_time_ms: 19283.106
    sample_throughput: 23743.455
    sample_time_ms: 6814.173
    update_time_ms: 24.591
  timestamp: 1602714642
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: a052f_00000
  
2020-10-14 22:30:43,476	WARNING util.py:136 -- The `process_trial` operation took 0.5199470520019531 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    195 |          5181.96 | 31549440 |  286.076 |              316.172 |              145.717 |            789.482 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3166.4792425527667
    time_step_min: 2977
  date: 2020-10-14_22-31-09
  done: false
  episode_len_mean: 789.5151333831963
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 286.22332524950303
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 239
  episodes_total: 40110
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.610909607143193e-22
        cur_lr: 5.0e-05
        entropy: 0.07016462832689285
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0066338973653425155
        total_loss: .inf
        vf_explained_var: 0.999370813369751
        vf_loss: 0.35221123198668164
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.683870967741935
    gpu_util_percent0: 0.3096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645066685009503
    mean_env_wait_ms: 1.2155878662174284
    mean_inference_ms: 4.306475317856655
    mean_raw_obs_processing_ms: 0.3782380985835641
  time_since_restore: 5208.394623756409
  time_this_iter_s: 26.436331510543823
  time_total_s: 5208.394623756409
  timers:
    learn_throughput: 8383.648
    learn_time_ms: 19298.519
    sample_throughput: 23782.651
    sample_time_ms: 6802.942
    update_time_ms: 22.949
  timestamp: 1602714669
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:31:10,907	WARNING util.py:136 -- The `process_trial` operation took 0.5854675769805908 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    196 |          5208.39 | 31711232 |  286.223 |              316.172 |              145.717 |            789.515 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3165.767828311682
    time_step_min: 2977
  date: 2020-10-14_22-31-37
  done: false
  episode_len_mean: 789.5446918360761
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 286.3322095780951
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 40287
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.916364410714789e-22
        cur_lr: 5.0e-05
        entropy: 0.061286669224500656
        entropy_coeff: 0.0005000000000000001
        kl: 0.004341180824364225
        model: {}
        policy_loss: -0.006022599042807997
        total_loss: 0.4316723694403966
        vf_explained_var: 0.9991136193275452
        vf_loss: 0.43772560109694797
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0741935483871
    gpu_util_percent0: 0.35580645161290325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644736597495786
    mean_env_wait_ms: 1.2154957616314304
    mean_inference_ms: 4.306306052609625
    mean_raw_obs_processing_ms: 0.37822738535659883
  time_since_restore: 5234.891001224518
  time_this_iter_s: 26.49637746810913
  time_total_s: 5234.891001224518
  timers:
    learn_throughput: 8384.049
    learn_time_ms: 19297.596
    sample_throughput: 23742.129
    sample_time_ms: 6814.553
    update_time_ms: 22.436
  timestamp: 1602714697
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: a052f_00000
  
2020-10-14 22:31:38,179	WARNING util.py:136 -- The `process_trial` operation took 0.5759913921356201 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    197 |          5234.89 | 31873024 |  286.332 |              316.172 |              145.717 |            789.545 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3165.0291266936997
    time_step_min: 2977
  date: 2020-10-14_22-32-04
  done: false
  episode_len_mean: 789.5708885155169
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 286.4435961213237
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 185
  episodes_total: 40472
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9581822053573944e-22
        cur_lr: 5.0e-05
        entropy: 0.06908261030912399
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00889921048656106
        total_loss: .inf
        vf_explained_var: 0.9992902278900146
        vf_loss: 0.34738121430079144
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.103225806451615
    gpu_util_percent0: 0.3067741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644471271276685
    mean_env_wait_ms: 1.2153990673039974
    mean_inference_ms: 4.306136042351609
    mean_raw_obs_processing_ms: 0.3782161830920192
  time_since_restore: 5261.235164880753
  time_this_iter_s: 26.34416365623474
  time_total_s: 5261.235164880753
  timers:
    learn_throughput: 8375.015
    learn_time_ms: 19318.413
    sample_throughput: 23736.844
    sample_time_ms: 6816.071
    update_time_ms: 22.72
  timestamp: 1602714724
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:32:05,243	WARNING util.py:136 -- The `process_trial` operation took 0.5388517379760742 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    198 |          5261.24 | 32034816 |  286.444 |              316.172 |              145.717 |            789.571 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3164.100098328417
    time_step_min: 2977
  date: 2020-10-14_22-32-31
  done: false
  episode_len_mean: 789.6072270806721
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 286.58704391860016
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 236
  episodes_total: 40708
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.937273308036092e-22
        cur_lr: 5.0e-05
        entropy: 0.06931895638505618
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008696384820116995
        total_loss: .inf
        vf_explained_var: 0.9994643330574036
        vf_loss: 0.323911818365256
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.219354838709677
    gpu_util_percent0: 0.29354838709677417
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644033625079386
    mean_env_wait_ms: 1.2152754090603837
    mean_inference_ms: 4.305926599373994
    mean_raw_obs_processing_ms: 0.378200300457511
  time_since_restore: 5287.749067783356
  time_this_iter_s: 26.51390290260315
  time_total_s: 5287.749067783356
  timers:
    learn_throughput: 8364.081
    learn_time_ms: 19343.667
    sample_throughput: 23758.374
    sample_time_ms: 6809.894
    update_time_ms: 23.928
  timestamp: 1602714751
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:32:32,482	WARNING util.py:136 -- The `process_trial` operation took 0.5360980033874512 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    199 |          5287.75 | 32196608 |  286.587 |              316.172 |              145.717 |            789.607 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3163.303795920364
    time_step_min: 2977
  date: 2020-10-14_22-32-59
  done: false
  episode_len_mean: 789.6361881018722
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 286.70641967408335
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 40914
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.405909962054139e-22
        cur_lr: 5.0e-05
        entropy: 0.06667429891725381
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009285335734602995
        total_loss: .inf
        vf_explained_var: 0.9991855621337891
        vf_loss: 0.41676413267850876
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.196774193548393
    gpu_util_percent0: 0.25870967741935486
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464354297636211
    mean_env_wait_ms: 1.2151648517351867
    mean_inference_ms: 4.305727526283016
    mean_raw_obs_processing_ms: 0.378187318843512
  time_since_restore: 5314.510205030441
  time_this_iter_s: 26.76113724708557
  time_total_s: 5314.510205030441
  timers:
    learn_throughput: 8362.333
    learn_time_ms: 19347.71
    sample_throughput: 23755.962
    sample_time_ms: 6810.585
    update_time_ms: 24.888
  timestamp: 1602714779
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:33:00,005	WARNING util.py:136 -- The `process_trial` operation took 0.5704915523529053 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    200 |          5314.51 | 32358400 |  286.706 |              316.172 |              145.717 |            789.636 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3162.661600292291
    time_step_min: 2977
  date: 2020-10-14_22-33-26
  done: false
  episode_len_mean: 789.6618796095709
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 286.80605386926715
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 41083
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.608864943081206e-22
        cur_lr: 5.0e-05
        entropy: 0.06975780551632245
        entropy_coeff: 0.0005000000000000001
        kl: 0.005111270118504763
        model: {}
        policy_loss: -0.008521288487827405
        total_loss: 0.2807561978697777
        vf_explained_var: 0.9993758201599121
        vf_loss: 0.28931236763795215
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.438709677419357
    gpu_util_percent0: 0.3503225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464326134235512
    mean_env_wait_ms: 1.2150759836340945
    mean_inference_ms: 4.305575692167541
    mean_raw_obs_processing_ms: 0.3781771080711887
  time_since_restore: 5340.700523853302
  time_this_iter_s: 26.190318822860718
  time_total_s: 5340.700523853302
  timers:
    learn_throughput: 8370.145
    learn_time_ms: 19329.653
    sample_throughput: 23793.022
    sample_time_ms: 6799.977
    update_time_ms: 24.575
  timestamp: 1602714806
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: a052f_00000
  
2020-10-14 22:33:26,951	WARNING util.py:136 -- The `process_trial` operation took 0.5637385845184326 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    201 |           5340.7 | 32520192 |  286.806 |              316.172 |              145.717 |            789.662 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3161.881406450675
    time_step_min: 2977
  date: 2020-10-14_22-33-53
  done: false
  episode_len_mean: 789.6990434677322
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 286.92706212139535
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 212
  episodes_total: 41295
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.608864943081206e-22
        cur_lr: 5.0e-05
        entropy: 0.07126038211087386
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0076967305212747306
        total_loss: .inf
        vf_explained_var: 0.9993746876716614
        vf_loss: 0.35053883244593936
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.296774193548384
    gpu_util_percent0: 0.3351612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643003370145452
    mean_env_wait_ms: 1.2149642087445696
    mean_inference_ms: 4.305391466504394
    mean_raw_obs_processing_ms: 0.37816439005044605
  time_since_restore: 5366.958637475967
  time_this_iter_s: 26.258113622665405
  time_total_s: 5366.958637475967
  timers:
    learn_throughput: 8376.568
    learn_time_ms: 19314.831
    sample_throughput: 23772.373
    sample_time_ms: 6805.884
    update_time_ms: 24.25
  timestamp: 1602714833
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:33:53,934	WARNING util.py:136 -- The `process_trial` operation took 0.5289065837860107 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    202 |          5366.96 | 32681984 |  286.927 |              316.172 |              145.717 |            789.699 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3160.9770630044572
    time_step_min: 2977
  date: 2020-10-14_22-34-20
  done: false
  episode_len_mean: 789.7422290708594
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.06363663116105
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 238
  episodes_total: 41533
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.91329741462181e-22
        cur_lr: 5.0e-05
        entropy: 0.06647817355891068
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007133327262029828
        total_loss: .inf
        vf_explained_var: 0.9993767142295837
        vf_loss: 0.3547941943009694
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.99677419354839
    gpu_util_percent0: 0.33774193548387094
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642450069498672
    mean_env_wait_ms: 1.214836266712865
    mean_inference_ms: 4.3051852178176615
    mean_raw_obs_processing_ms: 0.37814873806404414
  time_since_restore: 5393.417923688889
  time_this_iter_s: 26.459286212921143
  time_total_s: 5393.417923688889
  timers:
    learn_throughput: 8370.304
    learn_time_ms: 19329.286
    sample_throughput: 23811.255
    sample_time_ms: 6794.77
    update_time_ms: 25.789
  timestamp: 1602714860
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:34:21,161	WARNING util.py:136 -- The `process_trial` operation took 0.5825798511505127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    203 |          5393.42 | 32843776 |  287.064 |              316.172 |              145.717 |            789.742 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3160.329534548944
    time_step_min: 2977
  date: 2020-10-14_22-34-47
  done: false
  episode_len_mean: 789.7753908123142
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.1630247037361
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 41708
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4869946121932717e-21
        cur_lr: 5.0e-05
        entropy: 0.06229041268428167
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006761294246340792
        total_loss: .inf
        vf_explained_var: 0.9994831681251526
        vf_loss: 0.24170799180865288
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116129032258062
    gpu_util_percent0: 0.3354838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642132711598776
    mean_env_wait_ms: 1.2147425617338634
    mean_inference_ms: 4.3050252209135795
    mean_raw_obs_processing_ms: 0.37813817679425044
  time_since_restore: 5419.9068241119385
  time_this_iter_s: 26.488900423049927
  time_total_s: 5419.9068241119385
  timers:
    learn_throughput: 8360.578
    learn_time_ms: 19351.773
    sample_throughput: 23835.002
    sample_time_ms: 6788.0
    update_time_ms: 27.691
  timestamp: 1602714887
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:34:48,440	WARNING util.py:136 -- The `process_trial` operation took 0.5876619815826416 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    204 |          5419.91 | 33005568 |  287.163 |              316.172 |              145.717 |            789.775 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3159.6147415687396
    time_step_min: 2977
  date: 2020-10-14_22-35-14
  done: false
  episode_len_mean: 789.8091941951499
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.2705781319011
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 41896
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.230491918289907e-21
        cur_lr: 5.0e-05
        entropy: 0.06706358740727107
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007965411631933724
        total_loss: .inf
        vf_explained_var: 0.9994296431541443
        vf_loss: 0.29940882076819736
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.46451612903226
    gpu_util_percent0: 0.33967741935483875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464188518977306
    mean_env_wait_ms: 1.2146429579033295
    mean_inference_ms: 4.304867819420026
    mean_raw_obs_processing_ms: 0.37812752686726525
  time_since_restore: 5446.3253264427185
  time_this_iter_s: 26.41850233078003
  time_total_s: 5446.3253264427185
  timers:
    learn_throughput: 8353.516
    learn_time_ms: 19368.131
    sample_throughput: 23839.009
    sample_time_ms: 6786.859
    update_time_ms: 28.359
  timestamp: 1602714914
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:35:15,594	WARNING util.py:136 -- The `process_trial` operation took 0.5440418720245361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    205 |          5446.33 | 33167360 |  287.271 |              316.172 |              145.717 |            789.809 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3158.7715839726384
    time_step_min: 2977
  date: 2020-10-14_22-35-42
  done: false
  episode_len_mean: 789.846265220384
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.39764093187915
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 235
  episodes_total: 42131
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3457378774348604e-21
        cur_lr: 5.0e-05
        entropy: 0.07176326836148898
        entropy_coeff: 0.0005000000000000001
        kl: 0.004491733154281974
        model: {}
        policy_loss: -0.00694414495728779
        total_loss: 0.574038398762544
        vf_explained_var: 0.9990028738975525
        vf_loss: 0.5810184304912885
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612904
    gpu_util_percent0: 0.3783870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464147542035023
    mean_env_wait_ms: 1.2145180402916547
    mean_inference_ms: 4.3046727378723295
    mean_raw_obs_processing_ms: 0.37811259099557665
  time_since_restore: 5472.736062526703
  time_this_iter_s: 26.410736083984375
  time_total_s: 5472.736062526703
  timers:
    learn_throughput: 8355.673
    learn_time_ms: 19363.132
    sample_throughput: 23834.159
    sample_time_ms: 6788.24
    update_time_ms: 28.07
  timestamp: 1602714942
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: a052f_00000
  
2020-10-14 22:35:42,738	WARNING util.py:136 -- The `process_trial` operation took 0.5388951301574707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    206 |          5472.74 | 33329152 |  287.398 |              316.172 |              145.717 |            789.846 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3158.041152555193
    time_step_min: 2977
  date: 2020-10-14_22-36-08
  done: false
  episode_len_mean: 789.8804507015637
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.51033245479766
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 203
  episodes_total: 42334
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6728689387174302e-21
        cur_lr: 5.0e-05
        entropy: 0.06414977957804997
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0080535972956568
        total_loss: .inf
        vf_explained_var: 0.9995991587638855
        vf_loss: 0.20632774755358696
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10645161290323
    gpu_util_percent0: 0.3141935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641022582889582
    mean_env_wait_ms: 1.214406837829834
    mean_inference_ms: 4.304496164786301
    mean_raw_obs_processing_ms: 0.37810062402103073
  time_since_restore: 5498.977472305298
  time_this_iter_s: 26.24140977859497
  time_total_s: 5498.977472305298
  timers:
    learn_throughput: 8359.807
    learn_time_ms: 19353.558
    sample_throughput: 23896.989
    sample_time_ms: 6770.393
    update_time_ms: 28.893
  timestamp: 1602714968
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:36:09,743	WARNING util.py:136 -- The `process_trial` operation took 0.5678205490112305 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    207 |          5498.98 | 33490944 |   287.51 |              316.172 |              145.717 |             789.88 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3157.426994067238
    time_step_min: 2977
  date: 2020-10-14_22-36-36
  done: false
  episode_len_mean: 789.9148785996612
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.60358240793
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 42504
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5093034080761453e-21
        cur_lr: 5.0e-05
        entropy: 0.0645601786673069
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008155066372031191
        total_loss: .inf
        vf_explained_var: 0.9994916319847107
        vf_loss: 0.23546982929110527
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.580645161290327
    gpu_util_percent0: 0.33709677419354844
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640742851379346
    mean_env_wait_ms: 1.2143159018294853
    mean_inference_ms: 4.3043513390377965
    mean_raw_obs_processing_ms: 0.3780907836049246
  time_since_restore: 5525.343689441681
  time_this_iter_s: 26.366217136383057
  time_total_s: 5525.343689441681
  timers:
    learn_throughput: 8357.747
    learn_time_ms: 19358.327
    sample_throughput: 23922.433
    sample_time_ms: 6763.192
    update_time_ms: 30.665
  timestamp: 1602714996
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:36:36,867	WARNING util.py:136 -- The `process_trial` operation took 0.5537467002868652 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    208 |          5525.34 | 33652736 |  287.604 |              316.172 |              145.717 |            789.915 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3156.6728980720127
    time_step_min: 2977
  date: 2020-10-14_22-37-03
  done: false
  episode_len_mean: 789.9565960435444
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.7207188353153
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 211
  episodes_total: 42715
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.763955112114218e-21
        cur_lr: 5.0e-05
        entropy: 0.0640172337492307
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006049461095244624
        total_loss: .inf
        vf_explained_var: 0.9995409846305847
        vf_loss: 0.25144512578845024
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.283870967741937
    gpu_util_percent0: 0.2603225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464047233526393
    mean_env_wait_ms: 1.2142028774394609
    mean_inference_ms: 4.304183003207495
    mean_raw_obs_processing_ms: 0.37807872366890616
  time_since_restore: 5551.82018995285
  time_this_iter_s: 26.476500511169434
  time_total_s: 5551.82018995285
  timers:
    learn_throughput: 8361.958
    learn_time_ms: 19348.579
    sample_throughput: 23901.519
    sample_time_ms: 6769.109
    update_time_ms: 29.095
  timestamp: 1602715023
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:37:04,111	WARNING util.py:136 -- The `process_trial` operation took 0.5691590309143066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    209 |          5551.82 | 33814528 |  287.721 |              316.172 |              145.717 |            789.957 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3155.832964472918
    time_step_min: 2977
  date: 2020-10-14_22-37-30
  done: false
  episode_len_mean: 790.0016529695248
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.8494812394188
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 238
  episodes_total: 42953
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.645932668171327e-21
        cur_lr: 5.0e-05
        entropy: 0.06425965204834938
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009998717398654359
        total_loss: .inf
        vf_explained_var: 0.999563992023468
        vf_loss: 0.24980363746484122
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64516129032258
    gpu_util_percent0: 0.334516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639992954566602
    mean_env_wait_ms: 1.2140734627406906
    mean_inference_ms: 4.303989859000639
    mean_raw_obs_processing_ms: 0.3780642258389521
  time_since_restore: 5578.261322021484
  time_this_iter_s: 26.441132068634033
  time_total_s: 5578.261322021484
  timers:
    learn_throughput: 8372.387
    learn_time_ms: 19324.477
    sample_throughput: 23929.681
    sample_time_ms: 6761.143
    update_time_ms: 27.774
  timestamp: 1602715050
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:37:31,333	WARNING util.py:136 -- The `process_trial` operation took 0.5850481986999512 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    210 |          5578.26 | 33976320 |  287.849 |              316.172 |              145.717 |            790.002 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3155.2303087952114
    time_step_min: 2977
  date: 2020-10-14_22-37-58
  done: false
  episode_len_mean: 790.0342213257286
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 287.940479427368
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 178
  episodes_total: 43131
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.46889900225699e-21
        cur_lr: 5.0e-05
        entropy: 0.0637212252865235
        entropy_coeff: 0.0005000000000000001
        kl: 0.003750344602546344
        model: {}
        policy_loss: -0.007788174678959574
        total_loss: 0.32559432337681454
        vf_explained_var: 0.9993079304695129
        vf_loss: 0.33341435094674426
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3375
    gpu_util_percent0: 0.3121875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463970175636933
    mean_env_wait_ms: 1.213977174013373
    mean_inference_ms: 4.303842545390295
    mean_raw_obs_processing_ms: 0.37805399038074183
  time_since_restore: 5605.06227517128
  time_this_iter_s: 26.800953149795532
  time_total_s: 5605.06227517128
  timers:
    learn_throughput: 8353.696
    learn_time_ms: 19367.716
    sample_throughput: 23871.295
    sample_time_ms: 6777.68
    update_time_ms: 28.08
  timestamp: 1602715078
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: a052f_00000
  
2020-10-14 22:37:58,931	WARNING util.py:136 -- The `process_trial` operation took 0.5520987510681152 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    211 |          5605.06 | 34138112 |   287.94 |              316.172 |              145.717 |            790.034 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3154.6031973386316
    time_step_min: 2977
  date: 2020-10-14_22-38-25
  done: false
  episode_len_mean: 790.0695848917209
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.0380463451524
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 183
  episodes_total: 43314
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.234449501128495e-21
        cur_lr: 5.0e-05
        entropy: 0.06228856214632591
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00655596710445631
        total_loss: .inf
        vf_explained_var: 0.9993744492530823
        vf_loss: 0.31849023948113125
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.012903225806454
    gpu_util_percent0: 0.3206451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639460694679837
    mean_env_wait_ms: 1.2138781928772249
    mean_inference_ms: 4.3037019775953365
    mean_raw_obs_processing_ms: 0.37804445522096386
  time_since_restore: 5631.6153020858765
  time_this_iter_s: 26.553026914596558
  time_total_s: 5631.6153020858765
  timers:
    learn_throughput: 8343.09
    learn_time_ms: 19392.334
    sample_throughput: 23829.49
    sample_time_ms: 6789.57
    update_time_ms: 28.57
  timestamp: 1602715105
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:38:26,290	WARNING util.py:136 -- The `process_trial` operation took 0.6004049777984619 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    212 |          5631.62 | 34299904 |  288.038 |              316.172 |              145.717 |             790.07 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3153.7992830717617
    time_step_min: 2977
  date: 2020-10-14_22-38-52
  done: false
  episode_len_mean: 790.10815900062
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.15746437205985
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 233
  episodes_total: 43547
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.351674251692741e-21
        cur_lr: 5.0e-05
        entropy: 0.06492053220669429
        entropy_coeff: 0.0005000000000000001
        kl: 0.003438775660470128
        model: {}
        policy_loss: -0.00757945822988404
        total_loss: 0.3875013788541158
        vf_explained_var: 0.9993364810943604
        vf_loss: 0.395113284389178
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20322580645162
    gpu_util_percent0: 0.3790322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639063812630998
    mean_env_wait_ms: 1.21375242498985
    mean_inference_ms: 4.303515827524844
    mean_raw_obs_processing_ms: 0.3780303822764203
  time_since_restore: 5658.1771404743195
  time_this_iter_s: 26.561838388442993
  time_total_s: 5658.1771404743195
  timers:
    learn_throughput: 8344.688
    learn_time_ms: 19388.621
    sample_throughput: 23780.942
    sample_time_ms: 6803.431
    update_time_ms: 27.095
  timestamp: 1602715132
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: a052f_00000
  
2020-10-14 22:38:53,664	WARNING util.py:136 -- The `process_trial` operation took 0.6080577373504639 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    213 |          5658.18 | 34461696 |  288.157 |              316.172 |              145.717 |            790.108 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3153.0776847786315
    time_step_min: 2977
  date: 2020-10-14_22-39-20
  done: false
  episode_len_mean: 790.1477283115458
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.26729817601904
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 209
  episodes_total: 43756
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1758371258463703e-21
        cur_lr: 5.0e-05
        entropy: 0.06137976050376892
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006120224720992458
        total_loss: .inf
        vf_explained_var: 0.9996586441993713
        vf_loss: 0.17854325597484907
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.046875
    gpu_util_percent0: 0.346875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463867440782738
    mean_env_wait_ms: 1.2136385637112332
    mean_inference_ms: 4.303355938883429
    mean_raw_obs_processing_ms: 0.3780191875391203
  time_since_restore: 5685.054126501083
  time_this_iter_s: 26.876986026763916
  time_total_s: 5685.054126501083
  timers:
    learn_throughput: 8337.395
    learn_time_ms: 19405.581
    sample_throughput: 23726.023
    sample_time_ms: 6819.179
    update_time_ms: 25.48
  timestamp: 1602715160
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:39:21,400	WARNING util.py:136 -- The `process_trial` operation took 0.6104409694671631 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    214 |          5685.05 | 34623488 |  288.267 |              316.172 |              145.717 |            790.148 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3152.5041802401074
    time_step_min: 2977
  date: 2020-10-14_22-39-47
  done: false
  episode_len_mean: 790.1765964712579
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.35049987639604
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 43925
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.763755688769558e-21
        cur_lr: 5.0e-05
        entropy: 0.07656259958942731
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008517861269259205
        total_loss: .inf
        vf_explained_var: 0.9987399578094482
        vf_loss: 0.5770902087291082
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.396774193548385
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638428236126963
    mean_env_wait_ms: 1.213547863928344
    mean_inference_ms: 4.303225223917436
    mean_raw_obs_processing_ms: 0.37801006451229535
  time_since_restore: 5711.520039319992
  time_this_iter_s: 26.46591281890869
  time_total_s: 5711.520039319992
  timers:
    learn_throughput: 8335.353
    learn_time_ms: 19410.336
    sample_throughput: 23726.834
    sample_time_ms: 6818.946
    update_time_ms: 24.385
  timestamp: 1602715187
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:39:48,638	WARNING util.py:136 -- The `process_trial` operation took 0.5725610256195068 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    215 |          5711.52 | 34785280 |   288.35 |              316.172 |              145.717 |            790.177 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3152.036775875751
    time_step_min: 2977
  date: 2020-10-14_22-40-15
  done: false
  episode_len_mean: 790.182765730859
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.4164780151454
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 208
  episodes_total: 44133
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.145633533154335e-21
        cur_lr: 5.0e-05
        entropy: 0.083622004215916
        entropy_coeff: 0.0005000000000000001
        kl: 0.004571271051342289
        model: {}
        policy_loss: -0.0072959238483842155
        total_loss: 2.485251168409983
        vf_explained_var: 0.9955576062202454
        vf_loss: 2.492588917414347
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.712903225806453
    gpu_util_percent0: 0.3645161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638182751429538
    mean_env_wait_ms: 1.2134356545336804
    mean_inference_ms: 4.303069930736658
    mean_raw_obs_processing_ms: 0.3779990943906808
  time_since_restore: 5738.017151594162
  time_this_iter_s: 26.497112274169922
  time_total_s: 5738.017151594162
  timers:
    learn_throughput: 8335.043
    learn_time_ms: 19411.057
    sample_throughput: 23712.909
    sample_time_ms: 6822.95
    update_time_ms: 26.502
  timestamp: 1602715215
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: a052f_00000
  
2020-10-14 22:40:15,946	WARNING util.py:136 -- The `process_trial` operation took 0.5807301998138428 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    216 |          5738.02 | 34947072 |  288.416 |              316.172 |              145.717 |            790.183 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3151.4500090195725
    time_step_min: 2977
  date: 2020-10-14_22-40-42
  done: false
  episode_len_mean: 790.2098656931674
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.5175388279767
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 243
  episodes_total: 44376
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5728167665771675e-21
        cur_lr: 5.0e-05
        entropy: 0.06858866351346175
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031505223208417497
        model: {}
        policy_loss: -0.01108791809141015
        total_loss: 0.9894310832023621
        vf_explained_var: 0.998295247554779
        vf_loss: 1.0005533148845036
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.340624999999996
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463775310948499
    mean_env_wait_ms: 1.213304851758994
    mean_inference_ms: 4.3028893166068825
    mean_raw_obs_processing_ms: 0.37798532608514523
  time_since_restore: 5764.80818772316
  time_this_iter_s: 26.791036128997803
  time_total_s: 5764.80818772316
  timers:
    learn_throughput: 8317.901
    learn_time_ms: 19451.061
    sample_throughput: 23701.554
    sample_time_ms: 6826.219
    update_time_ms: 27.086
  timestamp: 1602715242
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: a052f_00000
  
2020-10-14 22:40:43,588	WARNING util.py:136 -- The `process_trial` operation took 0.5865757465362549 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    217 |          5764.81 | 35108864 |  288.518 |              316.172 |              145.717 |             790.21 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3150.8897697922516
    time_step_min: 2977
  date: 2020-10-14_22-41-10
  done: false
  episode_len_mean: 790.2383228963257
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.604325525812
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 44553
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7864083832885838e-21
        cur_lr: 5.0e-05
        entropy: 0.05693757999688387
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006064588708492617
        total_loss: .inf
        vf_explained_var: 0.9992852210998535
        vf_loss: 0.338612196346124
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02258064516129
    gpu_util_percent0: 0.29129032258064513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637456846282937
    mean_env_wait_ms: 1.2132091527315811
    mean_inference_ms: 4.302749525748023
    mean_raw_obs_processing_ms: 0.3779758138549522
  time_since_restore: 5791.449971437454
  time_this_iter_s: 26.641783714294434
  time_total_s: 5791.449971437454
  timers:
    learn_throughput: 8308.918
    learn_time_ms: 19472.091
    sample_throughput: 23677.056
    sample_time_ms: 6833.282
    update_time_ms: 27.014
  timestamp: 1602715270
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:41:11,057	WARNING util.py:136 -- The `process_trial` operation took 0.616307258605957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    218 |          5791.45 | 35270656 |  288.604 |              316.172 |              145.717 |            790.238 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3150.284531931551
    time_step_min: 2977
  date: 2020-10-14_22-41-37
  done: false
  episode_len_mean: 790.2721927883218
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.69410759733313
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 180
  episodes_total: 44733
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6796125749328756e-21
        cur_lr: 5.0e-05
        entropy: 0.05963824316859245
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00955948673072271
        total_loss: .inf
        vf_explained_var: 0.9994375109672546
        vf_loss: 0.27781935408711433
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.49354838709678
    gpu_util_percent0: 0.2990322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463721444829089
    mean_env_wait_ms: 1.2131123045119947
    mean_inference_ms: 4.302620990109142
    mean_raw_obs_processing_ms: 0.37796670539675004
  time_since_restore: 5818.118643283844
  time_this_iter_s: 26.66867184638977
  time_total_s: 5818.118643283844
  timers:
    learn_throughput: 8302.075
    learn_time_ms: 19488.14
    sample_throughput: 23669.116
    sample_time_ms: 6835.574
    update_time_ms: 28.796
  timestamp: 1602715297
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:41:38,617	WARNING util.py:136 -- The `process_trial` operation took 0.5900421142578125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    219 |          5818.12 | 35432448 |  288.694 |              316.172 |              145.717 |            790.272 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3149.5480741416523
    time_step_min: 2977
  date: 2020-10-14_22-42-04
  done: false
  episode_len_mean: 790.310213702773
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.80823377541094
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 236
  episodes_total: 44969
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.019418862399314e-21
        cur_lr: 5.0e-05
        entropy: 0.06499733806898196
        entropy_coeff: 0.0005000000000000001
        kl: 0.003910120499009888
        model: {}
        policy_loss: -0.006768485169838338
        total_loss: 0.34090528388818103
        vf_explained_var: 0.9994122982025146
        vf_loss: 0.34770626326402027
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.00967741935484
    gpu_util_percent0: 0.3706451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636868581819085
    mean_env_wait_ms: 1.2129852771463352
    mean_inference_ms: 4.302446175097644
    mean_raw_obs_processing_ms: 0.3779537828258757
  time_since_restore: 5844.2744863033295
  time_this_iter_s: 26.155843019485474
  time_total_s: 5844.2744863033295
  timers:
    learn_throughput: 8316.992
    learn_time_ms: 19453.186
    sample_throughput: 23678.414
    sample_time_ms: 6832.89
    update_time_ms: 27.67
  timestamp: 1602715324
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: a052f_00000
  
2020-10-14 22:42:05,563	WARNING util.py:136 -- The `process_trial` operation took 0.5823538303375244 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    220 |          5844.27 | 35594240 |  288.808 |              316.172 |              145.717 |             790.31 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3148.891497419769
    time_step_min: 2977
  date: 2020-10-14_22-42-32
  done: false
  episode_len_mean: 790.3480156709976
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.9073908254056
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 210
  episodes_total: 45179
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.009709431199657e-21
        cur_lr: 5.0e-05
        entropy: 0.05826550194372734
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030161606070275107
        model: {}
        policy_loss: -0.009464059849657739
        total_loss: 0.3120213771859805
        vf_explained_var: 0.9994010925292969
        vf_loss: 0.3215145692229271
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.606451612903225
    gpu_util_percent0: 0.3548387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636523665448264
    mean_env_wait_ms: 1.212871511816796
    mean_inference_ms: 4.302298952001322
    mean_raw_obs_processing_ms: 0.3779430383099862
  time_since_restore: 5870.9852175712585
  time_this_iter_s: 26.710731267929077
  time_total_s: 5870.9852175712585
  timers:
    learn_throughput: 8321.531
    learn_time_ms: 19442.575
    sample_throughput: 23653.829
    sample_time_ms: 6839.992
    update_time_ms: 28.728
  timestamp: 1602715352
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: a052f_00000
  
2020-10-14 22:42:33,108	WARNING util.py:136 -- The `process_trial` operation took 0.6166892051696777 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    221 |          5870.99 | 35756032 |  288.907 |              316.172 |              145.717 |            790.348 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3148.3408945475408
    time_step_min: 2977
  date: 2020-10-14_22-42-59
  done: false
  episode_len_mean: 790.3752839217589
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 288.9906935364626
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 168
  episodes_total: 45347
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0048547155998286e-21
        cur_lr: 5.0e-05
        entropy: 0.0612164114912351
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007120055621877934
        total_loss: .inf
        vf_explained_var: 0.9996138215065002
        vf_loss: 0.1808678780992826
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.548387096774192
    gpu_util_percent0: 0.3283870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146362787162496
    mean_env_wait_ms: 1.2127809191660053
    mean_inference_ms: 4.302178247466654
    mean_raw_obs_processing_ms: 0.3779346665618857
  time_since_restore: 5897.438598155975
  time_this_iter_s: 26.453380584716797
  time_total_s: 5897.438598155975
  timers:
    learn_throughput: 8324.893
    learn_time_ms: 19434.724
    sample_throughput: 23661.496
    sample_time_ms: 6837.776
    update_time_ms: 27.868
  timestamp: 1602715379
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:43:00,395	WARNING util.py:136 -- The `process_trial` operation took 0.6280241012573242 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    222 |          5897.44 | 35917824 |  288.991 |              316.172 |              145.717 |            790.375 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3147.7283909939592
    time_step_min: 2977
  date: 2020-10-14_22-43-26
  done: false
  episode_len_mean: 790.4035958114723
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.08332396473656
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 45553
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5072820733997426e-21
        cur_lr: 5.0e-05
        entropy: 0.07635863746205966
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050400605735679465
        model: {}
        policy_loss: -0.008741711838714158
        total_loss: 0.6843346605698267
        vf_explained_var: 0.9987028241157532
        vf_loss: 0.6931145290533701
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.506451612903227
    gpu_util_percent0: 0.3067741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636051004926862
    mean_env_wait_ms: 1.2126694673617697
    mean_inference_ms: 4.302030892950872
    mean_raw_obs_processing_ms: 0.37792409340327593
  time_since_restore: 5923.6096568107605
  time_this_iter_s: 26.171058654785156
  time_total_s: 5923.6096568107605
  timers:
    learn_throughput: 8334.599
    learn_time_ms: 19412.092
    sample_throughput: 23713.077
    sample_time_ms: 6822.902
    update_time_ms: 28.191
  timestamp: 1602715406
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: a052f_00000
  
2020-10-14 22:43:27,393	WARNING util.py:136 -- The `process_trial` operation took 0.6197638511657715 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    223 |          5923.61 | 36079616 |  289.083 |              316.172 |              145.717 |            790.404 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3147.087744980227
    time_step_min: 2977
  date: 2020-10-14_22-43-53
  done: false
  episode_len_mean: 790.4290018996877
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.17596649068116
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 244
  episodes_total: 45797
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5072820733997426e-21
        cur_lr: 5.0e-05
        entropy: 0.07233192337056
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033690641478945813
        model: {}
        policy_loss: -0.007779909221426351
        total_loss: 0.7809588114420573
        vf_explained_var: 0.9986439347267151
        vf_loss: 0.788774902621905
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.890322580645165
    gpu_util_percent0: 0.37129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463565515962007
    mean_env_wait_ms: 1.212538670345619
    mean_inference_ms: 4.3018688371082625
    mean_raw_obs_processing_ms: 0.3779116363430221
  time_since_restore: 5949.745703935623
  time_this_iter_s: 26.13604712486267
  time_total_s: 5949.745703935623
  timers:
    learn_throughput: 8353.417
    learn_time_ms: 19368.361
    sample_throughput: 23787.37
    sample_time_ms: 6801.592
    update_time_ms: 28.84
  timestamp: 1602715433
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: a052f_00000
  
2020-10-14 22:43:54,349	WARNING util.py:136 -- The `process_trial` operation took 0.5938360691070557 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    224 |          5949.75 | 36241408 |  289.176 |              316.172 |              145.717 |            790.429 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3146.557151562636
    time_step_min: 2977
  date: 2020-10-14_22-44-20
  done: false
  episode_len_mean: 790.4590655994432
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.25631071459304
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 45976
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.536410366998713e-22
        cur_lr: 5.0e-05
        entropy: 0.062443134374916553
        entropy_coeff: 0.0005000000000000001
        kl: 0.003458559357871612
        model: {}
        policy_loss: -0.006946903750455628
        total_loss: 0.3901308899124463
        vf_explained_var: 0.9991994500160217
        vf_loss: 0.397109014292558
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.174193548387095
    gpu_util_percent0: 0.32967741935483874
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635381838585593
    mean_env_wait_ms: 1.212442130026214
    mean_inference_ms: 4.301736935810358
    mean_raw_obs_processing_ms: 0.3779025934973045
  time_since_restore: 5976.099730968475
  time_this_iter_s: 26.354027032852173
  time_total_s: 5976.099730968475
  timers:
    learn_throughput: 8354.864
    learn_time_ms: 19365.007
    sample_throughput: 23817.324
    sample_time_ms: 6793.038
    update_time_ms: 28.811
  timestamp: 1602715460
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: a052f_00000
  
2020-10-14 22:44:21,558	WARNING util.py:136 -- The `process_trial` operation took 0.6467525959014893 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    225 |           5976.1 | 36403200 |  289.256 |              316.172 |              145.717 |            790.459 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3146.055698892093
    time_step_min: 2977
  date: 2020-10-14_22-44-48
  done: false
  episode_len_mean: 790.4871183722996
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.3322252010252
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 46151
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7682051834993564e-22
        cur_lr: 5.0e-05
        entropy: 0.06475949939340353
        entropy_coeff: 0.0005000000000000001
        kl: 0.00414882181212306
        model: {}
        policy_loss: -0.009134516275177399
        total_loss: 0.5043710693717003
        vf_explained_var: 0.9989582896232605
        vf_loss: 0.5135379731655121
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.019354838709678
    gpu_util_percent0: 0.33419354838709686
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635175159262243
    mean_env_wait_ms: 1.2123482495810154
    mean_inference_ms: 4.301621432652172
    mean_raw_obs_processing_ms: 0.3778941796754742
  time_since_restore: 6002.56235575676
  time_this_iter_s: 26.4626247882843
  time_total_s: 6002.56235575676
  timers:
    learn_throughput: 8360.284
    learn_time_ms: 19352.452
    sample_throughput: 23777.863
    sample_time_ms: 6804.312
    update_time_ms: 27.079
  timestamp: 1602715488
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: a052f_00000
  
2020-10-14 22:44:48,860	WARNING util.py:136 -- The `process_trial` operation took 0.6218292713165283 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    226 |          6002.56 | 36564992 |  289.332 |              316.172 |              145.717 |            790.487 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3145.3893310899716
    time_step_min: 2977
  date: 2020-10-14_22-45-15
  done: false
  episode_len_mean: 790.5345894323841
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.43724807085204
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 236
  episodes_total: 46387
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8841025917496782e-22
        cur_lr: 5.0e-05
        entropy: 0.06267723441123962
        entropy_coeff: 0.0005000000000000001
        kl: 0.004109859117306769
        model: {}
        policy_loss: -0.005983959233465915
        total_loss: 0.5411780153711637
        vf_explained_var: 0.9990895390510559
        vf_loss: 0.5471933061877886
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.803125
    gpu_util_percent0: 0.31843750000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463484455958205
    mean_env_wait_ms: 1.2122214944916194
    mean_inference_ms: 4.301455191638346
    mean_raw_obs_processing_ms: 0.3778819282094053
  time_since_restore: 6029.212426185608
  time_this_iter_s: 26.650070428848267
  time_total_s: 6029.212426185608
  timers:
    learn_throughput: 8359.332
    learn_time_ms: 19354.655
    sample_throughput: 23797.37
    sample_time_ms: 6798.735
    update_time_ms: 25.774
  timestamp: 1602715515
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: a052f_00000
  
2020-10-14 22:45:16,539	WARNING util.py:136 -- The `process_trial` operation took 0.629051685333252 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    227 |          6029.21 | 36726784 |  289.437 |              316.172 |              145.717 |            790.535 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3144.7584007901573
    time_step_min: 2977
  date: 2020-10-14_22-45-43
  done: false
  episode_len_mean: 790.5746014034033
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.5345725662884
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 214
  episodes_total: 46601
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.420512958748391e-23
        cur_lr: 5.0e-05
        entropy: 0.05753831285983324
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008623852394521236
        total_loss: .inf
        vf_explained_var: 0.9994902610778809
        vf_loss: 0.26857705290118855
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61935483870968
    gpu_util_percent0: 0.31677419354838715
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146345133089425
    mean_env_wait_ms: 1.2121060562650963
    mean_inference_ms: 4.301317237723994
    mean_raw_obs_processing_ms: 0.37787168454054426
  time_since_restore: 6055.6775023937225
  time_this_iter_s: 26.465076208114624
  time_total_s: 6055.6775023937225
  timers:
    learn_throughput: 8370.012
    learn_time_ms: 19329.96
    sample_throughput: 23797.085
    sample_time_ms: 6798.816
    update_time_ms: 23.784
  timestamp: 1602715543
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:45:43,849	WARNING util.py:136 -- The `process_trial` operation took 0.6336343288421631 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    228 |          6055.68 | 36888576 |  289.535 |              316.172 |              145.717 |            790.575 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3144.2612323491658
    time_step_min: 2977
  date: 2020-10-14_22-46-10
  done: false
  episode_len_mean: 790.6088564830653
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.61056813430207
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 46768
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4130769438122587e-22
        cur_lr: 5.0e-05
        entropy: 0.05678058974444866
        entropy_coeff: 0.0005000000000000001
        kl: 0.004009064539180447
        model: {}
        policy_loss: -0.005572611715858026
        total_loss: 0.18233930071194968
        vf_explained_var: 0.9996137022972107
        vf_loss: 0.18794030075271925
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.980645161290322
    gpu_util_percent0: 0.38032258064516133
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634292213328323
    mean_env_wait_ms: 1.2120166496846687
    mean_inference_ms: 4.301206038995531
    mean_raw_obs_processing_ms: 0.3778637786883543
  time_since_restore: 6081.994033336639
  time_this_iter_s: 26.31653094291687
  time_total_s: 6081.994033336639
  timers:
    learn_throughput: 8383.845
    learn_time_ms: 19298.066
    sample_throughput: 23809.397
    sample_time_ms: 6795.3
    update_time_ms: 21.921
  timestamp: 1602715570
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: a052f_00000
  
2020-10-14 22:46:11,022	WARNING util.py:136 -- The `process_trial` operation took 0.6295719146728516 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    229 |          6081.99 | 37050368 |  289.611 |              316.172 |              145.717 |            790.609 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3143.644602569292
    time_step_min: 2977
  date: 2020-10-14_22-46-37
  done: false
  episode_len_mean: 790.6513509485384
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.7028360553173
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 46967
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.065384719061294e-23
        cur_lr: 5.0e-05
        entropy: 0.05850799381732941
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006246535582855965
        total_loss: .inf
        vf_explained_var: 0.9996557831764221
        vf_loss: 0.18153289332985878
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10645161290323
    gpu_util_percent0: 0.34096774193548385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463409755534341
    mean_env_wait_ms: 1.2119097610986573
    mean_inference_ms: 4.301070669722494
    mean_raw_obs_processing_ms: 0.377854527799438
  time_since_restore: 6108.4408276081085
  time_this_iter_s: 26.446794271469116
  time_total_s: 6108.4408276081085
  timers:
    learn_throughput: 8364.907
    learn_time_ms: 19341.756
    sample_throughput: 23834.537
    sample_time_ms: 6788.133
    update_time_ms: 23.089
  timestamp: 1602715597
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:46:38,344	WARNING util.py:136 -- The `process_trial` operation took 0.5950145721435547 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    230 |          6108.44 | 37212160 |  289.703 |              316.172 |              145.717 |            790.651 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3142.921537874613
    time_step_min: 2977
  date: 2020-10-14_22-47-04
  done: false
  episode_len_mean: 790.6934124126244
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.81114256310167
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 243
  episodes_total: 47210
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.059807707859194e-22
        cur_lr: 5.0e-05
        entropy: 0.06000959066053232
        entropy_coeff: 0.0005000000000000001
        kl: 0.002772193014000853
        model: {}
        policy_loss: -0.007353436934257236
        total_loss: 0.3685971051454544
        vf_explained_var: 0.9993557929992676
        vf_loss: 0.3759805386265119
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.859375
    gpu_util_percent0: 0.3421875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633737620265985
    mean_env_wait_ms: 1.2117793226604046
    mean_inference_ms: 4.300922322636114
    mean_raw_obs_processing_ms: 0.3778426752077141
  time_since_restore: 6135.057198047638
  time_this_iter_s: 26.61637043952942
  time_total_s: 6135.057198047638
  timers:
    learn_throughput: 8358.306
    learn_time_ms: 19357.034
    sample_throughput: 23922.099
    sample_time_ms: 6763.286
    update_time_ms: 22.525
  timestamp: 1602715624
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: a052f_00000
  
2020-10-14 22:47:05,977	WARNING util.py:136 -- The `process_trial` operation took 0.609257698059082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    231 |          6135.06 | 37373952 |  289.811 |              316.172 |              145.717 |            790.693 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3142.374213570916
    time_step_min: 2977
  date: 2020-10-14_22-47-32
  done: false
  episode_len_mean: 790.7264843651095
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.8945384980324
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 184
  episodes_total: 47394
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.29903853929597e-23
        cur_lr: 5.0e-05
        entropy: 0.054370359828074775
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007899173554809144
        total_loss: .inf
        vf_explained_var: 0.9996131062507629
        vf_loss: 0.186643964300553
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.477419354838712
    gpu_util_percent0: 0.2993548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463344918230224
    mean_env_wait_ms: 1.2116798040922248
    mean_inference_ms: 4.300794227281836
    mean_raw_obs_processing_ms: 0.3778339746533508
  time_since_restore: 6161.579926252365
  time_this_iter_s: 26.522728204727173
  time_total_s: 6161.579926252365
  timers:
    learn_throughput: 8351.978
    learn_time_ms: 19371.697
    sample_throughput: 23991.278
    sample_time_ms: 6743.784
    update_time_ms: 22.407
  timestamp: 1602715652
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:47:33,328	WARNING util.py:136 -- The `process_trial` operation took 0.6085836887359619 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    232 |          6161.58 | 37535744 |  289.895 |              316.172 |              145.717 |            790.726 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3141.850051532297
    time_step_min: 2977
  date: 2020-10-14_22-47-59
  done: false
  episode_len_mean: 790.7572260410755
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 289.9718328520747
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 47571
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.948557808943956e-23
        cur_lr: 5.0e-05
        entropy: 0.05847520505388578
        entropy_coeff: 0.0005000000000000001
        kl: 0.004587109899148345
        model: {}
        policy_loss: -0.007160096405035195
        total_loss: 0.3294684365391731
        vf_explained_var: 0.999302089214325
        vf_loss: 0.33665777494510013
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.858064516129037
    gpu_util_percent0: 0.30838709677419357
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633239964038275
    mean_env_wait_ms: 1.2115857551395766
    mean_inference_ms: 4.300685145283127
    mean_raw_obs_processing_ms: 0.3778260641250353
  time_since_restore: 6188.235923051834
  time_this_iter_s: 26.655996799468994
  time_total_s: 6188.235923051834
  timers:
    learn_throughput: 8329.565
    learn_time_ms: 19423.823
    sample_throughput: 24019.338
    sample_time_ms: 6735.906
    update_time_ms: 22.47
  timestamp: 1602715679
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: a052f_00000
  
2020-10-14 22:48:00,864	WARNING util.py:136 -- The `process_trial` operation took 0.6448802947998047 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    233 |          6188.24 | 37697536 |  289.972 |              316.172 |              145.717 |            790.757 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3141.197542493511
    time_step_min: 2977
  date: 2020-10-14_22-48-27
  done: false
  episode_len_mean: 790.8021966527197
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.0726966315876
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 229
  episodes_total: 47800
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.974278904471978e-23
        cur_lr: 5.0e-05
        entropy: 0.06043405427287022
        entropy_coeff: 0.0005000000000000001
        kl: 0.005865330109372735
        model: {}
        policy_loss: -0.008524551701460345
        total_loss: 0.32738794138034183
        vf_explained_var: 0.999474048614502
        vf_loss: 0.33594271788994473
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73225806451613
    gpu_util_percent0: 0.33322580645161287
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463297055015511
    mean_env_wait_ms: 1.211460560250012
    mean_inference_ms: 4.300534751005388
    mean_raw_obs_processing_ms: 0.3778145554678151
  time_since_restore: 6214.778030395508
  time_this_iter_s: 26.542107343673706
  time_total_s: 6214.778030395508
  timers:
    learn_throughput: 8313.642
    learn_time_ms: 19461.025
    sample_throughput: 24013.193
    sample_time_ms: 6737.63
    update_time_ms: 22.635
  timestamp: 1602715707
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: a052f_00000
  
2020-10-14 22:48:28,237	WARNING util.py:136 -- The `process_trial` operation took 0.6114566326141357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    234 |          6214.78 | 37859328 |  290.073 |              316.172 |              145.717 |            790.802 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3140.553353753829
    time_step_min: 2977
  date: 2020-10-14_22-48-54
  done: false
  episode_len_mean: 790.847393739978
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.1683296237324
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 219
  episodes_total: 48019
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.974278904471978e-23
        cur_lr: 5.0e-05
        entropy: 0.05680483113974333
        entropy_coeff: 0.0005000000000000001
        kl: 0.004843908982972304
        model: {}
        policy_loss: -0.005174934442038648
        total_loss: 0.14714375014106432
        vf_explained_var: 0.9997263550758362
        vf_loss: 0.15234709158539772
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.065624999999997
    gpu_util_percent0: 0.411875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463264100297604
    mean_env_wait_ms: 1.2113450813950977
    mean_inference_ms: 4.300406090220063
    mean_raw_obs_processing_ms: 0.3778050634610242
  time_since_restore: 6241.515906095505
  time_this_iter_s: 26.73787569999695
  time_total_s: 6241.515906095505
  timers:
    learn_throughput: 8302.545
    learn_time_ms: 19487.037
    sample_throughput: 24007.522
    sample_time_ms: 6739.221
    update_time_ms: 24.554
  timestamp: 1602715734
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: a052f_00000
  
2020-10-14 22:48:55,816	WARNING util.py:136 -- The `process_trial` operation took 0.617173433303833 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    235 |          6241.52 | 38021120 |  290.168 |              316.172 |              145.717 |            790.847 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3140.0698282842964
    time_step_min: 2977
  date: 2020-10-14_22-49-22
  done: false
  episode_len_mean: 790.8811969536616
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.24062010882625
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 48189
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.987139452235989e-23
        cur_lr: 5.0e-05
        entropy: 0.05556761690725883
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00760531200406452
        total_loss: .inf
        vf_explained_var: 0.9994058609008789
        vf_loss: 0.281840721766154
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0483870967742
    gpu_util_percent0: 0.33903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463241524107101
    mean_env_wait_ms: 1.2112539239859985
    mean_inference_ms: 4.3002958081607465
    mean_raw_obs_processing_ms: 0.3777973694718448
  time_since_restore: 6268.154893398285
  time_this_iter_s: 26.63898730278015
  time_total_s: 6268.154893398285
  timers:
    learn_throughput: 8293.978
    learn_time_ms: 19507.166
    sample_throughput: 24024.369
    sample_time_ms: 6734.495
    update_time_ms: 26.264
  timestamp: 1602715762
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:49:23,340	WARNING util.py:136 -- The `process_trial` operation took 0.6604833602905273 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    236 |          6268.15 | 38182912 |  290.241 |              316.172 |              145.717 |            790.881 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3139.5522233712513
    time_step_min: 2977
  date: 2020-10-14_22-49-50
  done: false
  episode_len_mean: 790.9203356897764
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.3181705433346
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 189
  episodes_total: 48378
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9807091783539834e-23
        cur_lr: 5.0e-05
        entropy: 0.05953216304381689
        entropy_coeff: 0.0005000000000000001
        kl: 0.004113680295025309
        model: {}
        policy_loss: -0.008218848097991819
        total_loss: 0.4276200756430626
        vf_explained_var: 0.9991757869720459
        vf_loss: 0.4358686978618304
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.428125
    gpu_util_percent0: 0.320625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632231601029852
    mean_env_wait_ms: 1.2111524856036837
    mean_inference_ms: 4.300180074328205
    mean_raw_obs_processing_ms: 0.37778914949466724
  time_since_restore: 6294.958522081375
  time_this_iter_s: 26.80362868309021
  time_total_s: 6294.958522081375
  timers:
    learn_throughput: 8295.867
    learn_time_ms: 19502.724
    sample_throughput: 23989.424
    sample_time_ms: 6744.305
    update_time_ms: 34.283
  timestamp: 1602715790
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: a052f_00000
  
2020-10-14 22:49:51,073	WARNING util.py:136 -- The `process_trial` operation took 0.6846520900726318 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    237 |          6294.96 | 38344704 |  290.318 |              316.172 |              145.717 |             790.92 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3138.8951040316106
    time_step_min: 2977
  date: 2020-10-14_22-50-17
  done: false
  episode_len_mean: 790.9720891009688
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.4172297441182
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 241
  episodes_total: 48619
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4903545891769917e-23
        cur_lr: 5.0e-05
        entropy: 0.05784952205916246
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008575311236199923
        total_loss: .inf
        vf_explained_var: 0.9994927048683167
        vf_loss: 0.30193454523881275
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.800000000000004
    gpu_util_percent0: 0.3429032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631891301525862
    mean_env_wait_ms: 1.2110233534799943
    mean_inference_ms: 4.300030762748776
    mean_raw_obs_processing_ms: 0.3777778686905517
  time_since_restore: 6321.277307748795
  time_this_iter_s: 26.318785667419434
  time_total_s: 6321.277307748795
  timers:
    learn_throughput: 8301.737
    learn_time_ms: 19488.932
    sample_throughput: 23999.794
    sample_time_ms: 6741.391
    update_time_ms: 34.61
  timestamp: 1602715817
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:50:18,277	WARNING util.py:136 -- The `process_trial` operation took 0.6681373119354248 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    238 |          6321.28 | 38506496 |  290.417 |              316.172 |              145.717 |            790.972 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3138.357451780187
    time_step_min: 2977
  date: 2020-10-14_22-50-44
  done: false
  episode_len_mean: 791.0129263546041
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.49862550528303
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 196
  episodes_total: 48815
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2355318837654873e-23
        cur_lr: 5.0e-05
        entropy: 0.05323144793510437
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039177883105973406
        model: {}
        policy_loss: -0.008639993253382272
        total_loss: 0.2432916002968947
        vf_explained_var: 0.999504029750824
        vf_loss: 0.2519582062959671
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33225806451613
    gpu_util_percent0: 0.3764516129032259
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463161635587887
    mean_env_wait_ms: 1.2109179601923539
    mean_inference_ms: 4.299915885142016
    mean_raw_obs_processing_ms: 0.37776935126046024
  time_since_restore: 6347.60239315033
  time_this_iter_s: 26.325085401535034
  time_total_s: 6347.60239315033
  timers:
    learn_throughput: 8300.406
    learn_time_ms: 19492.059
    sample_throughput: 24014.349
    sample_time_ms: 6737.305
    update_time_ms: 34.62
  timestamp: 1602715844
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: a052f_00000
  
2020-10-14 22:50:45,508	WARNING util.py:136 -- The `process_trial` operation took 0.6694157123565674 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    239 |           6347.6 | 38668288 |  290.499 |              316.172 |              145.717 |            791.013 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3137.891973039216
    time_step_min: 2977
  date: 2020-10-14_22-51-11
  done: false
  episode_len_mean: 791.0508696007186
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.56917319681645
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 173
  episodes_total: 48988
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1177659418827436e-23
        cur_lr: 5.0e-05
        entropy: 0.057342853086690106
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009714411239353163
        total_loss: .inf
        vf_explained_var: 0.9989402890205383
        vf_loss: 0.5139600162704786
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.519354838709674
    gpu_util_percent0: 0.35677419354838713
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631413663724813
    mean_env_wait_ms: 1.2108252205675634
    mean_inference_ms: 4.2998112931074015
    mean_raw_obs_processing_ms: 0.37776207948931023
  time_since_restore: 6374.012024641037
  time_this_iter_s: 26.409631490707397
  time_total_s: 6374.012024641037
  timers:
    learn_throughput: 8310.929
    learn_time_ms: 19467.378
    sample_throughput: 23939.688
    sample_time_ms: 6758.317
    update_time_ms: 33.822
  timestamp: 1602715871
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:51:12,801	WARNING util.py:136 -- The `process_trial` operation took 0.6655983924865723 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    240 |          6374.01 | 38830080 |  290.569 |              316.172 |              145.717 |            791.051 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3137.300896833628
    time_step_min: 2977
  date: 2020-10-14_22-51-39
  done: false
  episode_len_mean: 791.0977215910245
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.6542079809084
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 49201
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6766489128241154e-23
        cur_lr: 5.0e-05
        entropy: 0.0594628993421793
        entropy_coeff: 0.0005000000000000001
        kl: 0.005068829748779535
        model: {}
        policy_loss: -0.007663162551277007
        total_loss: 0.4169003367424011
        vf_explained_var: 0.9992856383323669
        vf_loss: 0.4245932350556056
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.45806451612904
    gpu_util_percent0: 0.35806451612903223
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631184994635626
    mean_env_wait_ms: 1.210711273456045
    mean_inference_ms: 4.299680557501711
    mean_raw_obs_processing_ms: 0.37775232229342903
  time_since_restore: 6400.42835688591
  time_this_iter_s: 26.416332244873047
  time_total_s: 6400.42835688591
  timers:
    learn_throughput: 8321.188
    learn_time_ms: 19443.377
    sample_throughput: 23922.896
    sample_time_ms: 6763.061
    update_time_ms: 34.058
  timestamp: 1602715899
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: a052f_00000
  
2020-10-14 22:51:40,118	WARNING util.py:136 -- The `process_trial` operation took 0.674243688583374 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    241 |          6400.43 | 38991872 |  290.654 |              316.172 |              145.717 |            791.098 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3136.6558919969234
    time_step_min: 2977
  date: 2020-10-14_22-52-06
  done: false
  episode_len_mean: 791.1493911073351
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.7509204191444
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 233
  episodes_total: 49434
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6766489128241154e-23
        cur_lr: 5.0e-05
        entropy: 0.05950562500705322
        entropy_coeff: 0.0005000000000000001
        kl: 0.007765766427231331
        model: {}
        policy_loss: -0.007173362304456532
        total_loss: 0.28485876818497974
        vf_explained_var: 0.999483585357666
        vf_loss: 0.29206188147266704
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.434375
    gpu_util_percent0: 0.33625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463087609957843
    mean_env_wait_ms: 1.2105878959033933
    mean_inference_ms: 4.299558691777949
    mean_raw_obs_processing_ms: 0.3777425494005004
  time_since_restore: 6426.824672222137
  time_this_iter_s: 26.396315336227417
  time_total_s: 6426.824672222137
  timers:
    learn_throughput: 8323.041
    learn_time_ms: 19439.048
    sample_throughput: 23920.566
    sample_time_ms: 6763.72
    update_time_ms: 34.53
  timestamp: 1602715926
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: a052f_00000
  
2020-10-14 22:52:07,656	WARNING util.py:136 -- The `process_trial` operation took 0.7277765274047852 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    242 |          6426.82 | 39153664 |  290.751 |              316.172 |              145.717 |            791.149 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3136.199705544239
    time_step_min: 2977
  date: 2020-10-14_22-52-34
  done: false
  episode_len_mean: 791.1793553848945
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.8143947792611
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 49611
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6766489128241154e-23
        cur_lr: 5.0e-05
        entropy: 0.07249705555538337
        entropy_coeff: 0.0005000000000000001
        kl: 0.00535208029517283
        model: {}
        policy_loss: -0.011093523799596975
        total_loss: 0.7444133361180624
        vf_explained_var: 0.9984539151191711
        vf_loss: 0.7555431127548218
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93225806451613
    gpu_util_percent0: 0.32516129032258057
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630654161806306
    mean_env_wait_ms: 1.21049313256574
    mean_inference_ms: 4.299449905033735
    mean_raw_obs_processing_ms: 0.37773491631003614
  time_since_restore: 6453.252086639404
  time_this_iter_s: 26.427414417266846
  time_total_s: 6453.252086639404
  timers:
    learn_throughput: 8337.269
    learn_time_ms: 19405.875
    sample_throughput: 23881.921
    sample_time_ms: 6774.664
    update_time_ms: 34.138
  timestamp: 1602715954
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: a052f_00000
  
2020-10-14 22:52:34,958	WARNING util.py:136 -- The `process_trial` operation took 0.6418309211730957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    243 |          6453.25 | 39315456 |  290.814 |              316.172 |              145.717 |            791.179 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3135.781216091307
    time_step_min: 2977
  date: 2020-10-14_22-53-01
  done: false
  episode_len_mean: 791.2089408362453
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.87772714492775
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 183
  episodes_total: 49794
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6766489128241154e-23
        cur_lr: 5.0e-05
        entropy: 0.07329424346486728
        entropy_coeff: 0.0005000000000000001
        kl: 0.003925652340209733
        model: {}
        policy_loss: -0.008984929212601855
        total_loss: 0.6388332198063532
        vf_explained_var: 0.9986963868141174
        vf_loss: 0.6478547801574072
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20645161290323
    gpu_util_percent0: 0.302258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630465047995275
    mean_env_wait_ms: 1.2103953522043929
    mean_inference_ms: 4.299345959084164
    mean_raw_obs_processing_ms: 0.377727253793838
  time_since_restore: 6479.5771696567535
  time_this_iter_s: 26.325083017349243
  time_total_s: 6479.5771696567535
  timers:
    learn_throughput: 8346.343
    learn_time_ms: 19384.778
    sample_throughput: 23883.977
    sample_time_ms: 6774.081
    update_time_ms: 33.684
  timestamp: 1602715981
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: a052f_00000
  
2020-10-14 22:53:02,162	WARNING util.py:136 -- The `process_trial` operation took 0.6458413600921631 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    244 |          6479.58 | 39477248 |  290.878 |              316.172 |              145.717 |            791.209 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3135.18598140186
    time_step_min: 2977
  date: 2020-10-14_22-53-28
  done: false
  episode_len_mean: 791.2401814802231
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 290.96574402308585
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 239
  episodes_total: 50033
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.383244564120577e-24
        cur_lr: 5.0e-05
        entropy: 0.07204069631795089
        entropy_coeff: 0.0005000000000000001
        kl: 0.003468099069626381
        model: {}
        policy_loss: -0.010170111840125173
        total_loss: 0.5636664306124052
        vf_explained_var: 0.9990128874778748
        vf_loss: 0.5738725488384565
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.403125
    gpu_util_percent0: 0.2925
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630179506575788
    mean_env_wait_ms: 1.2102695423612
    mean_inference_ms: 4.299211137355733
    mean_raw_obs_processing_ms: 0.3777170188791452
  time_since_restore: 6506.066538095474
  time_this_iter_s: 26.489368438720703
  time_total_s: 6506.066538095474
  timers:
    learn_throughput: 8353.134
    learn_time_ms: 19369.017
    sample_throughput: 23886.799
    sample_time_ms: 6773.281
    update_time_ms: 31.914
  timestamp: 1602716008
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: a052f_00000
  
2020-10-14 22:53:29,726	WARNING util.py:136 -- The `process_trial` operation took 0.6955647468566895 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    245 |          6506.07 | 39639040 |  290.966 |              316.172 |              145.717 |             791.24 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3134.6416905672404
    time_step_min: 2977
  date: 2020-10-14_22-53-56
  done: false
  episode_len_mean: 791.2742057488654
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.04504838173887
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 203
  episodes_total: 50236
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1916222820602885e-24
        cur_lr: 5.0e-05
        entropy: 0.06470355826119582
        entropy_coeff: 0.0005000000000000001
        kl: 0.0064908110070973635
        model: {}
        policy_loss: -0.007252788188149377
        total_loss: 0.29044119268655777
        vf_explained_var: 0.9994251728057861
        vf_loss: 0.2977263381083806
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.04193548387097
    gpu_util_percent0: 0.31322580645161285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629892910675615
    mean_env_wait_ms: 1.2101601655747882
    mean_inference_ms: 4.29909563157913
    mean_raw_obs_processing_ms: 0.3777085736383811
  time_since_restore: 6532.433878421783
  time_this_iter_s: 26.367340326309204
  time_total_s: 6532.433878421783
  timers:
    learn_throughput: 8363.052
    learn_time_ms: 19346.047
    sample_throughput: 23908.084
    sample_time_ms: 6767.251
    update_time_ms: 30.122
  timestamp: 1602716036
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: a052f_00000
  
2020-10-14 22:53:56,990	WARNING util.py:136 -- The `process_trial` operation took 0.6462140083312988 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    246 |          6532.43 | 39800832 |  291.045 |              316.172 |              145.717 |            791.274 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3134.2188653777444
    time_step_min: 2977
  date: 2020-10-14_22-54-23
  done: false
  episode_len_mean: 791.2991310558267
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.1062998753152
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 50406
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1916222820602885e-24
        cur_lr: 5.0e-05
        entropy: 0.06731427150468032
        entropy_coeff: 0.0005000000000000001
        kl: 0.003263428710245838
        model: {}
        policy_loss: -0.00970881989148135
        total_loss: 0.6125120669603348
        vf_explained_var: 0.998795747756958
        vf_loss: 0.6222545305887858
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81612903225807
    gpu_util_percent0: 0.2619354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629721942557117
    mean_env_wait_ms: 1.210070255214728
    mean_inference_ms: 4.299001984099945
    mean_raw_obs_processing_ms: 0.3777016784734775
  time_since_restore: 6559.026163339615
  time_this_iter_s: 26.59228491783142
  time_total_s: 6559.026163339615
  timers:
    learn_throughput: 8365.734
    learn_time_ms: 19339.845
    sample_throughput: 23931.308
    sample_time_ms: 6760.684
    update_time_ms: 22.281
  timestamp: 1602716063
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: a052f_00000
  
2020-10-14 22:54:24,477	WARNING util.py:136 -- The `process_trial` operation took 0.6646249294281006 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    247 |          6559.03 | 39962624 |  291.106 |              316.172 |              145.717 |            791.299 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3133.7110548790133
    time_step_min: 2977
  date: 2020-10-14_22-54-50
  done: false
  episode_len_mean: 791.3328657235438
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.1824793816611
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 50612
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0958111410301442e-24
        cur_lr: 5.0e-05
        entropy: 0.06819658105572064
        entropy_coeff: 0.0005000000000000001
        kl: 0.004329927576084931
        model: {}
        policy_loss: -0.008024208781231815
        total_loss: 0.48250362277030945
        vf_explained_var: 0.9991130828857422
        vf_loss: 0.49056191990772885
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06129032258065
    gpu_util_percent0: 0.29709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629568817840902
    mean_env_wait_ms: 1.2099632518896166
    mean_inference_ms: 4.298891694378333
    mean_raw_obs_processing_ms: 0.37769367961963335
  time_since_restore: 6585.293956756592
  time_this_iter_s: 26.26779341697693
  time_total_s: 6585.293956756592
  timers:
    learn_throughput: 8363.948
    learn_time_ms: 19343.975
    sample_throughput: 23938.759
    sample_time_ms: 6758.579
    update_time_ms: 22.351
  timestamp: 1602716090
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: a052f_00000
  
2020-10-14 22:54:51,685	WARNING util.py:136 -- The `process_trial` operation took 0.7099084854125977 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    248 |          6585.29 | 40124416 |  291.182 |              316.172 |              145.717 |            791.333 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3133.1141305417264
    time_step_min: 2977
  date: 2020-10-14_22-55-18
  done: false
  episode_len_mean: 791.3721163490471
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.27208065074575
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 235
  episodes_total: 50847
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0479055705150721e-24
        cur_lr: 5.0e-05
        entropy: 0.06869937976201375
        entropy_coeff: 0.0005000000000000001
        kl: 0.005013968523902197
        model: {}
        policy_loss: -0.008747026925751319
        total_loss: 0.40289847056070965
        vf_explained_var: 0.9992833137512207
        vf_loss: 0.41167985399564105
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096773
    gpu_util_percent0: 0.3338709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629222280926363
    mean_env_wait_ms: 1.209837133616835
    mean_inference_ms: 4.298764757343419
    mean_raw_obs_processing_ms: 0.377683571727655
  time_since_restore: 6611.68748664856
  time_this_iter_s: 26.393529891967773
  time_total_s: 6611.68748664856
  timers:
    learn_throughput: 8359.92
    learn_time_ms: 19353.294
    sample_throughput: 23943.048
    sample_time_ms: 6757.369
    update_time_ms: 22.27
  timestamp: 1602716118
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: a052f_00000
  
2020-10-14 22:55:19,052	WARNING util.py:136 -- The `process_trial` operation took 0.6857132911682129 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    249 |          6611.69 | 40286208 |  291.272 |              316.172 |              145.717 |            791.372 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3132.656582687972
    time_step_min: 2977
  date: 2020-10-14_22-55-45
  done: false
  episode_len_mean: 791.4008582681794
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.337524917032
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 186
  episodes_total: 51033
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0479055705150721e-24
        cur_lr: 5.0e-05
        entropy: 0.06677634579439957
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034776865310656526
        model: {}
        policy_loss: -0.007223827977819989
        total_loss: 0.6682364990313848
        vf_explained_var: 0.9986411929130554
        vf_loss: 0.6754937122265497
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.849999999999998
    gpu_util_percent0: 0.2578125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629006143677475
    mean_env_wait_ms: 1.2097392531735256
    mean_inference_ms: 4.298664442355472
    mean_raw_obs_processing_ms: 0.37767632535508167
  time_since_restore: 6638.16920876503
  time_this_iter_s: 26.481722116470337
  time_total_s: 6638.16920876503
  timers:
    learn_throughput: 8354.834
    learn_time_ms: 19365.076
    sample_throughput: 23955.955
    sample_time_ms: 6753.728
    update_time_ms: 22.426
  timestamp: 1602716145
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: a052f_00000
  
2020-10-14 22:55:46,617	WARNING util.py:136 -- The `process_trial` operation took 0.6884121894836426 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    250 |          6638.17 | 40448000 |  291.338 |              316.172 |              145.717 |            791.401 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3132.239269317183
    time_step_min: 2977
  date: 2020-10-14_22-56-13
  done: false
  episode_len_mean: 791.4282896920704
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.40318657253783
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 180
  episodes_total: 51213
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.239527852575361e-25
        cur_lr: 5.0e-05
        entropy: 0.06213802626977364
        entropy_coeff: 0.0005000000000000001
        kl: 0.007112214729810755
        model: {}
        policy_loss: -0.007133312775598218
        total_loss: 0.28502097974220914
        vf_explained_var: 0.9994173049926758
        vf_loss: 0.29218536367019016
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.832258064516132
    gpu_util_percent0: 0.3119354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628826548958931
    mean_env_wait_ms: 1.2096441688258082
    mean_inference_ms: 4.298564632392948
    mean_raw_obs_processing_ms: 0.37766900675001525
  time_since_restore: 6664.94566655159
  time_this_iter_s: 26.77645778656006
  time_total_s: 6664.94566655159
  timers:
    learn_throughput: 8344.58
    learn_time_ms: 19388.873
    sample_throughput: 23927.415
    sample_time_ms: 6761.783
    update_time_ms: 22.952
  timestamp: 1602716173
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: a052f_00000
  
2020-10-14 22:56:14,376	WARNING util.py:136 -- The `process_trial` operation took 0.6596713066101074 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    251 |          6664.95 | 40609792 |  291.403 |              316.172 |              145.717 |            791.428 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3131.6620830496936
    time_step_min: 2977
  date: 2020-10-14_22-56-40
  done: false
  episode_len_mean: 791.4659720467313
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.4911056799746
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 230
  episodes_total: 51443
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.239527852575361e-25
        cur_lr: 5.0e-05
        entropy: 0.06528872313598792
        entropy_coeff: 0.0005000000000000001
        kl: 0.00425023310041676
        model: {}
        policy_loss: -0.008617962341910848
        total_loss: 0.37940268963575363
        vf_explained_var: 0.9993340373039246
        vf_loss: 0.3880532955129941
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.974193548387095
    gpu_util_percent0: 0.3503225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628596910962152
    mean_env_wait_ms: 1.209522664946355
    mean_inference_ms: 4.298447914440637
    mean_raw_obs_processing_ms: 0.377659961622088
  time_since_restore: 6691.312099695206
  time_this_iter_s: 26.366433143615723
  time_total_s: 6691.312099695206
  timers:
    learn_throughput: 8348.047
    learn_time_ms: 19380.821
    sample_throughput: 23910.129
    sample_time_ms: 6766.672
    update_time_ms: 23.682
  timestamp: 1602716200
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: a052f_00000
  
2020-10-14 22:56:41,676	WARNING util.py:136 -- The `process_trial` operation took 0.7001492977142334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    252 |          6691.31 | 40771584 |  291.491 |              316.172 |              145.717 |            791.466 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3131.1636772169063
    time_step_min: 2977
  date: 2020-10-14_22-57-08
  done: false
  episode_len_mean: 791.5004646300383
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.56865319474184
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 211
  episodes_total: 51654
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6197639262876803e-25
        cur_lr: 5.0e-05
        entropy: 0.06110932460675637
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036739154214349887
        model: {}
        policy_loss: -0.00837684857348601
        total_loss: 0.36804259320100147
        vf_explained_var: 0.9992972016334534
        vf_loss: 0.37644999970992404
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.340625
    gpu_util_percent0: 0.2665625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628311220876902
    mean_env_wait_ms: 1.2094122878189473
    mean_inference_ms: 4.29833670507756
    mean_raw_obs_processing_ms: 0.3776513622202537
  time_since_restore: 6718.2194991111755
  time_this_iter_s: 26.90739941596985
  time_total_s: 6718.2194991111755
  timers:
    learn_throughput: 8332.484
    learn_time_ms: 19417.018
    sample_throughput: 23876.291
    sample_time_ms: 6776.262
    update_time_ms: 24.563
  timestamp: 1602716228
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: a052f_00000
  
2020-10-14 22:57:09,501	WARNING util.py:136 -- The `process_trial` operation took 0.6790616512298584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    253 |          6718.22 | 40933376 |  291.569 |              316.172 |              145.717 |              791.5 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3130.7327889107687
    time_step_min: 2977
  date: 2020-10-14_22-57-35
  done: false
  episode_len_mean: 791.5285763902289
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.63498937976965
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 172
  episodes_total: 51826
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3098819631438401e-25
        cur_lr: 5.0e-05
        entropy: 0.05893310500929753
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007101180274427558
        total_loss: .inf
        vf_explained_var: 0.9996033310890198
        vf_loss: 0.18761430184046426
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.125806451612902
    gpu_util_percent0: 0.29709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628132211520345
    mean_env_wait_ms: 1.2093221016474338
    mean_inference_ms: 4.298247493148397
    mean_raw_obs_processing_ms: 0.3776449593288664
  time_since_restore: 6744.642548799515
  time_this_iter_s: 26.423049688339233
  time_total_s: 6744.642548799515
  timers:
    learn_throughput: 8331.194
    learn_time_ms: 19420.025
    sample_throughput: 23856.956
    sample_time_ms: 6781.754
    update_time_ms: 24.666
  timestamp: 1602716255
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 22:57:36,842	WARNING util.py:136 -- The `process_trial` operation took 0.6813948154449463 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    254 |          6744.64 | 41095168 |  291.635 |              316.172 |              145.717 |            791.529 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3130.253144351706
    time_step_min: 2977
  date: 2020-10-14_22-58-03
  done: false
  episode_len_mean: 791.5594510437089
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.7063973840582
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 200
  episodes_total: 52026
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9648229447157607e-25
        cur_lr: 5.0e-05
        entropy: 0.06378294217089812
        entropy_coeff: 0.0005000000000000001
        kl: 0.004937962706511219
        model: {}
        policy_loss: -0.008569791621994227
        total_loss: 0.30098799616098404
        vf_explained_var: 0.9994199275970459
        vf_loss: 0.3095896939436595
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.256249999999998
    gpu_util_percent0: 0.33625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627988702855346
    mean_env_wait_ms: 1.209218661994785
    mean_inference_ms: 4.298143606451548
    mean_raw_obs_processing_ms: 0.3776376821077904
  time_since_restore: 6771.459598064423
  time_this_iter_s: 26.817049264907837
  time_total_s: 6771.459598064423
  timers:
    learn_throughput: 8335.658
    learn_time_ms: 19409.626
    sample_throughput: 23738.899
    sample_time_ms: 6815.48
    update_time_ms: 25.068
  timestamp: 1602716283
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: a052f_00000
  
2020-10-14 22:58:04,576	WARNING util.py:136 -- The `process_trial` operation took 0.6794910430908203 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    255 |          6771.46 | 41256960 |  291.706 |              316.172 |              145.717 |            791.559 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3129.6961690885073
    time_step_min: 2977
  date: 2020-10-14_22-58-31
  done: false
  episode_len_mean: 791.5997014982491
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.7919026858003
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 235
  episodes_total: 52261
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.824114723578803e-26
        cur_lr: 5.0e-05
        entropy: 0.05900139641016722
        entropy_coeff: 0.0005000000000000001
        kl: 0.004837856162339449
        model: {}
        policy_loss: -0.007207886306180929
        total_loss: 0.31343720604976016
        vf_explained_var: 0.9994411468505859
        vf_loss: 0.3206745907664299
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08387096774194
    gpu_util_percent0: 0.25322580645161297
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627674573938795
    mean_env_wait_ms: 1.2090943432940053
    mean_inference_ms: 4.298023503955692
    mean_raw_obs_processing_ms: 0.3776276774492233
  time_since_restore: 6798.180140018463
  time_this_iter_s: 26.720541954040527
  time_total_s: 6798.180140018463
  timers:
    learn_throughput: 8323.355
    learn_time_ms: 19438.316
    sample_throughput: 23721.991
    sample_time_ms: 6820.338
    update_time_ms: 26.637
  timestamp: 1602716311
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: a052f_00000
  
2020-10-14 22:58:32,364	WARNING util.py:136 -- The `process_trial` operation took 0.7086796760559082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    256 |          6798.18 | 41418752 |  291.792 |              316.172 |              145.717 |              791.6 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3129.25148318422
    time_step_min: 2977
  date: 2020-10-14_22-58-58
  done: false
  episode_len_mean: 791.6280577322733
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.8585719923016
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 52449
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.9120573617894017e-26
        cur_lr: 5.0e-05
        entropy: 0.05705253345270952
        entropy_coeff: 0.0005000000000000001
        kl: 0.004039288808902104
        model: {}
        policy_loss: -0.006795141554903239
        total_loss: 0.3004603870213032
        vf_explained_var: 0.9993810057640076
        vf_loss: 0.30728405714035034
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7125
    gpu_util_percent0: 0.34531249999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462744712593973
    mean_env_wait_ms: 1.208995588790288
    mean_inference_ms: 4.2979290285366085
    mean_raw_obs_processing_ms: 0.37762075957082647
  time_since_restore: 6824.672504663467
  time_this_iter_s: 26.492364645004272
  time_total_s: 6824.672504663467
  timers:
    learn_throughput: 8332.811
    learn_time_ms: 19416.258
    sample_throughput: 23684.508
    sample_time_ms: 6831.132
    update_time_ms: 27.035
  timestamp: 1602716338
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: a052f_00000
  
2020-10-14 22:58:59,791	WARNING util.py:136 -- The `process_trial` operation took 0.6877915859222412 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    257 |          6824.67 | 41580544 |  291.859 |              316.172 |              145.717 |            791.628 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3128.811303537821
    time_step_min: 2977
  date: 2020-10-14_22-59-26
  done: false
  episode_len_mean: 791.6595352548878
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 291.9267624469119
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 182
  episodes_total: 52631
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4560286808947009e-26
        cur_lr: 5.0e-05
        entropy: 0.055551089656849705
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032313160481862724
        model: {}
        policy_loss: -0.0070699538725117845
        total_loss: 0.1865320478876432
        vf_explained_var: 0.999606192111969
        vf_loss: 0.19362977519631386
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.170967741935485
    gpu_util_percent0: 0.2651612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627294911780628
    mean_env_wait_ms: 1.2089014849868038
    mean_inference_ms: 4.297840702939606
    mean_raw_obs_processing_ms: 0.3776141950591508
  time_since_restore: 6851.318161487579
  time_this_iter_s: 26.64565682411194
  time_total_s: 6851.318161487579
  timers:
    learn_throughput: 8316.547
    learn_time_ms: 19454.228
    sample_throughput: 23687.164
    sample_time_ms: 6830.366
    update_time_ms: 27.563
  timestamp: 1602716366
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: a052f_00000
  
2020-10-14 22:59:27,496	WARNING util.py:136 -- The `process_trial` operation took 0.7118206024169922 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    258 |          6851.32 | 41742336 |  291.927 |              316.172 |              145.717 |             791.66 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3128.2811736867015
    time_step_min: 2977
  date: 2020-10-14_22-59-53
  done: false
  episode_len_mean: 791.6978033413429
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.00770939485835
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 222
  episodes_total: 52853
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2280143404473504e-26
        cur_lr: 5.0e-05
        entropy: 0.05631849014510711
        entropy_coeff: 0.0005000000000000001
        kl: 0.004174116261613865
        model: {}
        policy_loss: -0.009104615630349144
        total_loss: 0.1824015329281489
        vf_explained_var: 0.9996941089630127
        vf_loss: 0.19153429567813873
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.438709677419357
    gpu_util_percent0: 0.29612903225806453
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462710230837374
    mean_env_wait_ms: 1.2087856699643538
    mean_inference_ms: 4.29773088789853
    mean_raw_obs_processing_ms: 0.37760563743289555
  time_since_restore: 6877.683092832565
  time_this_iter_s: 26.364931344985962
  time_total_s: 6877.683092832565
  timers:
    learn_throughput: 8319.221
    learn_time_ms: 19447.976
    sample_throughput: 23681.859
    sample_time_ms: 6831.896
    update_time_ms: 28.943
  timestamp: 1602716393
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: a052f_00000
  
2020-10-14 22:59:54,941	WARNING util.py:136 -- The `process_trial` operation took 0.713778018951416 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    259 |          6877.68 | 41904128 |  292.008 |              316.172 |              145.717 |            791.698 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3127.7879010669985
    time_step_min: 2977
  date: 2020-10-14_23-00-20
  done: false
  episode_len_mean: 791.7270038060067
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.08240961828375
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 221
  episodes_total: 53074
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.140071702236752e-27
        cur_lr: 5.0e-05
        entropy: 0.06119880173355341
        entropy_coeff: 0.0005000000000000001
        kl: 0.004079618976296236
        model: {}
        policy_loss: -0.00873115905172502
        total_loss: 0.405729574461778
        vf_explained_var: 0.9992461800575256
        vf_loss: 0.41449134051799774
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.24193548387097
    gpu_util_percent0: 0.3477419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626804634607155
    mean_env_wait_ms: 1.2086711732729174
    mean_inference_ms: 4.297625299778626
    mean_raw_obs_processing_ms: 0.3775972719684031
  time_since_restore: 6903.409420490265
  time_this_iter_s: 25.726327657699585
  time_total_s: 6903.409420490265
  timers:
    learn_throughput: 8349.042
    learn_time_ms: 19378.511
    sample_throughput: 23740.063
    sample_time_ms: 6815.146
    update_time_ms: 28.264
  timestamp: 1602716420
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: a052f_00000
  
2020-10-14 23:00:21,686	WARNING util.py:136 -- The `process_trial` operation took 0.7403202056884766 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    260 |          6903.41 | 42065920 |  292.082 |              316.172 |              145.717 |            791.727 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3127.4252696456088
    time_step_min: 2977
  date: 2020-10-14_23-00-48
  done: false
  episode_len_mean: 791.7463471434475
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.13888613817215
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 172
  episodes_total: 53246
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.070035851118376e-27
        cur_lr: 5.0e-05
        entropy: 0.06098350013295809
        entropy_coeff: 0.0005000000000000001
        kl: 0.004752346935371558
        model: {}
        policy_loss: -0.009156292255890245
        total_loss: 0.35180841634670895
        vf_explained_var: 0.9992807507514954
        vf_loss: 0.36099519828955334
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.496774193548394
    gpu_util_percent0: 0.34129032258064523
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626631048149413
    mean_env_wait_ms: 1.2085819151867119
    mean_inference_ms: 4.297539556468509
    mean_raw_obs_processing_ms: 0.37759098819866843
  time_since_restore: 6930.15624833107
  time_this_iter_s: 26.746827840805054
  time_total_s: 6930.15624833107
  timers:
    learn_throughput: 8348.419
    learn_time_ms: 19379.957
    sample_throughput: 23761.012
    sample_time_ms: 6809.138
    update_time_ms: 28.455
  timestamp: 1602716448
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: a052f_00000
  
2020-10-14 23:00:49,515	WARNING util.py:136 -- The `process_trial` operation took 0.7505035400390625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    261 |          6930.16 | 42227712 |  292.139 |              316.172 |              145.717 |            791.746 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3127.054727579105
    time_step_min: 2977
  date: 2020-10-14_23-01-16
  done: false
  episode_len_mean: 791.7591601482092
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.19013613813166
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 192
  episodes_total: 53438
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.535017925559188e-27
        cur_lr: 5.0e-05
        entropy: 0.07790789566934109
        entropy_coeff: 0.0005000000000000001
        kl: 0.005166844348423183
        model: {}
        policy_loss: -0.010329283424653113
        total_loss: 1.2089351018269856
        vf_explained_var: 0.9976357817649841
        vf_loss: 1.2193033198515575
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3125
    gpu_util_percent0: 0.27625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462647590678292
    mean_env_wait_ms: 1.2084832583249292
    mean_inference_ms: 4.297446277664316
    mean_raw_obs_processing_ms: 0.3775841493789358
  time_since_restore: 6956.798627853394
  time_this_iter_s: 26.64237952232361
  time_total_s: 6956.798627853394
  timers:
    learn_throughput: 8342.349
    learn_time_ms: 19394.057
    sample_throughput: 23749.273
    sample_time_ms: 6812.503
    update_time_ms: 27.817
  timestamp: 1602716476
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: a052f_00000
  
2020-10-14 23:01:17,093	WARNING util.py:136 -- The `process_trial` operation took 0.6866040229797363 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    262 |           6956.8 | 42389504 |   292.19 |              316.172 |              145.717 |            791.759 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3126.6722399299147
    time_step_min: 2977
  date: 2020-10-14_23-01-43
  done: false
  episode_len_mean: 791.7718948525439
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.2518987968246
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 239
  episodes_total: 53677
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.535017925559188e-27
        cur_lr: 5.0e-05
        entropy: 0.06879807325700919
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038383555171700814
        model: {}
        policy_loss: -0.00983032772152607
        total_loss: 1.1334912180900574
        vf_explained_var: 0.9980325698852539
        vf_loss: 1.1433559556802113
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.574193548387104
    gpu_util_percent0: 0.3641935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626224492959972
    mean_env_wait_ms: 1.2083603626519757
    mean_inference_ms: 4.29733279337324
    mean_raw_obs_processing_ms: 0.37757537219393783
  time_since_restore: 6983.392250061035
  time_this_iter_s: 26.5936222076416
  time_total_s: 6983.392250061035
  timers:
    learn_throughput: 8354.868
    learn_time_ms: 19364.997
    sample_throughput: 23758.425
    sample_time_ms: 6809.879
    update_time_ms: 27.16
  timestamp: 1602716503
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: a052f_00000
  
2020-10-14 23:01:44,642	WARNING util.py:136 -- The `process_trial` operation took 0.7148683071136475 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    263 |          6983.39 | 42551296 |  292.252 |              316.172 |              145.717 |            791.772 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3126.28128076073
    time_step_min: 2977
  date: 2020-10-14_23-02-11
  done: false
  episode_len_mean: 791.7931540160754
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.3152204790003
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 194
  episodes_total: 53871
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.67508962779594e-28
        cur_lr: 5.0e-05
        entropy: 0.056870803236961365
        entropy_coeff: 0.0005000000000000001
        kl: 0.003445705476527413
        model: {}
        policy_loss: -0.0069159532091968385
        total_loss: 0.42314131806294125
        vf_explained_var: 0.9991340637207031
        vf_loss: 0.4300856987635295
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.971874999999997
    gpu_util_percent0: 0.35375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625985423390578
    mean_env_wait_ms: 1.208260138668601
    mean_inference_ms: 4.297240023090751
    mean_raw_obs_processing_ms: 0.37756828199658565
  time_since_restore: 7010.1686952114105
  time_this_iter_s: 26.776445150375366
  time_total_s: 7010.1686952114105
  timers:
    learn_throughput: 8344.5
    learn_time_ms: 19389.058
    sample_throughput: 23757.473
    sample_time_ms: 6810.152
    update_time_ms: 29.239
  timestamp: 1602716531
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: a052f_00000
  
2020-10-14 23:02:12,422	WARNING util.py:136 -- The `process_trial` operation took 0.6872546672821045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    264 |          7010.17 | 42713088 |  292.315 |              316.172 |              145.717 |            791.793 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3125.863426011625
    time_step_min: 2977
  date: 2020-10-14_23-02-38
  done: false
  episode_len_mean: 791.8140425531915
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.3761855371473
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 54050
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.83754481389797e-28
        cur_lr: 5.0e-05
        entropy: 0.05727981651822726
        entropy_coeff: 0.0005000000000000001
        kl: 0.003274915060804536
        model: {}
        policy_loss: -0.008006683647787819
        total_loss: 0.3954242914915085
        vf_explained_var: 0.999176561832428
        vf_loss: 0.4034596135218938
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.961290322580645
    gpu_util_percent0: 0.37548387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462582614447915
    mean_env_wait_ms: 1.2081685571177456
    mean_inference_ms: 4.297154836621174
    mean_raw_obs_processing_ms: 0.3775623464482272
  time_since_restore: 7036.688972234726
  time_this_iter_s: 26.52027702331543
  time_total_s: 7036.688972234726
  timers:
    learn_throughput: 8343.496
    learn_time_ms: 19391.391
    sample_throughput: 23849.57
    sample_time_ms: 6783.854
    update_time_ms: 29.144
  timestamp: 1602716558
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: a052f_00000
  
2020-10-14 23:02:39,897	WARNING util.py:136 -- The `process_trial` operation took 0.7093067169189453 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    265 |          7036.69 | 42874880 |  292.376 |              316.172 |              145.717 |            791.814 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3125.3826923785996
    time_step_min: 2977
  date: 2020-10-14_23-03-06
  done: false
  episode_len_mean: 791.8412566795652
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.4503436800283
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 220
  episodes_total: 54270
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.918772406948985e-28
        cur_lr: 5.0e-05
        entropy: 0.05622805841267109
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037303607484015324
        model: {}
        policy_loss: -0.005999943247540311
        total_loss: 0.35903750608364743
        vf_explained_var: 0.9993727803230286
        vf_loss: 0.3650655597448349
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.941935483870974
    gpu_util_percent0: 0.31161290322580654
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625653340994768
    mean_env_wait_ms: 1.208057508191999
    mean_inference_ms: 4.297056160915545
    mean_raw_obs_processing_ms: 0.3775539356315993
  time_since_restore: 7063.044026374817
  time_this_iter_s: 26.355054140090942
  time_total_s: 7063.044026374817
  timers:
    learn_throughput: 8354.176
    learn_time_ms: 19366.603
    sample_throughput: 23882.801
    sample_time_ms: 6774.415
    update_time_ms: 27.478
  timestamp: 1602716586
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: a052f_00000
  
2020-10-14 23:03:07,331	WARNING util.py:136 -- The `process_trial` operation took 0.7431473731994629 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    266 |          7063.04 | 43036672 |   292.45 |              316.172 |              145.717 |            791.841 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3124.8783232961223
    time_step_min: 2977
  date: 2020-10-14_23-03-34
  done: false
  episode_len_mean: 791.8735961242018
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.52683407517117
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 222
  episodes_total: 54492
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.593862034744925e-29
        cur_lr: 5.0e-05
        entropy: 0.05423162939647833
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034540111276631555
        model: {}
        policy_loss: -0.005319747773076718
        total_loss: 0.4316905165712039
        vf_explained_var: 0.9992195963859558
        vf_loss: 0.4370373884836833
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61875
    gpu_util_percent0: 0.250625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625376597941614
    mean_env_wait_ms: 1.2079436763181672
    mean_inference_ms: 4.296954547023818
    mean_raw_obs_processing_ms: 0.3775459588417337
  time_since_restore: 7089.795992612839
  time_this_iter_s: 26.75196623802185
  time_total_s: 7089.795992612839
  timers:
    learn_throughput: 8341.858
    learn_time_ms: 19395.199
    sample_throughput: 23901.056
    sample_time_ms: 6769.241
    update_time_ms: 29.165
  timestamp: 1602716614
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: a052f_00000
  
2020-10-14 23:03:35,050	WARNING util.py:136 -- The `process_trial` operation took 0.7064976692199707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    267 |           7089.8 | 43198464 |  292.527 |              316.172 |              145.717 |            791.874 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3124.4796215296205
    time_step_min: 2977
  date: 2020-10-14_23-04-01
  done: false
  episode_len_mean: 791.8965227093965
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.58683692547453
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 54669
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.7969310173724626e-29
        cur_lr: 5.0e-05
        entropy: 0.054979877235988774
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042038058551649255
        model: {}
        policy_loss: -0.0077580517827300355
        total_loss: 0.20821364099780718
        vf_explained_var: 0.9995492100715637
        vf_loss: 0.2159991835554441
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.375
    gpu_util_percent0: 0.3146875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625207881602362
    mean_env_wait_ms: 1.2078541328095311
    mean_inference_ms: 4.29687500292578
    mean_raw_obs_processing_ms: 0.3775400452597918
  time_since_restore: 7116.196452379227
  time_this_iter_s: 26.40045976638794
  time_total_s: 7116.196452379227
  timers:
    learn_throughput: 8353.269
    learn_time_ms: 19368.705
    sample_throughput: 23901.309
    sample_time_ms: 6769.169
    update_time_ms: 30.475
  timestamp: 1602716641
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: a052f_00000
  
2020-10-14 23:04:02,609	WARNING util.py:136 -- The `process_trial` operation took 0.7396445274353027 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    268 |           7116.2 | 43360256 |  292.587 |              316.172 |              145.717 |            791.897 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3124.0454678095934
    time_step_min: 2977
  date: 2020-10-14_23-04-29
  done: false
  episode_len_mean: 791.92165226585
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.65131960532784
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 189
  episodes_total: 54858
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3984655086862313e-29
        cur_lr: 5.0e-05
        entropy: 0.060194797813892365
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0062907383350345
        total_loss: .inf
        vf_explained_var: 0.9995661377906799
        vf_loss: 0.2914021350443363
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81290322580646
    gpu_util_percent0: 0.27
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625087197956166
    mean_env_wait_ms: 1.2077592287532315
    mean_inference_ms: 4.296790358630448
    mean_raw_obs_processing_ms: 0.3775336697286666
  time_since_restore: 7143.03019618988
  time_this_iter_s: 26.833743810653687
  time_total_s: 7143.03019618988
  timers:
    learn_throughput: 8337.859
    learn_time_ms: 19404.503
    sample_throughput: 23871.746
    sample_time_ms: 6777.552
    update_time_ms: 31.174
  timestamp: 1602716669
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:04:30,533	WARNING util.py:136 -- The `process_trial` operation took 0.7473328113555908 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    269 |          7143.03 | 43522048 |  292.651 |              316.172 |              145.717 |            791.922 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3123.5279750122577
    time_step_min: 2977
  date: 2020-10-14_23-04-57
  done: false
  episode_len_mean: 791.951683455849
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.7291482755678
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 237
  episodes_total: 55095
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5976982630293465e-29
        cur_lr: 5.0e-05
        entropy: 0.060178048287828766
        entropy_coeff: 0.0005000000000000001
        kl: 0.004100641352124512
        model: {}
        policy_loss: -0.007802962369169109
        total_loss: 0.32540976256132126
        vf_explained_var: 0.9994660019874573
        vf_loss: 0.33324281871318817
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.221875
    gpu_util_percent0: 0.28375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624829180684817
    mean_env_wait_ms: 1.2076385998372938
    mean_inference_ms: 4.296682567590351
    mean_raw_obs_processing_ms: 0.3775250263649883
  time_since_restore: 7169.780901432037
  time_this_iter_s: 26.750705242156982
  time_total_s: 7169.780901432037
  timers:
    learn_throughput: 8304.261
    learn_time_ms: 19483.01
    sample_throughput: 23768.836
    sample_time_ms: 6806.896
    update_time_ms: 31.86
  timestamp: 1602716697
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: a052f_00000
  
2020-10-14 23:04:58,399	WARNING util.py:136 -- The `process_trial` operation took 0.7341020107269287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    270 |          7169.78 | 43683840 |  292.729 |              316.172 |              145.717 |            791.952 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3123.0930064235954
    time_step_min: 2977
  date: 2020-10-14_23-05-25
  done: false
  episode_len_mean: 791.9762718608142
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.7943237924245
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 55293
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7988491315146733e-29
        cur_lr: 5.0e-05
        entropy: 0.05619082941363255
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037666975210110345
        model: {}
        policy_loss: -0.005019864761076557
        total_loss: 0.2395685389637947
        vf_explained_var: 0.999523937702179
        vf_loss: 0.24461649358272552
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.724999999999998
    gpu_util_percent0: 0.331875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624592155507646
    mean_env_wait_ms: 1.207537893113089
    mean_inference_ms: 4.296594356998051
    mean_raw_obs_processing_ms: 0.37751837720259707
  time_since_restore: 7196.602823019028
  time_this_iter_s: 26.821921586990356
  time_total_s: 7196.602823019028
  timers:
    learn_throughput: 8302.657
    learn_time_ms: 19486.774
    sample_throughput: 23774.661
    sample_time_ms: 6805.229
    update_time_ms: 32.209
  timestamp: 1602716725
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: a052f_00000
  
2020-10-14 23:05:26,296	WARNING util.py:136 -- The `process_trial` operation took 0.7081196308135986 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    271 |           7196.6 | 43845632 |  292.794 |              316.172 |              145.717 |            791.976 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3122.712064362508
    time_step_min: 2977
  date: 2020-10-14_23-05-52
  done: false
  episode_len_mean: 792.0009555747872
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.8512395700839
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 171
  episodes_total: 55464
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.994245657573366e-30
        cur_lr: 5.0e-05
        entropy: 0.054649573750793934
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038708472275175154
        model: {}
        policy_loss: -0.007184198613686021
        total_loss: 0.1827894685169061
        vf_explained_var: 0.9996063113212585
        vf_loss: 0.19000099102656046
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.66774193548387
    gpu_util_percent0: 0.3206451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624446207343897
    mean_env_wait_ms: 1.2074522406434651
    mean_inference_ms: 4.296520296049606
    mean_raw_obs_processing_ms: 0.377512695176479
  time_since_restore: 7223.150166988373
  time_this_iter_s: 26.547343969345093
  time_total_s: 7223.150166988373
  timers:
    learn_throughput: 8308.968
    learn_time_ms: 19471.974
    sample_throughput: 23754.521
    sample_time_ms: 6810.998
    update_time_ms: 32.072
  timestamp: 1602716752
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: a052f_00000
  
2020-10-14 23:05:53,819	WARNING util.py:136 -- The `process_trial` operation took 0.7210092544555664 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    272 |          7223.15 | 44007424 |  292.851 |              316.172 |              145.717 |            792.001 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3122.2368794581093
    time_step_min: 2977
  date: 2020-10-14_23-06-20
  done: false
  episode_len_mean: 792.028373888839
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.9231408998848
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 221
  episodes_total: 55685
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.497122828786683e-30
        cur_lr: 5.0e-05
        entropy: 0.06279692674676578
        entropy_coeff: 0.0005000000000000001
        kl: 0.005673448632781704
        model: {}
        policy_loss: -0.007950551326454539
        total_loss: 0.394114909072717
        vf_explained_var: 0.9992910027503967
        vf_loss: 0.402096855143706
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.853125
    gpu_util_percent0: 0.338125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624295666809878
    mean_env_wait_ms: 1.2073420433743278
    mean_inference_ms: 4.296426684232942
    mean_raw_obs_processing_ms: 0.3775051870658068
  time_since_restore: 7249.985733270645
  time_this_iter_s: 26.83556628227234
  time_total_s: 7249.985733270645
  timers:
    learn_throughput: 8298.451
    learn_time_ms: 19496.65
    sample_throughput: 23773.809
    sample_time_ms: 6805.472
    update_time_ms: 33.176
  timestamp: 1602716780
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: a052f_00000
  
2020-10-14 23:06:21,842	WARNING util.py:136 -- The `process_trial` operation took 0.7606635093688965 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    273 |          7249.99 | 44169216 |  292.923 |              316.172 |              145.717 |            792.028 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3121.767635374539
    time_step_min: 2977
  date: 2020-10-14_23-06-48
  done: false
  episode_len_mean: 792.058361652656
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 292.99468210995633
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 225
  episodes_total: 55910
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.497122828786683e-30
        cur_lr: 5.0e-05
        entropy: 0.059462227858603
        entropy_coeff: 0.0005000000000000001
        kl: 0.004724068528351684
        model: {}
        policy_loss: -0.006380362407071516
        total_loss: 0.3888514041900635
        vf_explained_var: 0.9993038773536682
        vf_loss: 0.395261491338412
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.751612903225812
    gpu_util_percent0: 0.26161290322580644
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462403261575445
    mean_env_wait_ms: 1.2072289592114176
    mean_inference_ms: 4.29632925157357
    mean_raw_obs_processing_ms: 0.3774974117574741
  time_since_restore: 7276.718945264816
  time_this_iter_s: 26.733211994171143
  time_total_s: 7276.718945264816
  timers:
    learn_throughput: 8297.911
    learn_time_ms: 19497.921
    sample_throughput: 23764.427
    sample_time_ms: 6808.159
    update_time_ms: 32.131
  timestamp: 1602716808
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: a052f_00000
  
2020-10-14 23:06:49,646	WARNING util.py:136 -- The `process_trial` operation took 0.7211627960205078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    274 |          7276.72 | 44331008 |  292.995 |              316.172 |              145.717 |            792.058 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3121.39905818558
    time_step_min: 2977
  date: 2020-10-14_23-07-16
  done: false
  episode_len_mean: 792.0814227134962
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.05062660839064
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 180
  episodes_total: 56090
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2485614143933416e-30
        cur_lr: 5.0e-05
        entropy: 0.060433530869583287
        entropy_coeff: 0.0005000000000000001
        kl: 0.003496014435465137
        model: {}
        policy_loss: -0.0099840450566262
        total_loss: 0.2593822826941808
        vf_explained_var: 0.9994398951530457
        vf_loss: 0.2693965385357539
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.409375000000004
    gpu_util_percent0: 0.253125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14623852952148658
    mean_env_wait_ms: 1.2071383662206896
    mean_inference_ms: 4.296252084584607
    mean_raw_obs_processing_ms: 0.3774913765510442
  time_since_restore: 7303.2316381931305
  time_this_iter_s: 26.51269292831421
  time_total_s: 7303.2316381931305
  timers:
    learn_throughput: 8298.631
    learn_time_ms: 19496.228
    sample_throughput: 23786.447
    sample_time_ms: 6801.856
    update_time_ms: 31.89
  timestamp: 1602716836
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: a052f_00000
  
2020-10-14 23:07:17,265	WARNING util.py:136 -- The `process_trial` operation took 0.7638463973999023 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    275 |          7303.23 | 44492800 |  293.051 |              316.172 |              145.717 |            792.081 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3121.0307916585184
    time_step_min: 2977
  date: 2020-10-14_23-07-43
  done: false
  episode_len_mean: 792.1039856424471
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.10794872333304
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 187
  episodes_total: 56277
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1242807071966708e-30
        cur_lr: 5.0e-05
        entropy: 0.057204028591513634
        entropy_coeff: 0.0005000000000000001
        kl: 0.004202285587477188
        model: {}
        policy_loss: -0.006872937859346469
        total_loss: 0.2567637500663598
        vf_explained_var: 0.999477207660675
        vf_loss: 0.26366528992851573
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.906451612903226
    gpu_util_percent0: 0.2925806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14623742525379946
    mean_env_wait_ms: 1.2070458113570504
    mean_inference_ms: 4.296173793917272
    mean_raw_obs_processing_ms: 0.37748534897055364
  time_since_restore: 7329.758726358414
  time_this_iter_s: 26.527088165283203
  time_total_s: 7329.758726358414
  timers:
    learn_throughput: 8293.798
    learn_time_ms: 19507.589
    sample_throughput: 23769.883
    sample_time_ms: 6806.596
    update_time_ms: 32.526
  timestamp: 1602716863
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: a052f_00000
  
2020-10-14 23:07:44,779	WARNING util.py:136 -- The `process_trial` operation took 0.7287130355834961 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    276 |          7329.76 | 44654592 |  293.108 |              316.172 |              145.717 |            792.104 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3120.549206742687
    time_step_min: 2977
  date: 2020-10-14_23-08-11
  done: false
  episode_len_mean: 792.1367336825712
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.18142739872144
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 227
  episodes_total: 56504
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.621403535983354e-31
        cur_lr: 5.0e-05
        entropy: 0.051898119350274406
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005831059977936093
        total_loss: .inf
        vf_explained_var: 0.9997653961181641
        vf_loss: 0.1348909636338552
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.329032258064515
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462351960232986
    mean_env_wait_ms: 1.2069324388632277
    mean_inference_ms: 4.296079471515737
    mean_raw_obs_processing_ms: 0.3774780492432347
  time_since_restore: 7356.033015727997
  time_this_iter_s: 26.27428936958313
  time_total_s: 7356.033015727997
  timers:
    learn_throughput: 8308.877
    learn_time_ms: 19472.186
    sample_throughput: 23808.922
    sample_time_ms: 6795.436
    update_time_ms: 30.995
  timestamp: 1602716891
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:08:12,076	WARNING util.py:136 -- The `process_trial` operation took 0.7599430084228516 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    277 |          7356.03 | 44816384 |  293.181 |              316.172 |              145.717 |            792.137 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3120.0962016830445
    time_step_min: 2977
  date: 2020-10-14_23-08-38
  done: false
  episode_len_mean: 792.1703373243286
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.2487793417945
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 207
  episodes_total: 56711
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.43210530397503e-31
        cur_lr: 5.0e-05
        entropy: 0.05053136063118776
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005251006650117536
        total_loss: .inf
        vf_explained_var: 0.9997861385345459
        vf_loss: 0.11312887445092201
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.924999999999997
    gpu_util_percent0: 0.34375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14623275483002127
    mean_env_wait_ms: 1.206828909608121
    mean_inference_ms: 4.295992094523692
    mean_raw_obs_processing_ms: 0.3774709058632437
  time_since_restore: 7382.617052078247
  time_this_iter_s: 26.584036350250244
  time_total_s: 7382.617052078247
  timers:
    learn_throughput: 8299.216
    learn_time_ms: 19494.855
    sample_throughput: 23823.378
    sample_time_ms: 6791.312
    update_time_ms: 29.221
  timestamp: 1602716918
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:08:39,862	WARNING util.py:136 -- The `process_trial` operation took 0.7470312118530273 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    278 |          7382.62 | 44978176 |  293.249 |              316.172 |              145.717 |             792.17 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3119.725600211063
    time_step_min: 2977
  date: 2020-10-14_23-09-06
  done: false
  episode_len_mean: 792.1993389940756
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.3055202624842
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 172
  episodes_total: 56883
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2648157955962549e-30
        cur_lr: 5.0e-05
        entropy: 0.05196822372575601
        entropy_coeff: 0.0005000000000000001
        kl: 0.005504032519335548
        model: {}
        policy_loss: -0.004529502883087844
        total_loss: 0.17555871916313967
        vf_explained_var: 0.9996540546417236
        vf_loss: 0.18011420344312987
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096777
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14623139297174773
    mean_env_wait_ms: 1.2067436642523952
    mean_inference_ms: 4.2959218220723425
    mean_raw_obs_processing_ms: 0.37746535564999256
  time_since_restore: 7409.046355962753
  time_this_iter_s: 26.429303884506226
  time_total_s: 7409.046355962753
  timers:
    learn_throughput: 8315.131
    learn_time_ms: 19457.54
    sample_throughput: 23836.875
    sample_time_ms: 6787.467
    update_time_ms: 28.983
  timestamp: 1602716946
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: a052f_00000
  
2020-10-14 23:09:07,314	WARNING util.py:136 -- The `process_trial` operation took 0.7604448795318604 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    279 |          7409.05 | 45139968 |  293.306 |              316.172 |              145.717 |            792.199 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3119.3064414382843
    time_step_min: 2977
  date: 2020-10-14_23-09-33
  done: false
  episode_len_mean: 792.2268109850077
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.36627200971435
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 57096
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2648157955962549e-30
        cur_lr: 5.0e-05
        entropy: 0.05950875207781792
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009483435385239622
        total_loss: .inf
        vf_explained_var: 0.9988502860069275
        vf_loss: 0.6341047883033752
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7741935483871
    gpu_util_percent0: 0.31129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462302124891966
    mean_env_wait_ms: 1.2066397135292022
    mean_inference_ms: 4.295838674873276
    mean_raw_obs_processing_ms: 0.37745867280666284
  time_since_restore: 7435.49533367157
  time_this_iter_s: 26.44897770881653
  time_total_s: 7435.49533367157
  timers:
    learn_throughput: 8319.16
    learn_time_ms: 19448.117
    sample_throughput: 23911.704
    sample_time_ms: 6766.226
    update_time_ms: 29.001
  timestamp: 1602716973
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:09:34,821	WARNING util.py:136 -- The `process_trial` operation took 0.7977252006530762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    280 |           7435.5 | 45301760 |  293.366 |              316.172 |              145.717 |            792.227 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3118.8902502879882
    time_step_min: 2977
  date: 2020-10-14_23-10-01
  done: false
  episode_len_mean: 792.2593419629461
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.4330204103063
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 226
  episodes_total: 57322
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8972236933943822e-30
        cur_lr: 5.0e-05
        entropy: 0.05464684156080087
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0062220122369277915
        total_loss: .inf
        vf_explained_var: 0.9993768334388733
        vf_loss: 0.3672540287176768
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.643749999999997
    gpu_util_percent0: 0.273125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14622752680400358
    mean_env_wait_ms: 1.2065266089220228
    mean_inference_ms: 4.295744930530063
    mean_raw_obs_processing_ms: 0.3774508829852417
  time_since_restore: 7462.041475534439
  time_this_iter_s: 26.546141862869263
  time_total_s: 7462.041475534439
  timers:
    learn_throughput: 8336.504
    learn_time_ms: 19407.656
    sample_throughput: 23894.387
    sample_time_ms: 6771.13
    update_time_ms: 26.926
  timestamp: 1602717001
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:10:02,437	WARNING util.py:136 -- The `process_trial` operation took 0.7824211120605469 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    281 |          7462.04 | 45463552 |  293.433 |              316.172 |              145.717 |            792.259 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3118.508715771894
    time_step_min: 2977
  date: 2020-10-14_23-10-28
  done: false
  episode_len_mean: 792.2843853242914
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.4912874177346
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 57510
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8458355400915735e-30
        cur_lr: 5.0e-05
        entropy: 0.05628476136674484
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008198123046895489
        total_loss: .inf
        vf_explained_var: 0.9995068907737732
        vf_loss: 0.2402379003663858
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.916129032258066
    gpu_util_percent0: 0.37903225806451607
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14622554077232025
    mean_env_wait_ms: 1.2064341114332071
    mean_inference_ms: 4.29566814715192
    mean_raw_obs_processing_ms: 0.37744494581670246
  time_since_restore: 7488.510948419571
  time_this_iter_s: 26.469472885131836
  time_total_s: 7488.510948419571
  timers:
    learn_throughput: 8334.447
    learn_time_ms: 19412.446
    sample_throughput: 23906.358
    sample_time_ms: 6767.739
    update_time_ms: 26.681
  timestamp: 1602717028
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:10:29,920	WARNING util.py:136 -- The `process_trial` operation took 0.7572965621948242 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    282 |          7488.51 | 45625344 |  293.491 |              316.172 |              145.717 |            792.284 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3118.142222915908
    time_step_min: 2977
  date: 2020-10-14_23-10-56
  done: false
  episode_len_mean: 792.3093376783207
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.54670555024137
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 181
  episodes_total: 57691
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2687533101373594e-30
        cur_lr: 5.0e-05
        entropy: 0.061152843137582145
        entropy_coeff: 0.0005000000000000001
        kl: 0.004430043666313092
        model: {}
        policy_loss: -0.0078003883245401084
        total_loss: 0.27684808398286503
        vf_explained_var: 0.9994304180145264
        vf_loss: 0.28467904527982074
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.59375
    gpu_util_percent0: 0.31562500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14622441149894722
    mean_env_wait_ms: 1.2063452222965696
    mean_inference_ms: 4.295597613996398
    mean_raw_obs_processing_ms: 0.3774395900477023
  time_since_restore: 7515.140010595322
  time_this_iter_s: 26.629062175750732
  time_total_s: 7515.140010595322
  timers:
    learn_throughput: 8341.119
    learn_time_ms: 19396.917
    sample_throughput: 23918.032
    sample_time_ms: 6764.436
    update_time_ms: 26.963
  timestamp: 1602717056
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: a052f_00000
  
2020-10-14 23:10:57,758	WARNING util.py:136 -- The `process_trial` operation took 0.7862548828125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    283 |          7515.14 | 45787136 |  293.547 |              316.172 |              145.717 |            792.309 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3117.72938798777
    time_step_min: 2977
  date: 2020-10-14_23-11-24
  done: false
  episode_len_mean: 792.3380928538131
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.6113023395087
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 228
  episodes_total: 57919
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1343766550686797e-30
        cur_lr: 5.0e-05
        entropy: 0.06097988256563743
        entropy_coeff: 0.0005000000000000001
        kl: 0.003905188544498136
        model: {}
        policy_loss: -0.008617532623854155
        total_loss: 0.7112613221009573
        vf_explained_var: 0.9987397789955139
        vf_loss: 0.7199093153079351
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.28387096774194
    gpu_util_percent0: 0.3490322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146222632114994
    mean_env_wait_ms: 1.2062346411927913
    mean_inference_ms: 4.29551032193891
    mean_raw_obs_processing_ms: 0.3774324228306602
  time_since_restore: 7541.647116661072
  time_this_iter_s: 26.507106065750122
  time_total_s: 7541.647116661072
  timers:
    learn_throughput: 8350.394
    learn_time_ms: 19375.373
    sample_throughput: 23926.267
    sample_time_ms: 6762.108
    update_time_ms: 26.908
  timestamp: 1602717084
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: a052f_00000
  
2020-10-14 23:11:25,311	WARNING util.py:136 -- The `process_trial` operation took 0.7783136367797852 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    284 |          7541.65 | 45948928 |  293.611 |              316.172 |              145.717 |            792.338 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3117.314384551838
    time_step_min: 2977
  date: 2020-10-14_23-11-51
  done: false
  episode_len_mean: 792.364463634487
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.67416979260685
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 58132
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0671883275343399e-30
        cur_lr: 5.0e-05
        entropy: 0.05744898443420728
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035321548930369318
        model: {}
        policy_loss: -0.009139513470775759
        total_loss: 0.3997315342227618
        vf_explained_var: 0.9992391467094421
        vf_loss: 0.40889976918697357
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.525806451612905
    gpu_util_percent0: 0.3612903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14622022280469418
    mean_env_wait_ms: 1.2061295877356024
    mean_inference_ms: 4.295425905412619
    mean_raw_obs_processing_ms: 0.3774254535159622
  time_since_restore: 7568.067651510239
  time_this_iter_s: 26.42053484916687
  time_total_s: 7568.067651510239
  timers:
    learn_throughput: 8349.484
    learn_time_ms: 19377.485
    sample_throughput: 23941.204
    sample_time_ms: 6757.889
    update_time_ms: 26.695
  timestamp: 1602717111
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: a052f_00000
  
2020-10-14 23:11:52,747	WARNING util.py:136 -- The `process_trial` operation took 0.7435903549194336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    285 |          7568.07 | 46110720 |  293.674 |              316.172 |              145.717 |            792.364 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3116.999725444437
    time_step_min: 2977
  date: 2020-10-14_23-12-19
  done: false
  episode_len_mean: 792.3847420417125
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.7241227450129
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 172
  episodes_total: 58304
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.335941637671699e-31
        cur_lr: 5.0e-05
        entropy: 0.05818386779477199
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047273284289985895
        model: {}
        policy_loss: -0.006164547870866954
        total_loss: 0.29597846418619156
        vf_explained_var: 0.9993605613708496
        vf_loss: 0.30217210700114566
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.409374999999997
    gpu_util_percent0: 0.2903125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14621885753063218
    mean_env_wait_ms: 1.2060455468561373
    mean_inference_ms: 4.295357535127827
    mean_raw_obs_processing_ms: 0.3774200976207794
  time_since_restore: 7594.786272287369
  time_this_iter_s: 26.718620777130127
  time_total_s: 7594.786272287369
  timers:
    learn_throughput: 8354.912
    learn_time_ms: 19364.895
    sample_throughput: 23870.676
    sample_time_ms: 6777.856
    update_time_ms: 27.111
  timestamp: 1602717139
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: a052f_00000
  
2020-10-14 23:12:20,476	WARNING util.py:136 -- The `process_trial` operation took 0.7430605888366699 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    286 |          7594.79 | 46272512 |  293.724 |              316.172 |              145.717 |            792.385 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3116.6029856871696
    time_step_min: 2977
  date: 2020-10-14_23-12-46
  done: false
  episode_len_mean: 792.4096603825184
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.78247962386587
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 203
  episodes_total: 58507
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6679708188358496e-31
        cur_lr: 5.0e-05
        entropy: 0.06302285970499118
        entropy_coeff: 0.0005000000000000001
        kl: 0.004029669294444223
        model: {}
        policy_loss: -0.006659353661234491
        total_loss: 0.3893429910143216
        vf_explained_var: 0.9992954730987549
        vf_loss: 0.3960338483254115
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.42258064516129
    gpu_util_percent0: 0.2964516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462175508994676
    mean_env_wait_ms: 1.2059475206087762
    mean_inference_ms: 4.295277935836819
    mean_raw_obs_processing_ms: 0.3774139701967837
  time_since_restore: 7621.2717089653015
  time_this_iter_s: 26.48543667793274
  time_total_s: 7621.2717089653015
  timers:
    learn_throughput: 8354.294
    learn_time_ms: 19366.329
    sample_throughput: 23835.554
    sample_time_ms: 6787.843
    update_time_ms: 26.706
  timestamp: 1602717166
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: a052f_00000
  
2020-10-14 23:12:47,990	WARNING util.py:136 -- The `process_trial` operation took 0.751859188079834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    287 |          7621.27 | 46434304 |  293.782 |              316.172 |              145.717 |             792.41 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3116.200190771432
    time_step_min: 2977
  date: 2020-10-14_23-13-14
  done: false
  episode_len_mean: 792.4327425643121
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.84507468061247
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 230
  episodes_total: 58737
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3339854094179248e-31
        cur_lr: 5.0e-05
        entropy: 0.06118481823553642
        entropy_coeff: 0.0005000000000000001
        kl: 0.004640437779016793
        model: {}
        policy_loss: -0.008816241150877127
        total_loss: 0.48408886790275574
        vf_explained_var: 0.9991456866264343
        vf_loss: 0.4929356773694356
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.987096774193553
    gpu_util_percent0: 0.3519354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14621526573679952
    mean_env_wait_ms: 1.205835905726541
    mean_inference_ms: 4.295190467149868
    mean_raw_obs_processing_ms: 0.37740675196927664
  time_since_restore: 7647.903096199036
  time_this_iter_s: 26.63138723373413
  time_total_s: 7647.903096199036
  timers:
    learn_throughput: 8364.349
    learn_time_ms: 19343.047
    sample_throughput: 23752.687
    sample_time_ms: 6811.524
    update_time_ms: 28.928
  timestamp: 1602717194
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: a052f_00000
  
2020-10-14 23:13:15,735	WARNING util.py:136 -- The `process_trial` operation took 0.7805831432342529 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    288 |           7647.9 | 46596096 |  293.845 |              316.172 |              145.717 |            792.433 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3115.842308998302
    time_step_min: 2977
  date: 2020-10-14_23-13-42
  done: false
  episode_len_mean: 792.4531971219114
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.89765030840545
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 191
  episodes_total: 58928
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.669927047089624e-32
        cur_lr: 5.0e-05
        entropy: 0.060254870603481926
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008878909997292794
        total_loss: .inf
        vf_explained_var: 0.9992330074310303
        vf_loss: 0.3781590387225151
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.26451612903226
    gpu_util_percent0: 0.3509677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462133099080684
    mean_env_wait_ms: 1.205742386682631
    mean_inference_ms: 4.295120009393076
    mean_raw_obs_processing_ms: 0.377401022262093
  time_since_restore: 7674.4518394470215
  time_this_iter_s: 26.54874324798584
  time_total_s: 7674.4518394470215
  timers:
    learn_throughput: 8355.117
    learn_time_ms: 19364.42
    sample_throughput: 23781.634
    sample_time_ms: 6803.233
    update_time_ms: 27.706
  timestamp: 1602717222
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:13:43,454	WARNING util.py:136 -- The `process_trial` operation took 0.8066470623016357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    289 |          7674.45 | 46757888 |  293.898 |              316.172 |              145.717 |            792.453 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3115.508886255924
    time_step_min: 2977
  date: 2020-10-14_23-14-09
  done: false
  episode_len_mean: 792.472068078771
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 293.9489933851608
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 180
  episodes_total: 59108
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0004890570634433e-31
        cur_lr: 5.0e-05
        entropy: 0.05836738118280967
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049597189063206315
        model: {}
        policy_loss: -0.007285191260355835
        total_loss: 0.2536200446387132
        vf_explained_var: 0.9994787573814392
        vf_loss: 0.26093441992998123
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.278125000000003
    gpu_util_percent0: 0.3053125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14621199749927663
    mean_env_wait_ms: 1.2056552997458079
    mean_inference_ms: 4.295051991769886
    mean_raw_obs_processing_ms: 0.37739578830799847
  time_since_restore: 7700.889623880386
  time_this_iter_s: 26.437784433364868
  time_total_s: 7700.889623880386
  timers:
    learn_throughput: 8360.948
    learn_time_ms: 19350.916
    sample_throughput: 23759.924
    sample_time_ms: 6809.449
    update_time_ms: 27.659
  timestamp: 1602717249
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: a052f_00000
  
2020-10-14 23:14:10,940	WARNING util.py:136 -- The `process_trial` operation took 0.7625861167907715 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    290 |          7700.89 | 46919680 |  293.949 |              316.172 |              145.717 |            792.472 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3115.07344461305
    time_step_min: 2977
  date: 2020-10-14_23-14-37
  done: false
  episode_len_mean: 792.4964440999023
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.01497669063116
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 230
  episodes_total: 59338
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.002445285317217e-32
        cur_lr: 5.0e-05
        entropy: 0.05956661514937878
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0078764757126919
        total_loss: .inf
        vf_explained_var: 0.9992141127586365
        vf_loss: 0.44344160457452136
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89032258064516
    gpu_util_percent0: 0.3190322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462106561001831
    mean_env_wait_ms: 1.2055459539172366
    mean_inference_ms: 4.294969930093151
    mean_raw_obs_processing_ms: 0.3773889379154096
  time_since_restore: 7727.413649559021
  time_this_iter_s: 26.524025678634644
  time_total_s: 7727.413649559021
  timers:
    learn_throughput: 8356.256
    learn_time_ms: 19361.781
    sample_throughput: 23761.311
    sample_time_ms: 6809.052
    update_time_ms: 28.009
  timestamp: 1602717277
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:14:38,517	WARNING util.py:136 -- The `process_trial` operation took 0.7753117084503174 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    291 |          7727.41 | 47081472 |  294.015 |              316.172 |              145.717 |            792.496 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3114.6736500789625
    time_step_min: 2977
  date: 2020-10-14_23-15-05
  done: false
  episode_len_mean: 792.523946263644
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.0761086939925
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 212
  episodes_total: 59550
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.503667927975826e-32
        cur_lr: 5.0e-05
        entropy: 0.05709661139796177
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006159159335462998
        total_loss: .inf
        vf_explained_var: 0.9995601773262024
        vf_loss: 0.24428335949778557
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.15625
    gpu_util_percent0: 0.30781250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14620832080825663
    mean_env_wait_ms: 1.205443637242567
    mean_inference_ms: 4.29489106646833
    mean_raw_obs_processing_ms: 0.37738209633826325
  time_since_restore: 7754.193971157074
  time_this_iter_s: 26.78032159805298
  time_total_s: 7754.193971157074
  timers:
    learn_throughput: 8345.457
    learn_time_ms: 19386.834
    sample_throughput: 23781.87
    sample_time_ms: 6803.166
    update_time_ms: 35.381
  timestamp: 1602717305
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:15:06,391	WARNING util.py:136 -- The `process_trial` operation took 0.7953050136566162 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    292 |          7754.19 | 47243264 |  294.076 |              316.172 |              145.717 |            792.524 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3114.363110991993
    time_step_min: 2977
  date: 2020-10-14_23-15-32
  done: false
  episode_len_mean: 792.5392124033084
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.123578652276
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 176
  episodes_total: 59726
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1255501891963737e-31
        cur_lr: 5.0e-05
        entropy: 0.05914490421613058
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007759780710330233
        total_loss: .inf
        vf_explained_var: 0.999126672744751
        vf_loss: 0.41702838987112045
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.36451612903226
    gpu_util_percent0: 0.3574193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14620692617098735
    mean_env_wait_ms: 1.2053590769429128
    mean_inference_ms: 4.294825165900863
    mean_raw_obs_processing_ms: 0.3773769877982078
  time_since_restore: 7780.575129032135
  time_this_iter_s: 26.381157875061035
  time_total_s: 7780.575129032135
  timers:
    learn_throughput: 8354.633
    learn_time_ms: 19365.543
    sample_throughput: 23791.475
    sample_time_ms: 6800.419
    update_time_ms: 34.073
  timestamp: 1602717332
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:15:33,869	WARNING util.py:136 -- The `process_trial` operation took 0.8123929500579834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    293 |          7780.58 | 47405056 |  294.124 |              316.172 |              145.717 |            792.539 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3113.9963938695782
    time_step_min: 2977
  date: 2020-10-14_23-16-00
  done: false
  episode_len_mean: 792.55463404866
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.18064788323073
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 200
  episodes_total: 59926
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6883252837945606e-31
        cur_lr: 5.0e-05
        entropy: 0.0630206090087692
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005745178049740692
        total_loss: .inf
        vf_explained_var: 0.9996446967124939
        vf_loss: 0.19668332735697427
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.33125
    gpu_util_percent0: 0.27218749999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14620576993055523
    mean_env_wait_ms: 1.2052647559707903
    mean_inference_ms: 4.2947515489640296
    mean_raw_obs_processing_ms: 0.3773712560720791
  time_since_restore: 7807.283318758011
  time_this_iter_s: 26.708189725875854
  time_total_s: 7807.283318758011
  timers:
    learn_throughput: 8348.209
    learn_time_ms: 19380.445
    sample_throughput: 23801.006
    sample_time_ms: 6797.696
    update_time_ms: 32.759
  timestamp: 1602717360
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:16:01,725	WARNING util.py:136 -- The `process_trial` operation took 0.8684098720550537 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    294 |          7807.28 | 47566848 |  294.181 |              316.172 |              145.717 |            792.555 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3113.555827220864
    time_step_min: 2977
  date: 2020-10-14_23-16-28
  done: false
  episode_len_mean: 792.5855264251634
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.246559626287
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 225
  episodes_total: 60151
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5324879256918406e-31
        cur_lr: 5.0e-05
        entropy: 0.05822804570198059
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005492397938117695
        total_loss: .inf
        vf_explained_var: 0.9997287392616272
        vf_loss: 0.18021756783127785
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73225806451613
    gpu_util_percent0: 0.3393548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14620368837498862
    mean_env_wait_ms: 1.205157586687283
    mean_inference_ms: 4.294672516173975
    mean_raw_obs_processing_ms: 0.37736440288519235
  time_since_restore: 7833.839502096176
  time_this_iter_s: 26.556183338165283
  time_total_s: 7833.839502096176
  timers:
    learn_throughput: 8346.124
    learn_time_ms: 19385.286
    sample_throughput: 23777.827
    sample_time_ms: 6804.322
    update_time_ms: 34.412
  timestamp: 1602717388
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:16:29,380	WARNING util.py:136 -- The `process_trial` operation took 0.8159074783325195 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    295 |          7833.84 | 47728640 |  294.247 |              316.172 |              145.717 |            792.586 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3113.192092175066
    time_step_min: 2977
  date: 2020-10-14_23-16-55
  done: false
  episode_len_mean: 792.6107410353284
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.30269638119074
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 197
  episodes_total: 60348
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7987318885377607e-31
        cur_lr: 5.0e-05
        entropy: 0.05898841594656309
        entropy_coeff: 0.0005000000000000001
        kl: 0.004386427851083378
        model: {}
        policy_loss: -0.007457565555038552
        total_loss: 0.1609960806866487
        vf_explained_var: 0.9996671080589294
        vf_loss: 0.16848314180970192
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.19354838709678
    gpu_util_percent0: 0.2906451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462017114683141
    mean_env_wait_ms: 1.2050623380003416
    mean_inference_ms: 4.294600679676515
    mean_raw_obs_processing_ms: 0.37735867148747354
  time_since_restore: 7860.341908931732
  time_this_iter_s: 26.50240683555603
  time_total_s: 7860.341908931732
  timers:
    learn_throughput: 8341.635
    learn_time_ms: 19395.719
    sample_throughput: 23857.954
    sample_time_ms: 6781.47
    update_time_ms: 33.653
  timestamp: 1602717415
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: a052f_00000
  
2020-10-14 23:16:57,084	WARNING util.py:136 -- The `process_trial` operation took 0.8184714317321777 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    296 |          7860.34 | 47890432 |  294.303 |              316.172 |              145.717 |            792.611 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3112.8579268393473
    time_step_min: 2977
  date: 2020-10-14_23-17-23
  done: false
  episode_len_mean: 792.6323667905824
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.3518756336597
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 60525
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8993659442688803e-31
        cur_lr: 5.0e-05
        entropy: 0.060839926823973656
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006943760323338211
        total_loss: .inf
        vf_explained_var: 0.9996166825294495
        vf_loss: 0.18854885920882225
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.659375
    gpu_util_percent0: 0.2884375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14620049629474163
    mean_env_wait_ms: 1.2049784102187069
    mean_inference_ms: 4.294540404765186
    mean_raw_obs_processing_ms: 0.37735353652116316
  time_since_restore: 7886.964926242828
  time_this_iter_s: 26.62301731109619
  time_total_s: 7886.964926242828
  timers:
    learn_throughput: 8331.976
    learn_time_ms: 19418.202
    sample_throughput: 23864.208
    sample_time_ms: 6779.693
    update_time_ms: 33.413
  timestamp: 1602717443
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:17:24,828	WARNING util.py:136 -- The `process_trial` operation took 0.8433096408843994 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    297 |          7886.96 | 48052224 |  294.352 |              316.172 |              145.717 |            792.632 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3112.4525722943154
    time_step_min: 2977
  date: 2020-10-14_23-17-51
  done: false
  episode_len_mean: 792.6569989465368
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.41183496806326
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 227
  episodes_total: 60752
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.849048916403321e-31
        cur_lr: 5.0e-05
        entropy: 0.06412857646743457
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009330665793337781
        total_loss: .inf
        vf_explained_var: 0.9993536472320557
        vf_loss: 0.3626266767581304
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.471875
    gpu_util_percent0: 0.265625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14619925389616012
    mean_env_wait_ms: 1.2048719423990144
    mean_inference_ms: 4.294465138664234
    mean_raw_obs_processing_ms: 0.3773472186642237
  time_since_restore: 7913.43562746048
  time_this_iter_s: 26.470701217651367
  time_total_s: 7913.43562746048
  timers:
    learn_throughput: 8333.657
    learn_time_ms: 19414.287
    sample_throughput: 23905.763
    sample_time_ms: 6767.908
    update_time_ms: 31.197
  timestamp: 1602717471
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:17:52,579	WARNING util.py:136 -- The `process_trial` operation took 0.8053882122039795 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    298 |          7913.44 | 48214016 |  294.412 |              316.172 |              145.717 |            792.657 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3112.0875644097277
    time_step_min: 2977
  date: 2020-10-14_23-18-19
  done: false
  episode_len_mean: 792.67645900994
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.46623105377137
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 214
  episodes_total: 60966
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2735733746049805e-31
        cur_lr: 5.0e-05
        entropy: 0.06358475672701995
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009138115613798922
        total_loss: .inf
        vf_explained_var: 0.9991839528083801
        vf_loss: 0.45169899115959805
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.590625000000003
    gpu_util_percent0: 0.27125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14619701303978586
    mean_env_wait_ms: 1.204770223035243
    mean_inference_ms: 4.294388484940779
    mean_raw_obs_processing_ms: 0.37734065903804753
  time_since_restore: 7940.139231920242
  time_this_iter_s: 26.703604459762573
  time_total_s: 7940.139231920242
  timers:
    learn_throughput: 8327.586
    learn_time_ms: 19428.439
    sample_throughput: 23908.426
    sample_time_ms: 6767.154
    update_time_ms: 30.877
  timestamp: 1602717499
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:18:20,539	WARNING util.py:136 -- The `process_trial` operation took 0.7854692935943604 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    299 |          7940.14 | 48375808 |  294.466 |              316.172 |              145.717 |            792.676 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3111.7634941671167
    time_step_min: 2977
  date: 2020-10-14_23-18-47
  done: false
  episode_len_mean: 792.6961420838308
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.5146349590064
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 181
  episodes_total: 61147
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.410360061907473e-31
        cur_lr: 5.0e-05
        entropy: 0.06040771988530954
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008512294768782644
        total_loss: .inf
        vf_explained_var: 0.9993852972984314
        vf_loss: 0.2983529654641946
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.453125
    gpu_util_percent0: 0.33875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14619543371008348
    mean_env_wait_ms: 1.2046851046091995
    mean_inference_ms: 4.294324232915274
    mean_raw_obs_processing_ms: 0.37733558989396226
  time_since_restore: 7966.964317560196
  time_this_iter_s: 26.825085639953613
  time_total_s: 7966.964317560196
  timers:
    learn_throughput: 8312.498
    learn_time_ms: 19463.704
    sample_throughput: 23912.066
    sample_time_ms: 6766.124
    update_time_ms: 31.138
  timestamp: 1602717527
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:18:48,482	WARNING util.py:136 -- The `process_trial` operation took 0.8110954761505127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    300 |          7966.96 | 48537600 |  294.515 |              316.172 |              145.717 |            792.696 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3111.4139578882127
    time_step_min: 2977
  date: 2020-10-14_23-19-15
  done: false
  episode_len_mean: 792.7114654146493
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.56515168805447
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 194
  episodes_total: 61341
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.615540092861206e-31
        cur_lr: 5.0e-05
        entropy: 0.06135720821718375
        entropy_coeff: 0.0005000000000000001
        kl: 0.004598445608280599
        model: {}
        policy_loss: -0.008458747459978136
        total_loss: 0.5931808948516846
        vf_explained_var: 0.9988665580749512
        vf_loss: 0.6016703198353449
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15806451612903
    gpu_util_percent0: 0.2725806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461945090647996
    mean_env_wait_ms: 1.2045952494598595
    mean_inference_ms: 4.294254688224099
    mean_raw_obs_processing_ms: 0.3773300675083907
  time_since_restore: 7993.615207910538
  time_this_iter_s: 26.650890350341797
  time_total_s: 7993.615207910538
  timers:
    learn_throughput: 8310.599
    learn_time_ms: 19468.15
    sample_throughput: 23886.761
    sample_time_ms: 6773.292
    update_time_ms: 30.85
  timestamp: 1602717555
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: a052f_00000
  
2020-10-14 23:19:16,323	WARNING util.py:136 -- The `process_trial` operation took 0.8126528263092041 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    301 |          7993.62 | 48699392 |  294.565 |              316.172 |              145.717 |            792.711 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3111.017160128701
    time_step_min: 2977
  date: 2020-10-14_23-19-42
  done: false
  episode_len_mean: 792.7327420979112
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.62723276030914
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 225
  episodes_total: 61566
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.807770046430603e-31
        cur_lr: 5.0e-05
        entropy: 0.06139330410708984
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00524083372632352
        total_loss: .inf
        vf_explained_var: 0.9996421933174133
        vf_loss: 0.2046442317465941
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.262500000000003
    gpu_util_percent0: 0.26093750000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461923968218978
    mean_env_wait_ms: 1.2044899518845595
    mean_inference_ms: 4.2941776498337525
    mean_raw_obs_processing_ms: 0.3773237723253872
  time_since_restore: 8020.143649339676
  time_this_iter_s: 26.528441429138184
  time_total_s: 8020.143649339676
  timers:
    learn_throughput: 8319.399
    learn_time_ms: 19447.56
    sample_throughput: 23878.998
    sample_time_ms: 6775.494
    update_time_ms: 24.452
  timestamp: 1602717582
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:19:43,994	WARNING util.py:136 -- The `process_trial` operation took 0.8578934669494629 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    302 |          8020.14 | 48861184 |  294.627 |              316.172 |              145.717 |            792.733 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3110.6602471695364
    time_step_min: 2977
  date: 2020-10-14_23-20-10
  done: false
  episode_len_mean: 792.7503359399032
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.68205735042363
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 201
  episodes_total: 61767
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.211655069645907e-31
        cur_lr: 5.0e-05
        entropy: 0.06087139621376991
        entropy_coeff: 0.0005000000000000001
        kl: 0.0067653409981479245
        model: {}
        policy_loss: -0.0077785900987995165
        total_loss: 0.18994318197170892
        vf_explained_var: 0.9996169209480286
        vf_loss: 0.1977522075176239
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.174193548387095
    gpu_util_percent0: 0.3132258064516128
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146190484599764
    mean_env_wait_ms: 1.2043950372720702
    mean_inference_ms: 4.294113124292588
    mean_raw_obs_processing_ms: 0.3773180095107654
  time_since_restore: 8046.579409837723
  time_this_iter_s: 26.435760498046875
  time_total_s: 8046.579409837723
  timers:
    learn_throughput: 8316.166
    learn_time_ms: 19455.119
    sample_throughput: 23889.307
    sample_time_ms: 6772.57
    update_time_ms: 24.077
  timestamp: 1602717610
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: a052f_00000
  
2020-10-14 23:20:11,552	WARNING util.py:136 -- The `process_trial` operation took 0.8367455005645752 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    303 |          8046.58 | 49022976 |  294.682 |              316.172 |              145.717 |             792.75 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3110.353176782196
    time_step_min: 2977
  date: 2020-10-14_23-20-37
  done: false
  episode_len_mean: 792.7656830142382
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.72946639415784
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 61946
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.211655069645907e-31
        cur_lr: 5.0e-05
        entropy: 0.06337680295109749
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00772029875467221
        total_loss: .inf
        vf_explained_var: 0.9996450543403625
        vf_loss: 0.20037476097544035
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.234375
    gpu_util_percent0: 0.2928125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461892495950274
    mean_env_wait_ms: 1.2043115173355792
    mean_inference_ms: 4.294052253911179
    mean_raw_obs_processing_ms: 0.37731309480268876
  time_since_restore: 8072.979758739471
  time_this_iter_s: 26.400348901748657
  time_total_s: 8072.979758739471
  timers:
    learn_throughput: 8326.221
    learn_time_ms: 19431.624
    sample_throughput: 23893.811
    sample_time_ms: 6771.293
    update_time_ms: 25.606
  timestamp: 1602717637
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:20:39,278	WARNING util.py:136 -- The `process_trial` operation took 0.8714616298675537 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    304 |          8072.98 | 49184768 |  294.729 |              316.172 |              145.717 |            792.766 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3109.9776304354823
    time_step_min: 2977
  date: 2020-10-14_23-21-05
  done: false
  episode_len_mean: 792.7820191101245
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.78615515252875
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 220
  episodes_total: 62166
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.081748260446886e-30
        cur_lr: 5.0e-05
        entropy: 0.0691356435418129
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036323206732049584
        model: {}
        policy_loss: -0.008650666626635939
        total_loss: 0.4714470331867536
        vf_explained_var: 0.999146044254303
        vf_loss: 0.4801322544614474
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.629032258064516
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461882539066178
    mean_env_wait_ms: 1.204210748273723
    mean_inference_ms: 4.293987167580744
    mean_raw_obs_processing_ms: 0.37730744824078816
  time_since_restore: 8099.334144353867
  time_this_iter_s: 26.35438561439514
  time_total_s: 8099.334144353867
  timers:
    learn_throughput: 8335.964
    learn_time_ms: 19408.914
    sample_throughput: 23881.405
    sample_time_ms: 6774.811
    update_time_ms: 23.778
  timestamp: 1602717665
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: a052f_00000
  
2020-10-14 23:21:06,764	WARNING util.py:136 -- The `process_trial` operation took 0.8448662757873535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    305 |          8099.33 | 49346560 |  294.786 |              316.172 |              145.717 |            792.782 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3109.6392180759794
    time_step_min: 2977
  date: 2020-10-14_23-21-33
  done: false
  episode_len_mean: 792.7977463253562
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.83878359144006
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 221
  episodes_total: 62387
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.40874130223443e-31
        cur_lr: 5.0e-05
        entropy: 0.06464044190943241
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006323775971395662
        total_loss: .inf
        vf_explained_var: 0.9993930459022522
        vf_loss: 0.34687017401059467
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.775
    gpu_util_percent0: 0.291875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14618589676638025
    mean_env_wait_ms: 1.2041069186036097
    mean_inference_ms: 4.293909420098973
    mean_raw_obs_processing_ms: 0.377300638912707
  time_since_restore: 8125.872774362564
  time_this_iter_s: 26.53863000869751
  time_total_s: 8125.872774362564
  timers:
    learn_throughput: 8333.946
    learn_time_ms: 19413.612
    sample_throughput: 23887.082
    sample_time_ms: 6773.201
    update_time_ms: 23.623
  timestamp: 1602717693
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:21:34,570	WARNING util.py:136 -- The `process_trial` operation took 0.8075792789459229 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    306 |          8125.87 | 49508352 |  294.839 |              316.172 |              145.717 |            792.798 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3109.3245071233273
    time_step_min: 2977
  date: 2020-10-14_23-22-01
  done: false
  episode_len_mean: 792.817065959181
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.8872176833944
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 182
  episodes_total: 62569
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.113111953351644e-31
        cur_lr: 5.0e-05
        entropy: 0.0600414980823795
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005785029750162114
        total_loss: .inf
        vf_explained_var: 0.9996640682220459
        vf_loss: 0.1633708563943704
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.683870967741935
    gpu_util_percent0: 0.3783870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14618448383905078
    mean_env_wait_ms: 1.2040232997844795
    mean_inference_ms: 4.293850825278912
    mean_raw_obs_processing_ms: 0.377295725282297
  time_since_restore: 8152.321974277496
  time_this_iter_s: 26.44919991493225
  time_total_s: 8152.321974277496
  timers:
    learn_throughput: 8342.274
    learn_time_ms: 19394.233
    sample_throughput: 23881.613
    sample_time_ms: 6774.752
    update_time_ms: 23.558
  timestamp: 1602717721
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:22:02,165	WARNING util.py:136 -- The `process_trial` operation took 0.8499622344970703 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    307 |          8152.32 | 49670144 |  294.887 |              316.172 |              145.717 |            792.817 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3109.011510011478
    time_step_min: 2977
  date: 2020-10-14_23-22-28
  done: false
  episode_len_mean: 792.8335776658805
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.9338154957694
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 187
  episodes_total: 62756
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2169667930027464e-30
        cur_lr: 5.0e-05
        entropy: 0.05931513570249081
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009415427348964537
        total_loss: .inf
        vf_explained_var: 0.9994047284126282
        vf_loss: 0.311582679549853
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.709375
    gpu_util_percent0: 0.268125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461833311955105
    mean_env_wait_ms: 1.2039375083631774
    mean_inference_ms: 4.293787019740742
    mean_raw_obs_processing_ms: 0.3772906517445834
  time_since_restore: 8179.109265327454
  time_this_iter_s: 26.787291049957275
  time_total_s: 8179.109265327454
  timers:
    learn_throughput: 8329.565
    learn_time_ms: 19423.824
    sample_throughput: 23881.138
    sample_time_ms: 6774.887
    update_time_ms: 23.466
  timestamp: 1602717748
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:22:30,231	WARNING util.py:136 -- The `process_trial` operation took 0.8219046592712402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    308 |          8179.11 | 49831936 |  294.934 |              316.172 |              145.717 |            792.834 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3108.614256897346
    time_step_min: 2977
  date: 2020-10-14_23-22-56
  done: false
  episode_len_mean: 792.8570339911411
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 294.9949471696337
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 231
  episodes_total: 62987
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8254501895041197e-30
        cur_lr: 5.0e-05
        entropy: 0.054085856614013515
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0048223676255171695
        total_loss: .inf
        vf_explained_var: 0.9997598528862
        vf_loss: 0.15894906719525656
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.328125
    gpu_util_percent0: 0.32375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14618166841478614
    mean_env_wait_ms: 1.2038328755049235
    mean_inference_ms: 4.293716284283562
    mean_raw_obs_processing_ms: 0.37728464874728784
  time_since_restore: 8205.833106040955
  time_this_iter_s: 26.723840713500977
  time_total_s: 8205.833106040955
  timers:
    learn_throughput: 8328.387
    learn_time_ms: 19426.572
    sample_throughput: 23886.575
    sample_time_ms: 6773.344
    update_time_ms: 24.139
  timestamp: 1602717776
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:22:58,047	WARNING util.py:136 -- The `process_trial` operation took 0.7970514297485352 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    309 |          8205.83 | 49993728 |  294.995 |              316.172 |              145.717 |            792.857 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3108.271374287524
    time_step_min: 2977
  date: 2020-10-14_23-23-24
  done: false
  episode_len_mean: 792.8754035576376
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.0471197702156
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 201
  episodes_total: 63188
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.73817528425618e-30
        cur_lr: 5.0e-05
        entropy: 0.05563986425598463
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052166549721732736
        model: {}
        policy_loss: -0.005476788889306287
        total_loss: 0.27150391787290573
        vf_explained_var: 0.9995012283325195
        vf_loss: 0.2770085148513317
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.05625
    gpu_util_percent0: 0.273125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617977411816419
    mean_env_wait_ms: 1.2037390302468647
    mean_inference_ms: 4.293651704655248
    mean_raw_obs_processing_ms: 0.37727893243379934
  time_since_restore: 8232.435846805573
  time_this_iter_s: 26.60274076461792
  time_total_s: 8232.435846805573
  timers:
    learn_throughput: 8332.447
    learn_time_ms: 19417.104
    sample_throughput: 23901.194
    sample_time_ms: 6769.202
    update_time_ms: 23.246
  timestamp: 1602717804
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: a052f_00000
  
2020-10-14 23:23:25,969	WARNING util.py:136 -- The `process_trial` operation took 0.8548951148986816 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    310 |          8232.44 | 50155520 |  295.047 |              316.172 |              145.717 |            792.875 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3107.9697955316965
    time_step_min: 2977
  date: 2020-10-14_23-23-52
  done: false
  episode_len_mean: 792.8934236068368
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.0925886869258
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 63363
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.73817528425618e-30
        cur_lr: 5.0e-05
        entropy: 0.05352420390894016
        entropy_coeff: 0.0005000000000000001
        kl: 0.006828452421662708
        model: {}
        policy_loss: -0.0068857607123694224
        total_loss: 0.16468176494042078
        vf_explained_var: 0.9996424317359924
        vf_loss: 0.17159428695837656
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.554838709677416
    gpu_util_percent0: 0.33967741935483864
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461786954555085
    mean_env_wait_ms: 1.2036596685285181
    mean_inference_ms: 4.293597422658783
    mean_raw_obs_processing_ms: 0.3772744396659416
  time_since_restore: 8258.802194833755
  time_this_iter_s: 26.366348028182983
  time_total_s: 8258.802194833755
  timers:
    learn_throughput: 8345.095
    learn_time_ms: 19387.676
    sample_throughput: 23930.249
    sample_time_ms: 6760.983
    update_time_ms: 23.765
  timestamp: 1602717832
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: a052f_00000
  
2020-10-14 23:23:53,442	WARNING util.py:136 -- The `process_trial` operation took 0.8094799518585205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    311 |           8258.8 | 50317312 |  295.093 |              316.172 |              145.717 |            792.893 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3107.5954778613464
    time_step_min: 2977
  date: 2020-10-14_23-24-19
  done: false
  episode_len_mean: 792.9157623226699
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.1497393487576
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 219
  episodes_total: 63582
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.73817528425618e-30
        cur_lr: 5.0e-05
        entropy: 0.0558264059945941
        entropy_coeff: 0.0005000000000000001
        kl: 0.004460077383555472
        model: {}
        policy_loss: -0.004277492839416179
        total_loss: 0.11180868806938331
        vf_explained_var: 0.999790608882904
        vf_loss: 0.11611409299075603
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612904
    gpu_util_percent0: 0.31967741935483873
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617764931925914
    mean_env_wait_ms: 1.2035599715025533
    mean_inference_ms: 4.293533579570219
    mean_raw_obs_processing_ms: 0.3772690343852304
  time_since_restore: 8285.122552156448
  time_this_iter_s: 26.32035732269287
  time_total_s: 8285.122552156448
  timers:
    learn_throughput: 8353.898
    learn_time_ms: 19367.247
    sample_throughput: 23933.896
    sample_time_ms: 6759.952
    update_time_ms: 24.38
  timestamp: 1602717859
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: a052f_00000
  
2020-10-14 23:24:20,962	WARNING util.py:136 -- The `process_trial` operation took 0.8304953575134277 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    312 |          8285.12 | 50479104 |   295.15 |              316.172 |              145.717 |            792.916 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3107.223319848214
    time_step_min: 2977
  date: 2020-10-14_23-24-47
  done: false
  episode_len_mean: 792.9359424469452
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.20594047430166
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 220
  episodes_total: 63802
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.36908764212809e-30
        cur_lr: 5.0e-05
        entropy: 0.059459867576758065
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0069597487842353685
        total_loss: .inf
        vf_explained_var: 0.9996414184570312
        vf_loss: 0.202747726192077
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.371875
    gpu_util_percent0: 0.2703125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617537211266574
    mean_env_wait_ms: 1.2034604988672197
    mean_inference_ms: 4.293461413777282
    mean_raw_obs_processing_ms: 0.3772626674089661
  time_since_restore: 8311.410354852676
  time_this_iter_s: 26.287802696228027
  time_total_s: 8311.410354852676
  timers:
    learn_throughput: 8359.925
    learn_time_ms: 19353.285
    sample_throughput: 23939.958
    sample_time_ms: 6758.241
    update_time_ms: 24.508
  timestamp: 1602717887
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:24:48,575	WARNING util.py:136 -- The `process_trial` operation took 0.8613970279693604 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    313 |          8311.41 | 50640896 |  295.206 |              316.172 |              145.717 |            792.936 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3106.9083077981363
    time_step_min: 2977
  date: 2020-10-14_23-25-15
  done: false
  episode_len_mean: 792.94911863983
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.24944753195126
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 190
  episodes_total: 63992
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.053631463192135e-30
        cur_lr: 5.0e-05
        entropy: 0.06382253393530846
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011027928775244314
        total_loss: .inf
        vf_explained_var: 0.9989699721336365
        vf_loss: 0.5284469500184059
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.490322580645167
    gpu_util_percent0: 0.3574193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617412640423438
    mean_env_wait_ms: 1.203374297359612
    mean_inference_ms: 4.2934042972256545
    mean_raw_obs_processing_ms: 0.3772578162874788
  time_since_restore: 8337.982955217361
  time_this_iter_s: 26.57260036468506
  time_total_s: 8337.982955217361
  timers:
    learn_throughput: 8357.259
    learn_time_ms: 19359.458
    sample_throughput: 23906.645
    sample_time_ms: 6767.658
    update_time_ms: 24.452
  timestamp: 1602717915
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:25:16,293	WARNING util.py:136 -- The `process_trial` operation took 0.8437957763671875 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    314 |          8337.98 | 50802688 |  295.249 |              316.172 |              145.717 |            792.949 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3106.6542160974013
    time_step_min: 2977
  date: 2020-10-14_23-25-42
  done: false
  episode_len_mean: 792.9610907674328
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.2870103449765
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 183
  episodes_total: 64175
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.080447194788202e-30
        cur_lr: 5.0e-05
        entropy: 0.061449442990124226
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010162004201750582
        total_loss: .inf
        vf_explained_var: 0.9991795420646667
        vf_loss: 0.4167913521329562
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.059375
    gpu_util_percent0: 0.256875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617292361755863
    mean_env_wait_ms: 1.2032918507678525
    mean_inference_ms: 4.29334648776581
    mean_raw_obs_processing_ms: 0.377253150839109
  time_since_restore: 8364.572237491608
  time_this_iter_s: 26.589282274246216
  time_total_s: 8364.572237491608
  timers:
    learn_throughput: 8350.217
    learn_time_ms: 19375.783
    sample_throughput: 23915.408
    sample_time_ms: 6765.178
    update_time_ms: 24.846
  timestamp: 1602717942
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:25:44,091	WARNING util.py:136 -- The `process_trial` operation took 0.861060619354248 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    315 |          8364.57 | 50964480 |  295.287 |              316.172 |              145.717 |            792.961 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3106.320072079909
    time_step_min: 2977
  date: 2020-10-14_23-26-10
  done: false
  episode_len_mean: 792.9795810624058
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.33942490070467
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 226
  episodes_total: 64401
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.620670792182304e-30
        cur_lr: 5.0e-05
        entropy: 0.06191285575429598
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009454470542550553
        total_loss: .inf
        vf_explained_var: 0.9991454482078552
        vf_loss: 0.500280499458313
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.983870967741936
    gpu_util_percent0: 0.302258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461714890107389
    mean_env_wait_ms: 1.2031917320014498
    mean_inference_ms: 4.293284028795256
    mean_raw_obs_processing_ms: 0.37724747803764924
  time_since_restore: 8391.027878284454
  time_this_iter_s: 26.45564079284668
  time_total_s: 8391.027878284454
  timers:
    learn_throughput: 8355.067
    learn_time_ms: 19364.536
    sample_throughput: 23906.617
    sample_time_ms: 6767.666
    update_time_ms: 24.903
  timestamp: 1602717970
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:26:11,812	WARNING util.py:136 -- The `process_trial` operation took 0.8732898235321045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    316 |          8391.03 | 51126272 |  295.339 |              316.172 |              145.717 |             792.98 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3106.038852240701
    time_step_min: 2977
  date: 2020-10-14_23-26-38
  done: false
  episode_len_mean: 792.99387053834
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.38120986354863
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 205
  episodes_total: 64606
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.931006188273455e-30
        cur_lr: 5.0e-05
        entropy: 0.058815891233583294
        entropy_coeff: 0.0005000000000000001
        kl: 0.004263617699810614
        model: {}
        policy_loss: -0.009599829635893306
        total_loss: 0.43771858016649884
        vf_explained_var: 0.9991701245307922
        vf_loss: 0.44734781483809155
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.325
    gpu_util_percent0: 0.3078125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616964297003426
    mean_env_wait_ms: 1.2030983381670537
    mean_inference_ms: 4.2932191216094
    mean_raw_obs_processing_ms: 0.37724170667917356
  time_since_restore: 8417.723221063614
  time_this_iter_s: 26.695342779159546
  time_total_s: 8417.723221063614
  timers:
    learn_throughput: 8351.958
    learn_time_ms: 19371.744
    sample_throughput: 23840.998
    sample_time_ms: 6786.293
    update_time_ms: 24.852
  timestamp: 1602717998
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: a052f_00000
  
2020-10-14 23:26:39,627	WARNING util.py:136 -- The `process_trial` operation took 0.8162264823913574 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    317 |          8417.72 | 51288064 |  295.381 |              316.172 |              145.717 |            792.994 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3105.7789737016847
    time_step_min: 2977
  date: 2020-10-14_23-27-06
  done: false
  episode_len_mean: 793.0115304468627
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.42383782254086
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 64785
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4655030941367274e-30
        cur_lr: 5.0e-05
        entropy: 0.053205463414390884
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007051198666886194
        total_loss: .inf
        vf_explained_var: 0.9994745850563049
        vf_loss: 0.27017498140533763
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.58125
    gpu_util_percent0: 0.360625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461684898703952
    mean_env_wait_ms: 1.2030181562347533
    mean_inference_ms: 4.293165360328895
    mean_raw_obs_processing_ms: 0.37723741791939513
  time_since_restore: 8444.345288991928
  time_this_iter_s: 26.62206792831421
  time_total_s: 8444.345288991928
  timers:
    learn_throughput: 8358.126
    learn_time_ms: 19357.449
    sample_throughput: 23874.289
    sample_time_ms: 6776.83
    update_time_ms: 26.627
  timestamp: 1602718026
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:27:07,433	WARNING util.py:136 -- The `process_trial` operation took 0.8616957664489746 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    318 |          8444.35 | 51449856 |  295.424 |              316.172 |              145.717 |            793.012 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3105.4385081886467
    time_step_min: 2977
  date: 2020-10-14_23-27-34
  done: false
  episode_len_mean: 793.0353098652225
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.47608461997004
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 211
  episodes_total: 64996
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.198254641205092e-30
        cur_lr: 5.0e-05
        entropy: 0.05276140539596478
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005286158906528726
        total_loss: .inf
        vf_explained_var: 0.9998298287391663
        vf_loss: 0.09394494816660881
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.983870967741936
    gpu_util_percent0: 0.2887096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616745943897413
    mean_env_wait_ms: 1.2029249261093988
    mean_inference_ms: 4.293107142224298
    mean_raw_obs_processing_ms: 0.3772321983470782
  time_since_restore: 8471.146889925003
  time_this_iter_s: 26.80160093307495
  time_total_s: 8471.146889925003
  timers:
    learn_throughput: 8360.95
    learn_time_ms: 19350.911
    sample_throughput: 23826.169
    sample_time_ms: 6790.517
    update_time_ms: 25.807
  timestamp: 1602718054
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:27:35,509	WARNING util.py:136 -- The `process_trial` operation took 0.864551305770874 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    319 |          8471.15 | 51611648 |  295.476 |              316.172 |              145.717 |            793.035 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3105.0690892914668
    time_step_min: 2977
  date: 2020-10-14_23-28-02
  done: false
  episode_len_mean: 793.0600591852068
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.53104822740937
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 65219
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.797381961807638e-30
        cur_lr: 5.0e-05
        entropy: 0.058163536712527275
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008090541184931984
        total_loss: .inf
        vf_explained_var: 0.9996874928474426
        vf_loss: 0.17390885576605797
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.675
    gpu_util_percent0: 0.349375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616546432335745
    mean_env_wait_ms: 1.2028261301806171
    mean_inference_ms: 4.293038811433042
    mean_raw_obs_processing_ms: 0.3772263980912189
  time_since_restore: 8497.873263120651
  time_this_iter_s: 26.726373195648193
  time_total_s: 8497.873263120651
  timers:
    learn_throughput: 8367.637
    learn_time_ms: 19335.447
    sample_throughput: 23731.463
    sample_time_ms: 6817.616
    update_time_ms: 26.038
  timestamp: 1602718082
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:28:03,428	WARNING util.py:136 -- The `process_trial` operation took 0.8870763778686523 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    320 |          8497.87 | 51773440 |  295.531 |              316.172 |              145.717 |             793.06 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3104.7729003839036
    time_step_min: 2977
  date: 2020-10-14_23-28-29
  done: false
  episode_len_mean: 793.079255148374
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.57724333181807
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 190
  episodes_total: 65409
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1696072942711455e-29
        cur_lr: 5.0e-05
        entropy: 0.05397353104005257
        entropy_coeff: 0.0005000000000000001
        kl: 0.004272616041513781
        model: {}
        policy_loss: -0.006707917788238167
        total_loss: 0.11107913715143998
        vf_explained_var: 0.9997761249542236
        vf_loss: 0.11781403608620167
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.253125000000004
    gpu_util_percent0: 0.31156249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616414801426608
    mean_env_wait_ms: 1.2027408126558135
    mean_inference_ms: 4.292983362709788
    mean_raw_obs_processing_ms: 0.3772218145229439
  time_since_restore: 8524.253066062927
  time_this_iter_s: 26.379802942276
  time_total_s: 8524.253066062927
  timers:
    learn_throughput: 8361.475
    learn_time_ms: 19349.695
    sample_throughput: 23748.07
    sample_time_ms: 6812.849
    update_time_ms: 26.558
  timestamp: 1602718109
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: a052f_00000
  
2020-10-14 23:28:31,152	WARNING util.py:136 -- The `process_trial` operation took 0.8376493453979492 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    321 |          8524.25 | 51935232 |  295.577 |              316.172 |              145.717 |            793.079 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3104.4815594399197
    time_step_min: 2977
  date: 2020-10-14_23-28-58
  done: false
  episode_len_mean: 793.0988412867815
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.62172340880954
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 181
  episodes_total: 65590
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.848036471355727e-30
        cur_lr: 5.0e-05
        entropy: 0.05607154158254465
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006872929333136805
        total_loss: .inf
        vf_explained_var: 0.999772846698761
        vf_loss: 0.13191872338453928
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.071875
    gpu_util_percent0: 0.33906250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616287506583775
    mean_env_wait_ms: 1.202660277104604
    mean_inference_ms: 4.292929340374977
    mean_raw_obs_processing_ms: 0.3772171122306122
  time_since_restore: 8551.11448597908
  time_this_iter_s: 26.861419916152954
  time_total_s: 8551.11448597908
  timers:
    learn_throughput: 8340.8
    learn_time_ms: 19397.659
    sample_throughput: 23755.059
    sample_time_ms: 6810.844
    update_time_ms: 24.983
  timestamp: 1602718138
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:28:59,275	WARNING util.py:136 -- The `process_trial` operation took 0.8744120597839355 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    322 |          8551.11 | 52097024 |  295.622 |              316.172 |              145.717 |            793.099 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3104.1424423907097
    time_step_min: 2977
  date: 2020-10-14_23-29-25
  done: false
  episode_len_mean: 793.1224170414489
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.67323379657734
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 226
  episodes_total: 65816
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.77205470703359e-30
        cur_lr: 5.0e-05
        entropy: 0.06139319638411204
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038210239727050066
        model: {}
        policy_loss: -0.00837220703882243
        total_loss: 0.3874652832746506
        vf_explained_var: 0.9993316531181335
        vf_loss: 0.3958681896328926
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.403225806451616
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616179273155497
    mean_env_wait_ms: 1.202560829819054
    mean_inference_ms: 4.2928701295335125
    mean_raw_obs_processing_ms: 0.3772115204372529
  time_since_restore: 8577.729234933853
  time_this_iter_s: 26.61474895477295
  time_total_s: 8577.729234933853
  timers:
    learn_throughput: 8331.281
    learn_time_ms: 19419.824
    sample_throughput: 23723.163
    sample_time_ms: 6820.001
    update_time_ms: 24.849
  timestamp: 1602718165
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: a052f_00000
  
2020-10-14 23:29:27,095	WARNING util.py:136 -- The `process_trial` operation took 0.8966593742370605 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    323 |          8577.73 | 52258816 |  295.673 |              316.172 |              145.717 |            793.122 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3103.82825497356
    time_step_min: 2977
  date: 2020-10-14_23-29-53
  done: false
  episode_len_mean: 793.1423205658291
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.720976404969
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 211
  episodes_total: 66027
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.386027353516795e-30
        cur_lr: 5.0e-05
        entropy: 0.06069737859070301
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008268727285515828
        total_loss: .inf
        vf_explained_var: 0.9995930194854736
        vf_loss: 0.2195302409430345
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.23125
    gpu_util_percent0: 0.2521875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615995953014957
    mean_env_wait_ms: 1.2024685264416344
    mean_inference_ms: 4.292810311775741
    mean_raw_obs_processing_ms: 0.3772060356235662
  time_since_restore: 8604.218438625336
  time_this_iter_s: 26.489203691482544
  time_total_s: 8604.218438625336
  timers:
    learn_throughput: 8334.76
    learn_time_ms: 19411.717
    sample_throughput: 23755.343
    sample_time_ms: 6810.762
    update_time_ms: 23.645
  timestamp: 1602718193
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:29:54,820	WARNING util.py:136 -- The `process_trial` operation took 0.8822083473205566 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    324 |          8604.22 | 52420608 |  295.721 |              316.172 |              145.717 |            793.142 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3103.567052992641
    time_step_min: 2977
  date: 2020-10-14_23-30-21
  done: false
  episode_len_mean: 793.1625809959672
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.7618776921415
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 180
  episodes_total: 66207
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.579041030275194e-30
        cur_lr: 5.0e-05
        entropy: 0.0587598467245698
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007467714272327915
        total_loss: .inf
        vf_explained_var: 0.99945467710495
        vf_loss: 0.2729887093106906
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.390322580645165
    gpu_util_percent0: 0.2903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461588172860978
    mean_env_wait_ms: 1.2023892357887878
    mean_inference_ms: 4.292758317196255
    mean_raw_obs_processing_ms: 0.37720192833538313
  time_since_restore: 8630.805742740631
  time_this_iter_s: 26.58730411529541
  time_total_s: 8630.805742740631
  timers:
    learn_throughput: 8333.655
    learn_time_ms: 19414.291
    sample_throughput: 23749.738
    sample_time_ms: 6812.37
    update_time_ms: 25.364
  timestamp: 1602718221
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:30:22,701	WARNING util.py:136 -- The `process_trial` operation took 0.8722789287567139 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    325 |          8630.81 | 52582400 |  295.762 |              316.172 |              145.717 |            793.163 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3103.269570326624
    time_step_min: 2977
  date: 2020-10-14_23-30-49
  done: false
  episode_len_mean: 793.180034335281
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.8083788003518
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 197
  episodes_total: 66404
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.868561545412791e-30
        cur_lr: 5.0e-05
        entropy: 0.0605205682416757
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008640856106164089
        total_loss: .inf
        vf_explained_var: 0.9995664954185486
        vf_loss: 0.23634432504574457
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.340625000000003
    gpu_util_percent0: 0.3109375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615786952175963
    mean_env_wait_ms: 1.202303896079473
    mean_inference_ms: 4.292704730037526
    mean_raw_obs_processing_ms: 0.3771969747668398
  time_since_restore: 8657.322571277618
  time_this_iter_s: 26.516828536987305
  time_total_s: 8657.322571277618
  timers:
    learn_throughput: 8336.343
    learn_time_ms: 19408.031
    sample_throughput: 23744.16
    sample_time_ms: 6813.97
    update_time_ms: 25.339
  timestamp: 1602718249
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:30:50,425	WARNING util.py:136 -- The `process_trial` operation took 0.8946022987365723 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    326 |          8657.32 | 52744192 |  295.808 |              316.172 |              145.717 |             793.18 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3102.9300771748594
    time_step_min: 2977
  date: 2020-10-14_23-31-16
  done: false
  episode_len_mean: 793.1986342488368
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.8559321263055
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 226
  episodes_total: 66630
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.480284231811919e-29
        cur_lr: 5.0e-05
        entropy: 0.06368644380321105
        entropy_coeff: 0.0005000000000000001
        kl: 0.004555212799459696
        model: {}
        policy_loss: -0.008768696832703426
        total_loss: 0.32557331025600433
        vf_explained_var: 0.999426543712616
        vf_loss: 0.3343738416830699
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.05625
    gpu_util_percent0: 0.3196875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615611291617736
    mean_env_wait_ms: 1.2022060247519926
    mean_inference_ms: 4.292643353266362
    mean_raw_obs_processing_ms: 0.37719159321250206
  time_since_restore: 8683.889548301697
  time_this_iter_s: 26.56697702407837
  time_total_s: 8683.889548301697
  timers:
    learn_throughput: 8338.705
    learn_time_ms: 19402.533
    sample_throughput: 23810.56
    sample_time_ms: 6794.968
    update_time_ms: 25.848
  timestamp: 1602718276
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: a052f_00000
  
2020-10-14 23:31:18,267	WARNING util.py:136 -- The `process_trial` operation took 0.9049122333526611 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    327 |          8683.89 | 52905984 |  295.856 |              316.172 |              145.717 |            793.199 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3102.652794203506
    time_step_min: 2977
  date: 2020-10-14_23-31-44
  done: false
  episode_len_mean: 793.2116809074176
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.897993658584
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 197
  episodes_total: 66827
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.401421159059595e-30
        cur_lr: 5.0e-05
        entropy: 0.06343969081838925
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00741518268478103
        total_loss: .inf
        vf_explained_var: 0.9993374347686768
        vf_loss: 0.3410523906350136
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.693548387096776
    gpu_util_percent0: 0.37064516129032254
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615448222950858
    mean_env_wait_ms: 1.2021185837722972
    mean_inference_ms: 4.29258794333903
    mean_raw_obs_processing_ms: 0.37718670242991337
  time_since_restore: 8710.254784107208
  time_this_iter_s: 26.365235805511475
  time_total_s: 8710.254784107208
  timers:
    learn_throughput: 8346.029
    learn_time_ms: 19385.506
    sample_throughput: 23809.243
    sample_time_ms: 6795.344
    update_time_ms: 23.88
  timestamp: 1602718304
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:31:45,897	WARNING util.py:136 -- The `process_trial` operation took 0.9518868923187256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    328 |          8710.25 | 53067776 |  295.898 |              316.172 |              145.717 |            793.212 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3102.3810355639166
    time_step_min: 2977
  date: 2020-10-14_23-32-12
  done: false
  episode_len_mean: 793.22556188998
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.9388795877465
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 67006
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102131738589393e-29
        cur_lr: 5.0e-05
        entropy: 0.06386591276774804
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006015485181706026
        total_loss: .inf
        vf_explained_var: 0.9992617964744568
        vf_loss: 0.36560867230097455
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.98125
    gpu_util_percent0: 0.2478125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615351226443976
    mean_env_wait_ms: 1.2020412133728593
    mean_inference_ms: 4.292538986789863
    mean_raw_obs_processing_ms: 0.37718252907688804
  time_since_restore: 8736.70654463768
  time_this_iter_s: 26.4517605304718
  time_total_s: 8736.70654463768
  timers:
    learn_throughput: 8359.839
    learn_time_ms: 19353.484
    sample_throughput: 23851.872
    sample_time_ms: 6783.199
    update_time_ms: 23.674
  timestamp: 1602718332
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:32:13,597	WARNING util.py:136 -- The `process_trial` operation took 0.8864531517028809 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    329 |          8736.71 | 53229568 |  295.939 |              316.172 |              145.717 |            793.226 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3102.04893908282
    time_step_min: 2977
  date: 2020-10-14_23-32-40
  done: false
  episode_len_mean: 793.2451884463218
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 295.989206699472
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 228
  episodes_total: 67234
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6653197607884083e-29
        cur_lr: 5.0e-05
        entropy: 0.06568253599107265
        entropy_coeff: 0.0005000000000000001
        kl: 0.004322548707326253
        model: {}
        policy_loss: -0.008014384909377744
        total_loss: 0.3053908869624138
        vf_explained_var: 0.9994626641273499
        vf_loss: 0.3134381175041199
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.09354838709678
    gpu_util_percent0: 0.28161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615245546318678
    mean_env_wait_ms: 1.2019423668940907
    mean_inference_ms: 4.292481742039852
    mean_raw_obs_processing_ms: 0.3771768978836187
  time_since_restore: 8763.336065769196
  time_this_iter_s: 26.629521131515503
  time_total_s: 8763.336065769196
  timers:
    learn_throughput: 8352.214
    learn_time_ms: 19371.152
    sample_throughput: 23955.24
    sample_time_ms: 6753.93
    update_time_ms: 25.253
  timestamp: 1602718360
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: a052f_00000
  
2020-10-14 23:32:41,591	WARNING util.py:136 -- The `process_trial` operation took 0.9589076042175293 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    330 |          8763.34 | 53391360 |  295.989 |              316.172 |              145.717 |            793.245 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3101.7508825536206
    time_step_min: 2977
  date: 2020-10-14_23-33-08
  done: false
  episode_len_mean: 793.2585772321561
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.0352376177032
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 212
  episodes_total: 67446
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.326598803942042e-30
        cur_lr: 5.0e-05
        entropy: 0.06517302555342515
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008420284342719242
        total_loss: .inf
        vf_explained_var: 0.9993956685066223
        vf_loss: 0.37001768747965497
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.178125
    gpu_util_percent0: 0.30375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615056749935876
    mean_env_wait_ms: 1.2018515443011117
    mean_inference_ms: 4.292422393121034
    mean_raw_obs_processing_ms: 0.3771717164295696
  time_since_restore: 8789.85949754715
  time_this_iter_s: 26.5234317779541
  time_total_s: 8789.85949754715
  timers:
    learn_throughput: 8349.295
    learn_time_ms: 19377.924
    sample_throughput: 23933.295
    sample_time_ms: 6760.122
    update_time_ms: 25.41
  timestamp: 1602718388
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:33:09,310	WARNING util.py:136 -- The `process_trial` operation took 0.8774263858795166 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    331 |          8789.86 | 53553152 |  296.035 |              316.172 |              145.717 |            793.259 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3101.4704516205384
    time_step_min: 2977
  date: 2020-10-14_23-33-35
  done: false
  episode_len_mean: 793.2745419864259
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.0773341362878
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 183
  episodes_total: 67629
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2489898205913064e-29
        cur_lr: 5.0e-05
        entropy: 0.06055667965362469
        entropy_coeff: 0.0005000000000000001
        kl: 0.007877361727878451
        model: {}
        policy_loss: -0.006682702403243941
        total_loss: 0.12707629427313805
        vf_explained_var: 0.9997246265411377
        vf_loss: 0.13378927670419216
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.25161290322581
    gpu_util_percent0: 0.322258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461494692505575
    mean_env_wait_ms: 1.2017725552943916
    mean_inference_ms: 4.292373308877353
    mean_raw_obs_processing_ms: 0.37716754577235967
  time_since_restore: 8816.304193496704
  time_this_iter_s: 26.444695949554443
  time_total_s: 8816.304193496704
  timers:
    learn_throughput: 8364.945
    learn_time_ms: 19341.669
    sample_throughput: 23929.227
    sample_time_ms: 6761.271
    update_time_ms: 25.412
  timestamp: 1602718415
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: a052f_00000
  
2020-10-14 23:33:37,197	WARNING util.py:136 -- The `process_trial` operation took 0.9166231155395508 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    332 |           8816.3 | 53714944 |  296.077 |              316.172 |              145.717 |            793.275 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3101.195843289967
    time_step_min: 2977
  date: 2020-10-14_23-34-03
  done: false
  episode_len_mean: 793.289050750494
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.1200444776861
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 193
  episodes_total: 67822
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2489898205913064e-29
        cur_lr: 5.0e-05
        entropy: 0.06282771130402882
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007866850326536223
        total_loss: .inf
        vf_explained_var: 0.9994271397590637
        vf_loss: 0.3100174715121587
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.8875
    gpu_util_percent0: 0.3059375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614841961694827
    mean_env_wait_ms: 1.201690641476925
    mean_inference_ms: 4.292325272814775
    mean_raw_obs_processing_ms: 0.3771629772425613
  time_since_restore: 8842.789723396301
  time_this_iter_s: 26.485529899597168
  time_total_s: 8842.789723396301
  timers:
    learn_throughput: 8371.739
    learn_time_ms: 19325.974
    sample_throughput: 23957.913
    sample_time_ms: 6753.176
    update_time_ms: 26.247
  timestamp: 1602718443
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:34:04,925	WARNING util.py:136 -- The `process_trial` operation took 0.9279007911682129 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    333 |          8842.79 | 53876736 |   296.12 |              316.172 |              145.717 |            793.289 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3100.876775160977
    time_step_min: 2977
  date: 2020-10-14_23-34-31
  done: false
  episode_len_mean: 793.3012490815577
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.1671483386395
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 228
  episodes_total: 68050
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8734847308869596e-29
        cur_lr: 5.0e-05
        entropy: 0.06534644588828087
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043384523984665675
        model: {}
        policy_loss: -0.007639289503761877
        total_loss: 0.38292550295591354
        vf_explained_var: 0.9993100166320801
        vf_loss: 0.39059745768706006
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.96129032258065
    gpu_util_percent0: 0.3454838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614699920004967
    mean_env_wait_ms: 1.2015932953113142
    mean_inference_ms: 4.292263849408969
    mean_raw_obs_processing_ms: 0.37715771934014003
  time_since_restore: 8869.451143741608
  time_this_iter_s: 26.661420345306396
  time_total_s: 8869.451143741608
  timers:
    learn_throughput: 8363.111
    learn_time_ms: 19345.911
    sample_throughput: 23940.04
    sample_time_ms: 6758.218
    update_time_ms: 27.735
  timestamp: 1602718471
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: a052f_00000
  
2020-10-14 23:34:32,914	WARNING util.py:136 -- The `process_trial` operation took 0.9087350368499756 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    334 |          8869.45 | 54038528 |  296.167 |              316.172 |              145.717 |            793.301 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3100.5948255643507
    time_step_min: 2977
  date: 2020-10-14_23-34-59
  done: false
  episode_len_mean: 793.3165807056617
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.2119270302363
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 68248
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.367423654434798e-30
        cur_lr: 5.0e-05
        entropy: 0.06130501224348942
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007527168568534155
        total_loss: .inf
        vf_explained_var: 0.9996950030326843
        vf_loss: 0.15526330719391504
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.1030303030303
    gpu_util_percent0: 0.2687878787878788
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8727272727272726
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614528079950853
    mean_env_wait_ms: 1.2015078976551727
    mean_inference_ms: 4.292211025191384
    mean_raw_obs_processing_ms: 0.3771528455822541
  time_since_restore: 8896.206447124481
  time_this_iter_s: 26.755303382873535
  time_total_s: 8896.206447124481
  timers:
    learn_throughput: 8355.225
    learn_time_ms: 19364.169
    sample_throughput: 23940.359
    sample_time_ms: 6758.128
    update_time_ms: 26.128
  timestamp: 1602718499
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:35:01,048	WARNING util.py:136 -- The `process_trial` operation took 0.8889102935791016 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    335 |          8896.21 | 54200320 |  296.212 |              316.172 |              145.717 |            793.317 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3100.3397517434973
    time_step_min: 2977
  date: 2020-10-14_23-35-27
  done: false
  episode_len_mean: 793.3286518085495
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.2519554330292
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 68425
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4051135481652197e-29
        cur_lr: 5.0e-05
        entropy: 0.06556733387211959
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008991218152611205
        total_loss: .inf
        vf_explained_var: 0.9996011853218079
        vf_loss: 0.19746844843029976
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.296774193548387
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614427552477696
    mean_env_wait_ms: 1.2014329992537436
    mean_inference_ms: 4.292164039306969
    mean_raw_obs_processing_ms: 0.37714899258236056
  time_since_restore: 8922.827974796295
  time_this_iter_s: 26.621527671813965
  time_total_s: 8922.827974796295
  timers:
    learn_throughput: 8348.851
    learn_time_ms: 19378.954
    sample_throughput: 23926.486
    sample_time_ms: 6762.046
    update_time_ms: 25.932
  timestamp: 1602718527
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:35:28,975	WARNING util.py:136 -- The `process_trial` operation took 0.89363694190979 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    336 |          8922.83 | 54362112 |  296.252 |              316.172 |              145.717 |            793.329 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3100.0106672884394
    time_step_min: 2977
  date: 2020-10-14_23-35-55
  done: false
  episode_len_mean: 793.3483371935498
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.30091413633744
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 224
  episodes_total: 68649
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1076703222478298e-29
        cur_lr: 5.0e-05
        entropy: 0.0675811842083931
        entropy_coeff: 0.0005000000000000001
        kl: 0.004493701426933209
        model: {}
        policy_loss: -0.0070287523364337785
        total_loss: 0.3019183923800786
        vf_explained_var: 0.9994705319404602
        vf_loss: 0.30898092935482663
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.309375
    gpu_util_percent0: 0.2959375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614344817472893
    mean_env_wait_ms: 1.2013382065345368
    mean_inference_ms: 4.292114407893513
    mean_raw_obs_processing_ms: 0.37714384533801515
  time_since_restore: 8949.437768220901
  time_this_iter_s: 26.609793424606323
  time_total_s: 8949.437768220901
  timers:
    learn_throughput: 8349.717
    learn_time_ms: 19376.944
    sample_throughput: 23912.949
    sample_time_ms: 6765.874
    update_time_ms: 25.56
  timestamp: 1602718555
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: a052f_00000
  
2020-10-14 23:35:56,842	WARNING util.py:136 -- The `process_trial` operation took 0.9424188137054443 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    337 |          8949.44 | 54523904 |  296.301 |              316.172 |              145.717 |            793.348 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3099.7228815898425
    time_step_min: 2977
  date: 2020-10-14_23-36-23
  done: false
  episode_len_mean: 793.3638132578233
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.3430339113193
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 216
  episodes_total: 68865
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0538351611239149e-29
        cur_lr: 5.0e-05
        entropy: 0.06906356165806453
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043660216421509785
        model: {}
        policy_loss: -0.009307101849117316
        total_loss: 0.37808013210693997
        vf_explained_var: 0.9993137717247009
        vf_loss: 0.38742176194985706
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.89375
    gpu_util_percent0: 0.364375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614144334724072
    mean_env_wait_ms: 1.2012479154830278
    mean_inference_ms: 4.292053167527364
    mean_raw_obs_processing_ms: 0.3771385908727881
  time_since_restore: 8976.342634439468
  time_this_iter_s: 26.904866218566895
  time_total_s: 8976.342634439468
  timers:
    learn_throughput: 8328.098
    learn_time_ms: 19427.246
    sample_throughput: 23905.829
    sample_time_ms: 6767.889
    update_time_ms: 25.954
  timestamp: 1602718583
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: a052f_00000
  
2020-10-14 23:36:25,029	WARNING util.py:136 -- The `process_trial` operation took 0.920438289642334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    338 |          8976.34 | 54685696 |  296.343 |              316.172 |              145.717 |            793.364 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3099.461932978862
    time_step_min: 2977
  date: 2020-10-14_23-36-51
  done: false
  episode_len_mean: 793.3816744145631
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.38234980468945
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 186
  episodes_total: 69051
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.2691758056195745e-30
        cur_lr: 5.0e-05
        entropy: 0.06578620212773482
        entropy_coeff: 0.0005000000000000001
        kl: 0.006026940691905717
        model: {}
        policy_loss: -0.007797442308704679
        total_loss: 0.3553468808531761
        vf_explained_var: 0.9992611408233643
        vf_loss: 0.36317722251017887
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.796875
    gpu_util_percent0: 0.3046875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614042985205472
    mean_env_wait_ms: 1.2011691128129052
    mean_inference_ms: 4.292005578902806
    mean_raw_obs_processing_ms: 0.37713444220737247
  time_since_restore: 9003.110589027405
  time_this_iter_s: 26.7679545879364
  time_total_s: 9003.110589027405
  timers:
    learn_throughput: 8316.471
    learn_time_ms: 19454.406
    sample_throughput: 23894.782
    sample_time_ms: 6771.018
    update_time_ms: 27.059
  timestamp: 1602718611
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: a052f_00000
  
2020-10-14 23:36:53,123	WARNING util.py:136 -- The `process_trial` operation took 0.978515625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    339 |          9003.11 | 54847488 |  296.382 |              316.172 |              145.717 |            793.382 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3099.238239713361
    time_step_min: 2977
  date: 2020-10-14_23-37-19
  done: false
  episode_len_mean: 793.3913263243024
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.4139567356304
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 193
  episodes_total: 69244
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.2691758056195745e-30
        cur_lr: 5.0e-05
        entropy: 0.07737822271883488
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010509691356370846
        total_loss: .inf
        vf_explained_var: 0.9982126355171204
        vf_loss: 0.9633058706919352
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.56129032258065
    gpu_util_percent0: 0.2783870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613964213180974
    mean_env_wait_ms: 1.2010893008327106
    mean_inference_ms: 4.291961737515204
    mean_raw_obs_processing_ms: 0.3771297955250981
  time_since_restore: 9029.455507040024
  time_this_iter_s: 26.34491801261902
  time_total_s: 9029.455507040024
  timers:
    learn_throughput: 8326.17
    learn_time_ms: 19431.743
    sample_throughput: 23923.133
    sample_time_ms: 6762.994
    update_time_ms: 25.553
  timestamp: 1602718639
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:37:20,807	WARNING util.py:136 -- The `process_trial` operation took 0.9200503826141357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    340 |          9029.46 | 55009280 |  296.414 |              316.172 |              145.717 |            793.391 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3099.0698742781433
    time_step_min: 2977
  date: 2020-10-14_23-37-47
  done: false
  episode_len_mean: 793.3876660860552
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.43644500629796
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 69467
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.903763708429362e-30
        cur_lr: 5.0e-05
        entropy: 0.08237588219344616
        entropy_coeff: 0.0005000000000000001
        kl: 0.004210521893886228
        model: {}
        policy_loss: -0.011938422879514595
        total_loss: 1.8612256944179535
        vf_explained_var: 0.9967197775840759
        vf_loss: 1.873205264409383
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.196875
    gpu_util_percent0: 0.27125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461382144511371
    mean_env_wait_ms: 1.200997174010864
    mean_inference_ms: 4.2919101102535775
    mean_raw_obs_processing_ms: 0.3771252581323972
  time_since_restore: 9056.277537345886
  time_this_iter_s: 26.822030305862427
  time_total_s: 9056.277537345886
  timers:
    learn_throughput: 8314.308
    learn_time_ms: 19459.467
    sample_throughput: 23917.143
    sample_time_ms: 6764.688
    update_time_ms: 24.249
  timestamp: 1602718667
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: a052f_00000
  
2020-10-14 23:37:48,972	WARNING util.py:136 -- The `process_trial` operation took 0.9113116264343262 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    341 |          9056.28 | 55171072 |  296.436 |              316.172 |              145.717 |            793.388 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3098.8919761063726
    time_step_min: 2977
  date: 2020-10-14_23-38-15
  done: false
  episode_len_mean: 793.3985503085977
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.46929536501773
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 203
  episodes_total: 69670
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.951881854214681e-30
        cur_lr: 5.0e-05
        entropy: 0.06333661824464798
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036466696959299347
        model: {}
        policy_loss: -0.0107672456773192
        total_loss: 0.6235898037751516
        vf_explained_var: 0.9988053441047668
        vf_loss: 0.6343887398640314
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.046875
    gpu_util_percent0: 0.34843749999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613646661419216
    mean_env_wait_ms: 1.200911124390426
    mean_inference_ms: 4.291853659145986
    mean_raw_obs_processing_ms: 0.3771203060351173
  time_since_restore: 9082.896679401398
  time_this_iter_s: 26.619142055511475
  time_total_s: 9082.896679401398
  timers:
    learn_throughput: 8308.243
    learn_time_ms: 19473.673
    sample_throughput: 23906.485
    sample_time_ms: 6767.703
    update_time_ms: 25.23
  timestamp: 1602718695
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: a052f_00000
  
2020-10-14 23:38:16,845	WARNING util.py:136 -- The `process_trial` operation took 0.930532693862915 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    342 |           9082.9 | 55332864 |  296.469 |              316.172 |              145.717 |            793.399 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3098.6640980835887
    time_step_min: 2977
  date: 2020-10-14_23-38-43
  done: false
  episode_len_mean: 793.4187211866106
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.5056704837218
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 176
  episodes_total: 69846
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9759409271073405e-30
        cur_lr: 5.0e-05
        entropy: 0.06130173026273648
        entropy_coeff: 0.0005000000000000001
        kl: 0.004533059739818175
        model: {}
        policy_loss: -0.007162310943385819
        total_loss: 0.2629428518315156
        vf_explained_var: 0.9994615912437439
        vf_loss: 0.2701358248790105
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.799999999999997
    gpu_util_percent0: 0.31625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613559903393886
    mean_env_wait_ms: 1.2008388116306339
    mean_inference_ms: 4.291810217780365
    mean_raw_obs_processing_ms: 0.37711669948451654
  time_since_restore: 9109.631677865982
  time_this_iter_s: 26.73499846458435
  time_total_s: 9109.631677865982
  timers:
    learn_throughput: 8298.507
    learn_time_ms: 19496.52
    sample_throughput: 23904.296
    sample_time_ms: 6768.323
    update_time_ms: 25.922
  timestamp: 1602718723
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: a052f_00000
  
2020-10-14 23:38:44,817	WARNING util.py:136 -- The `process_trial` operation took 0.9123632907867432 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    343 |          9109.63 | 55494656 |  296.506 |              316.172 |              145.717 |            793.419 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3098.364985078463
    time_step_min: 2977
  date: 2020-10-14_23-39-11
  done: false
  episode_len_mean: 793.4452263027932
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.5513884797934
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 215
  episodes_total: 70061
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.879704635536703e-31
        cur_lr: 5.0e-05
        entropy: 0.05942468220988909
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033927701491241655
        model: {}
        policy_loss: -0.008535379864042625
        total_loss: 0.15139379352331161
        vf_explained_var: 0.9997141361236572
        vf_loss: 0.15995888536175093
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.953125
    gpu_util_percent0: 0.3346875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613464017394912
    mean_env_wait_ms: 1.200750060002893
    mean_inference_ms: 4.29176097495647
    mean_raw_obs_processing_ms: 0.3771117670832487
  time_since_restore: 9136.461262226105
  time_this_iter_s: 26.82958436012268
  time_total_s: 9136.461262226105
  timers:
    learn_throughput: 8295.741
    learn_time_ms: 19503.02
    sample_throughput: 23905.725
    sample_time_ms: 6767.918
    update_time_ms: 25.386
  timestamp: 1602718751
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: a052f_00000
  
2020-10-14 23:39:12,899	WARNING util.py:136 -- The `process_trial` operation took 0.9281148910522461 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    344 |          9136.46 | 55656448 |  296.551 |              316.172 |              145.717 |            793.445 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3098.073276966436
    time_step_min: 2977
  date: 2020-10-14_23-39-39
  done: false
  episode_len_mean: 793.4720127486412
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.59627089597745
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 221
  episodes_total: 70282
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.939852317768351e-31
        cur_lr: 5.0e-05
        entropy: 0.06050061651815971
        entropy_coeff: 0.0005000000000000001
        kl: 0.004063179289611678
        model: {}
        policy_loss: -0.008367203583475202
        total_loss: 0.29309064894914627
        vf_explained_var: 0.9995009899139404
        vf_loss: 0.3014880989988645
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.87741935483871
    gpu_util_percent0: 0.30451612903225805
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613283787877704
    mean_env_wait_ms: 1.2006600306624828
    mean_inference_ms: 4.291704627542137
    mean_raw_obs_processing_ms: 0.3771067726277731
  time_since_restore: 9163.111501932144
  time_this_iter_s: 26.65023970603943
  time_total_s: 9163.111501932144
  timers:
    learn_throughput: 8301.269
    learn_time_ms: 19490.033
    sample_throughput: 23900.368
    sample_time_ms: 6769.436
    update_time_ms: 26.046
  timestamp: 1602718779
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: a052f_00000
  
2020-10-14 23:39:40,870	WARNING util.py:136 -- The `process_trial` operation took 0.9362401962280273 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    345 |          9163.11 | 55818240 |  296.596 |              316.172 |              145.717 |            793.472 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3097.8442876611207
    time_step_min: 2977
  date: 2020-10-14_23-40-07
  done: false
  episode_len_mean: 793.4965092519014
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.6324223905529
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 190
  episodes_total: 70472
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4699261588841756e-31
        cur_lr: 5.0e-05
        entropy: 0.06349831788490216
        entropy_coeff: 0.0005000000000000001
        kl: 0.005597064465594788
        model: {}
        policy_loss: -0.007455482559938294
        total_loss: 0.3366287300984065
        vf_explained_var: 0.9993565678596497
        vf_loss: 0.3441159551342328
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.228125
    gpu_util_percent0: 0.26687500000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461318306426801
    mean_env_wait_ms: 1.2005812917054173
    mean_inference_ms: 4.291661059738656
    mean_raw_obs_processing_ms: 0.3771025146193806
  time_since_restore: 9189.6378531456
  time_this_iter_s: 26.5263512134552
  time_total_s: 9189.6378531456
  timers:
    learn_throughput: 8307.37
    learn_time_ms: 19475.719
    sample_throughput: 23910.235
    sample_time_ms: 6766.642
    update_time_ms: 25.851
  timestamp: 1602718807
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: a052f_00000
  
2020-10-14 23:40:08,682	WARNING util.py:136 -- The `process_trial` operation took 0.9657077789306641 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    346 |          9189.64 | 55980032 |  296.632 |              316.172 |              145.717 |            793.497 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3097.595489232773
    time_step_min: 2977
  date: 2020-10-14_23-40-35
  done: false
  episode_len_mean: 793.5170042032862
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.6700624038539
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 187
  episodes_total: 70659
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4699261588841756e-31
        cur_lr: 5.0e-05
        entropy: 0.06146581657230854
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038711871408546963
        model: {}
        policy_loss: -0.007379610238937555
        total_loss: 0.23969333618879318
        vf_explained_var: 0.9995208382606506
        vf_loss: 0.24710367992520332
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.996774193548394
    gpu_util_percent0: 0.3635483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613092405346179
    mean_env_wait_ms: 1.2005052031043
    mean_inference_ms: 4.291613941438098
    mean_raw_obs_processing_ms: 0.37709828136764983
  time_since_restore: 9215.992297649384
  time_this_iter_s: 26.35444450378418
  time_total_s: 9215.992297649384
  timers:
    learn_throughput: 8311.609
    learn_time_ms: 19465.786
    sample_throughput: 23930.724
    sample_time_ms: 6760.848
    update_time_ms: 25.824
  timestamp: 1602718835
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: a052f_00000
  
2020-10-14 23:40:36,358	WARNING util.py:136 -- The `process_trial` operation took 0.9055123329162598 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    347 |          9215.99 | 56141824 |   296.67 |              316.172 |              145.717 |            793.517 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3097.2836273817925
    time_step_min: 2977
  date: 2020-10-14_23-41-03
  done: false
  episode_len_mean: 793.5468692683202
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.7163726488618
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 219
  episodes_total: 70878
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2349630794420878e-31
        cur_lr: 5.0e-05
        entropy: 0.059471454471349716
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005874602502444759
        total_loss: .inf
        vf_explained_var: 0.9997609257698059
        vf_loss: 0.14329223210612932
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.09375
    gpu_util_percent0: 0.34500000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612985225231728
    mean_env_wait_ms: 1.2004166514913843
    mean_inference_ms: 4.291566020059676
    mean_raw_obs_processing_ms: 0.37709380907940065
  time_since_restore: 9242.762085676193
  time_this_iter_s: 26.769788026809692
  time_total_s: 9242.762085676193
  timers:
    learn_throughput: 8326.693
    learn_time_ms: 19430.524
    sample_throughput: 23857.832
    sample_time_ms: 6781.505
    update_time_ms: 26.504
  timestamp: 1602718863
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:41:04,593	WARNING util.py:136 -- The `process_trial` operation took 0.9102060794830322 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    348 |          9242.76 | 56303616 |  296.716 |              316.172 |              145.717 |            793.547 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3096.9912328844234
    time_step_min: 2977
  date: 2020-10-14_23-41-31
  done: false
  episode_len_mean: 793.5711150810955
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.75834417832436
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 211
  episodes_total: 71089
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8524446191631315e-31
        cur_lr: 5.0e-05
        entropy: 0.0631385628754894
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065335897185529275
        model: {}
        policy_loss: -0.00823537380589793
        total_loss: 0.32887102911869687
        vf_explained_var: 0.9993740916252136
        vf_loss: 0.33713797231515247
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.390625
    gpu_util_percent0: 0.2946875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461281400802958
    mean_env_wait_ms: 1.2003301686318844
    mean_inference_ms: 4.291514851067665
    mean_raw_obs_processing_ms: 0.37708921820487856
  time_since_restore: 9269.40534901619
  time_this_iter_s: 26.643263339996338
  time_total_s: 9269.40534901619
  timers:
    learn_throughput: 8337.515
    learn_time_ms: 19405.301
    sample_throughput: 23785.509
    sample_time_ms: 6802.125
    update_time_ms: 25.241
  timestamp: 1602718891
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: a052f_00000
  
2020-10-14 23:41:32,627	WARNING util.py:136 -- The `process_trial` operation took 0.9831669330596924 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    349 |          9269.41 | 56465408 |  296.758 |              316.172 |              145.717 |            793.571 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3096.7591420188946
    time_step_min: 2977
  date: 2020-10-14_23-41-59
  done: false
  episode_len_mean: 793.592647162001
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.79311319892213
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 176
  episodes_total: 71265
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8524446191631315e-31
        cur_lr: 5.0e-05
        entropy: 0.05773022739837567
        entropy_coeff: 0.0005000000000000001
        kl: 0.004838933721960832
        model: {}
        policy_loss: -0.0072749395039863884
        total_loss: 0.2888688842455546
        vf_explained_var: 0.9994171261787415
        vf_loss: 0.2961726784706116
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.015151515151516
    gpu_util_percent0: 0.28181818181818186
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8727272727272726
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461272006723679
    mean_env_wait_ms: 1.2002586902108339
    mean_inference_ms: 4.291472666205223
    mean_raw_obs_processing_ms: 0.37708552326243056
  time_since_restore: 9296.391649246216
  time_this_iter_s: 26.986300230026245
  time_total_s: 9296.391649246216
  timers:
    learn_throughput: 8326.753
    learn_time_ms: 19430.383
    sample_throughput: 23674.728
    sample_time_ms: 6833.954
    update_time_ms: 24.974
  timestamp: 1602718919
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: a052f_00000
  
2020-10-14 23:42:00,961	WARNING util.py:136 -- The `process_trial` operation took 0.9179205894470215 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    350 |          9296.39 | 56627200 |  296.793 |              316.172 |              145.717 |            793.593 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3096.499944010526
    time_step_min: 2977
  date: 2020-10-14_23-42-27
  done: false
  episode_len_mean: 793.617615782846
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.8336265975832
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 205
  episodes_total: 71470
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.262223095815658e-32
        cur_lr: 5.0e-05
        entropy: 0.058625999838113785
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007457571333361557
        total_loss: .inf
        vf_explained_var: 0.9995145201683044
        vf_loss: 0.28811677545309067
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096777
    gpu_util_percent0: 0.3574193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612623085023163
    mean_env_wait_ms: 1.200175730157743
    mean_inference_ms: 4.29142303837122
    mean_raw_obs_processing_ms: 0.37708073886623317
  time_since_restore: 9322.916204452515
  time_this_iter_s: 26.524555206298828
  time_total_s: 9322.916204452515
  timers:
    learn_throughput: 8338.672
    learn_time_ms: 19402.61
    sample_throughput: 23683.804
    sample_time_ms: 6831.335
    update_time_ms: 25.63
  timestamp: 1602718947
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:42:28,825	WARNING util.py:136 -- The `process_trial` operation took 0.9200444221496582 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    351 |          9322.92 | 56788992 |  296.834 |              316.172 |              145.717 |            793.618 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3096.206307563494
    time_step_min: 2977
  date: 2020-10-14_23-42-55
  done: false
  episode_len_mean: 793.6486720232117
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.8772323164686
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 218
  episodes_total: 71688
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3893334643723486e-31
        cur_lr: 5.0e-05
        entropy: 0.05426539480686188
        entropy_coeff: 0.0005000000000000001
        kl: 0.003680938136919091
        model: {}
        policy_loss: -0.00855390208986743
        total_loss: 0.23876414199670157
        vf_explained_var: 0.9995905756950378
        vf_loss: 0.24734518056114516
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.34375
    gpu_util_percent0: 0.30937499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612505475850113
    mean_env_wait_ms: 1.200089874089585
    mean_inference_ms: 4.291379857290181
    mean_raw_obs_processing_ms: 0.37707650450020475
  time_since_restore: 9349.585072755814
  time_this_iter_s: 26.66886830329895
  time_total_s: 9349.585072755814
  timers:
    learn_throughput: 8336.755
    learn_time_ms: 19407.072
    sample_throughput: 23686.01
    sample_time_ms: 6830.699
    update_time_ms: 25.119
  timestamp: 1602718975
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: a052f_00000
  
2020-10-14 23:42:56,859	WARNING util.py:136 -- The `process_trial` operation took 0.9440093040466309 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    352 |          9349.59 | 56950784 |  296.877 |              316.172 |              145.717 |            793.649 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3095.943876202007
    time_step_min: 2977
  date: 2020-10-14_23-43-23
  done: false
  episode_len_mean: 793.6766174690835
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.9176205697687
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 71887
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.946667321861743e-32
        cur_lr: 5.0e-05
        entropy: 0.053452868945896626
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050773267867043614
        model: {}
        policy_loss: -0.007172615095138705
        total_loss: 0.20521489282449087
        vf_explained_var: 0.9995869994163513
        vf_loss: 0.2124142311513424
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.603125
    gpu_util_percent0: 0.2521875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612350916574987
    mean_env_wait_ms: 1.2000076343312847
    mean_inference_ms: 4.291329558319792
    mean_raw_obs_processing_ms: 0.3770721978829894
  time_since_restore: 9376.279317378998
  time_this_iter_s: 26.694244623184204
  time_total_s: 9376.279317378998
  timers:
    learn_throughput: 8338.495
    learn_time_ms: 19403.022
    sample_throughput: 23684.229
    sample_time_ms: 6831.212
    update_time_ms: 23.716
  timestamp: 1602719003
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: a052f_00000
  
2020-10-14 23:43:24,825	WARNING util.py:136 -- The `process_trial` operation took 0.9372138977050781 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    353 |          9376.28 | 57112576 |  296.918 |              316.172 |              145.717 |            793.677 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3095.7106804741943
    time_step_min: 2977
  date: 2020-10-14_23-43-51
  done: false
  episode_len_mean: 793.6962784114562
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.9520204122648
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 72066
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.946667321861743e-32
        cur_lr: 5.0e-05
        entropy: 0.05741896169881026
        entropy_coeff: 0.0005000000000000001
        kl: 0.00478822198541214
        model: {}
        policy_loss: -0.011413306036653617
        total_loss: 0.2605178455511729
        vf_explained_var: 0.9994891285896301
        vf_loss: 0.27195985491077107
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.20645161290323
    gpu_util_percent0: 0.35258064516129034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612278613075155
    mean_env_wait_ms: 1.199936859879655
    mean_inference_ms: 4.291293745478692
    mean_raw_obs_processing_ms: 0.3770686825139144
  time_since_restore: 9402.651728868484
  time_this_iter_s: 26.372411489486694
  time_total_s: 9402.651728868484
  timers:
    learn_throughput: 8350.697
    learn_time_ms: 19374.669
    sample_throughput: 23705.332
    sample_time_ms: 6825.131
    update_time_ms: 22.437
  timestamp: 1602719031
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: a052f_00000
  
2020-10-14 23:43:52,465	WARNING util.py:136 -- The `process_trial` operation took 0.9359104633331299 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    354 |          9402.65 | 57274368 |  296.952 |              316.172 |              145.717 |            793.696 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3095.4289906022063
    time_step_min: 2977
  date: 2020-10-14_23-44-19
  done: false
  episode_len_mean: 793.7269331341054
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 296.9954234580056
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 72279
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4733336609308716e-32
        cur_lr: 5.0e-05
        entropy: 0.05630615974466006
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006405569409253076
        total_loss: .inf
        vf_explained_var: 0.9997084736824036
        vf_loss: 0.17392988751331964
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.412499999999998
    gpu_util_percent0: 0.30312500000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612174684880222
    mean_env_wait_ms: 1.199850963347008
    mean_inference_ms: 4.291246925768317
    mean_raw_obs_processing_ms: 0.37706426572853674
  time_since_restore: 9429.267405509949
  time_this_iter_s: 26.615676641464233
  time_total_s: 9429.267405509949
  timers:
    learn_throughput: 8354.146
    learn_time_ms: 19366.672
    sample_throughput: 23723.191
    sample_time_ms: 6819.993
    update_time_ms: 21.831
  timestamp: 1602719059
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:44:20,449	WARNING util.py:136 -- The `process_trial` operation took 1.0033934116363525 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    355 |          9429.27 | 57436160 |  296.995 |              316.172 |              145.717 |            793.727 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3095.136712982766
    time_step_min: 2977
  date: 2020-10-14_23-44-46
  done: false
  episode_len_mean: 793.757451621357
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.0391910999761
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 222
  episodes_total: 72501
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.210000491396308e-32
        cur_lr: 5.0e-05
        entropy: 0.056829413399100304
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006917582380992826
        total_loss: .inf
        vf_explained_var: 0.9996049404144287
        vf_loss: 0.22120977689822516
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.512903225806454
    gpu_util_percent0: 0.35258064516129023
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611987390964395
    mean_env_wait_ms: 1.1997630348781645
    mean_inference_ms: 4.29119051024482
    mean_raw_obs_processing_ms: 0.3770595458175393
  time_since_restore: 9455.535111188889
  time_this_iter_s: 26.26770567893982
  time_total_s: 9455.535111188889
  timers:
    learn_throughput: 8360.343
    learn_time_ms: 19352.316
    sample_throughput: 23746.076
    sample_time_ms: 6813.421
    update_time_ms: 22.205
  timestamp: 1602719086
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:44:48,173	WARNING util.py:136 -- The `process_trial` operation took 1.0012962818145752 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    356 |          9455.54 | 57597952 |  297.039 |              316.172 |              145.717 |            793.757 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3094.9086792556704
    time_step_min: 2977
  date: 2020-10-14_23-45-14
  done: false
  episode_len_mean: 793.7817126190083
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.0728241637104
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 183
  episodes_total: 72684
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.815000737094461e-32
        cur_lr: 5.0e-05
        entropy: 0.05449420399963856
        entropy_coeff: 0.0005000000000000001
        kl: 0.004970248439349234
        model: {}
        policy_loss: -0.008427266181873469
        total_loss: 0.25762742509444553
        vf_explained_var: 0.9994832873344421
        vf_loss: 0.26608194038271904
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.987499999999997
    gpu_util_percent0: 0.2909375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461191963943905
    mean_env_wait_ms: 1.1996901935777484
    mean_inference_ms: 4.291155200587853
    mean_raw_obs_processing_ms: 0.37705587096319476
  time_since_restore: 9482.286239862442
  time_this_iter_s: 26.751128673553467
  time_total_s: 9482.286239862442
  timers:
    learn_throughput: 8346.054
    learn_time_ms: 19385.448
    sample_throughput: 23731.165
    sample_time_ms: 6817.702
    update_time_ms: 22.236
  timestamp: 1602719114
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: a052f_00000
  
2020-10-14 23:45:16,314	WARNING util.py:136 -- The `process_trial` operation took 0.9579312801361084 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    357 |          9482.29 | 57759744 |  297.073 |              316.172 |              145.717 |            793.782 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3094.6951746859772
    time_step_min: 2977
  date: 2020-10-14_23-45-42
  done: false
  episode_len_mean: 793.8046601622
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.10690786669505
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 189
  episodes_total: 72873
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9075003685472305e-32
        cur_lr: 5.0e-05
        entropy: 0.05754050457229217
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0073823366352977855
        total_loss: .inf
        vf_explained_var: 0.9991450309753418
        vf_loss: 0.48056190957625705
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.428125
    gpu_util_percent0: 0.33125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146118363669963
    mean_env_wait_ms: 1.1996156250314625
    mean_inference_ms: 4.291113274531339
    mean_raw_obs_processing_ms: 0.37705189531300304
  time_since_restore: 9508.628410100937
  time_this_iter_s: 26.342170238494873
  time_total_s: 9508.628410100937
  timers:
    learn_throughput: 8362.153
    learn_time_ms: 19348.127
    sample_throughput: 23784.397
    sample_time_ms: 6802.443
    update_time_ms: 23.004
  timestamp: 1602719142
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:45:43,987	WARNING util.py:136 -- The `process_trial` operation took 0.9914751052856445 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    358 |          9508.63 | 57921536 |  297.107 |              316.172 |              145.717 |            793.805 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3094.4368943997374
    time_step_min: 2977
  date: 2020-10-14_23-46-10
  done: false
  episode_len_mean: 793.8312766772464
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.1464067455773
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 73096
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.861250552820847e-32
        cur_lr: 5.0e-05
        entropy: 0.05763770888249079
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00888269444112666
        total_loss: .inf
        vf_explained_var: 0.9992542266845703
        vf_loss: 0.4503108685215314
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3483870967742
    gpu_util_percent0: 0.40290322580645155
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611715434199685
    mean_env_wait_ms: 1.1995281881383222
    mean_inference_ms: 4.291067086794425
    mean_raw_obs_processing_ms: 0.3770476659237885
  time_since_restore: 9534.898722171783
  time_this_iter_s: 26.270312070846558
  time_total_s: 9534.898722171783
  timers:
    learn_throughput: 8370.535
    learn_time_ms: 19328.753
    sample_throughput: 23851.681
    sample_time_ms: 6783.254
    update_time_ms: 23.399
  timestamp: 1602719170
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:46:11,688	WARNING util.py:136 -- The `process_trial` operation took 0.9804244041442871 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    359 |           9534.9 | 58083328 |  297.146 |              316.172 |              145.717 |            793.831 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3094.182872739679
    time_step_min: 2977
  date: 2020-10-14_23-46-37
  done: false
  episode_len_mean: 793.8612880782506
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.185783320566
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 207
  episodes_total: 73303
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.791875829231269e-32
        cur_lr: 5.0e-05
        entropy: 0.0536729758605361
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006009495031321421
        total_loss: .inf
        vf_explained_var: 0.9994203448295593
        vf_loss: 0.36571721235911053
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.6375
    gpu_util_percent0: 0.241875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611559777946534
    mean_env_wait_ms: 1.19944547889839
    mean_inference_ms: 4.291018950636396
    mean_raw_obs_processing_ms: 0.3770432616357701
  time_since_restore: 9561.166570425034
  time_this_iter_s: 26.267848253250122
  time_total_s: 9561.166570425034
  timers:
    learn_throughput: 8386.918
    learn_time_ms: 19290.995
    sample_throughput: 23950.897
    sample_time_ms: 6755.154
    update_time_ms: 25.967
  timestamp: 1602719197
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:46:39,459	WARNING util.py:136 -- The `process_trial` operation took 0.9834418296813965 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    360 |          9561.17 | 58245120 |  297.186 |              316.172 |              145.717 |            793.861 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3093.982028590878
    time_step_min: 2977
  date: 2020-10-14_23-47-05
  done: false
  episode_len_mean: 793.8807942513405
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.21830543107626
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 73478
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3187813743846904e-31
        cur_lr: 5.0e-05
        entropy: 0.05787413567304611
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007117118589424838
        total_loss: .inf
        vf_explained_var: 0.999517023563385
        vf_loss: 0.23725286995371184
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.99032258064516
    gpu_util_percent0: 0.34161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611471519607636
    mean_env_wait_ms: 1.1993767004368565
    mean_inference_ms: 4.290980087132088
    mean_raw_obs_processing_ms: 0.3770398748701587
  time_since_restore: 9587.694075107574
  time_this_iter_s: 26.527504682540894
  time_total_s: 9587.694075107574
  timers:
    learn_throughput: 8384.433
    learn_time_ms: 19296.712
    sample_throughput: 23975.059
    sample_time_ms: 6748.346
    update_time_ms: 25.848
  timestamp: 1602719225
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:47:07,377	WARNING util.py:136 -- The `process_trial` operation took 0.9783501625061035 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    361 |          9587.69 | 58406912 |  297.218 |              316.172 |              145.717 |            793.881 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3093.724525535554
    time_step_min: 2977
  date: 2020-10-14_23-47-33
  done: false
  episode_len_mean: 793.9062423666712
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.2588430100979
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 212
  episodes_total: 73690
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9781720615770353e-31
        cur_lr: 5.0e-05
        entropy: 0.05787798079351584
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004630507144611329
        total_loss: .inf
        vf_explained_var: 0.999623715877533
        vf_loss: 0.20473009596268335
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.603125
    gpu_util_percent0: 0.244375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461137076826222
    mean_env_wait_ms: 1.1992938564434508
    mean_inference_ms: 4.29093726009559
    mean_raw_obs_processing_ms: 0.3770352589765149
  time_since_restore: 9614.130667448044
  time_this_iter_s: 26.43659234046936
  time_total_s: 9614.130667448044
  timers:
    learn_throughput: 8396.331
    learn_time_ms: 19269.37
    sample_throughput: 23994.005
    sample_time_ms: 6743.018
    update_time_ms: 25.564
  timestamp: 1602719253
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:47:35,163	WARNING util.py:136 -- The `process_trial` operation took 1.0081942081451416 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    362 |          9614.13 | 58568704 |  297.259 |              316.172 |              145.717 |            793.906 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3093.457755820249
    time_step_min: 2977
  date: 2020-10-14_23-48-01
  done: false
  episode_len_mean: 793.9352844076419
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.299968757226
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 218
  episodes_total: 73908
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9672580923655536e-31
        cur_lr: 5.0e-05
        entropy: 0.05280969695498546
        entropy_coeff: 0.0005000000000000001
        kl: 0.004381697818947335
        model: {}
        policy_loss: -0.006313406326322972
        total_loss: 0.20173545430103937
        vf_explained_var: 0.999642550945282
        vf_loss: 0.2080752675731977
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.590322580645164
    gpu_util_percent0: 0.31000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611256601286063
    mean_env_wait_ms: 1.1992092513475163
    mean_inference_ms: 4.29089170631117
    mean_raw_obs_processing_ms: 0.3770312751295953
  time_since_restore: 9640.503136396408
  time_this_iter_s: 26.372468948364258
  time_total_s: 9640.503136396408
  timers:
    learn_throughput: 8406.292
    learn_time_ms: 19246.536
    sample_throughput: 23988.492
    sample_time_ms: 6744.567
    update_time_ms: 25.72
  timestamp: 1602719281
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: a052f_00000
  
2020-10-14 23:48:02,975	WARNING util.py:136 -- The `process_trial` operation took 1.0066180229187012 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    363 |           9640.5 | 58730496 |    297.3 |              316.172 |              145.717 |            793.935 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3093.2159250459013
    time_step_min: 2977
  date: 2020-10-14_23-48-29
  done: false
  episode_len_mean: 793.9634008097166
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.3373553347234
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 192
  episodes_total: 74100
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4836290461827768e-31
        cur_lr: 5.0e-05
        entropy: 0.049925824937721096
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005909375298263815
        total_loss: .inf
        vf_explained_var: 0.9997458457946777
        vf_loss: 0.1355716505398353
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.709375
    gpu_util_percent0: 0.2990625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611130306812278
    mean_env_wait_ms: 1.1991335279022874
    mean_inference_ms: 4.290849741873617
    mean_raw_obs_processing_ms: 0.3770273968400634
  time_since_restore: 9667.44416809082
  time_this_iter_s: 26.94103169441223
  time_total_s: 9667.44416809082
  timers:
    learn_throughput: 8388.96
    learn_time_ms: 19286.299
    sample_throughput: 23942.517
    sample_time_ms: 6757.518
    update_time_ms: 26.077
  timestamp: 1602719309
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:48:31,399	WARNING util.py:136 -- The `process_trial` operation took 1.0145034790039062 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    364 |          9667.44 | 58892288 |  297.337 |              316.172 |              145.717 |            793.963 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3092.9933201352133
    time_step_min: 2977
  date: 2020-10-14_23-48-58
  done: false
  episode_len_mean: 793.9880723199742
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.3716895942091
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 181
  episodes_total: 74281
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2254435692741655e-31
        cur_lr: 5.0e-05
        entropy: 0.05465832383682331
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005471556200063787
        total_loss: .inf
        vf_explained_var: 0.9997373223304749
        vf_loss: 0.1391865313053131
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.30625
    gpu_util_percent0: 0.28937499999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611058607223526
    mean_env_wait_ms: 1.1990633825671995
    mean_inference_ms: 4.29081566506714
    mean_raw_obs_processing_ms: 0.37702395088770274
  time_since_restore: 9694.216431617737
  time_this_iter_s: 26.772263526916504
  time_total_s: 9694.216431617737
  timers:
    learn_throughput: 8387.055
    learn_time_ms: 19290.681
    sample_throughput: 23880.782
    sample_time_ms: 6774.987
    update_time_ms: 26.883
  timestamp: 1602719338
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:48:59,633	WARNING util.py:136 -- The `process_trial` operation took 1.0153813362121582 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    365 |          9694.22 | 59054080 |  297.372 |              316.172 |              145.717 |            793.988 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3092.73286871047
    time_step_min: 2977
  date: 2020-10-14_23-49-26
  done: false
  episode_len_mean: 794.0106715706672
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.4105488350622
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 216
  episodes_total: 74497
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3381653539112474e-31
        cur_lr: 5.0e-05
        entropy: 0.06017392811675867
        entropy_coeff: 0.0005000000000000001
        kl: 0.005472181752944986
        model: {}
        policy_loss: -0.00805678572457206
        total_loss: 0.2731111707786719
        vf_explained_var: 0.999495267868042
        vf_loss: 0.28119805082678795
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.5625
    gpu_util_percent0: 0.248125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610966382464244
    mean_env_wait_ms: 1.1989811686401395
    mean_inference_ms: 4.290774378576409
    mean_raw_obs_processing_ms: 0.3770199344179391
  time_since_restore: 9720.664158344269
  time_this_iter_s: 26.447726726531982
  time_total_s: 9720.664158344269
  timers:
    learn_throughput: 8382.915
    learn_time_ms: 19300.207
    sample_throughput: 23878.067
    sample_time_ms: 6775.758
    update_time_ms: 26.855
  timestamp: 1602719366
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: a052f_00000
  
2020-10-14 23:49:27,487	WARNING util.py:136 -- The `process_trial` operation took 0.9793968200683594 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    366 |          9720.66 | 59215872 |  297.411 |              316.172 |              145.717 |            794.011 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3092.454009907618
    time_step_min: 2977
  date: 2020-10-14_23-49-54
  done: false
  episode_len_mean: 794.0405524773147
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.4521092236097
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 221
  episodes_total: 74718
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3381653539112474e-31
        cur_lr: 5.0e-05
        entropy: 0.05816090448449055
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006203005024872255
        total_loss: .inf
        vf_explained_var: 0.9996395111083984
        vf_loss: 0.20675778264800707
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.31612903225807
    gpu_util_percent0: 0.26741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610793826741955
    mean_env_wait_ms: 1.1988947102580008
    mean_inference_ms: 4.290721761533726
    mean_raw_obs_processing_ms: 0.3770153145193993
  time_since_restore: 9747.319639205933
  time_this_iter_s: 26.65548086166382
  time_total_s: 9747.319639205933
  timers:
    learn_throughput: 8385.646
    learn_time_ms: 19293.921
    sample_throughput: 23893.128
    sample_time_ms: 6771.487
    update_time_ms: 26.938
  timestamp: 1602719394
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:49:55,551	WARNING util.py:136 -- The `process_trial` operation took 0.9717733860015869 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    367 |          9747.32 | 59377664 |  297.452 |              316.172 |              145.717 |            794.041 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3092.258040818507
    time_step_min: 2977
  date: 2020-10-14_23-50-22
  done: false
  episode_len_mean: 794.0579603717155
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.48160223253643
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 178
  episodes_total: 74896
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.007248030866871e-31
        cur_lr: 5.0e-05
        entropy: 0.06004595446089903
        entropy_coeff: 0.0005000000000000001
        kl: 0.004715288950440784
        model: {}
        policy_loss: -0.01000713906250894
        total_loss: 0.5848141883810362
        vf_explained_var: 0.9988470673561096
        vf_loss: 0.5948513398567835
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.796875
    gpu_util_percent0: 0.2596875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610725473743064
    mean_env_wait_ms: 1.1988260106733528
    mean_inference_ms: 4.290688607319503
    mean_raw_obs_processing_ms: 0.37701196019343025
  time_since_restore: 9774.120095014572
  time_this_iter_s: 26.800455808639526
  time_total_s: 9774.120095014572
  timers:
    learn_throughput: 8364.216
    learn_time_ms: 19343.356
    sample_throughput: 23875.358
    sample_time_ms: 6776.527
    update_time_ms: 25.43
  timestamp: 1602719422
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: a052f_00000
  
2020-10-14 23:50:23,802	WARNING util.py:136 -- The `process_trial` operation took 1.0155887603759766 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    368 |          9774.12 | 59539456 |  297.482 |              316.172 |              145.717 |            794.058 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3092.061642375636
    time_step_min: 2977
  date: 2020-10-14_23-50-50
  done: false
  episode_len_mean: 794.0778931948329
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.5128916007857
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 194
  episodes_total: 75090
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5036240154334355e-31
        cur_lr: 5.0e-05
        entropy: 0.058149436178306736
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008335834466076145
        total_loss: .inf
        vf_explained_var: 0.9994199275970459
        vf_loss: 0.3182927370071411
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.89375
    gpu_util_percent0: 0.3765625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610654400022627
    mean_env_wait_ms: 1.1987521811548814
    mean_inference_ms: 4.290649139127143
    mean_raw_obs_processing_ms: 0.3770080616580994
  time_since_restore: 9800.630022525787
  time_this_iter_s: 26.50992751121521
  time_total_s: 9800.630022525787
  timers:
    learn_throughput: 8357.385
    learn_time_ms: 19359.166
    sample_throughput: 23854.41
    sample_time_ms: 6782.477
    update_time_ms: 27.167
  timestamp: 1602719450
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:50:51,632	WARNING util.py:136 -- The `process_trial` operation took 0.9691581726074219 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    369 |          9800.63 | 59701248 |  297.513 |              316.172 |              145.717 |            794.078 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3091.7992136443336
    time_step_min: 2977
  date: 2020-10-14_23-51-18
  done: false
  episode_len_mean: 794.1082032079881
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.5542909174598
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 222
  episodes_total: 75312
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.755436023150153e-31
        cur_lr: 5.0e-05
        entropy: 0.056066238631804786
        entropy_coeff: 0.0005000000000000001
        kl: 0.005611350333007674
        model: {}
        policy_loss: -0.0052828191519438406
        total_loss: 0.1838942915201187
        vf_explained_var: 0.9996773600578308
        vf_loss: 0.18920514608422914
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.121875
    gpu_util_percent0: 0.26937500000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610525177696745
    mean_env_wait_ms: 1.1986670711765455
    mean_inference_ms: 4.290607039759741
    mean_raw_obs_processing_ms: 0.37700400749720264
  time_since_restore: 9827.184224128723
  time_this_iter_s: 26.55420160293579
  time_total_s: 9827.184224128723
  timers:
    learn_throughput: 8349.954
    learn_time_ms: 19376.395
    sample_throughput: 23851.458
    sample_time_ms: 6783.317
    update_time_ms: 27.224
  timestamp: 1602719478
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: a052f_00000
  
2020-10-14 23:51:19,553	WARNING util.py:136 -- The `process_trial` operation took 0.9733555316925049 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    370 |          9827.18 | 59863040 |  297.554 |              316.172 |              145.717 |            794.108 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3091.571521301399
    time_step_min: 2977
  date: 2020-10-14_23-51-46
  done: false
  episode_len_mean: 794.1336935219026
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.59085852967905
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 204
  episodes_total: 75516
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.755436023150153e-31
        cur_lr: 5.0e-05
        entropy: 0.05998164570579926
        entropy_coeff: 0.0005000000000000001
        kl: 0.004842932607668142
        model: {}
        policy_loss: -0.00726580640184693
        total_loss: 0.3228081415096919
        vf_explained_var: 0.9994247555732727
        vf_loss: 0.33010393877824146
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.100000000000005
    gpu_util_percent0: 0.3064516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610396699444422
    mean_env_wait_ms: 1.1985880503984796
    mean_inference_ms: 4.290563076574562
    mean_raw_obs_processing_ms: 0.3769999736193242
  time_since_restore: 9853.749536514282
  time_this_iter_s: 26.565312385559082
  time_total_s: 9853.749536514282
  timers:
    learn_throughput: 8351.304
    learn_time_ms: 19373.263
    sample_throughput: 23826.825
    sample_time_ms: 6790.33
    update_time_ms: 27.023
  timestamp: 1602719506
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: a052f_00000
  
2020-10-14 23:51:47,579	WARNING util.py:136 -- The `process_trial` operation took 1.003483533859253 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    371 |          9853.75 | 60024832 |  297.591 |              316.172 |              145.717 |            794.134 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3091.360329879467
    time_step_min: 2977
  date: 2020-10-14_23-52-14
  done: false
  episode_len_mean: 794.1536754214449
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.62063535529643
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 176
  episodes_total: 75692
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8777180115750766e-31
        cur_lr: 5.0e-05
        entropy: 0.06254499995460112
        entropy_coeff: 0.0005000000000000001
        kl: 0.006550203310325742
        model: {}
        policy_loss: -0.007455484251598439
        total_loss: 0.42502578099568683
        vf_explained_var: 0.999161422252655
        vf_loss: 0.4325125341614087
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.465625
    gpu_util_percent0: 0.2525
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610306341489568
    mean_env_wait_ms: 1.1985213592490793
    mean_inference_ms: 4.290528190329915
    mean_raw_obs_processing_ms: 0.3769967781564062
  time_since_restore: 9880.231082201004
  time_this_iter_s: 26.4815456867218
  time_total_s: 9880.231082201004
  timers:
    learn_throughput: 8354.059
    learn_time_ms: 19366.873
    sample_throughput: 23790.525
    sample_time_ms: 6800.691
    update_time_ms: 26.63
  timestamp: 1602719534
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: a052f_00000
  
2020-10-14 23:52:15,418	WARNING util.py:136 -- The `process_trial` operation took 1.0043320655822754 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    372 |          9880.23 | 60186624 |  297.621 |              316.172 |              145.717 |            794.154 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3091.164503077349
    time_step_min: 2977
  date: 2020-10-14_23-52-41
  done: false
  episode_len_mean: 794.1745866543706
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.6509485873819
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 75905
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8777180115750766e-31
        cur_lr: 5.0e-05
        entropy: 0.059429747983813286
        entropy_coeff: 0.0005000000000000001
        kl: 0.00791717281875511
        model: {}
        policy_loss: -0.009694739244878292
        total_loss: 0.6807864656051
        vf_explained_var: 0.99892258644104
        vf_loss: 0.6905109484990438
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.803124999999998
    gpu_util_percent0: 0.2953125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610226032129334
    mean_env_wait_ms: 1.1984410367130154
    mean_inference_ms: 4.2904887498597395
    mean_raw_obs_processing_ms: 0.37699276598901904
  time_since_restore: 9906.808052539825
  time_this_iter_s: 26.57697033882141
  time_total_s: 9906.808052539825
  timers:
    learn_throughput: 8348.49
    learn_time_ms: 19379.793
    sample_throughput: 23778.198
    sample_time_ms: 6804.216
    update_time_ms: 26.555
  timestamp: 1602719561
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: a052f_00000
  
2020-10-14 23:52:43,543	WARNING util.py:136 -- The `process_trial` operation took 1.0169646739959717 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    373 |          9906.81 | 60348416 |  297.651 |              316.172 |              145.717 |            794.175 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3090.931902703129
    time_step_min: 2977
  date: 2020-10-14_23-53-10
  done: false
  episode_len_mean: 794.2003546798029
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.68733641837053
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 220
  episodes_total: 76125
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8777180115750766e-31
        cur_lr: 5.0e-05
        entropy: 0.05564382734398047
        entropy_coeff: 0.0005000000000000001
        kl: 0.004743205655055742
        model: {}
        policy_loss: -0.00747959664689309
        total_loss: 0.4181689073642095
        vf_explained_var: 0.9992935061454773
        vf_loss: 0.42567632844050723
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.703225806451616
    gpu_util_percent0: 0.3674193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610096093544667
    mean_env_wait_ms: 1.1983577911515584
    mean_inference_ms: 4.2904469810755765
    mean_raw_obs_processing_ms: 0.37698863837488533
  time_since_restore: 9933.272503614426
  time_this_iter_s: 26.46445107460022
  time_total_s: 9933.272503614426
  timers:
    learn_throughput: 8366.653
    learn_time_ms: 19337.721
    sample_throughput: 23788.843
    sample_time_ms: 6801.171
    update_time_ms: 26.05
  timestamp: 1602719590
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: a052f_00000
  
2020-10-14 23:53:11,458	WARNING util.py:136 -- The `process_trial` operation took 0.9973330497741699 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    374 |          9933.27 | 60510208 |  297.687 |              316.172 |              145.717 |              794.2 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3090.7188773971975
    time_step_min: 2977
  date: 2020-10-14_23-53-38
  done: false
  episode_len_mean: 794.2261285461574
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.72109485075435
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 190
  episodes_total: 76315
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.388590057875383e-32
        cur_lr: 5.0e-05
        entropy: 0.05459365497032801
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010453447214482972
        total_loss: .inf
        vf_explained_var: 0.9996538162231445
        vf_loss: 0.17427268624305725
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.446875
    gpu_util_percent0: 0.33562499999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460998292402806
    mean_env_wait_ms: 1.1982855721882018
    mean_inference_ms: 4.290409143933071
    mean_raw_obs_processing_ms: 0.3769850315017426
  time_since_restore: 9960.01085639
  time_this_iter_s: 26.73835277557373
  time_total_s: 9960.01085639
  timers:
    learn_throughput: 8361.972
    learn_time_ms: 19348.547
    sample_throughput: 23860.31
    sample_time_ms: 6780.8
    update_time_ms: 25.046
  timestamp: 1602719618
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:53:39,557	WARNING util.py:136 -- The `process_trial` operation took 1.0070092678070068 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    375 |          9960.01 | 60672000 |  297.721 |              316.172 |              145.717 |            794.226 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3090.5165749107505
    time_step_min: 2977
  date: 2020-10-14_23-54-06
  done: false
  episode_len_mean: 794.2494803853646
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.7530232454345
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 184
  episodes_total: 76499
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4082885086813076e-31
        cur_lr: 5.0e-05
        entropy: 0.05618108933170637
        entropy_coeff: 0.0005000000000000001
        kl: 0.00305322171576942
        model: {}
        policy_loss: -0.00579473125981167
        total_loss: 0.28295011321703595
        vf_explained_var: 0.9994378685951233
        vf_loss: 0.28877294063568115
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.35
    gpu_util_percent0: 0.275
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609945149640227
    mean_env_wait_ms: 1.198217209398833
    mean_inference_ms: 4.290376451093084
    mean_raw_obs_processing_ms: 0.37698175903680375
  time_since_restore: 9986.873464345932
  time_this_iter_s: 26.862607955932617
  time_total_s: 9986.873464345932
  timers:
    learn_throughput: 8347.944
    learn_time_ms: 19381.061
    sample_throughput: 23817.178
    sample_time_ms: 6793.08
    update_time_ms: 27.179
  timestamp: 1602719646
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: a052f_00000
  
2020-10-14 23:54:07,856	WARNING util.py:136 -- The `process_trial` operation took 0.9845466613769531 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    376 |          9986.87 | 60833792 |  297.753 |              316.172 |              145.717 |            794.249 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3090.2601648279997
    time_step_min: 2977
  date: 2020-10-14_23-54-34
  done: false
  episode_len_mean: 794.2764423703627
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.79094079728867
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 215
  episodes_total: 76714
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.041442543406538e-32
        cur_lr: 5.0e-05
        entropy: 0.056260092494388424
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006567445506031315
        total_loss: .inf
        vf_explained_var: 0.9995599389076233
        vf_loss: 0.26980039353171986
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.51212121212121
    gpu_util_percent0: 0.2718181818181818
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8696969696969696
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609822482190565
    mean_env_wait_ms: 1.1981364141697433
    mean_inference_ms: 4.290336270321847
    mean_raw_obs_processing_ms: 0.37697789244248014
  time_since_restore: 10013.976362228394
  time_this_iter_s: 27.102897882461548
  time_total_s: 10013.976362228394
  timers:
    learn_throughput: 8344.527
    learn_time_ms: 19388.995
    sample_throughput: 23729.606
    sample_time_ms: 6818.15
    update_time_ms: 27.56
  timestamp: 1602719674
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:54:36,438	WARNING util.py:136 -- The `process_trial` operation took 1.0540385246276855 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    377 |            10014 | 60995584 |  297.791 |              316.172 |              145.717 |            794.276 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3090.0124444097682
    time_step_min: 2977
  date: 2020-10-14_23-55-02
  done: false
  episode_len_mean: 794.3021968022878
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.82727377768276
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 216
  episodes_total: 76930
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0562163815109807e-31
        cur_lr: 5.0e-05
        entropy: 0.055669725562135376
        entropy_coeff: 0.0005000000000000001
        kl: 0.004704963105420272
        model: {}
        policy_loss: -0.008044822424077816
        total_loss: 0.36663875977198285
        vf_explained_var: 0.9993283152580261
        vf_loss: 0.37471139927705127
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.529032258064515
    gpu_util_percent0: 0.31935483870967746
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609668283271646
    mean_env_wait_ms: 1.1980545846817656
    mean_inference_ms: 4.29028990734574
    mean_raw_obs_processing_ms: 0.37697364229298236
  time_since_restore: 10040.464388370514
  time_this_iter_s: 26.48802614212036
  time_total_s: 10040.464388370514
  timers:
    learn_throughput: 8355.004
    learn_time_ms: 19364.683
    sample_throughput: 23756.582
    sample_time_ms: 6810.407
    update_time_ms: 27.094
  timestamp: 1602719702
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: a052f_00000
  
2020-10-14 23:55:04,398	WARNING util.py:136 -- The `process_trial` operation took 1.0204463005065918 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    378 |          10040.5 | 61157376 |  297.827 |              316.172 |              145.717 |            794.302 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3089.813697634894
    time_step_min: 2977
  date: 2020-10-14_23-55-31
  done: false
  episode_len_mean: 794.3258459024472
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.85858009458934
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 77107
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.281081907554903e-32
        cur_lr: 5.0e-05
        entropy: 0.05110831093043089
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0068184423004519585
        total_loss: .inf
        vf_explained_var: 0.9995319247245789
        vf_loss: 0.22743881369630495
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.078125
    gpu_util_percent0: 0.295
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460960342480483
    mean_env_wait_ms: 1.1979884623228658
    mean_inference_ms: 4.290259501673946
    mean_raw_obs_processing_ms: 0.3769704355006722
  time_since_restore: 10067.102005958557
  time_this_iter_s: 26.637617588043213
  time_total_s: 10067.102005958557
  timers:
    learn_throughput: 8349.206
    learn_time_ms: 19378.13
    sample_throughput: 23753.014
    sample_time_ms: 6811.43
    update_time_ms: 25.179
  timestamp: 1602719731
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:55:32,492	WARNING util.py:136 -- The `process_trial` operation took 1.0023703575134277 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    379 |          10067.1 | 61319168 |  297.859 |              316.172 |              145.717 |            794.326 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3089.584207870185
    time_step_min: 2977
  date: 2020-10-14_23-55-59
  done: false
  episode_len_mean: 794.3491016337459
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.89259704813236
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 200
  episodes_total: 77307
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.921622861332355e-32
        cur_lr: 5.0e-05
        entropy: 0.054080450596908726
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007005679644256209
        total_loss: .inf
        vf_explained_var: 0.9994975924491882
        vf_loss: 0.27496982490022975
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.646875
    gpu_util_percent0: 0.33656250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609548769947495
    mean_env_wait_ms: 1.1979148127647672
    mean_inference_ms: 4.290225110763869
    mean_raw_obs_processing_ms: 0.376966779726577
  time_since_restore: 10093.805670976639
  time_this_iter_s: 26.703665018081665
  time_total_s: 10093.805670976639
  timers:
    learn_throughput: 8342.963
    learn_time_ms: 19392.63
    sample_throughput: 23726.077
    sample_time_ms: 6819.164
    update_time_ms: 25.182
  timestamp: 1602719759
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:56:00,540	WARNING util.py:136 -- The `process_trial` operation took 0.9762120246887207 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    380 |          10093.8 | 61480960 |  297.893 |              316.172 |              145.717 |            794.349 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3089.3397762609516
    time_step_min: 2977
  date: 2020-10-14_23-56-27
  done: false
  episode_len_mean: 794.3755497942706
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.9294230858673
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 222
  episodes_total: 77529
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.188243429199853e-31
        cur_lr: 5.0e-05
        entropy: 0.05175605043768883
        entropy_coeff: 0.0005000000000000001
        kl: 0.003275777504313737
        model: {}
        policy_loss: -0.009282395825721323
        total_loss: 0.3009416162967682
        vf_explained_var: 0.9994567036628723
        vf_loss: 0.3102498898903529
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.35625
    gpu_util_percent0: 0.30937500000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609407052807216
    mean_env_wait_ms: 1.1978313648523176
    mean_inference_ms: 4.290183095249902
    mean_raw_obs_processing_ms: 0.3769628970637402
  time_since_restore: 10120.476508140564
  time_this_iter_s: 26.67083716392517
  time_total_s: 10120.476508140564
  timers:
    learn_throughput: 8346.679
    learn_time_ms: 19383.998
    sample_throughput: 23691.431
    sample_time_ms: 6829.136
    update_time_ms: 31.747
  timestamp: 1602719787
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: a052f_00000
  
2020-10-14 23:56:28,575	WARNING util.py:136 -- The `process_trial` operation took 0.9911458492279053 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    381 |          10120.5 | 61642752 |  297.929 |              316.172 |              145.717 |            794.376 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3089.1151193019487
    time_step_min: 2977
  date: 2020-10-14_23-56-55
  done: false
  episode_len_mean: 794.4018139714397
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.964697145129
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 201
  episodes_total: 77730
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.941217145999265e-32
        cur_lr: 5.0e-05
        entropy: 0.046780054457485676
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006118431473926951
        total_loss: .inf
        vf_explained_var: 0.9997798800468445
        vf_loss: 0.11907331707576911
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.459374999999998
    gpu_util_percent0: 0.336875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609287558455486
    mean_env_wait_ms: 1.1977563304814554
    mean_inference_ms: 4.2901423850142715
    mean_raw_obs_processing_ms: 0.3769591311855631
  time_since_restore: 10147.140531301498
  time_this_iter_s: 26.66402316093445
  time_total_s: 10147.140531301498
  timers:
    learn_throughput: 8340.168
    learn_time_ms: 19399.129
    sample_throughput: 23684.941
    sample_time_ms: 6831.007
    update_time_ms: 31.921
  timestamp: 1602719815
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:56:56,651	WARNING util.py:136 -- The `process_trial` operation took 1.0402328968048096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    382 |          10147.1 | 61804544 |  297.965 |              316.172 |              145.717 |            794.402 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3088.9073819643295
    time_step_min: 2977
  date: 2020-10-14_23-57-23
  done: false
  episode_len_mean: 794.4246858433773
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 297.99521418505543
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 77907
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.911825718998897e-32
        cur_lr: 5.0e-05
        entropy: 0.05127061903476715
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008152377520066997
        total_loss: .inf
        vf_explained_var: 0.9995221495628357
        vf_loss: 0.242381169150273
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.146875
    gpu_util_percent0: 0.35750000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609219419428998
    mean_env_wait_ms: 1.197691208907206
    mean_inference_ms: 4.290111914450425
    mean_raw_obs_processing_ms: 0.3769559879261118
  time_since_restore: 10173.764901161194
  time_this_iter_s: 26.624369859695435
  time_total_s: 10173.764901161194
  timers:
    learn_throughput: 8341.086
    learn_time_ms: 19396.995
    sample_throughput: 23669.595
    sample_time_ms: 6835.436
    update_time_ms: 32.898
  timestamp: 1602719843
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:57:24,650	WARNING util.py:136 -- The `process_trial` operation took 1.0153605937957764 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    383 |          10173.8 | 61966336 |  297.995 |              316.172 |              145.717 |            794.425 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3088.677418941761
    time_step_min: 2977
  date: 2020-10-14_23-57-51
  done: false
  episode_len_mean: 794.4503712237583
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.0277428664521
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 78120
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3367738578498348e-31
        cur_lr: 5.0e-05
        entropy: 0.05660749971866608
        entropy_coeff: 0.0005000000000000001
        kl: 0.006111283398543795
        model: {}
        policy_loss: -0.008730516346986406
        total_loss: 0.5457866142193476
        vf_explained_var: 0.9990383982658386
        vf_loss: 0.5545454248785973
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.375
    gpu_util_percent0: 0.29374999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609149434880103
    mean_env_wait_ms: 1.1976145103691633
    mean_inference_ms: 4.2900798711008035
    mean_raw_obs_processing_ms: 0.37695241156343035
  time_since_restore: 10200.477223396301
  time_this_iter_s: 26.712322235107422
  time_total_s: 10200.477223396301
  timers:
    learn_throughput: 8329.81
    learn_time_ms: 19423.252
    sample_throughput: 23681.589
    sample_time_ms: 6831.974
    update_time_ms: 32.978
  timestamp: 1602719871
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: a052f_00000
  
2020-10-14 23:57:52,733	WARNING util.py:136 -- The `process_trial` operation took 1.0032176971435547 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    384 |          10200.5 | 62128128 |  298.028 |              316.172 |              145.717 |             794.45 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3088.4675264972543
    time_step_min: 2977
  date: 2020-10-14_23-58-19
  done: false
  episode_len_mean: 794.472171870612
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.0584925823886
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 218
  episodes_total: 78338
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3367738578498348e-31
        cur_lr: 5.0e-05
        entropy: 0.06033367974062761
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009152304958358096
        total_loss: .inf
        vf_explained_var: 0.998649537563324
        vf_loss: 0.8376990457375845
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.45
    gpu_util_percent0: 0.33687500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609008097068932
    mean_env_wait_ms: 1.1975338790435415
    mean_inference_ms: 4.2900368391245784
    mean_raw_obs_processing_ms: 0.37694883641295807
  time_since_restore: 10227.288994312286
  time_this_iter_s: 26.811770915985107
  time_total_s: 10227.288994312286
  timers:
    learn_throughput: 8330.262
    learn_time_ms: 19422.199
    sample_throughput: 23643.582
    sample_time_ms: 6842.956
    update_time_ms: 34.962
  timestamp: 1602719899
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:58:21,068	WARNING util.py:136 -- The `process_trial` operation took 1.0428755283355713 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    385 |          10227.3 | 62289920 |  298.058 |              316.172 |              145.717 |            794.472 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3088.344891719745
    time_step_min: 2977
  date: 2020-10-14_23-58-47
  done: false
  episode_len_mean: 794.4858012428688
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.0807004179937
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 190
  episodes_total: 78528
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.005160786774752e-31
        cur_lr: 5.0e-05
        entropy: 0.06060913639763991
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01016493421047926
        total_loss: .inf
        vf_explained_var: 0.9984003901481628
        vf_loss: 0.832159032424291
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.246875
    gpu_util_percent0: 0.32437499999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460891601544017
    mean_env_wait_ms: 1.1974633890696205
    mean_inference_ms: 4.29000193460861
    mean_raw_obs_processing_ms: 0.3769451844922303
  time_since_restore: 10253.872016429901
  time_this_iter_s: 26.583022117614746
  time_total_s: 10253.872016429901
  timers:
    learn_throughput: 8339.505
    learn_time_ms: 19400.673
    sample_throughput: 23657.898
    sample_time_ms: 6838.816
    update_time_ms: 33.194
  timestamp: 1602719927
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:58:49,081	WARNING util.py:136 -- The `process_trial` operation took 1.0714685916900635 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    386 |          10253.9 | 62451712 |  298.081 |              316.172 |              145.717 |            794.486 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3088.1599735664904
    time_step_min: 2977
  date: 2020-10-14_23-59-15
  done: false
  episode_len_mean: 794.5059962396464
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.1091731892835
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 78716
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.007741180162129e-31
        cur_lr: 5.0e-05
        entropy: 0.05556652881205082
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055948966182768345
        model: {}
        policy_loss: -0.010029586740226174
        total_loss: 0.2815673140188058
        vf_explained_var: 0.9994375705718994
        vf_loss: 0.2916246863702933
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3125
    gpu_util_percent0: 0.32999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608875634833332
    mean_env_wait_ms: 1.1973954202451982
    mean_inference_ms: 4.289969774585609
    mean_raw_obs_processing_ms: 0.37694192689195655
  time_since_restore: 10280.476027011871
  time_this_iter_s: 26.604010581970215
  time_total_s: 10280.476027011871
  timers:
    learn_throughput: 8353.352
    learn_time_ms: 19368.512
    sample_throughput: 23713.816
    sample_time_ms: 6822.689
    update_time_ms: 32.496
  timestamp: 1602719955
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: a052f_00000
  
2020-10-14 23:59:17,107	WARNING util.py:136 -- The `process_trial` operation took 1.0582659244537354 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    387 |          10280.5 | 62613504 |  298.109 |              316.172 |              145.717 |            794.506 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3087.9294830361328
    time_step_min: 2977
  date: 2020-10-14_23-59-43
  done: false
  episode_len_mean: 794.5319202848057
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.1443415160328
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 215
  episodes_total: 78931
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.007741180162129e-31
        cur_lr: 5.0e-05
        entropy: 0.052544912633796535
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007429640041664243
        total_loss: .inf
        vf_explained_var: 0.999578058719635
        vf_loss: 0.2945559037228425
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3375
    gpu_util_percent0: 0.2921875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608755777160418
    mean_env_wait_ms: 1.1973172308953752
    mean_inference_ms: 4.289934400066829
    mean_raw_obs_processing_ms: 0.37693850873737306
  time_since_restore: 10307.117371797562
  time_this_iter_s: 26.641344785690308
  time_total_s: 10307.117371797562
  timers:
    learn_throughput: 8351.457
    learn_time_ms: 19372.907
    sample_throughput: 23711.154
    sample_time_ms: 6823.455
    update_time_ms: 33.046
  timestamp: 1602719983
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-14 23:59:45,157	WARNING util.py:136 -- The `process_trial` operation took 1.0293552875518799 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    388 |          10307.1 | 62775296 |  298.144 |              316.172 |              145.717 |            794.532 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3087.7079678454966
    time_step_min: 2977
  date: 2020-10-15_00-00-11
  done: false
  episode_len_mean: 794.559459732646
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.1790775620656
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 215
  episodes_total: 79146
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.511611770243193e-31
        cur_lr: 5.0e-05
        entropy: 0.05223243683576584
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006523460401998212
        total_loss: .inf
        vf_explained_var: 0.9992337226867676
        vf_loss: 0.4362075626850128
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.671875
    gpu_util_percent0: 0.35375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608626667503952
    mean_env_wait_ms: 1.1972379319546402
    mean_inference_ms: 4.28989229306317
    mean_raw_obs_processing_ms: 0.37693454156460915
  time_since_restore: 10333.671075105667
  time_this_iter_s: 26.55370330810547
  time_total_s: 10333.671075105667
  timers:
    learn_throughput: 8352.795
    learn_time_ms: 19369.803
    sample_throughput: 23735.009
    sample_time_ms: 6816.597
    update_time_ms: 33.243
  timestamp: 1602720011
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:00:13,268	WARNING util.py:136 -- The `process_trial` operation took 1.0189995765686035 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    389 |          10333.7 | 62937088 |  298.179 |              316.172 |              145.717 |            794.559 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3087.5168865235705
    time_step_min: 2977
  date: 2020-10-15_00-00-39
  done: false
  episode_len_mean: 794.5819066589345
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.20708051239257
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 176
  episodes_total: 79322
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.76741765536479e-31
        cur_lr: 5.0e-05
        entropy: 0.050660982728004456
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041265281033702195
        model: {}
        policy_loss: -0.008306438898822913
        total_loss: 0.2659478100637595
        vf_explained_var: 0.999445915222168
        vf_loss: 0.27427958448727924
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.441935483870974
    gpu_util_percent0: 0.3077419354838711
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608551706645287
    mean_env_wait_ms: 1.197174629762414
    mean_inference_ms: 4.28986334591658
    mean_raw_obs_processing_ms: 0.3769316314261495
  time_since_restore: 10360.144921064377
  time_this_iter_s: 26.473845958709717
  time_total_s: 10360.144921064377
  timers:
    learn_throughput: 8360.129
    learn_time_ms: 19352.811
    sample_throughput: 23748.77
    sample_time_ms: 6812.647
    update_time_ms: 31.406
  timestamp: 1602720039
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: a052f_00000
  
2020-10-15 00:00:41,317	WARNING util.py:136 -- The `process_trial` operation took 1.1157991886138916 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    390 |          10360.1 | 63098880 |  298.207 |              316.172 |              145.717 |            794.582 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3087.3168209721243
    time_step_min: 2977
  date: 2020-10-15_00-01-07
  done: false
  episode_len_mean: 794.6056787887933
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.23664554096837
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 202
  episodes_total: 79524
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.383708827682395e-31
        cur_lr: 5.0e-05
        entropy: 0.05082682427018881
        entropy_coeff: 0.0005000000000000001
        kl: 0.004214590958630045
        model: {}
        policy_loss: -0.008368752841003394
        total_loss: 0.37631993740797043
        vf_explained_var: 0.9992954134941101
        vf_loss: 0.38471410671869916
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.056250000000002
    gpu_util_percent0: 0.2703125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608506086497552
    mean_env_wait_ms: 1.1971031655939546
    mean_inference_ms: 4.2898349003465155
    mean_raw_obs_processing_ms: 0.37692850250287896
  time_since_restore: 10386.790776014328
  time_this_iter_s: 26.645854949951172
  time_total_s: 10386.790776014328
  timers:
    learn_throughput: 8351.402
    learn_time_ms: 19373.034
    sample_throughput: 23803.119
    sample_time_ms: 6797.092
    update_time_ms: 24.53
  timestamp: 1602720067
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: a052f_00000
  
2020-10-15 00:01:09,499	WARNING util.py:136 -- The `process_trial` operation took 1.0732991695404053 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    391 |          10386.8 | 63260672 |  298.237 |              316.172 |              145.717 |            794.606 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3087.0815258978637
    time_step_min: 2977
  date: 2020-10-15_00-01-36
  done: false
  episode_len_mean: 794.6336071227037
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.2729774388182
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 221
  episodes_total: 79745
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6918544138411976e-31
        cur_lr: 5.0e-05
        entropy: 0.04559079774965843
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008972606048700982
        total_loss: .inf
        vf_explained_var: 0.9997599124908447
        vf_loss: 0.1404727747042974
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.253125
    gpu_util_percent0: 0.289375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460836412422902
    mean_env_wait_ms: 1.197022370667845
    mean_inference_ms: 4.289794234000164
    mean_raw_obs_processing_ms: 0.3769250535939533
  time_since_restore: 10413.345429420471
  time_this_iter_s: 26.55465340614319
  time_total_s: 10413.345429420471
  timers:
    learn_throughput: 8347.493
    learn_time_ms: 19382.106
    sample_throughput: 23848.858
    sample_time_ms: 6784.057
    update_time_ms: 25.524
  timestamp: 1602720096
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:01:37,586	WARNING util.py:136 -- The `process_trial` operation took 1.0748589038848877 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    392 |          10413.3 | 63422464 |  298.273 |              316.172 |              145.717 |            794.634 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3086.8723784974222
    time_step_min: 2977
  date: 2020-10-15_00-02-04
  done: false
  episode_len_mean: 794.6570724507155
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.3042042308399
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 79944
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.537781620761796e-31
        cur_lr: 5.0e-05
        entropy: 0.04920199327170849
        entropy_coeff: 0.0005000000000000001
        kl: 0.005658271450859805
        model: {}
        policy_loss: -0.007346010420102782
        total_loss: 0.2005082219839096
        vf_explained_var: 0.999619722366333
        vf_loss: 0.20787883549928665
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.790625000000002
    gpu_util_percent0: 0.2996875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608252990459447
    mean_env_wait_ms: 1.196950398716273
    mean_inference_ms: 4.289757335004326
    mean_raw_obs_processing_ms: 0.3769211989623741
  time_since_restore: 10440.036368846893
  time_this_iter_s: 26.69093942642212
  time_total_s: 10440.036368846893
  timers:
    learn_throughput: 8348.121
    learn_time_ms: 19380.65
    sample_throughput: 23820.181
    sample_time_ms: 6792.224
    update_time_ms: 25.56
  timestamp: 1602720124
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: a052f_00000
  
2020-10-15 00:02:05,751	WARNING util.py:136 -- The `process_trial` operation took 1.096177577972412 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    393 |            10440 | 63584256 |  298.304 |              316.172 |              145.717 |            794.657 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3086.670865846807
    time_step_min: 2977
  date: 2020-10-15_00-02-32
  done: false
  episode_len_mean: 794.6804413214683
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.33372830686915
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 80123
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.537781620761796e-31
        cur_lr: 5.0e-05
        entropy: 0.04850280936807394
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005830896596307866
        total_loss: .inf
        vf_explained_var: 0.9997227191925049
        vf_loss: 0.1539867346485456
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.121875
    gpu_util_percent0: 0.31906249999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460819545098079
    mean_env_wait_ms: 1.1968867157692777
    mean_inference_ms: 4.289729278177505
    mean_raw_obs_processing_ms: 0.37691831120677843
  time_since_restore: 10466.694134235382
  time_this_iter_s: 26.65776538848877
  time_total_s: 10466.694134235382
  timers:
    learn_throughput: 8360.389
    learn_time_ms: 19352.21
    sample_throughput: 23775.626
    sample_time_ms: 6804.952
    update_time_ms: 25.816
  timestamp: 1602720152
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:02:33,854	WARNING util.py:136 -- The `process_trial` operation took 1.0546934604644775 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    394 |          10466.7 | 63746048 |  298.334 |              316.172 |              145.717 |             794.68 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3086.443962940825
    time_step_min: 2977
  date: 2020-10-15_00-03-00
  done: false
  episode_len_mean: 794.7087959966141
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.3684397880107
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 209
  episodes_total: 80332
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8066724311426944e-31
        cur_lr: 5.0e-05
        entropy: 0.047138823506732784
        entropy_coeff: 0.0005000000000000001
        kl: 0.006326191864597301
        model: {}
        policy_loss: -0.009382577147334814
        total_loss: 0.09006761604299147
        vf_explained_var: 0.9998289942741394
        vf_loss: 0.09947376698255539
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.175
    gpu_util_percent0: 0.2990625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608133236374
    mean_env_wait_ms: 1.1968128900330315
    mean_inference_ms: 4.289698600830083
    mean_raw_obs_processing_ms: 0.3769149456766659
  time_since_restore: 10493.7303109169
  time_this_iter_s: 27.036176681518555
  time_total_s: 10493.7303109169
  timers:
    learn_throughput: 8352.463
    learn_time_ms: 19370.575
    sample_throughput: 23777.757
    sample_time_ms: 6804.342
    update_time_ms: 25.865
  timestamp: 1602720180
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: a052f_00000
  
2020-10-15 00:03:02,440	WARNING util.py:136 -- The `process_trial` operation took 1.0573394298553467 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    395 |          10493.7 | 63907840 |  298.368 |              316.172 |              145.717 |            794.709 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3086.2133800280667
    time_step_min: 2977
  date: 2020-10-15_00-03-29
  done: false
  episode_len_mean: 794.735571252995
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.4025154275178
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 219
  episodes_total: 80551
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8066724311426944e-31
        cur_lr: 5.0e-05
        entropy: 0.04599378599474827
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006334368770088379
        total_loss: .inf
        vf_explained_var: 0.9996090531349182
        vf_loss: 0.22827293847997984
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.765625
    gpu_util_percent0: 0.2903125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607993237263067
    mean_env_wait_ms: 1.1967341312749145
    mean_inference_ms: 4.289660606887559
    mean_raw_obs_processing_ms: 0.3769118845191138
  time_since_restore: 10520.434611320496
  time_this_iter_s: 26.70430040359497
  time_total_s: 10520.434611320496
  timers:
    learn_throughput: 8351.45
    learn_time_ms: 19372.923
    sample_throughput: 23748.955
    sample_time_ms: 6812.594
    update_time_ms: 26.53
  timestamp: 1602720209
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:03:30,565	WARNING util.py:136 -- The `process_trial` operation took 1.0457687377929688 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    396 |          10520.4 | 64069632 |  298.403 |              316.172 |              145.717 |            794.736 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3086.009725939142
    time_step_min: 2977
  date: 2020-10-15_00-03-57
  done: false
  episode_len_mean: 794.7593881595244
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.43394122047783
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 189
  episodes_total: 80740
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.710008646714042e-31
        cur_lr: 5.0e-05
        entropy: 0.047122120236357055
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005733807956592803
        total_loss: .inf
        vf_explained_var: 0.9998292922973633
        vf_loss: 0.09110324271023273
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.05625
    gpu_util_percent0: 0.294375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607900157593698
    mean_env_wait_ms: 1.1966664613317697
    mean_inference_ms: 4.28963026462047
    mean_raw_obs_processing_ms: 0.37690824691128144
  time_since_restore: 10547.391422748566
  time_this_iter_s: 26.95681142807007
  time_total_s: 10547.391422748566
  timers:
    learn_throughput: 8342.526
    learn_time_ms: 19393.647
    sample_throughput: 23673.236
    sample_time_ms: 6834.385
    update_time_ms: 26.481
  timestamp: 1602720237
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:03:59,071	WARNING util.py:136 -- The `process_trial` operation took 1.0886297225952148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    397 |          10547.4 | 64231424 |  298.434 |              316.172 |              145.717 |            794.759 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3085.8105462231615
    time_step_min: 2977
  date: 2020-10-15_00-04-25
  done: false
  episode_len_mean: 794.7833409531812
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.46457245039903
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 189
  episodes_total: 80929
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.565012970071061e-31
        cur_lr: 5.0e-05
        entropy: 0.049051981108884014
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031219907687045634
        model: {}
        policy_loss: -0.008623141989422342
        total_loss: 0.14397887513041496
        vf_explained_var: 0.9997089505195618
        vf_loss: 0.15262654796242714
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.328125
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607860873172196
    mean_env_wait_ms: 1.1966001808944813
    mean_inference_ms: 4.289596678066117
    mean_raw_obs_processing_ms: 0.37690505889851034
  time_since_restore: 10573.924202919006
  time_this_iter_s: 26.532780170440674
  time_total_s: 10573.924202919006
  timers:
    learn_throughput: 8349.172
    learn_time_ms: 19378.209
    sample_throughput: 23638.086
    sample_time_ms: 6844.547
    update_time_ms: 29.418
  timestamp: 1602720265
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: a052f_00000
  
2020-10-15 00:04:27,050	WARNING util.py:136 -- The `process_trial` operation took 1.0757784843444824 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    398 |          10573.9 | 64393216 |  298.465 |              316.172 |              145.717 |            794.783 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3085.5909729756386
    time_step_min: 2977
  date: 2020-10-15_00-04-53
  done: false
  episode_len_mean: 794.808442198669
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.4984158817652
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 211
  episodes_total: 81140
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2825064850355305e-31
        cur_lr: 5.0e-05
        entropy: 0.052438030329843364
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007665548978062968
        total_loss: .inf
        vf_explained_var: 0.9996669888496399
        vf_loss: 0.19768254086375237
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.346875
    gpu_util_percent0: 0.35906249999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607760433046593
    mean_env_wait_ms: 1.1965253486286143
    mean_inference_ms: 4.289567391274282
    mean_raw_obs_processing_ms: 0.3769020474825674
  time_since_restore: 10600.625540494919
  time_this_iter_s: 26.701337575912476
  time_total_s: 10600.625540494919
  timers:
    learn_throughput: 8349.936
    learn_time_ms: 19376.437
    sample_throughput: 23587.838
    sample_time_ms: 6859.128
    update_time_ms: 29.735
  timestamp: 1602720293
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:04:55,366	WARNING util.py:136 -- The `process_trial` operation took 1.1102097034454346 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    399 |          10600.6 | 64555008 |  298.498 |              316.172 |              145.717 |            794.808 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3085.425348268188
    time_step_min: 2977
  date: 2020-10-15_00-05-22
  done: false
  episode_len_mean: 794.825428041151
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.520724396337
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 219
  episodes_total: 81359
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.423759727553295e-31
        cur_lr: 5.0e-05
        entropy: 0.07317631815870602
        entropy_coeff: 0.0005000000000000001
        kl: 0.006871112738735974
        model: {}
        policy_loss: -0.012375502847135067
        total_loss: 1.4743601779143016
        vf_explained_var: 0.9973661303520203
        vf_loss: 1.4867722590764363
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.7
    gpu_util_percent0: 0.2778125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607615137915533
    mean_env_wait_ms: 1.1964467437883408
    mean_inference_ms: 4.289528388268468
    mean_raw_obs_processing_ms: 0.37689827688568994
  time_since_restore: 10627.483373880386
  time_this_iter_s: 26.85783338546753
  time_total_s: 10627.483373880386
  timers:
    learn_throughput: 8340.442
    learn_time_ms: 19398.492
    sample_throughput: 23543.897
    sample_time_ms: 6871.929
    update_time_ms: 29.524
  timestamp: 1602720322
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: a052f_00000
  
2020-10-15 00:05:23,785	WARNING util.py:136 -- The `process_trial` operation took 1.0948469638824463 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    400 |          10627.5 | 64716800 |  298.521 |              316.172 |              145.717 |            794.825 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3085.391946605158
    time_step_min: 2977
  date: 2020-10-15_00-05-50
  done: false
  episode_len_mean: 794.8260480290431
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.51938808201186
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 81534
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.423759727553295e-31
        cur_lr: 5.0e-05
        entropy: 0.0774936197946469
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013239652616903186
        total_loss: .inf
        vf_explained_var: 0.9962027072906494
        vf_loss: 1.9817346930503845
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.959375
    gpu_util_percent0: 0.3190625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607561505235792
    mean_env_wait_ms: 1.1963860056596523
    mean_inference_ms: 4.289502451497982
    mean_raw_obs_processing_ms: 0.37689559341592344
  time_since_restore: 10654.247864246368
  time_this_iter_s: 26.764490365982056
  time_total_s: 10654.247864246368
  timers:
    learn_throughput: 8341.446
    learn_time_ms: 19396.157
    sample_throughput: 23499.577
    sample_time_ms: 6884.89
    update_time_ms: 29.593
  timestamp: 1602720350
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:05:52,157	WARNING util.py:136 -- The `process_trial` operation took 1.1405515670776367 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    401 |          10654.2 | 64878592 |  298.519 |              316.172 |              145.717 |            794.826 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3085.246998494658
    time_step_min: 2977
  date: 2020-10-15_00-06-18
  done: false
  episode_len_mean: 794.8485019024432
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.54534826716286
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 203
  episodes_total: 81737
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.635639591329943e-31
        cur_lr: 5.0e-05
        entropy: 0.05337563343346119
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00829991620654861
        total_loss: .inf
        vf_explained_var: 0.9992844462394714
        vf_loss: 0.39766117185354233
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.853125
    gpu_util_percent0: 0.37031250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460750757053521
    mean_env_wait_ms: 1.1963166742842821
    mean_inference_ms: 4.289474147774495
    mean_raw_obs_processing_ms: 0.3768924093834194
  time_since_restore: 10680.773192882538
  time_this_iter_s: 26.525328636169434
  time_total_s: 10680.773192882538
  timers:
    learn_throughput: 8344.169
    learn_time_ms: 19389.828
    sample_throughput: 23491.145
    sample_time_ms: 6887.361
    update_time_ms: 28.297
  timestamp: 1602720378
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:06:20,241	WARNING util.py:136 -- The `process_trial` operation took 1.0839407444000244 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    402 |          10680.8 | 65040384 |  298.545 |              316.172 |              145.717 |            794.849 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3085.031867112554
    time_step_min: 2977
  date: 2020-10-15_00-06-46
  done: false
  episode_len_mean: 794.8748688416583
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.58090272925153
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 225
  episodes_total: 81962
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4453459386994917e-30
        cur_lr: 5.0e-05
        entropy: 0.05049538208792607
        entropy_coeff: 0.0005000000000000001
        kl: 0.004538885647586237
        model: {}
        policy_loss: -0.005989701946721955
        total_loss: 0.2606619521975517
        vf_explained_var: 0.9995473027229309
        vf_loss: 0.2666769027709961
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.809375
    gpu_util_percent0: 0.32999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460736229801305
    mean_env_wait_ms: 1.1962365596620128
    mean_inference_ms: 4.2894361018378095
    mean_raw_obs_processing_ms: 0.37688909964315376
  time_since_restore: 10707.324377536774
  time_this_iter_s: 26.55118465423584
  time_total_s: 10707.324377536774
  timers:
    learn_throughput: 8350.131
    learn_time_ms: 19375.984
    sample_throughput: 23523.193
    sample_time_ms: 6877.978
    update_time_ms: 26.788
  timestamp: 1602720406
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: a052f_00000
  
2020-10-15 00:06:48,242	WARNING util.py:136 -- The `process_trial` operation took 1.0734214782714844 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    403 |          10707.3 | 65202176 |  298.581 |              316.172 |              145.717 |            794.875 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3084.8298408601104
    time_step_min: 2977
  date: 2020-10-15_00-07-15
  done: false
  episode_len_mean: 794.8981462322139
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.61049938507693
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 195
  episodes_total: 82157
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.2267296934974585e-31
        cur_lr: 5.0e-05
        entropy: 0.04719574407984813
        entropy_coeff: 0.0005000000000000001
        kl: 0.004900996185218294
        model: {}
        policy_loss: -0.007765007304745571
        total_loss: 0.20102989797790846
        vf_explained_var: 0.9995940327644348
        vf_loss: 0.20881849775711694
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.996875
    gpu_util_percent0: 0.2503125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607269794866487
    mean_env_wait_ms: 1.196167953703566
    mean_inference_ms: 4.289405028348417
    mean_raw_obs_processing_ms: 0.37688587989174804
  time_since_restore: 10734.13394498825
  time_this_iter_s: 26.80956745147705
  time_total_s: 10734.13394498825
  timers:
    learn_throughput: 8336.079
    learn_time_ms: 19408.646
    sample_throughput: 23583.526
    sample_time_ms: 6860.382
    update_time_ms: 26.364
  timestamp: 1602720435
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: a052f_00000
  
2020-10-15 00:07:16,502	WARNING util.py:136 -- The `process_trial` operation took 1.0635132789611816 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    404 |          10734.1 | 65363968 |   298.61 |              316.172 |              145.717 |            794.898 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3084.6557765439147
    time_step_min: 2977
  date: 2020-10-15_00-07-43
  done: false
  episode_len_mean: 794.9196089147993
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.6371034079173
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 178
  episodes_total: 82335
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6133648467487292e-31
        cur_lr: 5.0e-05
        entropy: 0.05154151966174444
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005915318201004993
        total_loss: .inf
        vf_explained_var: 0.9994816184043884
        vf_loss: 0.2618162805835406
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.95625
    gpu_util_percent0: 0.2459375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607215837669707
    mean_env_wait_ms: 1.1961065503553283
    mean_inference_ms: 4.289376836409755
    mean_raw_obs_processing_ms: 0.3768829092704441
  time_since_restore: 10760.77581834793
  time_this_iter_s: 26.641873359680176
  time_total_s: 10760.77581834793
  timers:
    learn_throughput: 8356.081
    learn_time_ms: 19362.186
    sample_throughput: 23577.781
    sample_time_ms: 6862.054
    update_time_ms: 26.151
  timestamp: 1602720463
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:07:44,631	WARNING util.py:136 -- The `process_trial` operation took 1.059859275817871 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    405 |          10760.8 | 65525760 |  298.637 |              316.172 |              145.717 |             794.92 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3084.4589013705604
    time_step_min: 2977
  date: 2020-10-15_00-08-11
  done: false
  episode_len_mean: 794.9446510557366
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.668610660506
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 214
  episodes_total: 82549
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.420047270123093e-31
        cur_lr: 5.0e-05
        entropy: 0.05205119370172421
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005658113465566809
        total_loss: .inf
        vf_explained_var: 0.9991742968559265
        vf_loss: 0.49400301029284793
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.021875
    gpu_util_percent0: 0.25875000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607147837999146
    mean_env_wait_ms: 1.19603326577136
    mean_inference_ms: 4.289349373821089
    mean_raw_obs_processing_ms: 0.3768795901922165
  time_since_restore: 10787.464892148972
  time_this_iter_s: 26.68907380104065
  time_total_s: 10787.464892148972
  timers:
    learn_throughput: 8354.788
    learn_time_ms: 19365.182
    sample_throughput: 23626.498
    sample_time_ms: 6847.904
    update_time_ms: 25.34
  timestamp: 1602720491
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: a052f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 00:08:12,876	WARNING util.py:136 -- The `process_trial` operation took 1.1288840770721436 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |    406 |          10787.5 | 65687552 |  298.669 |              316.172 |              145.717 |            794.945 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a052f_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3084.229426765449
    time_step_min: 2977
  date: 2020-10-15_00-08-39
  done: true
  episode_len_mean: 794.972139327526
  episode_reward_max: 316.1717171717166
  episode_reward_mean: 298.7025743181305
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 220
  episodes_total: 82769
  experiment_id: 165e19af1fec47018887d66ceb7af0b3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.130070905184642e-31
        cur_lr: 5.0e-05
        entropy: 0.044818478326002754
        entropy_coeff: 0.0005000000000000001
        kl: 0.005046803698254128
        model: {}
        policy_loss: -0.007625033986793521
        total_loss: 0.10592914931476116
        vf_explained_var: 0.9998006820678711
        vf_loss: 0.11357659101486206
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.478125
    gpu_util_percent0: 0.30593750000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 422
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607015536399434
    mean_env_wait_ms: 1.195956209379515
    mean_inference_ms: 4.289311747655062
    mean_raw_obs_processing_ms: 0.3768764733061813
  time_since_restore: 10814.195276737213
  time_this_iter_s: 26.730384588241577
  time_total_s: 10814.195276737213
  timers:
    learn_throughput: 8351.706
    learn_time_ms: 19372.328
    sample_throughput: 23764.909
    sample_time_ms: 6808.021
    update_time_ms: 25.584
  timestamp: 1602720519
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: a052f_00000
  
2020-10-15 00:08:41,279	WARNING util.py:136 -- The `process_trial` operation took 1.2749769687652588 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 25.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | TERMINATED |       |    407 |          10814.2 | 65849344 |  298.703 |              316.172 |              145.717 |            794.972 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a052f_00000 | TERMINATED |       |    407 |          10814.2 | 65849344 |  298.703 |              316.172 |              145.717 |            794.972 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


