2020-10-11 02:01:16,042	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8266[39m[22m
== Status ==
Memory usage on this node: 32.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_a6496_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=36795)[0m 2020-10-11 02:01:19,011	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=36770)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36770)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36708)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36708)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36829)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36829)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36712)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36712)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36821)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36821)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36710)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36710)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36801)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36801)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36696)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36696)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36778)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36778)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36762)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36762)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36784)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36784)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36730)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36730)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36738)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36738)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36814)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36814)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36711)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36711)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36716)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36716)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36726)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36726)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36720)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36720)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36792)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36792)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36765)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36765)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36723)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36723)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36773)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36773)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36718)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36718)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36703)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36703)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36702)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36702)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36763)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36763)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36756)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36756)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36759)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36759)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36783)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36783)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36822)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36822)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36815)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36815)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36775)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36775)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36813)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36813)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36789)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36789)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36771)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36771)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36728)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36728)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36764)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36764)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36735)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36735)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36767)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36767)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36733)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36733)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36806)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36806)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36704)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36704)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36700)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36700)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36790)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36790)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36833)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36833)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36772)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36772)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36698)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36698)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36714)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36714)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36787)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36787)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36766)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36766)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36774)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36774)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36780)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36780)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36776)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36776)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36697)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36697)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36782)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36782)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36777)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36777)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36781)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36781)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36705)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36705)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36701)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36701)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36707)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36707)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36798)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36798)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36811)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36811)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36699)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36699)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36836)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36836)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36713)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36713)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36737)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36737)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36794)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36794)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36769)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36769)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36825)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36825)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36803)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36803)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36761)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36761)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36779)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36779)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36805)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36805)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36785)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36785)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36725)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36725)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36820)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36820)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36786)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36786)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36715)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36715)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=36793)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=36793)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-11_02-01-58
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 1.1814798968178886
        entropy_coeff: 0.0
        kl: 0.007969200211976255
        model: {}
        policy_loss: -0.005780456870395158
        total_loss: 19.855808666774205
        vf_explained_var: 0.45542749762535095
        vf_loss: 19.85999502454485
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.5075
    gpu_util_percent0: 0.27599999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.00025
    ram_util_percent: 6.2675
    vram_util_percent0: 0.18964414562151524
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.18094502396331813
    mean_env_wait_ms: 1.2290000856640382
    mean_inference_ms: 6.863632973567614
    mean_raw_obs_processing_ms: 0.5000362413906322
  time_since_restore: 33.39646339416504
  time_this_iter_s: 33.39646339416504
  time_total_s: 33.39646339416504
  timers:
    learn_throughput: 7085.513
    learn_time_ms: 22834.196
    sample_throughput: 15415.494
    sample_time_ms: 10495.414
    update_time_ms: 31.012
  timestamp: 1602381718
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      1 |          33.3965 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3597.7048611111113
    time_step_min: 3312
  date: 2020-10-11_02-02-28
  done: false
  episode_len_mean: 880.5284810126582
  episode_reward_max: 264.2020202020202
  episode_reward_mean: 218.36772791203148
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 1.1465644921575273
        entropy_coeff: 0.0
        kl: 0.010220900177955627
        model: {}
        policy_loss: -0.005447401897981763
        total_loss: 12.240083490099225
        vf_explained_var: 0.7765488028526306
        vf_loss: 12.243486540658134
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.175675675675677
    gpu_util_percent0: 0.3240540540540541
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.462162162162162
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17402025393201898
    mean_env_wait_ms: 1.218100523836841
    mean_inference_ms: 6.349030309264849
    mean_raw_obs_processing_ms: 0.478093046478531
  time_since_restore: 64.0907084941864
  time_this_iter_s: 30.694245100021362
  time_total_s: 64.0907084941864
  timers:
    learn_throughput: 7114.293
    learn_time_ms: 22741.824
    sample_throughput: 17534.841
    sample_time_ms: 9226.887
    update_time_ms: 31.056
  timestamp: 1602381748
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      2 |          64.0907 | 323584 |  218.368 |              264.202 |              145.717 |            880.528 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3591.0784753363228
    time_step_min: 3312
  date: 2020-10-11_02-02-58
  done: false
  episode_len_mean: 870.0801687763714
  episode_reward_max: 264.2020202020202
  episode_reward_mean: 219.4705280654646
  episode_reward_min: 136.17171717171715
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 1.12765428849629
        entropy_coeff: 0.0
        kl: 0.009503740351647139
        model: {}
        policy_loss: -0.006432474557576435
        total_loss: 13.23840618133545
        vf_explained_var: 0.8649366497993469
        vf_loss: 13.242937360491071
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.999999999999996
    gpu_util_percent0: 0.3925
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4833333333333325
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1700344675617952
    mean_env_wait_ms: 1.2150702271902607
    mean_inference_ms: 6.025129200366446
    mean_raw_obs_processing_ms: 0.46455551969713593
  time_since_restore: 94.14343404769897
  time_this_iter_s: 30.052725553512573
  time_total_s: 94.14343404769897
  timers:
    learn_throughput: 7135.494
    learn_time_ms: 22674.252
    sample_throughput: 18749.264
    sample_time_ms: 8629.246
    update_time_ms: 30.754
  timestamp: 1602381778
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      3 |          94.1434 | 485376 |  219.471 |              264.202 |              136.172 |             870.08 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3577.8923841059604
    time_step_min: 3312
  date: 2020-10-11_02-03-28
  done: false
  episode_len_mean: 860.2246835443038
  episode_reward_max: 269.20202020202
  episode_reward_mean: 222.26229062779674
  episode_reward_min: 135.56565656565638
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 1.0929351363863264
        entropy_coeff: 0.0
        kl: 0.008082273516005703
        model: {}
        policy_loss: -0.0062619894228451555
        total_loss: 14.53056710106986
        vf_explained_var: 0.8974133133888245
        vf_loss: 14.535212653023857
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.57142857142857
    gpu_util_percent0: 0.4408571428571428
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.48
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16728177139986652
    mean_env_wait_ms: 1.2155902116291812
    mean_inference_ms: 5.800525095863286
    mean_raw_obs_processing_ms: 0.4547206498150975
  time_since_restore: 124.04704570770264
  time_this_iter_s: 29.903611660003662
  time_total_s: 124.04704570770264
  timers:
    learn_throughput: 7146.98
    learn_time_ms: 22637.812
    sample_throughput: 19498.05
    sample_time_ms: 8297.855
    update_time_ms: 29.753
  timestamp: 1602381808
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      4 |          124.047 | 647168 |  222.262 |              269.202 |              135.566 |            860.225 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3568.7917121046894
    time_step_min: 3278
  date: 2020-10-11_02-03-58
  done: false
  episode_len_mean: 843.7047619047619
  episode_reward_max: 271.4747474747473
  episode_reward_mean: 223.99855699855686
  episode_reward_min: 135.56565656565638
  episodes_this_iter: 313
  episodes_total: 945
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 1.0629850115094865
        entropy_coeff: 0.0
        kl: 0.006945903146905559
        model: {}
        policy_loss: -0.005337580951163545
        total_loss: 20.82646574292864
        vf_explained_var: 0.9366797208786011
        vf_loss: 20.830414090837753
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.769444444444446
    gpu_util_percent0: 0.35888888888888887
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4833333333333325
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16384172385233348
    mean_env_wait_ms: 1.2203857348119465
    mean_inference_ms: 5.524678683365137
    mean_raw_obs_processing_ms: 0.44254496694385714
  time_since_restore: 153.95776987075806
  time_this_iter_s: 29.91072416305542
  time_total_s: 153.95776987075806
  timers:
    learn_throughput: 7153.026
    learn_time_ms: 22618.679
    sample_throughput: 19982.176
    sample_time_ms: 8096.816
    update_time_ms: 28.532
  timestamp: 1602381838
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      5 |          153.958 | 808960 |  223.999 |              271.475 |              135.566 |            843.705 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3559.2495361781075
    time_step_min: 3253
  date: 2020-10-11_02-04-29
  done: false
  episode_len_mean: 836.7160940325497
  episode_reward_max: 273.14141414141375
  episode_reward_mean: 225.52455842329243
  episode_reward_min: 135.56565656565638
  episodes_this_iter: 161
  episodes_total: 1106
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 1.0428904039519173
        entropy_coeff: 0.0
        kl: 0.00885618883850319
        model: {}
        policy_loss: -0.005811814312689226
        total_loss: 13.506397792271205
        vf_explained_var: 0.9462892413139343
        vf_loss: 13.510438646589007
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.522222222222226
    gpu_util_percent0: 0.29638888888888887
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.486111111111111
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16263213244064495
    mean_env_wait_ms: 1.2225036247291543
    mean_inference_ms: 5.429255593516622
    mean_raw_obs_processing_ms: 0.4383208108094476
  time_since_restore: 184.05345749855042
  time_this_iter_s: 30.09568762779236
  time_total_s: 184.05345749855042
  timers:
    learn_throughput: 7153.649
    learn_time_ms: 22616.708
    sample_throughput: 20271.416
    sample_time_ms: 7981.287
    update_time_ms: 28.91
  timestamp: 1602381869
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      6 |          184.053 | 970752 |  225.525 |              273.141 |              135.566 |            836.716 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3552.73786407767
    time_step_min: 3253
  date: 2020-10-11_02-04-59
  done: false
  episode_len_mean: 830.757911392405
  episode_reward_max: 277.080808080808
  episode_reward_mean: 226.66522024037835
  episode_reward_min: 135.56565656565638
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 1.0015440327780587
        entropy_coeff: 0.0
        kl: 0.009049633384815283
        model: {}
        policy_loss: -0.007451840809413365
        total_loss: 11.743043422698975
        vf_explained_var: 0.9611858129501343
        vf_loss: 11.748685564313616
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.799999999999997
    gpu_util_percent0: 0.37888888888888883
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.488888888888889
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1616265793480362
    mean_env_wait_ms: 1.2245450479006885
    mean_inference_ms: 5.350160917914372
    mean_raw_obs_processing_ms: 0.43466902298417165
  time_since_restore: 214.0324149131775
  time_this_iter_s: 29.978957414627075
  time_total_s: 214.0324149131775
  timers:
    learn_throughput: 7155.721
    learn_time_ms: 22610.161
    sample_throughput: 20508.804
    sample_time_ms: 7888.905
    update_time_ms: 27.728
  timestamp: 1602381899
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      7 |          214.032 | 1132544 |  226.665 |              277.081 |              135.566 |            830.758 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3548.8888106966924
    time_step_min: 3253
  date: 2020-10-11_02-05-29
  done: false
  episode_len_mean: 825.552104899931
  episode_reward_max: 277.080808080808
  episode_reward_mean: 227.33333333333317
  episode_reward_min: 135.56565656565638
  episodes_this_iter: 185
  episodes_total: 1449
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 0.9336677193641663
        entropy_coeff: 0.0
        kl: 0.008578003384172916
        model: {}
        policy_loss: -0.005669708184931161
        total_loss: 11.386487483978271
        vf_explained_var: 0.9775973558425903
        vf_loss: 11.39044189453125
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87777777777778
    gpu_util_percent0: 0.43916666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.48888888888889
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16065129403270587
    mean_env_wait_ms: 1.2273843907153053
    mean_inference_ms: 5.272251481985681
    mean_raw_obs_processing_ms: 0.4310305030305043
  time_since_restore: 244.1627402305603
  time_this_iter_s: 30.130325317382812
  time_total_s: 244.1627402305603
  timers:
    learn_throughput: 7153.717
    learn_time_ms: 22616.494
    sample_throughput: 20692.933
    sample_time_ms: 7818.708
    update_time_ms: 34.861
  timestamp: 1602381929
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      8 |          244.163 | 1294336 |  227.333 |              277.081 |              135.566 |            825.552 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3544.5602339181287
    time_step_min: 3253
  date: 2020-10-11_02-05-59
  done: false
  episode_len_mean: 818.5569620253165
  episode_reward_max: 277.080808080808
  episode_reward_mean: 228.11412746568087
  episode_reward_min: 135.56565656565638
  episodes_this_iter: 289
  episodes_total: 1738
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 0.9017800944192069
        entropy_coeff: 0.0
        kl: 0.00852125437398042
        model: {}
        policy_loss: -0.005139594320228623
        total_loss: 9.969785485948835
        vf_explained_var: 0.9806135296821594
        vf_loss: 9.973220825195312
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.102777777777778
    gpu_util_percent0: 0.4097222222222222
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.477777777777778
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15943124372366282
    mean_env_wait_ms: 1.231145613309773
    mean_inference_ms: 5.177265066532192
    mean_raw_obs_processing_ms: 0.4265771062835323
  time_since_restore: 274.3121249675751
  time_this_iter_s: 30.14938473701477
  time_total_s: 274.3121249675751
  timers:
    learn_throughput: 7152.879
    learn_time_ms: 22619.145
    sample_throughput: 20809.508
    sample_time_ms: 7774.907
    update_time_ms: 34.553
  timestamp: 1602381959
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |      9 |          274.312 | 1456128 |  228.114 |              277.081 |              135.566 |            818.557 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3543.742505353319
    time_step_min: 3253
  date: 2020-10-11_02-06-29
  done: false
  episode_len_mean: 815.4451476793249
  episode_reward_max: 277.080808080808
  episode_reward_mean: 228.42598452883254
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 158
  episodes_total: 1896
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 0.8636346416813987
        entropy_coeff: 0.0
        kl: 0.007630075294790524
        model: {}
        policy_loss: -0.006490851257694885
        total_loss: 6.756034817014422
        vf_explained_var: 0.9856501221656799
        vf_loss: 6.760999645505633
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.57428571428572
    gpu_util_percent0: 0.412
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.488571428571428
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1588878950657669
    mean_env_wait_ms: 1.2329948250120784
    mean_inference_ms: 5.134727005849136
    mean_raw_obs_processing_ms: 0.42459093027570083
  time_since_restore: 304.14001417160034
  time_this_iter_s: 29.82788920402527
  time_total_s: 304.14001417160034
  timers:
    learn_throughput: 7158.508
    learn_time_ms: 22601.358
    sample_throughput: 20933.88
    sample_time_ms: 7728.715
    update_time_ms: 33.016
  timestamp: 1602381989
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     10 |           304.14 | 1617920 |  228.426 |              277.081 |              132.232 |            815.445 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3540.8440276406714
    time_step_min: 3231
  date: 2020-10-11_02-06-59
  done: false
  episode_len_mean: 812.7030185004869
  episode_reward_max: 277.080808080808
  episode_reward_mean: 228.90717791350693
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 0.8145505615643093
        entropy_coeff: 0.0
        kl: 0.007097901942740593
        model: {}
        policy_loss: -0.004515377588437072
        total_loss: 5.473442588533674
        vf_explained_var: 0.9899949431419373
        vf_loss: 5.476538419723511
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.86944444444445
    gpu_util_percent0: 0.3227777777777778
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.488888888888889
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15839649656714255
    mean_env_wait_ms: 1.2348684569299098
    mean_inference_ms: 5.0963948955990785
    mean_raw_obs_processing_ms: 0.42276320898105285
  time_since_restore: 334.2715263366699
  time_this_iter_s: 30.13151216506958
  time_total_s: 334.2715263366699
  timers:
    learn_throughput: 7164.641
    learn_time_ms: 22582.009
    sample_throughput: 21811.011
    sample_time_ms: 7417.905
    update_time_ms: 34.681
  timestamp: 1602382019
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     11 |          334.272 | 1779712 |  228.907 |              277.081 |              132.232 |            812.703 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3541.4906063193853
    time_step_min: 3231
  date: 2020-10-11_02-07-29
  done: false
  episode_len_mean: 807.9510548523207
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.10817031070192
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 316
  episodes_total: 2370
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 0.7612378682409014
        entropy_coeff: 0.0
        kl: 0.006143723614513874
        model: {}
        policy_loss: -0.0030021907379185514
        total_loss: 7.783948523657663
        vf_explained_var: 0.988917350769043
        vf_loss: 7.785721744809832
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.413888888888888
    gpu_util_percent0: 0.32194444444444437
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.475
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15755827058959693
    mean_env_wait_ms: 1.2385249090423744
    mean_inference_ms: 5.03180110182595
    mean_raw_obs_processing_ms: 0.4197472862266732
  time_since_restore: 364.3553400039673
  time_this_iter_s: 30.083813667297363
  time_total_s: 364.3553400039673
  timers:
    learn_throughput: 7167.423
    learn_time_ms: 22573.244
    sample_throughput: 21964.237
    sample_time_ms: 7366.156
    update_time_ms: 33.86
  timestamp: 1602382049
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     12 |          364.355 | 1941504 |  229.108 |              277.081 |              132.232 |            807.951 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3540.852
    time_step_min: 3231
  date: 2020-10-11_02-08-00
  done: false
  episode_len_mean: 805.9007120253165
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.22701300984525
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 158
  episodes_total: 2528
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 0.7188951671123505
        entropy_coeff: 0.0
        kl: 0.005677759713892426
        model: {}
        policy_loss: -0.0037253251710873364
        total_loss: 4.650596107755389
        vf_explained_var: 0.9912580251693726
        vf_loss: 4.653185912540981
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.13611111111111
    gpu_util_percent0: 0.41138888888888886
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.488888888888889
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15720830437268163
    mean_env_wait_ms: 1.2401347812432035
    mean_inference_ms: 5.004504865395884
    mean_raw_obs_processing_ms: 0.4184712769742799
  time_since_restore: 394.4415285587311
  time_this_iter_s: 30.086188554763794
  time_total_s: 394.4415285587311
  timers:
    learn_throughput: 7166.52
    learn_time_ms: 22576.09
    sample_throughput: 21961.959
    sample_time_ms: 7366.92
    update_time_ms: 32.733
  timestamp: 1602382080
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     13 |          394.442 | 2103296 |  229.227 |              277.081 |              132.232 |            805.901 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3539.4270127915725
    time_step_min: 3231
  date: 2020-10-11_02-08-30
  done: false
  episode_len_mean: 803.9657483246464
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.31444376753385
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 158
  episodes_total: 2686
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 1.0e-05
        entropy: 0.7053305080958775
        entropy_coeff: 0.0
        kl: 0.004527306516787836
        model: {}
        policy_loss: -0.0030261759363513973
        total_loss: 4.08461286340441
        vf_explained_var: 0.9925640225410461
        vf_loss: 4.08673357963562
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.913513513513518
    gpu_util_percent0: 0.3681081081081081
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.494594594594595
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15688866809301757
    mean_env_wait_ms: 1.2417762829378576
    mean_inference_ms: 4.979553399677917
    mean_raw_obs_processing_ms: 0.41729327461310917
  time_since_restore: 424.73452401161194
  time_this_iter_s: 30.29299545288086
  time_total_s: 424.73452401161194
  timers:
    learn_throughput: 7162.814
    learn_time_ms: 22587.772
    sample_throughput: 21886.343
    sample_time_ms: 7392.372
    update_time_ms: 33.187
  timestamp: 1602382110
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     14 |          424.735 | 2265088 |  229.314 |              277.081 |              132.232 |            803.966 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3540.131136516476
    time_step_min: 3231
  date: 2020-10-11_02-09-00
  done: false
  episode_len_mean: 800.5866089273817
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.2055027288205
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 316
  episodes_total: 3002
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 1.0e-05
        entropy: 0.6569898724555969
        entropy_coeff: 0.0
        kl: 0.007279561100793737
        model: {}
        policy_loss: -0.003451603281843875
        total_loss: 5.935438667024885
        vf_explained_var: 0.9921092987060547
        vf_loss: 5.93816225869315
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.225
    gpu_util_percent0: 0.34388888888888897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.477777777777778
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15632915342749998
    mean_env_wait_ms: 1.2449579002315672
    mean_inference_ms: 4.935867759909382
    mean_raw_obs_processing_ms: 0.4152609816198676
  time_since_restore: 454.88604521751404
  time_this_iter_s: 30.1515212059021
  time_total_s: 454.88604521751404
  timers:
    learn_throughput: 7158.505
    learn_time_ms: 22601.368
    sample_throughput: 21856.293
    sample_time_ms: 7402.536
    update_time_ms: 33.045
  timestamp: 1602382140
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     15 |          454.886 | 2426880 |  229.206 |              277.081 |              132.232 |            800.587 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3540.791826309068
    time_step_min: 3231
  date: 2020-10-11_02-09-30
  done: false
  episode_len_mean: 799.0797468354431
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.11082342411453
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 158
  episodes_total: 3160
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 1.0e-05
        entropy: 0.6153967167649951
        entropy_coeff: 0.0
        kl: 0.007886625028082303
        model: {}
        policy_loss: -0.004706729053785759
        total_loss: 3.9784744467054094
        vf_explained_var: 0.9925244450569153
        vf_loss: 3.982392566544669
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.18333333333333
    gpu_util_percent0: 0.36833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.488888888888889
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.156080875562241
    mean_env_wait_ms: 1.246391837272702
    mean_inference_ms: 4.916609894476877
    mean_raw_obs_processing_ms: 0.41436685619725916
  time_since_restore: 484.7389407157898
  time_this_iter_s: 29.852895498275757
  time_total_s: 484.7389407157898
  timers:
    learn_throughput: 7160.39
    learn_time_ms: 22595.416
    sample_throughput: 21912.298
    sample_time_ms: 7383.616
    update_time_ms: 33.463
  timestamp: 1602382170
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     16 |          484.739 | 2588672 |  229.111 |              277.081 |              132.232 |             799.08 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3540.731610942249
    time_step_min: 3231
  date: 2020-10-11_02-10-01
  done: false
  episode_len_mean: 797.7007233273056
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.20608435165397
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 158
  episodes_total: 3318
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 1.0e-05
        entropy: 0.6172731305871691
        entropy_coeff: 0.0
        kl: 0.006947778331648026
        model: {}
        policy_loss: -0.003697997337440029
        total_loss: 3.458932025091989
        vf_explained_var: 0.9934535026550293
        vf_loss: 3.4619351625442505
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78333333333333
    gpu_util_percent0: 0.3672222222222222
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.497222222222222
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15584463256692058
    mean_env_wait_ms: 1.2478123051337193
    mean_inference_ms: 4.898575529604403
    mean_raw_obs_processing_ms: 0.4135165664743834
  time_since_restore: 514.9878952503204
  time_this_iter_s: 30.24895453453064
  time_total_s: 514.9878952503204
  timers:
    learn_throughput: 7153.641
    learn_time_ms: 22616.735
    sample_throughput: 21899.272
    sample_time_ms: 7388.008
    update_time_ms: 33.8
  timestamp: 1602382201
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     17 |          514.988 | 2750464 |  229.206 |              277.081 |              132.232 |            797.701 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3539.5
    time_step_min: 3231
  date: 2020-10-11_02-10-31
  done: false
  episode_len_mean: 795.216565767749
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.39160176336839
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 316
  episodes_total: 3634
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 1.0e-05
        entropy: 0.5599193062101092
        entropy_coeff: 0.0
        kl: 0.007243110997868436
        model: {}
        policy_loss: -0.003690818665615682
        total_loss: 4.607808760234287
        vf_explained_var: 0.9937297105789185
        vf_loss: 4.61077526637486
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.897222222222226
    gpu_util_percent0: 0.33249999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.486111111111111
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15541847794512245
    mean_env_wait_ms: 1.2505479638408765
    mean_inference_ms: 4.866103451245176
    mean_raw_obs_processing_ms: 0.4120111335045687
  time_since_restore: 545.3063061237335
  time_this_iter_s: 30.318410873413086
  time_total_s: 545.3063061237335
  timers:
    learn_throughput: 7148.903
    learn_time_ms: 22631.724
    sample_throughput: 21870.963
    sample_time_ms: 7397.571
    update_time_ms: 27.65
  timestamp: 1602382231
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     18 |          545.306 | 2912256 |  229.392 |              277.081 |              132.232 |            795.217 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3539.2098831030817
    time_step_min: 3231
  date: 2020-10-11_02-11-01
  done: false
  episode_len_mean: 794.2434071729958
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.6001470400205
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 158
  episodes_total: 3792
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 1.0e-05
        entropy: 0.5371150544711522
        entropy_coeff: 0.0
        kl: 0.006628591567277908
        model: {}
        policy_loss: -0.00469395108354677
        total_loss: 2.7112883499690463
        vf_explained_var: 0.9945540428161621
        vf_loss: 2.715319343975612
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.988888888888894
    gpu_util_percent0: 0.2980555555555556
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.494444444444444
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15522442319538401
    mean_env_wait_ms: 1.251776253753229
    mean_inference_ms: 4.851448604981727
    mean_raw_obs_processing_ms: 0.41132998946342086
  time_since_restore: 575.256884098053
  time_this_iter_s: 29.950577974319458
  time_total_s: 575.256884098053
  timers:
    learn_throughput: 7150.737
    learn_time_ms: 22625.919
    sample_throughput: 21912.485
    sample_time_ms: 7383.553
    update_time_ms: 26.852
  timestamp: 1602382261
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: a6496_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | RUNNING  | 172.17.0.4:36795 |     19 |          575.257 | 3074048 |    229.6 |              277.081 |              132.232 |            794.243 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_a6496_00000:
  custom_metrics:
    time_step_max: 4161
    time_step_mean: 3537.6839153708897
    time_step_min: 3231
  date: 2020-10-11_02-11-31
  done: true
  episode_len_mean: 793.2394330549228
  episode_reward_max: 277.080808080808
  episode_reward_mean: 229.87726160619104
  episode_reward_min: 132.2323232323232
  episodes_this_iter: 159
  episodes_total: 3951
  experiment_id: 53c34cefdc154eaba8e36f529f1c7894
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 1.0e-05
        entropy: 0.534711914403098
        entropy_coeff: 0.0
        kl: 0.006015887118077704
        model: {}
        policy_loss: -0.0039605065373637316
        total_loss: 2.6766470159803117
        vf_explained_var: 0.9943894743919373
        vf_loss: 2.6800059931618825
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.21666666666667
    gpu_util_percent0: 0.2947222222222222
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.497222222222222
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 36795
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15503990317545346
    mean_env_wait_ms: 1.2529983565060634
    mean_inference_ms: 4.837439096041163
    mean_raw_obs_processing_ms: 0.410666385092114
  time_since_restore: 605.2815291881561
  time_this_iter_s: 30.02464509010315
  time_total_s: 605.2815291881561
  timers:
    learn_throughput: 7140.749
    learn_time_ms: 22657.566
    sample_throughput: 21950.657
    sample_time_ms: 7370.713
    update_time_ms: 26.864
  timestamp: 1602382291
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: a6496_00000
  
[2m[33m(pid=raylet)[0m E1011 02:11:32.124045 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.124342 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.124794 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.124914 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.124994 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.125944 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | TERMINATED |       |     20 |          605.282 | 3235840 |  229.877 |              277.081 |              132.232 |            793.239 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/537.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_a6496_00000 | TERMINATED |       |     20 |          605.282 | 3235840 |  229.877 |              277.081 |              132.232 |            793.239 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


[2m[33m(pid=raylet)[0m E1011 02:11:32.190822 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.196568 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.197432 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.197499 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.198065 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 02:11:32.198737 36654 36654 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
