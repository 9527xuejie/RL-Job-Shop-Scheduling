2020-10-12 09:20:14,254	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_2368d_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=38914)[0m 2020-10-12 09:20:16,993	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=38797)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38797)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38913)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38913)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38860)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38860)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38812)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38812)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38878)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38878)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38864)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38864)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38927)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38927)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38829)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38829)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38895)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38895)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38879)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38879)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38859)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38859)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38880)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38880)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38875)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38875)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38824)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38824)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38888)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38888)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38920)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38920)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38830)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38830)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38800)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38800)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38892)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38892)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38911)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38911)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38897)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38897)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38889)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38889)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38872)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38872)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38811)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38811)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38916)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38916)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38900)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38900)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38802)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38802)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38915)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38915)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38884)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38884)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38799)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38799)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38796)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38796)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38826)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38826)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38795)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38795)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38901)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38901)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38903)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38903)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38805)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38805)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38814)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38814)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38899)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38899)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38907)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38907)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38817)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38817)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38929)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38929)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38862)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38862)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38798)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38798)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38822)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38822)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38871)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38871)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38867)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38867)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38923)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38923)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38887)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38887)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38877)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38877)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38881)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38881)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38876)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38876)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38894)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38894)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38820)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38820)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38809)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38809)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38863)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38863)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38813)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38813)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38807)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38807)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38866)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38866)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38869)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38869)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38804)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38804)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38810)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38810)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38806)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38806)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38868)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38868)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38870)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38870)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38815)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38815)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38882)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38882)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38917)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38917)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38803)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38803)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38905)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38905)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38828)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38828)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38874)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38874)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38816)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38816)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38896)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38896)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38865)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38865)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38861)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38861)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38921)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38921)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38873)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38873)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38825)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38825)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38823)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38823)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-12_09-20-48
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1827744742234547
        entropy_coeff: 0.0005000000000000001
        kl: 0.007263169119444986
        model: {}
        policy_loss: -0.00761965052515734
        total_loss: 514.7333196004232
        vf_explained_var: 0.4917435944080353
        vf_loss: 514.7400767008463
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.823333333333338
    gpu_util_percent0: 0.319
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.550000000000001
    vram_util_percent0: 0.08396742101235381
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1686786140472516
    mean_env_wait_ms: 1.1653963037757602
    mean_inference_ms: 5.951796741020389
    mean_raw_obs_processing_ms: 0.45555506933247036
  time_since_restore: 25.677541971206665
  time_this_iter_s: 25.677541971206665
  time_total_s: 25.677541971206665
  timers:
    learn_throughput: 10161.034
    learn_time_ms: 15922.789
    sample_throughput: 16729.27
    sample_time_ms: 9671.193
    update_time_ms: 49.843
  timestamp: 1602494448
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      1 |          25.6775 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3627.9444444444443
    time_step_min: 3352
  date: 2020-10-12_09-21-12
  done: false
  episode_len_mean: 891.5791139240506
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.4263201636617
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1514901518821716
        entropy_coeff: 0.0005000000000000001
        kl: 0.008677494013682008
        model: {}
        policy_loss: -0.007752557110507041
        total_loss: 149.27600733439127
        vf_explained_var: 0.7886922955513
        vf_loss: 149.28260294596353
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.250000000000004
    gpu_util_percent0: 0.28285714285714286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.75
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16522475825236266
    mean_env_wait_ms: 1.1631367512626534
    mean_inference_ms: 5.757631300482575
    mean_raw_obs_processing_ms: 0.44612020259142066
  time_since_restore: 49.853564500808716
  time_this_iter_s: 24.17602252960205
  time_total_s: 49.853564500808716
  timers:
    learn_throughput: 10225.246
    learn_time_ms: 15822.798
    sample_throughput: 17935.916
    sample_time_ms: 9020.56
    update_time_ms: 41.407
  timestamp: 1602494472
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      2 |          49.8536 | 323584 |  216.426 |              258.596 |              138.293 |            891.579 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3619.372197309417
    time_step_min: 3306
  date: 2020-10-12_09-21-35
  done: false
  episode_len_mean: 885.7763713080169
  episode_reward_max: 265.111111111111
  episode_reward_mean: 218.3539828666409
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1501519680023193
        entropy_coeff: 0.0005000000000000001
        kl: 0.00987866218201816
        model: {}
        policy_loss: -0.011303798101531962
        total_loss: 68.9120267232259
        vf_explained_var: 0.8753736019134521
        vf_loss: 68.92193285624187
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.622222222222224
    gpu_util_percent0: 0.3292592592592593
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666666
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16271943504127687
    mean_env_wait_ms: 1.1637210719889728
    mean_inference_ms: 5.566098416995584
    mean_raw_obs_processing_ms: 0.43808816597207934
  time_since_restore: 72.73792386054993
  time_this_iter_s: 22.88435935974121
  time_total_s: 72.73792386054993
  timers:
    learn_throughput: 10294.333
    learn_time_ms: 15716.608
    sample_throughput: 19205.703
    sample_time_ms: 8424.165
    update_time_ms: 59.316
  timestamp: 1602494495
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      3 |          72.7379 | 485376 |  218.354 |              265.111 |              138.293 |            885.776 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3609.2218543046356
    time_step_min: 3306
  date: 2020-10-12_09-21-57
  done: false
  episode_len_mean: 881.5458860759494
  episode_reward_max: 271.9292929292925
  episode_reward_mean: 219.63067382687615
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.123563160498937
        entropy_coeff: 0.0005000000000000001
        kl: 0.01068496766189734
        model: {}
        policy_loss: -0.00788415075900654
        total_loss: 51.15607770284017
        vf_explained_var: 0.9072044491767883
        vf_loss: 51.162388483683266
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.31923076923077
    gpu_util_percent0: 0.2553846153846154
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769230769230769
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16081538205548576
    mean_env_wait_ms: 1.164579723782807
    mean_inference_ms: 5.416884148073274
    mean_raw_obs_processing_ms: 0.43160135943520156
  time_since_restore: 95.40110111236572
  time_this_iter_s: 22.663177251815796
  time_total_s: 95.40110111236572
  timers:
    learn_throughput: 10319.316
    learn_time_ms: 15678.558
    sample_throughput: 20045.004
    sample_time_ms: 8071.437
    update_time_ms: 53.564
  timestamp: 1602494517
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      4 |          95.4011 | 647168 |  219.631 |              271.929 |              138.293 |            881.546 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3597.922572178478
    time_step_min: 3220
  date: 2020-10-12_09-22-20
  done: false
  episode_len_mean: 878.0860759493671
  episode_reward_max: 278.14141414141426
  episode_reward_mean: 220.69051272215813
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.0902884205182393
        entropy_coeff: 0.0005000000000000001
        kl: 0.00903574850720664
        model: {}
        policy_loss: -0.009291995510769388
        total_loss: 44.12793127695719
        vf_explained_var: 0.927177906036377
        vf_loss: 44.13595962524414
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.16153846153846
    gpu_util_percent0: 0.28500000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.757692307692307
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15939243547503956
    mean_env_wait_ms: 1.165936563255046
    mean_inference_ms: 5.301039287467012
    mean_raw_obs_processing_ms: 0.4264011198442179
  time_since_restore: 117.91398453712463
  time_this_iter_s: 22.51288342475891
  time_total_s: 117.91398453712463
  timers:
    learn_throughput: 10355.112
    learn_time_ms: 15624.36
    sample_throughput: 20589.687
    sample_time_ms: 7857.915
    update_time_ms: 50.792
  timestamp: 1602494540
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      5 |          117.914 | 808960 |  220.691 |              278.141 |              138.293 |            878.086 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3581.089201877934
    time_step_min: 3220
  date: 2020-10-12_09-22-43
  done: false
  episode_len_mean: 871.1381518755718
  episode_reward_max: 278.14141414141426
  episode_reward_mean: 223.01693051281325
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 303
  episodes_total: 1093
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.0726720094680786
        entropy_coeff: 0.0005000000000000001
        kl: 0.008798854425549507
        model: {}
        policy_loss: -0.011426329845562577
        total_loss: 42.27958615620931
        vf_explained_var: 0.9519073963165283
        vf_loss: 42.28978888193766
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.003703703703703
    gpu_util_percent0: 0.3118518518518518
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.755555555555555
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1575550389084717
    mean_env_wait_ms: 1.1693032611932224
    mean_inference_ms: 5.14701749967029
    mean_raw_obs_processing_ms: 0.419540382097035
  time_since_restore: 140.6349482536316
  time_this_iter_s: 22.720963716506958
  time_total_s: 140.6349482536316
  timers:
    learn_throughput: 10360.276
    learn_time_ms: 15616.572
    sample_throughput: 20948.818
    sample_time_ms: 7723.204
    update_time_ms: 49.392
  timestamp: 1602494563
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      6 |          140.635 | 970752 |  223.017 |              278.141 |              138.293 |            871.138 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3575.8155339805826
    time_step_min: 3220
  date: 2020-10-12_09-23-06
  done: false
  episode_len_mean: 867.6273734177215
  episode_reward_max: 278.14141414141426
  episode_reward_mean: 224.21580520393792
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 171
  episodes_total: 1264
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.0701393087704976
        entropy_coeff: 0.0005000000000000001
        kl: 0.00972194162507852
        model: {}
        policy_loss: -0.012504054078211388
        total_loss: 24.834288756052654
        vf_explained_var: 0.9582985043525696
        vf_loss: 24.845383485158283
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.900000000000002
    gpu_util_percent0: 0.2566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777777777777778
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15679506772490143
    mean_env_wait_ms: 1.1709221026744456
    mean_inference_ms: 5.0848212374967305
    mean_raw_obs_processing_ms: 0.4167851193139076
  time_since_restore: 163.35206365585327
  time_this_iter_s: 22.71711540222168
  time_total_s: 163.35206365585327
  timers:
    learn_throughput: 10368.087
    learn_time_ms: 15604.807
    sample_throughput: 21210.289
    sample_time_ms: 7627.996
    update_time_ms: 48.447
  timestamp: 1602494586
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      7 |          163.352 | 1132544 |  224.216 |              278.141 |              138.293 |            867.627 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3567.4081779053085
    time_step_min: 3220
  date: 2020-10-12_09-23-29
  done: false
  episode_len_mean: 864.2721518987341
  episode_reward_max: 278.14141414141426
  episode_reward_mean: 225.9137578314792
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.053697516520818
        entropy_coeff: 0.0005000000000000001
        kl: 0.00854927790351212
        model: {}
        policy_loss: -0.009208010005143782
        total_loss: 20.169148763020832
        vf_explained_var: 0.9635282158851624
        vf_loss: 20.17717440923055
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.153846153846153
    gpu_util_percent0: 0.31384615384615383
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7807692307692307
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15620341712189031
    mean_env_wait_ms: 1.1722119121456387
    mean_inference_ms: 5.035066153614783
    mean_raw_obs_processing_ms: 0.4145090279785972
  time_since_restore: 186.05805230140686
  time_this_iter_s: 22.70598864555359
  time_total_s: 186.05805230140686
  timers:
    learn_throughput: 10361.75
    learn_time_ms: 15614.35
    sample_throughput: 21451.184
    sample_time_ms: 7542.334
    update_time_ms: 46.387
  timestamp: 1602494609
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      8 |          186.058 | 1294336 |  225.914 |              278.141 |              138.293 |            864.272 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3556.853092783505
    time_step_min: 3220
  date: 2020-10-12_09-23-51
  done: false
  episode_len_mean: 861.1063291139241
  episode_reward_max: 278.14141414141426
  episode_reward_mean: 227.5715062012529
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.0249782105286915
        entropy_coeff: 0.0005000000000000001
        kl: 0.008773986482992768
        model: {}
        policy_loss: -0.011871100180239106
        total_loss: 18.7819766998291
        vf_explained_var: 0.9634544849395752
        vf_loss: 18.792605717976887
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.815384615384612
    gpu_util_percent0: 0.25230769230769234
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7653846153846158
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1556915728808796
    mean_env_wait_ms: 1.1734060394791885
    mean_inference_ms: 4.991530438550621
    mean_raw_obs_processing_ms: 0.4124572359884706
  time_since_restore: 208.7378134727478
  time_this_iter_s: 22.679761171340942
  time_total_s: 208.7378134727478
  timers:
    learn_throughput: 10364.133
    learn_time_ms: 15610.761
    sample_throughput: 21617.833
    sample_time_ms: 7484.191
    update_time_ms: 44.225
  timestamp: 1602494631
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |      9 |          208.738 | 1456128 |  227.572 |              278.141 |              138.293 |            861.106 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3542.5145413870246
    time_step_min: 3209
  date: 2020-10-12_09-24-14
  done: false
  episode_len_mean: 855.078744493392
  episode_reward_max: 279.8080808080808
  episode_reward_mean: 229.86081075067844
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 236
  episodes_total: 1816
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.9820816020170847
        entropy_coeff: 0.0005000000000000001
        kl: 0.00821732710270832
        model: {}
        policy_loss: -0.010050915479951072
        total_loss: 21.17539644241333
        vf_explained_var: 0.9707738757133484
        vf_loss: 21.1842942237854
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.251851851851853
    gpu_util_percent0: 0.21962962962962962
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7592592592592586
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15505376808197696
    mean_env_wait_ms: 1.1757058986829956
    mean_inference_ms: 4.936731188829644
    mean_raw_obs_processing_ms: 0.4099086398609132
  time_since_restore: 231.40399146080017
  time_this_iter_s: 22.666177988052368
  time_total_s: 231.40399146080017
  timers:
    learn_throughput: 10372.554
    learn_time_ms: 15598.087
    sample_throughput: 21725.305
    sample_time_ms: 7447.168
    update_time_ms: 41.731
  timestamp: 1602494654
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     10 |          231.404 | 1617920 |  229.861 |              279.808 |              138.293 |            855.079 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3530.031095755183
    time_step_min: 3209
  date: 2020-10-12_09-24-37
  done: false
  episode_len_mean: 849.9323271665044
  episode_reward_max: 279.8080808080808
  episode_reward_mean: 231.8811975647417
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 238
  episodes_total: 2054
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.9930150409539541
        entropy_coeff: 0.0005000000000000001
        kl: 0.009119869054605564
        model: {}
        policy_loss: -0.008603869452296445
        total_loss: 16.28092400232951
        vf_explained_var: 0.9731142520904541
        vf_loss: 16.28820053736369
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.61111111111111
    gpu_util_percent0: 0.4014814814814815
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7592592592592586
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15450174672829692
    mean_env_wait_ms: 1.177575019054972
    mean_inference_ms: 4.890645710340337
    mean_raw_obs_processing_ms: 0.4077204848885823
  time_since_restore: 254.3549039363861
  time_this_iter_s: 22.950912475585938
  time_total_s: 254.3549039363861
  timers:
    learn_throughput: 10393.91
    learn_time_ms: 15566.038
    sample_throughput: 22456.294
    sample_time_ms: 7204.751
    update_time_ms: 40.844
  timestamp: 1602494677
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     11 |          254.355 | 1779712 |  231.881 |              279.808 |              138.293 |            849.932 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3522.0476190476193
    time_step_min: 3209
  date: 2020-10-12_09-25-00
  done: false
  episode_len_mean: 847.0786618444846
  episode_reward_max: 279.8080808080808
  episode_reward_mean: 233.0196266462088
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.9703877021869024
        entropy_coeff: 0.0005000000000000001
        kl: 0.007759922145244976
        model: {}
        policy_loss: -0.010197352380297767
        total_loss: 12.45139765739441
        vf_explained_var: 0.9765066504478455
        vf_loss: 12.460527817408243
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.723076923076924
    gpu_util_percent0: 0.27499999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7807692307692307
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15418325967439245
    mean_env_wait_ms: 1.178807434003838
    mean_inference_ms: 4.864168533056006
    mean_raw_obs_processing_ms: 0.4064531879388726
  time_since_restore: 277.23624324798584
  time_this_iter_s: 22.88133931159973
  time_total_s: 277.23624324798584
  timers:
    learn_throughput: 10401.052
    learn_time_ms: 15555.349
    sample_throughput: 22837.812
    sample_time_ms: 7084.391
    update_time_ms: 42.04
  timestamp: 1602494700
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     12 |          277.236 | 1941504 |   233.02 |              279.808 |              138.293 |            847.079 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3515.3185311699403
    time_step_min: 3209
  date: 2020-10-12_09-25-23
  done: false
  episode_len_mean: 845.0949367088608
  episode_reward_max: 279.8080808080808
  episode_reward_mean: 234.05101649405435
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 2370
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.9404816528161367
        entropy_coeff: 0.0005000000000000001
        kl: 0.007768297684378922
        model: {}
        policy_loss: -0.011234237026656047
        total_loss: 12.331538756688436
        vf_explained_var: 0.9758628010749817
        vf_loss: 12.341690063476562
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.47692307692307
    gpu_util_percent0: 0.4196153846153846
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769230769230769
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15389385011663606
    mean_env_wait_ms: 1.1799601908857826
    mean_inference_ms: 4.839993427048123
    mean_raw_obs_processing_ms: 0.4052788314029578
  time_since_restore: 299.8108310699463
  time_this_iter_s: 22.57458782196045
  time_total_s: 299.8108310699463
  timers:
    learn_throughput: 10404.547
    learn_time_ms: 15550.125
    sample_throughput: 22903.193
    sample_time_ms: 7064.168
    update_time_ms: 36.264
  timestamp: 1602494723
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     13 |          299.811 | 2103296 |  234.051 |              279.808 |              138.293 |            845.095 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3502.781992337165
    time_step_min: 3195
  date: 2020-10-12_09-25-45
  done: false
  episode_len_mean: 841.7566338134951
  episode_reward_max: 281.9292929292926
  episode_reward_mean: 235.95897565495736
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 268
  episodes_total: 2638
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8899559080600739
        entropy_coeff: 0.0005000000000000001
        kl: 0.008799789395804206
        model: {}
        policy_loss: -0.009050529527788361
        total_loss: 14.480824708938599
        vf_explained_var: 0.979515552520752
        vf_loss: 14.48855996131897
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.46666666666667
    gpu_util_percent0: 0.3048148148148148
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.762962962962962
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15346255248838944
    mean_env_wait_ms: 1.1818818859721714
    mean_inference_ms: 4.804153791082136
    mean_raw_obs_processing_ms: 0.40357690475691743
  time_since_restore: 322.49730682373047
  time_this_iter_s: 22.68647575378418
  time_total_s: 322.49730682373047
  timers:
    learn_throughput: 10409.233
    learn_time_ms: 15543.125
    sample_throughput: 22874.618
    sample_time_ms: 7072.992
    update_time_ms: 35.622
  timestamp: 1602494745
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     14 |          322.497 | 2265088 |  235.959 |              281.929 |              138.293 |            841.757 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3494.737215909091
    time_step_min: 3195
  date: 2020-10-12_09-26-08
  done: false
  episode_len_mean: 840.1722925457103
  episode_reward_max: 281.9292929292926
  episode_reward_mean: 237.35901206154358
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 206
  episodes_total: 2844
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8923394729693731
        entropy_coeff: 0.0005000000000000001
        kl: 0.00703572128744175
        model: {}
        policy_loss: -0.01187642919830978
        total_loss: 10.077098766962687
        vf_explained_var: 0.982374370098114
        vf_loss: 10.088014205296835
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.415384615384614
    gpu_util_percent0: 0.28692307692307695
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7807692307692307
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.153176065517477
    mean_env_wait_ms: 1.183078387126366
    mean_inference_ms: 4.779900284195376
    mean_raw_obs_processing_ms: 0.40240913871715844
  time_since_restore: 345.097039937973
  time_this_iter_s: 22.599733114242554
  time_total_s: 345.097039937973
  timers:
    learn_throughput: 10398.645
    learn_time_ms: 15558.951
    sample_throughput: 22890.538
    sample_time_ms: 7068.073
    update_time_ms: 34.473
  timestamp: 1602494768
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     15 |          345.097 | 2426880 |  237.359 |              281.929 |              138.293 |            840.172 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3488.5406859448553
    time_step_min: 3195
  date: 2020-10-12_09-26-31
  done: false
  episode_len_mean: 839.1568954030646
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 238.24548617420027
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 3002
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8800846089919409
        entropy_coeff: 0.0005000000000000001
        kl: 0.008667680822933713
        model: {}
        policy_loss: -0.010779316173435896
        total_loss: 9.988673686981201
        vf_explained_var: 0.9807379841804504
        vf_loss: 9.998159567515055
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.91923076923077
    gpu_util_percent0: 0.26153846153846155
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773076923076923
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15297371270614404
    mean_env_wait_ms: 1.1839169380455592
    mean_inference_ms: 4.763045426569329
    mean_raw_obs_processing_ms: 0.4016037863495797
  time_since_restore: 367.7543263435364
  time_this_iter_s: 22.657286405563354
  time_total_s: 367.7543263435364
  timers:
    learn_throughput: 10405.813
    learn_time_ms: 15548.232
    sample_throughput: 22877.624
    sample_time_ms: 7072.063
    update_time_ms: 34.536
  timestamp: 1602494791
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     16 |          367.754 | 2588672 |  238.245 |              291.626 |              138.293 |            839.157 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3482.6005747126437
    time_step_min: 3156
  date: 2020-10-12_09-26-54
  done: false
  episode_len_mean: 838.3003164556962
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 239.13158483569867
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 3160
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8536874353885651
        entropy_coeff: 0.0005000000000000001
        kl: 0.007795991492457688
        model: {}
        policy_loss: -0.0078069346491247416
        total_loss: 10.167111794153849
        vf_explained_var: 0.9799486994743347
        vf_loss: 10.173785924911499
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.26666666666667
    gpu_util_percent0: 0.42037037037037045
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77037037037037
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1527843375612678
    mean_env_wait_ms: 1.1846727712863188
    mean_inference_ms: 4.747365968268405
    mean_raw_obs_processing_ms: 0.40084186735936783
  time_since_restore: 390.42669439315796
  time_this_iter_s: 22.672368049621582
  time_total_s: 390.42669439315796
  timers:
    learn_throughput: 10400.664
    learn_time_ms: 15555.93
    sample_throughput: 22906.994
    sample_time_ms: 7062.996
    update_time_ms: 34.073
  timestamp: 1602494814
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     17 |          390.427 | 2750464 |  239.132 |              291.626 |              138.293 |              838.3 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3473.450309643173
    time_step_min: 3156
  date: 2020-10-12_09-27-17
  done: false
  episode_len_mean: 836.8543433752559
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 240.43895816899607
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 259
  episodes_total: 3419
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8139620572328568
        entropy_coeff: 0.0005000000000000001
        kl: 0.007261293552195032
        model: {}
        policy_loss: -0.010260122634160021
        total_loss: 13.31914758682251
        vf_explained_var: 0.9808043837547302
        vf_loss: 13.328362544377645
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.24615384615385
    gpu_util_percent0: 0.2373076923076923
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7615384615384615
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15250233321054504
    mean_env_wait_ms: 1.185879539178616
    mean_inference_ms: 4.724170317877592
    mean_raw_obs_processing_ms: 0.3997331904205269
  time_since_restore: 413.07285046577454
  time_this_iter_s: 22.646156072616577
  time_total_s: 413.07285046577454
  timers:
    learn_throughput: 10407.574
    learn_time_ms: 15545.601
    sample_throughput: 22901.294
    sample_time_ms: 7064.754
    update_time_ms: 35.488
  timestamp: 1602494837
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     18 |          413.073 | 2912256 |  240.439 |              291.626 |              138.293 |            836.854 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3467.253743760399
    time_step_min: 3156
  date: 2020-10-12_09-27-39
  done: false
  episode_len_mean: 835.5393505778757
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 241.34495477615997
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 215
  episodes_total: 3634
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8341793666283289
        entropy_coeff: 0.0005000000000000001
        kl: 0.008043301058933139
        model: {}
        policy_loss: -0.009528601134661585
        total_loss: 10.148140748341879
        vf_explained_var: 0.9823832511901855
        vf_loss: 10.156477610270182
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.949999999999996
    gpu_util_percent0: 0.26269230769230767
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773076923076923
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15229438406005505
    mean_env_wait_ms: 1.1867253963739983
    mean_inference_ms: 4.706751181471518
    mean_raw_obs_processing_ms: 0.39890343696658775
  time_since_restore: 435.53685665130615
  time_this_iter_s: 22.464006185531616
  time_total_s: 435.53685665130615
  timers:
    learn_throughput: 10426.012
    learn_time_ms: 15518.109
    sample_throughput: 22886.786
    sample_time_ms: 7069.232
    update_time_ms: 36.133
  timestamp: 1602494859
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     19 |          435.537 | 3074048 |  241.345 |              291.626 |              138.293 |            835.539 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3462.7428267800215
    time_step_min: 3156
  date: 2020-10-12_09-28-02
  done: false
  episode_len_mean: 834.5495780590717
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 242.07749168904223
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 3792
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8270436624685923
        entropy_coeff: 0.0005000000000000001
        kl: 0.007921267068013549
        model: {}
        policy_loss: -0.008027644410807019
        total_loss: 8.775661627451578
        vf_explained_var: 0.9819247722625732
        vf_loss: 8.78251854578654
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.361538461538462
    gpu_util_percent0: 0.2811538461538462
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7884615384615383
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15215330072412026
    mean_env_wait_ms: 1.1873365349754388
    mean_inference_ms: 4.694970511370541
    mean_raw_obs_processing_ms: 0.39833702433585705
  time_since_restore: 458.296884059906
  time_this_iter_s: 22.760027408599854
  time_total_s: 458.296884059906
  timers:
    learn_throughput: 10416.537
    learn_time_ms: 15532.226
    sample_throughput: 22906.783
    sample_time_ms: 7063.061
    update_time_ms: 36.6
  timestamp: 1602494882
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     20 |          458.297 | 3235840 |  242.077 |              291.626 |              138.293 |             834.55 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3458.382210927573
    time_step_min: 3156
  date: 2020-10-12_09-28-25
  done: false
  episode_len_mean: 833.3790058036841
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 242.7109933552022
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 171
  episodes_total: 3963
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.7922637710968653
        entropy_coeff: 0.0005000000000000001
        kl: 0.00846713703746597
        model: {}
        policy_loss: -0.009647412516642362
        total_loss: 10.379550457000732
        vf_explained_var: 0.9806429743766785
        vf_loss: 10.387900511423746
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.5
    gpu_util_percent0: 0.31037037037037035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7629629629629626
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520079365597308
    mean_env_wait_ms: 1.1879892191210335
    mean_inference_ms: 4.682970435105971
    mean_raw_obs_processing_ms: 0.39775380473998373
  time_since_restore: 480.8740713596344
  time_this_iter_s: 22.577187299728394
  time_total_s: 480.8740713596344
  timers:
    learn_throughput: 10424.895
    learn_time_ms: 15519.772
    sample_throughput: 23010.507
    sample_time_ms: 7031.223
    update_time_ms: 36.505
  timestamp: 1602494905
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     21 |          480.874 | 3397632 |  242.711 |              291.626 |              138.293 |            833.379 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3451.4755344418054
    time_step_min: 3156
  date: 2020-10-12_09-28-48
  done: false
  episode_len_mean: 831.5285512033978
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 243.81068352233987
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 275
  episodes_total: 4238
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.7635907779137293
        entropy_coeff: 0.0005000000000000001
        kl: 0.006186021181444327
        model: {}
        policy_loss: -0.00809120606087769
        total_loss: 12.58633295694987
        vf_explained_var: 0.9819806218147278
        vf_loss: 12.593568722407023
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.292307692307695
    gpu_util_percent0: 0.2896153846153846
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761538461538461
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15179261662292176
    mean_env_wait_ms: 1.1889850673180626
    mean_inference_ms: 4.665328442988377
    mean_raw_obs_processing_ms: 0.3969050824181129
  time_since_restore: 503.5492398738861
  time_this_iter_s: 22.67516851425171
  time_total_s: 503.5492398738861
  timers:
    learn_throughput: 10426.544
    learn_time_ms: 15517.318
    sample_throughput: 23073.669
    sample_time_ms: 7011.976
    update_time_ms: 36.777
  timestamp: 1602494928
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     22 |          503.549 | 3559424 |  243.811 |              291.626 |              138.293 |            831.529 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3447.7979981801636
    time_step_min: 3156
  date: 2020-10-12_09-29-10
  done: false
  episode_len_mean: 830.6717902350814
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 244.41100425594084
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 186
  episodes_total: 4424
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.7658579001824061
        entropy_coeff: 0.0005000000000000001
        kl: 0.006407496286556125
        model: {}
        policy_loss: -0.010411242435414655
        total_loss: 9.48198382059733
        vf_explained_var: 0.9826169013977051
        vf_loss: 9.491496562957764
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.492307692307694
    gpu_util_percent0: 0.2196153846153846
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7769230769230764
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15166053141927105
    mean_env_wait_ms: 1.1895626757331934
    mean_inference_ms: 4.654318975382179
    mean_raw_obs_processing_ms: 0.396383275691878
  time_since_restore: 526.1185171604156
  time_this_iter_s: 22.56927728652954
  time_total_s: 526.1185171604156
  timers:
    learn_throughput: 10425.191
    learn_time_ms: 15519.331
    sample_throughput: 23088.469
    sample_time_ms: 7007.481
    update_time_ms: 37.323
  timestamp: 1602494950
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     23 |          526.119 | 3721216 |  244.411 |              291.626 |              138.293 |            830.672 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3444.521519543259
    time_step_min: 3156
  date: 2020-10-12_09-29-33
  done: false
  episode_len_mean: 830.0261894369271
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 244.86456004832252
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 158
  episodes_total: 4582
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.7546412895123163
        entropy_coeff: 0.0005000000000000001
        kl: 0.007149649240697424
        model: {}
        policy_loss: -0.0075514697867523255
        total_loss: 8.021642128626505
        vf_explained_var: 0.9839253425598145
        vf_loss: 8.028141101201376
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.211111111111112
    gpu_util_percent0: 0.3381481481481482
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7777777777777777
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15155384005330969
    mean_env_wait_ms: 1.1900468522916654
    mean_inference_ms: 4.64553421142189
    mean_raw_obs_processing_ms: 0.3959627705576778
  time_since_restore: 548.8968155384064
  time_this_iter_s: 22.778298377990723
  time_total_s: 548.8968155384064
  timers:
    learn_throughput: 10417.224
    learn_time_ms: 15531.201
    sample_throughput: 23102.832
    sample_time_ms: 7003.124
    update_time_ms: 38.812
  timestamp: 1602494973
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     24 |          548.897 | 3883008 |  244.865 |              291.626 |              138.293 |            830.026 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3440.8998312948124
    time_step_min: 3156
  date: 2020-10-12_09-29-56
  done: false
  episode_len_mean: 829.0721174004193
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 245.37192681532292
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 188
  episodes_total: 4770
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.712892139951388
        entropy_coeff: 0.0005000000000000001
        kl: 0.007282022580814858
        model: {}
        policy_loss: -0.010588849492099447
        total_loss: 9.326696952184042
        vf_explained_var: 0.984372079372406
        vf_loss: 9.336185693740845
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.865384615384617
    gpu_util_percent0: 0.3211538461538461
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7653846153846158
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15143126271022408
    mean_env_wait_ms: 1.1906497688401347
    mean_inference_ms: 4.635565144109004
    mean_raw_obs_processing_ms: 0.3954749750444413
  time_since_restore: 571.5366971492767
  time_this_iter_s: 22.63988161087036
  time_total_s: 571.5366971492767
  timers:
    learn_throughput: 10428.547
    learn_time_ms: 15514.338
    sample_throughput: 23036.553
    sample_time_ms: 7023.273
    update_time_ms: 38.29
  timestamp: 1602494996
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     25 |          571.537 | 4044800 |  245.372 |              291.626 |              138.293 |            829.072 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3436.1666334065058
    time_step_min: 3156
  date: 2020-10-12_09-30-19
  done: false
  episode_len_mean: 828.2643381623338
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 246.06244825712966
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 269
  episodes_total: 5039
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.6960836946964264
        entropy_coeff: 0.0005000000000000001
        kl: 0.0067783767978350324
        model: {}
        policy_loss: -0.009497678205661941
        total_loss: 9.854984839757284
        vf_explained_var: 0.9858968257904053
        vf_loss: 9.86347476641337
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.766666666666666
    gpu_util_percent0: 0.26814814814814814
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7592592592592586
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1512703561173303
    mean_env_wait_ms: 1.1913865426440073
    mean_inference_ms: 4.622600936815719
    mean_raw_obs_processing_ms: 0.39486517336879373
  time_since_restore: 594.444696187973
  time_this_iter_s: 22.90799903869629
  time_total_s: 594.444696187973
  timers:
    learn_throughput: 10410.872
    learn_time_ms: 15540.677
    sample_throughput: 23045.044
    sample_time_ms: 7020.685
    update_time_ms: 38.302
  timestamp: 1602495019
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | RUNNING  | 172.17.0.4:38914 |     26 |          594.445 | 4206592 |  246.062 |              291.626 |              138.293 |            828.264 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_2368d_00000:
  custom_metrics:
    time_step_max: 4143
    time_step_mean: 3433.269571924412
    time_step_min: 3156
  date: 2020-10-12_09-30-42
  done: true
  episode_len_mean: 827.7301495972382
  episode_reward_max: 291.6262626262624
  episode_reward_mean: 246.5109262940102
  episode_reward_min: 138.29292929292922
  episodes_this_iter: 175
  episodes_total: 5214
  experiment_id: b35504ea99e6454b9b23eaebff6c8ae6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.7203244020541509
        entropy_coeff: 0.0005000000000000001
        kl: 0.006474755665597816
        model: {}
        policy_loss: -0.008001692759838383
        total_loss: 8.758366187413534
        vf_explained_var: 0.9835500717163086
        vf_loss: 8.765433549880981
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.35185185185185
    gpu_util_percent0: 0.2525925925925926
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777777777777778
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38914
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1511747547181723
    mean_env_wait_ms: 1.1918082964819434
    mean_inference_ms: 4.6145812163910245
    mean_raw_obs_processing_ms: 0.3944869495641862
  time_since_restore: 617.2291052341461
  time_this_iter_s: 22.784409046173096
  time_total_s: 617.2291052341461
  timers:
    learn_throughput: 10412.647
    learn_time_ms: 15538.028
    sample_throughput: 23025.938
    sample_time_ms: 7026.511
    update_time_ms: 38.592
  timestamp: 1602495042
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 2368d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | TERMINATED |       |     27 |          617.229 | 4368384 |  246.511 |              291.626 |              138.293 |             827.73 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


2020-10-12 09:30:42,885	WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffff61dc472601000000.
[2m[36m(pid=38870)[0m 2020-10-12 09:30:42,808	ERROR worker.py:372 -- SystemExit was raised from the worker
[2m[36m(pid=38870)[0m Traceback (most recent call last):
[2m[36m(pid=38870)[0m   File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
[2m[36m(pid=38870)[0m   File "python/ray/_raylet.pyx", line 483, in ray._raylet.execute_task
[2m[36m(pid=38870)[0m   File "python/ray/_raylet.pyx", line 484, in ray._raylet.execute_task
[2m[36m(pid=38870)[0m   File "python/ray/_raylet.pyx", line 438, in ray._raylet.execute_task.function_executor
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/function_manager.py", line 553, in actor_method_executor
[2m[36m(pid=38870)[0m     return method(actor, *args, **kwargs)
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 929, in __ray_terminate__
[2m[36m(pid=38870)[0m     ray.actor.exit_actor()
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 996, in exit_actor
[2m[36m(pid=38870)[0m     raise exit
[2m[36m(pid=38870)[0m SystemExit: 0
[2m[36m(pid=38870)[0m 
[2m[36m(pid=38870)[0m During handling of the above exception, another exception occurred:
[2m[36m(pid=38870)[0m 
[2m[36m(pid=38870)[0m Traceback (most recent call last):
[2m[36m(pid=38870)[0m   File "python/ray/_raylet.pyx", line 553, in ray._raylet.task_execution_handler
[2m[36m(pid=38870)[0m   File "python/ray/_raylet.pyx", line 440, in ray._raylet.execute_task
[2m[36m(pid=38870)[0m   File "python/ray/includes/libcoreworker.pxi", line 33, in ray._raylet.ProfileEvent.__exit__
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 167, in format_exc
[2m[36m(pid=38870)[0m     return "".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 120, in format_exception
[2m[36m(pid=38870)[0m     return list(TracebackException(
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 509, in __init__
[2m[36m(pid=38870)[0m     self.stack = StackSummary.extract(
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 359, in extract
[2m[36m(pid=38870)[0m     result.append(FrameSummary(
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 243, in __init__
[2m[36m(pid=38870)[0m     def __init__(self, filename, lineno, name, *, lookup_line=True,
[2m[36m(pid=38870)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/worker.py", line 369, in sigterm_handler
[2m[36m(pid=38870)[0m     sys.exit(1)
[2m[36m(pid=38870)[0m SystemExit: 1
[2m[33m(pid=raylet)[0m E1012 09:30:42.886673 38754 38754 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.76 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_2368d_00000 | TERMINATED |       |     27 |          617.229 | 4368384 |  246.511 |              291.626 |              138.293 |             827.73 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


