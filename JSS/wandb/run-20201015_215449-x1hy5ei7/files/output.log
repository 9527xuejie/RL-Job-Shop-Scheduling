2020-10-15 21:54:53,746	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_0f5d2_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=59344)[0m 2020-10-15 21:54:56,572	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=59363)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59363)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59309)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59309)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59306)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59306)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59355)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59355)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59337)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59337)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59350)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59350)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59319)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59319)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59370)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59370)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59349)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59349)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59312)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59312)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59352)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59352)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59303)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59303)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59334)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59334)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59333)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59333)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59316)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59316)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59300)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59300)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59324)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59324)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59310)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59310)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59311)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59311)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59291)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59291)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59326)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59326)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59241)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59241)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59346)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59346)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59296)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59296)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59267)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59267)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59320)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59320)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59243)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59243)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59237)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59237)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59307)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59307)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59247)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59247)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59365)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59365)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59302)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59302)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59244)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59244)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59252)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59252)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59331)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59331)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59255)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59255)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59234)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59234)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59308)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59308)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59259)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59259)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59266)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59266)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59262)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59262)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59368)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59368)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59322)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59322)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59357)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59357)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59248)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59248)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59313)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59313)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59239)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59239)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59251)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59251)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59235)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59235)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59250)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59250)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59249)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59249)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59317)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59317)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59256)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59256)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59265)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59265)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59323)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59323)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59233)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59233)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59264)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59264)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59360)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59360)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59292)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59292)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59339)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59339)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59293)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59293)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59254)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59254)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59246)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59246)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59340)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59340)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59257)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59257)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59304)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59304)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59345)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59345)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59341)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59341)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59258)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59258)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59314)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59314)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59242)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59242)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59253)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59253)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59289)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59289)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59236)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59236)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59315)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59315)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59347)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59347)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59238)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59238)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59351)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59351)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59361)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59361)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3522.9508196721313
    time_step_min: 3235
  date: 2020-10-15_21-55-30
  done: false
  episode_len_mean: 892.2341772151899
  episode_reward_max: 254.5757575757571
  episode_reward_mean: 208.69735327963153
  episode_reward_min: 139.27272727272737
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1810161769390106
        entropy_coeff: 0.0005000000000000001
        kl: 0.003497340988057355
        model: {}
        policy_loss: -0.007581961224786937
        total_loss: 425.68299611409503
        vf_explained_var: 0.5529740452766418
        vf_loss: 425.6904652913411
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.53333333333333
    gpu_util_percent0: 0.3606060606060606
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.557575757575757
    vram_util_percent0: 0.08582297226114873
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17405372447678982
    mean_env_wait_ms: 1.1756335691597477
    mean_inference_ms: 6.0277436507561575
    mean_raw_obs_processing_ms: 0.46486214157533184
  time_since_restore: 28.44771385192871
  time_this_iter_s: 28.44771385192871
  time_total_s: 28.44771385192871
  timers:
    learn_throughput: 8516.024
    learn_time_ms: 18998.537
    sample_throughput: 17265.06
    sample_time_ms: 9371.065
    update_time_ms: 46.794
  timestamp: 1602798930
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      1 |          28.4477 | 161792 |  208.697 |              254.576 |              139.273 |            892.234 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3532.1642857142856
    time_step_min: 3205
  date: 2020-10-15_21-55-56
  done: false
  episode_len_mean: 889.6677215189874
  episode_reward_max: 259.1212121212121
  episode_reward_mean: 208.27157652474088
  episode_reward_min: 139.27272727272737
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1515157421429951
        entropy_coeff: 0.0005000000000000001
        kl: 0.008129845761383573
        model: {}
        policy_loss: -0.010888179541022206
        total_loss: 106.39584986368816
        vf_explained_var: 0.8193144202232361
        vf_loss: 106.40650304158528
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.696774193548386
    gpu_util_percent0: 0.3419354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7548387096774185
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16899320150728717
    mean_env_wait_ms: 1.1699602570036185
    mean_inference_ms: 5.729917556566728
    mean_raw_obs_processing_ms: 0.45225780308524705
  time_since_restore: 54.841328620910645
  time_this_iter_s: 26.393614768981934
  time_total_s: 54.841328620910645
  timers:
    learn_throughput: 8595.936
    learn_time_ms: 18821.918
    sample_throughput: 18977.527
    sample_time_ms: 8525.452
    update_time_ms: 34.099
  timestamp: 1602798956
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      2 |          54.8413 | 323584 |  208.272 |              259.121 |              139.273 |            889.668 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3539.1141552511417
    time_step_min: 3205
  date: 2020-10-15_21-56-23
  done: false
  episode_len_mean: 883.3628691983122
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 208.68073136427546
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1436680853366852
        entropy_coeff: 0.0005000000000000001
        kl: 0.008007710915990174
        model: {}
        policy_loss: -0.011135648053217059
        total_loss: 57.12984085083008
        vf_explained_var: 0.8894199728965759
        vf_loss: 57.1407470703125
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.353333333333335
    gpu_util_percent0: 0.312
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16553138984535243
    mean_env_wait_ms: 1.1687448005030068
    mean_inference_ms: 5.509744806716748
    mean_raw_obs_processing_ms: 0.442168294954408
  time_since_restore: 81.05949759483337
  time_this_iter_s: 26.21816897392273
  time_total_s: 81.05949759483337
  timers:
    learn_throughput: 8567.326
    learn_time_ms: 18884.773
    sample_throughput: 20080.3
    sample_time_ms: 8057.25
    update_time_ms: 36.887
  timestamp: 1602798983
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      3 |          81.0595 | 485376 |  208.681 |                  267 |              136.242 |            883.363 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3544.412751677852
    time_step_min: 3205
  date: 2020-10-15_21-56-49
  done: false
  episode_len_mean: 879.0142405063291
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 207.51759685462198
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1258440613746643
        entropy_coeff: 0.0005000000000000001
        kl: 0.008104618444728354
        model: {}
        policy_loss: -0.012452503734190637
        total_loss: 42.44776312510172
        vf_explained_var: 0.9244468212127686
        vf_loss: 42.45996824900309
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.363333333333333
    gpu_util_percent0: 0.353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16302969727754632
    mean_env_wait_ms: 1.1688712686163385
    mean_inference_ms: 5.3507881397626305
    mean_raw_obs_processing_ms: 0.4342546099008795
  time_since_restore: 107.02397608757019
  time_this_iter_s: 25.964478492736816
  time_total_s: 107.02397608757019
  timers:
    learn_throughput: 8576.399
    learn_time_ms: 18864.794
    sample_throughput: 20721.112
    sample_time_ms: 7808.075
    update_time_ms: 39.766
  timestamp: 1602799009
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      4 |          107.024 | 647168 |  207.518 |                  267 |              136.242 |            879.014 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3537.816976127321
    time_step_min: 3205
  date: 2020-10-15_21-57-14
  done: false
  episode_len_mean: 873.3012658227848
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 208.89221327195992
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0834401945273082
        entropy_coeff: 0.0005000000000000001
        kl: 0.008102510396080712
        model: {}
        policy_loss: -0.0116378908775611
        total_loss: 34.47308222452799
        vf_explained_var: 0.941558837890625
        vf_loss: 34.48445192972819
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.79666666666667
    gpu_util_percent0: 0.31600000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16117481676945328
    mean_env_wait_ms: 1.1702542116562014
    mean_inference_ms: 5.2312639389375395
    mean_raw_obs_processing_ms: 0.42805560525397096
  time_since_restore: 132.72996926307678
  time_this_iter_s: 25.705993175506592
  time_total_s: 132.72996926307678
  timers:
    learn_throughput: 8603.964
    learn_time_ms: 18804.356
    sample_throughput: 21123.283
    sample_time_ms: 7659.416
    update_time_ms: 38.004
  timestamp: 1602799034
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      5 |           132.73 | 808960 |  208.892 |                  267 |              136.242 |            873.301 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3512.602432179607
    time_step_min: 3204
  date: 2020-10-15_21-57-40
  done: false
  episode_len_mean: 862.558371040724
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 212.5103523927052
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 315
  episodes_total: 1105
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0828134020169575
        entropy_coeff: 0.0005000000000000001
        kl: 0.007772813783958554
        model: {}
        policy_loss: -0.010905310687424693
        total_loss: 29.839585304260254
        vf_explained_var: 0.9594511389732361
        vf_loss: 29.85025469462077
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.943333333333335
    gpu_util_percent0: 0.385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1586672007258682
    mean_env_wait_ms: 1.173530969024907
    mean_inference_ms: 5.071121759422573
    mean_raw_obs_processing_ms: 0.4199800420750728
  time_since_restore: 158.36627507209778
  time_this_iter_s: 25.636305809020996
  time_total_s: 158.36627507209778
  timers:
    learn_throughput: 8609.043
    learn_time_ms: 18793.261
    sample_throughput: 21513.988
    sample_time_ms: 7520.317
    update_time_ms: 35.969
  timestamp: 1602799060
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      6 |          158.366 | 970752 |   212.51 |                  267 |              136.242 |            862.558 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3501.2597719869705
    time_step_min: 3175
  date: 2020-10-15_21-58-06
  done: false
  episode_len_mean: 857.7254746835443
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 214.3693421557344
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 159
  episodes_total: 1264
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0574177900950115
        entropy_coeff: 0.0005000000000000001
        kl: 0.00769848582179596
        model: {}
        policy_loss: -0.01350904762512073
        total_loss: 17.78907855351766
        vf_explained_var: 0.9648939967155457
        vf_loss: 17.802346070607502
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.043333333333337
    gpu_util_percent0: 0.353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15778095034618095
    mean_env_wait_ms: 1.1749219765880123
    mean_inference_ms: 5.013665954156916
    mean_raw_obs_processing_ms: 0.41701779760810886
  time_since_restore: 184.20225977897644
  time_this_iter_s: 25.835984706878662
  time_total_s: 184.20225977897644
  timers:
    learn_throughput: 8600.579
    learn_time_ms: 18811.757
    sample_throughput: 21803.541
    sample_time_ms: 7420.446
    update_time_ms: 36.426
  timestamp: 1602799086
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      7 |          184.202 | 1132544 |  214.369 |                  267 |              136.242 |            857.725 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3493.8376623376626
    time_step_min: 3158
  date: 2020-10-15_21-58-32
  done: false
  episode_len_mean: 853.4549929676512
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 215.6040787623065
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0409260789553325
        entropy_coeff: 0.0005000000000000001
        kl: 0.00779755685168008
        model: {}
        policy_loss: -0.011598864497500472
        total_loss: 19.022697925567627
        vf_explained_var: 0.9629490971565247
        vf_loss: 19.03403838475545
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.510344827586206
    gpu_util_percent0: 0.37724137931034485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15702082413216117
    mean_env_wait_ms: 1.176305766351755
    mean_inference_ms: 4.9640871913943245
    mean_raw_obs_processing_ms: 0.4143954729820178
  time_since_restore: 209.79473114013672
  time_this_iter_s: 25.59247136116028
  time_total_s: 209.79473114013672
  timers:
    learn_throughput: 8606.292
    learn_time_ms: 18799.27
    sample_throughput: 22032.153
    sample_time_ms: 7343.45
    update_time_ms: 34.737
  timestamp: 1602799112
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      8 |          209.795 | 1294336 |  215.604 |                  267 |              136.242 |            853.455 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3486.4576051779936
    time_step_min: 3158
  date: 2020-10-15_21-58-57
  done: false
  episode_len_mean: 850.1170145477546
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 216.83727215226253
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 159
  episodes_total: 1581
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0032605528831482
        entropy_coeff: 0.0005000000000000001
        kl: 0.007478718568260471
        model: {}
        policy_loss: -0.013388285306670392
        total_loss: 17.289390722910564
        vf_explained_var: 0.9693426489830017
        vf_loss: 17.302532354990642
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.27
    gpu_util_percent0: 0.351
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15635492901051687
    mean_env_wait_ms: 1.1776207893807928
    mean_inference_ms: 4.92050505250129
    mean_raw_obs_processing_ms: 0.41201792349723765
  time_since_restore: 235.37021589279175
  time_this_iter_s: 25.57548475265503
  time_total_s: 235.37021589279175
  timers:
    learn_throughput: 8616.048
    learn_time_ms: 18777.983
    sample_throughput: 22182.525
    sample_time_ms: 7293.669
    update_time_ms: 33.432
  timestamp: 1602799137
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |      9 |           235.37 | 1456128 |  216.837 |                  267 |              136.242 |            850.117 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3468.207212055974
    time_step_min: 3158
  date: 2020-10-15_21-59-23
  done: false
  episode_len_mean: 843.5290390707497
  episode_reward_max: 268.96969696969654
  episode_reward_mean: 219.8268695401746
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 313
  episodes_total: 1894
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9728382378816605
        entropy_coeff: 0.0005000000000000001
        kl: 0.0068471186483899755
        model: {}
        policy_loss: -0.010848217488576969
        total_loss: 18.585219701131184
        vf_explained_var: 0.9735828042030334
        vf_loss: 18.595869382222492
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.42666666666667
    gpu_util_percent0: 0.2996666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15528805388529687
    mean_env_wait_ms: 1.1802044910542342
    mean_inference_ms: 4.851275331000091
    mean_raw_obs_processing_ms: 0.4083369063833582
  time_since_restore: 261.502699136734
  time_this_iter_s: 26.13248324394226
  time_total_s: 261.502699136734
  timers:
    learn_throughput: 8602.557
    learn_time_ms: 18807.432
    sample_throughput: 22283.507
    sample_time_ms: 7260.617
    update_time_ms: 33.902
  timestamp: 1602799163
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     10 |          261.503 | 1617920 |  219.827 |               268.97 |              136.242 |            843.529 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3459.940039643211
    time_step_min: 3149
  date: 2020-10-15_21-59-49
  done: false
  episode_len_mean: 840.819376825706
  episode_reward_max: 268.96969696969654
  episode_reward_mean: 221.11539936856386
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 160
  episodes_total: 2054
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.952570786078771
        entropy_coeff: 0.0005000000000000001
        kl: 0.006939189663777749
        model: {}
        policy_loss: -0.01326275585355082
        total_loss: 12.24478809038798
        vf_explained_var: 0.9761103987693787
        vf_loss: 12.257833242416382
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.470000000000006
    gpu_util_percent0: 0.32133333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15485445461374284
    mean_env_wait_ms: 1.181326008450022
    mean_inference_ms: 4.822543598162095
    mean_raw_obs_processing_ms: 0.40679615917415707
  time_since_restore: 287.34496092796326
  time_this_iter_s: 25.842261791229248
  time_total_s: 287.34496092796326
  timers:
    learn_throughput: 8608.014
    learn_time_ms: 18795.509
    sample_throughput: 23080.111
    sample_time_ms: 7010.018
    update_time_ms: 33.353
  timestamp: 1602799189
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     11 |          287.345 | 1779712 |  221.115 |               268.97 |              136.242 |            840.819 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3451.996783088235
    time_step_min: 3149
  date: 2020-10-15_22-00-15
  done: false
  episode_len_mean: 838.5524412296564
  episode_reward_max: 276.5454545454551
  episode_reward_mean: 222.440161104718
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.93861157198747
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065783633617684245
        model: {}
        policy_loss: -0.01096414635533923
        total_loss: 11.370628595352173
        vf_explained_var: 0.9759250283241272
        vf_loss: 11.381404479344686
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.28666666666667
    gpu_util_percent0: 0.30700000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15446219230935415
    mean_env_wait_ms: 1.1823412137500564
    mean_inference_ms: 4.796729560357333
    mean_raw_obs_processing_ms: 0.4053908455958505
  time_since_restore: 313.2493386268616
  time_this_iter_s: 25.904377698898315
  time_total_s: 313.2493386268616
  timers:
    learn_throughput: 8593.664
    learn_time_ms: 18826.894
    sample_throughput: 23354.31
    sample_time_ms: 6927.715
    update_time_ms: 34.74
  timestamp: 1602799215
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     12 |          313.249 | 1941504 |   222.44 |              276.545 |              136.242 |            838.552 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3441.9974768713205
    time_step_min: 3121
  date: 2020-10-15_22-00-41
  done: false
  episode_len_mean: 835.330985915493
  episode_reward_max: 276.5454545454551
  episode_reward_mean: 223.75008159473768
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 202
  episodes_total: 2414
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8984403163194656
        entropy_coeff: 0.0005000000000000001
        kl: 0.006221253269662459
        model: {}
        policy_loss: -0.009880342788771182
        total_loss: 16.48670530319214
        vf_explained_var: 0.974186360836029
        vf_loss: 16.49641251564026
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.960000000000004
    gpu_util_percent0: 0.2826666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15401207205037934
    mean_env_wait_ms: 1.1837368220957822
    mean_inference_ms: 4.767182178008781
    mean_raw_obs_processing_ms: 0.4037584587545814
  time_since_restore: 339.19266986846924
  time_this_iter_s: 25.943331241607666
  time_total_s: 339.19266986846924
  timers:
    learn_throughput: 8599.854
    learn_time_ms: 18813.344
    sample_throughput: 23402.172
    sample_time_ms: 6913.546
    update_time_ms: 33.938
  timestamp: 1602799241
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     13 |          339.193 | 2103296 |   223.75 |              276.545 |              136.242 |            835.331 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3429.613061532654
    time_step_min: 3121
  date: 2020-10-15_22-01-07
  done: false
  episode_len_mean: 831.2167597765363
  episode_reward_max: 276.5454545454553
  episode_reward_mean: 225.59765250268038
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 271
  episodes_total: 2685
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8792577832937241
        entropy_coeff: 0.0005000000000000001
        kl: 0.006186536280438304
        model: {}
        policy_loss: -0.009713826865966743
        total_loss: 12.887195348739624
        vf_explained_var: 0.9778575897216797
        vf_loss: 12.896730581919352
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.780000000000005
    gpu_util_percent0: 0.38400000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15350358312686557
    mean_env_wait_ms: 1.1853865047672
    mean_inference_ms: 4.7333161540880075
    mean_raw_obs_processing_ms: 0.4019276196328734
  time_since_restore: 365.1201674938202
  time_this_iter_s: 25.927497625350952
  time_total_s: 365.1201674938202
  timers:
    learn_throughput: 8594.749
    learn_time_ms: 18824.517
    sample_throughput: 23458.716
    sample_time_ms: 6896.882
    update_time_ms: 34.837
  timestamp: 1602799267
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     14 |           365.12 | 2265088 |  225.598 |              276.545 |              136.242 |            831.217 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3422.2681623931626
    time_step_min: 3121
  date: 2020-10-15_22-01-33
  done: false
  episode_len_mean: 829.1111111111111
  episode_reward_max: 276.5454545454553
  episode_reward_mean: 226.5676170992626
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 159
  episodes_total: 2844
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8780574252208074
        entropy_coeff: 0.0005000000000000001
        kl: 0.006002183809566001
        model: {}
        policy_loss: -0.010840655624633655
        total_loss: 10.460712591807047
        vf_explained_var: 0.9777989387512207
        vf_loss: 10.471392234166464
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.133333333333333
    gpu_util_percent0: 0.38166666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15323726542612343
    mean_env_wait_ms: 1.1862819965774003
    mean_inference_ms: 4.715777459568087
    mean_raw_obs_processing_ms: 0.40096458752793984
  time_since_restore: 391.0727491378784
  time_this_iter_s: 25.952581644058228
  time_total_s: 391.0727491378784
  timers:
    learn_throughput: 8583.788
    learn_time_ms: 18848.554
    sample_throughput: 23489.058
    sample_time_ms: 6887.973
    update_time_ms: 35.505
  timestamp: 1602799293
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     15 |          391.073 | 2426880 |  226.568 |              276.545 |              136.242 |            829.111 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3415.3331087908387
    time_step_min: 3112
  date: 2020-10-15_22-02-00
  done: false
  episode_len_mean: 826.9144758735441
  episode_reward_max: 279.42424242424227
  episode_reward_mean: 227.6205818585186
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 3005
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8447853823502859
        entropy_coeff: 0.0005000000000000001
        kl: 0.0059578693471848965
        model: {}
        policy_loss: -0.009374744794816555
        total_loss: 11.64163851737976
        vf_explained_var: 0.9763595461845398
        vf_loss: 11.650839964548746
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.693333333333335
    gpu_util_percent0: 0.3506666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15298667898445195
    mean_env_wait_ms: 1.1872044640945547
    mean_inference_ms: 4.699274521940101
    mean_raw_obs_processing_ms: 0.4000432373893744
  time_since_restore: 417.1413652896881
  time_this_iter_s: 26.068616151809692
  time_total_s: 417.1413652896881
  timers:
    learn_throughput: 8565.912
    learn_time_ms: 18887.89
    sample_throughput: 23484.496
    sample_time_ms: 6889.311
    update_time_ms: 37.046
  timestamp: 1602799320
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     16 |          417.141 | 2588672 |  227.621 |              279.424 |              136.242 |            826.914 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3404.028981086028
    time_step_min: 3093
  date: 2020-10-15_22-02-26
  done: false
  episode_len_mean: 823.415509957755
  episode_reward_max: 279.42424242424227
  episode_reward_mean: 229.41505276055662
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 309
  episodes_total: 3314
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8260619839032491
        entropy_coeff: 0.0005000000000000001
        kl: 0.006421650837485989
        model: {}
        policy_loss: -0.009145767389175793
        total_loss: 13.133175134658813
        vf_explained_var: 0.9803077578544617
        vf_loss: 13.142091671625773
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.680000000000003
    gpu_util_percent0: 0.39399999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1525565616429332
    mean_env_wait_ms: 1.188898554365132
    mean_inference_ms: 4.671358274966672
    mean_raw_obs_processing_ms: 0.3985232633472342
  time_since_restore: 443.39058661460876
  time_this_iter_s: 26.249221324920654
  time_total_s: 443.39058661460876
  timers:
    learn_throughput: 8563.96
    learn_time_ms: 18892.195
    sample_throughput: 23360.71
    sample_time_ms: 6925.817
    update_time_ms: 37.176
  timestamp: 1602799346
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     17 |          443.391 | 2750464 |  229.415 |              279.424 |              136.242 |            823.416 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3398.445930232558
    time_step_min: 3088
  date: 2020-10-15_22-02-52
  done: false
  episode_len_mean: 821.7310126582279
  episode_reward_max: 279.42424242424227
  episode_reward_mean: 230.2455887993862
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 162
  episodes_total: 3476
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7999657342831293
        entropy_coeff: 0.0005000000000000001
        kl: 0.005766421129616599
        model: {}
        policy_loss: -0.012520057265646756
        total_loss: 8.275206168492636
        vf_explained_var: 0.9822821617126465
        vf_loss: 8.28754965464274
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.543333333333337
    gpu_util_percent0: 0.34166666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15235791965125442
    mean_env_wait_ms: 1.1896962885814055
    mean_inference_ms: 4.658350572822218
    mean_raw_obs_processing_ms: 0.3978090674228695
  time_since_restore: 469.1387298107147
  time_this_iter_s: 25.748143196105957
  time_total_s: 469.1387298107147
  timers:
    learn_throughput: 8563.532
    learn_time_ms: 18893.138
    sample_throughput: 23318.23
    sample_time_ms: 6938.434
    update_time_ms: 38.845
  timestamp: 1602799372
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     18 |          469.139 | 2912256 |  230.246 |              279.424 |              136.242 |            821.731 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3392.4371873262926
    time_step_min: 3088
  date: 2020-10-15_22-03-18
  done: false
  episode_len_mean: 820.1821684094662
  episode_reward_max: 279.42424242424227
  episode_reward_mean: 231.0936608795717
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 3634
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7937028606732687
        entropy_coeff: 0.0005000000000000001
        kl: 0.005935679965962966
        model: {}
        policy_loss: -0.012428551142268892
        total_loss: 9.381741841634115
        vf_explained_var: 0.978752613067627
        vf_loss: 9.393973429997763
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.50666666666667
    gpu_util_percent0: 0.31466666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15217586634129684
    mean_env_wait_ms: 1.1904548485253688
    mean_inference_ms: 4.64646273388499
    mean_raw_obs_processing_ms: 0.3971510929575071
  time_since_restore: 494.87887024879456
  time_this_iter_s: 25.740140438079834
  time_total_s: 494.87887024879456
  timers:
    learn_throughput: 8558.955
    learn_time_ms: 18903.243
    sample_throughput: 23303.618
    sample_time_ms: 6942.785
    update_time_ms: 40.073
  timestamp: 1602799398
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     19 |          494.879 | 3074048 |  231.094 |              279.424 |              136.242 |            820.182 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3384.686385946784
    time_step_min: 3076
  date: 2020-10-15_22-03-44
  done: false
  episode_len_mean: 817.9669823393908
  episode_reward_max: 280.181818181819
  episode_reward_mean: 232.309545415765
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 273
  episodes_total: 3907
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7688640058040619
        entropy_coeff: 0.0005000000000000001
        kl: 0.005751285973625879
        model: {}
        policy_loss: -0.008068479248322546
        total_loss: 13.53387713432312
        vf_explained_var: 0.9797406196594238
        vf_loss: 13.541754881540934
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.910000000000004
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15188586310632912
    mean_env_wait_ms: 1.1917378893733206
    mean_inference_ms: 4.627626079663076
    mean_raw_obs_processing_ms: 0.3961064735336415
  time_since_restore: 520.6503825187683
  time_this_iter_s: 25.771512269973755
  time_total_s: 520.6503825187683
  timers:
    learn_throughput: 8574.769
    learn_time_ms: 18868.38
    sample_throughput: 23304.538
    sample_time_ms: 6942.511
    update_time_ms: 39.081
  timestamp: 1602799424
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     20 |           520.65 | 3235840 |   232.31 |              280.182 |              136.242 |            817.967 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3378.114931237721
    time_step_min: 3076
  date: 2020-10-15_22-04-10
  done: false
  episode_len_mean: 816.805988315482
  episode_reward_max: 282.0000000000004
  episode_reward_mean: 233.28526009855122
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 4108
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7478803247213364
        entropy_coeff: 0.0005000000000000001
        kl: 0.005493286298587918
        model: {}
        policy_loss: -0.009481565718791293
        total_loss: 8.676706790924072
        vf_explained_var: 0.9827273488044739
        vf_loss: 8.686012983322144
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.04827586206897
    gpu_util_percent0: 0.3637931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1516949697419432
    mean_env_wait_ms: 1.1925425792336821
    mean_inference_ms: 4.615258433449084
    mean_raw_obs_processing_ms: 0.39544462422994614
  time_since_restore: 546.4401116371155
  time_this_iter_s: 25.789729118347168
  time_total_s: 546.4401116371155
  timers:
    learn_throughput: 8578.826
    learn_time_ms: 18859.457
    sample_throughput: 23293.118
    sample_time_ms: 6945.914
    update_time_ms: 39.078
  timestamp: 1602799450
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     21 |           546.44 | 3397632 |  233.285 |                  282 |              136.242 |            816.806 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3372.9333333333334
    time_step_min: 3076
  date: 2020-10-15_22-04-36
  done: false
  episode_len_mean: 815.9709329582747
  episode_reward_max: 282.0000000000004
  episode_reward_mean: 234.00482319680629
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 4266
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7441265483697256
        entropy_coeff: 0.0005000000000000001
        kl: 0.006011400294179718
        model: {}
        policy_loss: -0.012443608846903468
        total_loss: 8.12285848458608
        vf_explained_var: 0.981548547744751
        vf_loss: 8.135072708129883
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.00645161290323
    gpu_util_percent0: 0.3841935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1515532879791851
    mean_env_wait_ms: 1.1931294601664082
    mean_inference_ms: 4.606075940051427
    mean_raw_obs_processing_ms: 0.39494162808008837
  time_since_restore: 572.4726228713989
  time_this_iter_s: 26.032511234283447
  time_total_s: 572.4726228713989
  timers:
    learn_throughput: 8577.592
    learn_time_ms: 18862.169
    sample_throughput: 23262.662
    sample_time_ms: 6955.008
    update_time_ms: 38.811
  timestamp: 1602799476
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     22 |          572.473 | 3559424 |  234.005 |                  282 |              136.242 |            815.971 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3366.973940629957
    time_step_min: 3076
  date: 2020-10-15_22-05-02
  done: false
  episode_len_mean: 815.1571139581929
  episode_reward_max: 282.0000000000004
  episode_reward_mean: 234.8332890605311
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 183
  episodes_total: 4449
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7222094535827637
        entropy_coeff: 0.0005000000000000001
        kl: 0.005956807329008977
        model: {}
        policy_loss: -0.009062373486813158
        total_loss: 8.264012813568115
        vf_explained_var: 0.9844412207603455
        vf_loss: 8.27284061908722
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.67666666666667
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1513966932709459
    mean_env_wait_ms: 1.1937689059153636
    mean_inference_ms: 4.595881603256814
    mean_raw_obs_processing_ms: 0.39438138899632563
  time_since_restore: 598.4623374938965
  time_this_iter_s: 25.98971462249756
  time_total_s: 598.4623374938965
  timers:
    learn_throughput: 8574.477
    learn_time_ms: 18869.022
    sample_throughput: 23270.642
    sample_time_ms: 6952.623
    update_time_ms: 37.599
  timestamp: 1602799502
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     23 |          598.462 | 3721216 |  234.833 |                  282 |              136.242 |            815.157 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3358.6475218038713
    time_step_min: 3076
  date: 2020-10-15_22-05-28
  done: false
  episode_len_mean: 814.0994300189993
  episode_reward_max: 282.4545454545455
  episode_reward_mean: 236.12900378068204
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 288
  episodes_total: 4737
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6992855221033096
        entropy_coeff: 0.0005000000000000001
        kl: 0.005327076263104876
        model: {}
        policy_loss: -0.00848967404696547
        total_loss: 9.076302925745646
        vf_explained_var: 0.9848020672798157
        vf_loss: 9.084609985351562
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.700000000000003
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15117667559847844
    mean_env_wait_ms: 1.194706561446557
    mean_inference_ms: 4.581513172657573
    mean_raw_obs_processing_ms: 0.3936107915509205
  time_since_restore: 624.4964699745178
  time_this_iter_s: 26.034132480621338
  time_total_s: 624.4964699745178
  timers:
    learn_throughput: 8579.502
    learn_time_ms: 18857.97
    sample_throughput: 23193.832
    sample_time_ms: 6975.648
    update_time_ms: 35.72
  timestamp: 1602799528
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     24 |          624.496 | 3883008 |  236.129 |              282.455 |              136.242 |            814.099 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3354.6355409296584
    time_step_min: 3076
  date: 2020-10-15_22-05-55
  done: false
  episode_len_mean: 813.6945692119232
  episode_reward_max: 282.90909090909133
  episode_reward_mean: 236.7266973532796
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 4898
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6953709373871485
        entropy_coeff: 0.0005000000000000001
        kl: 0.005563315353356302
        model: {}
        policy_loss: -0.011534025679187229
        total_loss: 7.18784765402476
        vf_explained_var: 0.9845226407051086
        vf_loss: 7.199173092842102
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.16666666666667
    gpu_util_percent0: 0.27766666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15106187958556122
    mean_env_wait_ms: 1.1951646953678479
    mean_inference_ms: 4.574062891779079
    mean_raw_obs_processing_ms: 0.39320847414618765
  time_since_restore: 650.5541234016418
  time_this_iter_s: 26.057653427124023
  time_total_s: 650.5541234016418
  timers:
    learn_throughput: 8575.321
    learn_time_ms: 18867.165
    sample_throughput: 23164.513
    sample_time_ms: 6984.477
    update_time_ms: 35.559
  timestamp: 1602799555
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     25 |          650.554 | 4044800 |  236.727 |              282.909 |              136.242 |            813.695 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3350.9976109894487
    time_step_min: 3026
  date: 2020-10-15_22-06-21
  done: false
  episode_len_mean: 813.2152599327931
  episode_reward_max: 286.24242424242414
  episode_reward_mean: 237.2804482859829
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 5059
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6938041498263677
        entropy_coeff: 0.0005000000000000001
        kl: 0.00575402588583529
        model: {}
        policy_loss: -0.010851105515030213
        total_loss: 7.853219707806905
        vf_explained_var: 0.9833803176879883
        vf_loss: 7.863842288653056
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.04
    gpu_util_percent0: 0.37
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15095203515690678
    mean_env_wait_ms: 1.1955938271693074
    mean_inference_ms: 4.566930810830606
    mean_raw_obs_processing_ms: 0.39282088741386767
  time_since_restore: 676.3634505271912
  time_this_iter_s: 25.809327125549316
  time_total_s: 676.3634505271912
  timers:
    learn_throughput: 8597.951
    learn_time_ms: 18817.506
    sample_throughput: 23083.196
    sample_time_ms: 7009.082
    update_time_ms: 33.858
  timestamp: 1602799581
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     26 |          676.363 | 4206592 |   237.28 |              286.242 |              136.242 |            813.215 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3344.712121212121
    time_step_min: 3026
  date: 2020-10-15_22-06-47
  done: false
  episode_len_mean: 812.4174191121143
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 238.21776455297896
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 257
  episodes_total: 5316
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6754096349080404
        entropy_coeff: 0.0005000000000000001
        kl: 0.005268798054506381
        model: {}
        policy_loss: -0.01019210034670929
        total_loss: 9.916938702265421
        vf_explained_var: 0.9842239022254944
        vf_loss: 9.926941951115927
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60666666666667
    gpu_util_percent0: 0.3436666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15078293597332024
    mean_env_wait_ms: 1.19623839326394
    mean_inference_ms: 4.556195684069735
    mean_raw_obs_processing_ms: 0.39223135096711004
  time_since_restore: 702.2646071910858
  time_this_iter_s: 25.901156663894653
  time_total_s: 702.2646071910858
  timers:
    learn_throughput: 8603.029
    learn_time_ms: 18806.398
    sample_throughput: 23171.111
    sample_time_ms: 6982.488
    update_time_ms: 34.93
  timestamp: 1602799607
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     27 |          702.265 | 4368384 |  238.218 |              289.424 |              136.242 |            812.417 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3339.8709501274116
    time_step_min: 3026
  date: 2020-10-15_22-07-13
  done: false
  episode_len_mean: 811.7710669077758
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 238.8827880979779
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 5530
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6553066670894623
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051280740881338716
        model: {}
        policy_loss: -0.009533356826674813
        total_loss: 7.269570549329122
        vf_explained_var: 0.9859780669212341
        vf_loss: 7.278918703397115
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.33333333333333
    gpu_util_percent0: 0.3756666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15065974672606022
    mean_env_wait_ms: 1.196685191977989
    mean_inference_ms: 4.548060532458023
    mean_raw_obs_processing_ms: 0.39180580786160335
  time_since_restore: 728.2100241184235
  time_this_iter_s: 25.945416927337646
  time_total_s: 728.2100241184235
  timers:
    learn_throughput: 8601.615
    learn_time_ms: 18809.491
    sample_throughput: 23135.959
    sample_time_ms: 6993.097
    update_time_ms: 38.548
  timestamp: 1602799633
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     28 |           728.21 | 4530176 |  238.883 |              289.424 |              136.242 |            811.771 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3336.404812455768
    time_step_min: 3026
  date: 2020-10-15_22-07-39
  done: false
  episode_len_mean: 811.354606188467
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 239.4166773217406
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 5688
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6610687722762426
        entropy_coeff: 0.0005000000000000001
        kl: 0.005322185306188961
        model: {}
        policy_loss: -0.011243697178239623
        total_loss: 6.071420470873515
        vf_explained_var: 0.9862317442893982
        vf_loss: 6.082462549209595
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.833333333333336
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1505705545461702
    mean_env_wait_ms: 1.1970038836116796
    mean_inference_ms: 4.542299961453864
    mean_raw_obs_processing_ms: 0.3914964122015318
  time_since_restore: 753.9614369869232
  time_this_iter_s: 25.751412868499756
  time_total_s: 753.9614369869232
  timers:
    learn_throughput: 8599.708
    learn_time_ms: 18813.662
    sample_throughput: 23151.847
    sample_time_ms: 6988.298
    update_time_ms: 38.99
  timestamp: 1602799659
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     29 |          753.961 | 4691968 |  239.417 |              289.424 |              136.242 |            811.355 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3332.803390991608
    time_step_min: 3026
  date: 2020-10-15_22-08-04
  done: false
  episode_len_mean: 810.8731914893617
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 239.9534493874919
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 5875
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6483458032210668
        entropy_coeff: 0.0005000000000000001
        kl: 0.006111707344340782
        model: {}
        policy_loss: -0.0106555049715098
        total_loss: 8.00598931312561
        vf_explained_var: 0.9853588938713074
        vf_loss: 8.016357819239298
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.91034482758621
    gpu_util_percent0: 0.34068965517241384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15047104288788637
    mean_env_wait_ms: 1.1973600196978766
    mean_inference_ms: 4.535725856250525
    mean_raw_obs_processing_ms: 0.3911400742818682
  time_since_restore: 779.5211801528931
  time_this_iter_s: 25.55974316596985
  time_total_s: 779.5211801528931
  timers:
    learn_throughput: 8606.49
    learn_time_ms: 18798.837
    sample_throughput: 23182.176
    sample_time_ms: 6979.155
    update_time_ms: 40.488
  timestamp: 1602799684
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     30 |          779.521 | 4853760 |  239.953 |              289.424 |              136.242 |            810.873 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3327.303694017653
    time_step_min: 3026
  date: 2020-10-15_22-08-30
  done: false
  episode_len_mean: 810.2309067273318
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 240.72891245900666
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 279
  episodes_total: 6154
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6224197347958883
        entropy_coeff: 0.0005000000000000001
        kl: 0.005031730552824835
        model: {}
        policy_loss: -0.008331117608274022
        total_loss: 9.511263291041056
        vf_explained_var: 0.9848304390907288
        vf_loss: 9.519402503967285
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.62666666666667
    gpu_util_percent0: 0.3596666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15032782105139525
    mean_env_wait_ms: 1.1978437434064293
    mean_inference_ms: 4.526622355089264
    mean_raw_obs_processing_ms: 0.390655128119267
  time_since_restore: 805.2092418670654
  time_this_iter_s: 25.688061714172363
  time_total_s: 805.2092418670654
  timers:
    learn_throughput: 8616.986
    learn_time_ms: 18775.939
    sample_throughput: 23144.089
    sample_time_ms: 6990.64
    update_time_ms: 40.37
  timestamp: 1602799710
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     31 |          805.209 | 5015552 |  240.729 |              289.424 |              136.242 |            810.231 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3324.074952259707
    time_step_min: 3011
  date: 2020-10-15_22-08-56
  done: false
  episode_len_mean: 809.8982594936709
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 241.21406309934784
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 166
  episodes_total: 6320
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6152809311946233
        entropy_coeff: 0.0005000000000000001
        kl: 0.005592694894100229
        model: {}
        policy_loss: -0.010222488936657706
        total_loss: 6.031432429949443
        vf_explained_var: 0.9868378639221191
        vf_loss: 6.041403333346049
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.276666666666664
    gpu_util_percent0: 0.32499999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15024877670452308
    mean_env_wait_ms: 1.198106567796025
    mean_inference_ms: 4.521619480573685
    mean_raw_obs_processing_ms: 0.39039006081446054
  time_since_restore: 830.8288795948029
  time_this_iter_s: 25.619637727737427
  time_total_s: 830.8288795948029
  timers:
    learn_throughput: 8637.349
    learn_time_ms: 18731.673
    sample_throughput: 23160.606
    sample_time_ms: 6985.655
    update_time_ms: 39.089
  timestamp: 1602799736
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     32 |          830.829 | 5177344 |  241.214 |              289.424 |              136.242 |            809.898 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3321.071849782744
    time_step_min: 3011
  date: 2020-10-15_22-09-22
  done: false
  episode_len_mean: 809.7006172839506
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 241.67134306023192
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 160
  episodes_total: 6480
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6128020832935969
        entropy_coeff: 0.0005000000000000001
        kl: 0.005626710519815485
        model: {}
        policy_loss: -0.011386892847137156
        total_loss: 6.6085009177525835
        vf_explained_var: 0.9855687022209167
        vf_loss: 6.6196315685908
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.406896551724138
    gpu_util_percent0: 0.34896551724137936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15017633712410017
    mean_env_wait_ms: 1.198342463100914
    mean_inference_ms: 4.516890441251819
    mean_raw_obs_processing_ms: 0.39013934922714216
  time_since_restore: 856.4063284397125
  time_this_iter_s: 25.577448844909668
  time_total_s: 856.4063284397125
  timers:
    learn_throughput: 8652.736
    learn_time_ms: 18698.362
    sample_throughput: 23197.774
    sample_time_ms: 6974.462
    update_time_ms: 41.01
  timestamp: 1602799762
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     33 |          856.406 | 5339136 |  241.671 |              289.424 |              136.242 |            809.701 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3316.9723053892217
    time_step_min: 3011
  date: 2020-10-15_22-09-48
  done: false
  episode_len_mean: 809.5023823704586
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 242.33120815059468
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 236
  episodes_total: 6716
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5948016991217931
        entropy_coeff: 0.0005000000000000001
        kl: 0.005257600375140707
        model: {}
        policy_loss: -0.010794992907904088
        total_loss: 8.705323378245035
        vf_explained_var: 0.9856746792793274
        vf_loss: 8.715890169143677
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.503333333333337
    gpu_util_percent0: 0.28333333333333327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15007373644286484
    mean_env_wait_ms: 1.1986770579665917
    mean_inference_ms: 4.510267028876266
    mean_raw_obs_processing_ms: 0.3897866794613245
  time_since_restore: 882.1388766765594
  time_this_iter_s: 25.732548236846924
  time_total_s: 882.1388766765594
  timers:
    learn_throughput: 8655.584
    learn_time_ms: 18692.211
    sample_throughput: 23279.161
    sample_time_ms: 6950.079
    update_time_ms: 40.861
  timestamp: 1602799788
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     34 |          882.139 | 5500928 |  242.331 |              289.424 |              136.242 |            809.502 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3312.533198322002
    time_step_min: 3011
  date: 2020-10-15_22-10-14
  done: false
  episode_len_mean: 809.4299899266082
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 242.97450254451257
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 233
  episodes_total: 6949
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5731659332911173
        entropy_coeff: 0.0005000000000000001
        kl: 0.004918006753238539
        model: {}
        policy_loss: -0.013124067646761736
        total_loss: 6.96964172522227
        vf_explained_var: 0.9871130585670471
        vf_loss: 6.982560594876607
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.64
    gpu_util_percent0: 0.32166666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14997742577842102
    mean_env_wait_ms: 1.1989133183327547
    mean_inference_ms: 4.504101902171418
    mean_raw_obs_processing_ms: 0.38946308178581807
  time_since_restore: 907.732898235321
  time_this_iter_s: 25.594021558761597
  time_total_s: 907.732898235321
  timers:
    learn_throughput: 8669.71
    learn_time_ms: 18661.754
    sample_throughput: 23331.301
    sample_time_ms: 6934.547
    update_time_ms: 39.266
  timestamp: 1602799814
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     35 |          907.733 | 5662720 |  242.975 |              289.424 |              136.242 |             809.43 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3309.539016115352
    time_step_min: 2960
  date: 2020-10-15_22-10-40
  done: false
  episode_len_mean: 809.256258790436
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 243.4278012189404
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 7110
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5802024255196253
        entropy_coeff: 0.0005000000000000001
        kl: 0.005665121600031853
        model: {}
        policy_loss: -0.010481108650613654
        total_loss: 6.6643790404001875
        vf_explained_var: 0.9848917126655579
        vf_loss: 6.674866875012715
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.403333333333332
    gpu_util_percent0: 0.341
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1499147790035408
    mean_env_wait_ms: 1.199079072458209
    mean_inference_ms: 4.500078696894743
    mean_raw_obs_processing_ms: 0.38925100115978106
  time_since_restore: 933.5397572517395
  time_this_iter_s: 25.806859016418457
  time_total_s: 933.5397572517395
  timers:
    learn_throughput: 8663.809
    learn_time_ms: 18674.466
    sample_throughput: 23383.734
    sample_time_ms: 6918.998
    update_time_ms: 40.853
  timestamp: 1602799840
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     36 |           933.54 | 5824512 |  243.428 |              296.242 |              136.242 |            809.256 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3306.5756656090493
    time_step_min: 2960
  date: 2020-10-15_22-11-06
  done: false
  episode_len_mean: 809.1177762525738
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 243.86341798215506
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 175
  episodes_total: 7285
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5707181990146637
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055595429148525
        model: {}
        policy_loss: -0.010776971925224643
        total_loss: 6.928751269976298
        vf_explained_var: 0.9861412048339844
        vf_loss: 6.939535578091939
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.643333333333334
    gpu_util_percent0: 0.37099999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1498519756670719
    mean_env_wait_ms: 1.1992491051308087
    mean_inference_ms: 4.495813177010232
    mean_raw_obs_processing_ms: 0.38902562265812546
  time_since_restore: 959.4187581539154
  time_this_iter_s: 25.879000902175903
  time_total_s: 959.4187581539154
  timers:
    learn_throughput: 8667.45
    learn_time_ms: 18666.621
    sample_throughput: 23358.923
    sample_time_ms: 6926.347
    update_time_ms: 39.131
  timestamp: 1602799866
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     37 |          959.419 | 5986304 |  243.863 |              296.242 |              136.242 |            809.118 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3302.1002663115846
    time_step_min: 2960
  date: 2020-10-15_22-11-32
  done: false
  episode_len_mean: 808.9057778955738
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 244.52353243540622
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 261
  episodes_total: 7546
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5507474193970362
        entropy_coeff: 0.0005000000000000001
        kl: 0.004939862565758328
        model: {}
        policy_loss: -0.008943449628228942
        total_loss: 9.609710693359375
        vf_explained_var: 0.9844183921813965
        vf_loss: 9.618682702382406
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37333333333333
    gpu_util_percent0: 0.32500000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14975414755705924
    mean_env_wait_ms: 1.1994724026421286
    mean_inference_ms: 4.489718038416102
    mean_raw_obs_processing_ms: 0.3887041549061176
  time_since_restore: 985.1840415000916
  time_this_iter_s: 25.765283346176147
  time_total_s: 985.1840415000916
  timers:
    learn_throughput: 8676.558
    learn_time_ms: 18647.026
    sample_throughput: 23333.984
    sample_time_ms: 6933.749
    update_time_ms: 33.339
  timestamp: 1602799892
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     38 |          985.184 | 6148096 |  244.524 |              296.242 |              136.242 |            808.906 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3298.669218790553
    time_step_min: 2960
  date: 2020-10-15_22-11-58
  done: false
  episode_len_mean: 808.7885559287006
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 245.01595782156357
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 196
  episodes_total: 7742
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.538197527329127
        entropy_coeff: 0.0005000000000000001
        kl: 0.006036433197247486
        model: {}
        policy_loss: -0.010254086198983714
        total_loss: 5.736980835596721
        vf_explained_var: 0.9881876111030579
        vf_loss: 5.747352917989095
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.526666666666667
    gpu_util_percent0: 0.36166666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14968961281663165
    mean_env_wait_ms: 1.19960687332925
    mean_inference_ms: 4.485498662507034
    mean_raw_obs_processing_ms: 0.38848815300183065
  time_since_restore: 1011.0526096820831
  time_this_iter_s: 25.868568181991577
  time_total_s: 1011.0526096820831
  timers:
    learn_throughput: 8684.035
    learn_time_ms: 18630.971
    sample_throughput: 23243.045
    sample_time_ms: 6960.878
    update_time_ms: 32.955
  timestamp: 1602799918
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     39 |          1011.05 | 6309888 |  245.016 |              296.242 |              136.242 |            808.789 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3296.1144164759726
    time_step_min: 2960
  date: 2020-10-15_22-12-24
  done: false
  episode_len_mean: 808.5966843837003
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 245.392236717977
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 160
  episodes_total: 7902
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5471722483634949
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055558883274594946
        model: {}
        policy_loss: -0.009315188857726753
        total_loss: 6.58670171101888
        vf_explained_var: 0.9855363965034485
        vf_loss: 6.59615163008372
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.48
    gpu_util_percent0: 0.37000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.79
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14963850209549778
    mean_env_wait_ms: 1.1997185561054713
    mean_inference_ms: 4.482139423808852
    mean_raw_obs_processing_ms: 0.38831423971803725
  time_since_restore: 1037.015417098999
  time_this_iter_s: 25.962807416915894
  time_total_s: 1037.015417098999
  timers:
    learn_throughput: 8679.142
    learn_time_ms: 18641.475
    sample_throughput: 23143.23
    sample_time_ms: 6990.9
    update_time_ms: 31.979
  timestamp: 1602799944
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     40 |          1037.02 | 6471680 |  245.392 |              296.242 |              136.242 |            808.597 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3292.6551510648837
    time_step_min: 2960
  date: 2020-10-15_22-12-50
  done: false
  episode_len_mean: 808.4330621301775
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 245.87276985834674
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 8112
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.534255251288414
        entropy_coeff: 0.0005000000000000001
        kl: 0.005973616226886709
        model: {}
        policy_loss: -0.01130775130392673
        total_loss: 6.658358136812846
        vf_explained_var: 0.9877612590789795
        vf_loss: 6.669783552487691
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.720000000000006
    gpu_util_percent0: 0.29433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14957126911319157
    mean_env_wait_ms: 1.1998503765731339
    mean_inference_ms: 4.47781030994901
    mean_raw_obs_processing_ms: 0.38808517214994936
  time_since_restore: 1062.7946693897247
  time_this_iter_s: 25.779252290725708
  time_total_s: 1062.7946693897247
  timers:
    learn_throughput: 8668.106
    learn_time_ms: 18665.208
    sample_throughput: 23187.982
    sample_time_ms: 6977.408
    update_time_ms: 30.071
  timestamp: 1602799970
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     41 |          1062.79 | 6633472 |  245.873 |              296.242 |              136.242 |            808.433 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3289.2574174174174
    time_step_min: 2960
  date: 2020-10-15_22-13-16
  done: false
  episode_len_mean: 808.2537973926563
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 246.4274644543751
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 249
  episodes_total: 8361
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.511997178196907
        entropy_coeff: 0.0005000000000000001
        kl: 0.005024631430084507
        model: {}
        policy_loss: -0.010512914877229681
        total_loss: 6.375741799672444
        vf_explained_var: 0.9888591766357422
        vf_loss: 6.3863853216171265
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.480000000000004
    gpu_util_percent0: 0.3863333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1494980814646256
    mean_env_wait_ms: 1.199986043815297
    mean_inference_ms: 4.472974405489903
    mean_raw_obs_processing_ms: 0.38784062663597174
  time_since_restore: 1088.709882736206
  time_this_iter_s: 25.915213346481323
  time_total_s: 1088.709882736206
  timers:
    learn_throughput: 8657.395
    learn_time_ms: 18688.301
    sample_throughput: 23145.57
    sample_time_ms: 6990.193
    update_time_ms: 31.461
  timestamp: 1602799996
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     42 |          1088.71 | 6795264 |  246.427 |              296.242 |              136.242 |            808.254 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3287.04343220339
    time_step_min: 2960
  date: 2020-10-15_22-13-42
  done: false
  episode_len_mean: 808.2243319268636
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 246.76638750372925
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 171
  episodes_total: 8532
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5099739829699198
        entropy_coeff: 0.0005000000000000001
        kl: 0.005351927014999092
        model: {}
        policy_loss: -0.011824537985376082
        total_loss: 5.416117231051127
        vf_explained_var: 0.9883833527565002
        vf_loss: 5.4280628363291425
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.523333333333333
    gpu_util_percent0: 0.3350000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14944999359903788
    mean_env_wait_ms: 1.200063332393353
    mean_inference_ms: 4.469858621748696
    mean_raw_obs_processing_ms: 0.3876822280141201
  time_since_restore: 1114.422720193863
  time_this_iter_s: 25.71283745765686
  time_total_s: 1114.422720193863
  timers:
    learn_throughput: 8654.015
    learn_time_ms: 18695.6
    sample_throughput: 23127.904
    sample_time_ms: 6995.532
    update_time_ms: 31.393
  timestamp: 1602800022
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     43 |          1114.42 | 6957056 |  246.766 |              296.242 |              136.242 |            808.224 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3284.6597807270628
    time_step_min: 2960
  date: 2020-10-15_22-14-08
  done: false
  episode_len_mean: 808.2108952993909
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 247.1329697387621
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 169
  episodes_total: 8701
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5150950103998184
        entropy_coeff: 0.0005000000000000001
        kl: 0.005764365817109744
        model: {}
        policy_loss: -0.011448757897596806
        total_loss: 5.494906306266785
        vf_explained_var: 0.9883672595024109
        vf_loss: 5.506468613942464
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.19
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14940566304976735
    mean_env_wait_ms: 1.200131375119458
    mean_inference_ms: 4.466784597862623
    mean_raw_obs_processing_ms: 0.3875228716400522
  time_since_restore: 1140.205302476883
  time_this_iter_s: 25.78258228302002
  time_total_s: 1140.205302476883
  timers:
    learn_throughput: 8656.837
    learn_time_ms: 18689.506
    sample_throughput: 23118.545
    sample_time_ms: 6998.364
    update_time_ms: 38.196
  timestamp: 1602800048
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     44 |          1140.21 | 7118848 |  247.133 |              296.242 |              136.242 |            808.211 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3281.3542837868226
    time_step_min: 2960
  date: 2020-10-15_22-14-34
  done: false
  episode_len_mean: 808.1973124300112
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 247.62341782890488
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 229
  episodes_total: 8930
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4971722587943077
        entropy_coeff: 0.0005000000000000001
        kl: 0.005169067066162825
        model: {}
        policy_loss: -0.008737684285733849
        total_loss: 6.912025690078735
        vf_explained_var: 0.9879312515258789
        vf_loss: 6.920882701873779
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.730000000000004
    gpu_util_percent0: 0.337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14934153705512326
    mean_env_wait_ms: 1.2002198331783676
    mean_inference_ms: 4.462833198625915
    mean_raw_obs_processing_ms: 0.3873161632078113
  time_since_restore: 1165.7713694572449
  time_this_iter_s: 25.56606698036194
  time_total_s: 1165.7713694572449
  timers:
    learn_throughput: 8656.107
    learn_time_ms: 18691.081
    sample_throughput: 23135.824
    sample_time_ms: 6993.138
    update_time_ms: 38.16
  timestamp: 1602800074
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     45 |          1165.77 | 7280640 |  247.623 |              296.242 |              136.242 |            808.197 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3278.586082191781
    time_step_min: 2960
  date: 2020-10-15_22-15-00
  done: false
  episode_len_mean: 808.1566422879598
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 248.04835055058825
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 231
  episodes_total: 9161
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.48112762719392776
        entropy_coeff: 0.0005000000000000001
        kl: 0.005684606071251134
        model: {}
        policy_loss: -0.010469781302769357
        total_loss: 5.727801442146301
        vf_explained_var: 0.9894949793815613
        vf_loss: 5.738369425137837
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.72666666666667
    gpu_util_percent0: 0.3583333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14928533194079063
    mean_env_wait_ms: 1.200277888316377
    mean_inference_ms: 4.459023136522173
    mean_raw_obs_processing_ms: 0.38712812401971675
  time_since_restore: 1191.4961955547333
  time_this_iter_s: 25.724826097488403
  time_total_s: 1191.4961955547333
  timers:
    learn_throughput: 8657.237
    learn_time_ms: 18688.642
    sample_throughput: 23172.849
    sample_time_ms: 6981.964
    update_time_ms: 41.522
  timestamp: 1602800100
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     46 |           1191.5 | 7442432 |  248.048 |              296.242 |              136.242 |            808.157 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3276.515614904157
    time_step_min: 2960
  date: 2020-10-15_22-15-25
  done: false
  episode_len_mean: 808.1065222055353
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 248.3415316000598
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 9322
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.48768074065446854
        entropy_coeff: 0.0005000000000000001
        kl: 0.005590466239179174
        model: {}
        policy_loss: -0.011224654345520927
        total_loss: 4.9677205085754395
        vf_explained_var: 0.9889311790466309
        vf_loss: 4.979049324989319
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.17241379310345
    gpu_util_percent0: 0.3199999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7896551724137932
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14924632387143877
    mean_env_wait_ms: 1.200319526351857
    mean_inference_ms: 4.456482569114146
    mean_raw_obs_processing_ms: 0.3869991079703888
  time_since_restore: 1216.9556846618652
  time_this_iter_s: 25.459489107131958
  time_total_s: 1216.9556846618652
  timers:
    learn_throughput: 8667.935
    learn_time_ms: 18665.576
    sample_throughput: 23235.023
    sample_time_ms: 6963.281
    update_time_ms: 40.28
  timestamp: 1602800125
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     47 |          1216.96 | 7604224 |  248.342 |              296.242 |              136.242 |            808.107 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3274.290646114865
    time_step_min: 2960
  date: 2020-10-15_22-15-51
  done: false
  episode_len_mean: 808.0292385359697
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 248.69652987595768
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 9508
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.48659316698710126
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058739522549634176
        model: {}
        policy_loss: -0.011002960692470273
        total_loss: 6.946582198143005
        vf_explained_var: 0.9860231876373291
        vf_loss: 6.957681616147359
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.044827586206896
    gpu_util_percent0: 0.35137931034482756
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14920284725848115
    mean_env_wait_ms: 1.2003582160535355
    mean_inference_ms: 4.453528136978033
    mean_raw_obs_processing_ms: 0.38684709925998295
  time_since_restore: 1242.5419383049011
  time_this_iter_s: 25.58625364303589
  time_total_s: 1242.5419383049011
  timers:
    learn_throughput: 8661.908
    learn_time_ms: 18678.563
    sample_throughput: 23347.478
    sample_time_ms: 6929.742
    update_time_ms: 42.163
  timestamp: 1602800151
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     48 |          1242.54 | 7766016 |  248.697 |              296.394 |              136.242 |            808.029 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3271.225584027992
    time_step_min: 2960
  date: 2020-10-15_22-16-17
  done: false
  episode_len_mean: 807.914077719676
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 249.17858374579382
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 245
  episodes_total: 9753
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4660199632247289
        entropy_coeff: 0.0005000000000000001
        kl: 0.005540805713584025
        model: {}
        policy_loss: -0.009368036281860745
        total_loss: 7.123373548189799
        vf_explained_var: 0.9875916838645935
        vf_loss: 7.1328361829121905
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.93666666666667
    gpu_util_percent0: 0.294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14914376372953397
    mean_env_wait_ms: 1.200399015782105
    mean_inference_ms: 4.449827151892841
    mean_raw_obs_processing_ms: 0.38665764129350294
  time_since_restore: 1268.397170305252
  time_this_iter_s: 25.855232000350952
  time_total_s: 1268.397170305252
  timers:
    learn_throughput: 8653.023
    learn_time_ms: 18697.743
    sample_throughput: 23420.772
    sample_time_ms: 6908.056
    update_time_ms: 42.564
  timestamp: 1602800177
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     49 |           1268.4 | 7927808 |  249.179 |              296.394 |              136.242 |            807.914 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3268.990218816174
    time_step_min: 2960
  date: 2020-10-15_22-16-43
  done: false
  episode_len_mean: 807.9160052245554
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 249.51915822547792
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 200
  episodes_total: 9953
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.45666811615228653
        entropy_coeff: 0.0005000000000000001
        kl: 0.005614828163137038
        model: {}
        policy_loss: -0.010633054165130792
        total_loss: 7.585811694463094
        vf_explained_var: 0.9846763014793396
        vf_loss: 7.5965326229731245
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.355172413793106
    gpu_util_percent0: 0.3151724137931034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689654
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1491031485538944
    mean_env_wait_ms: 1.2004310977640777
    mean_inference_ms: 4.447056397582897
    mean_raw_obs_processing_ms: 0.3865222422959731
  time_since_restore: 1293.773687839508
  time_this_iter_s: 25.37651753425598
  time_total_s: 1293.773687839508
  timers:
    learn_throughput: 8660.602
    learn_time_ms: 18681.379
    sample_throughput: 23562.749
    sample_time_ms: 6866.431
    update_time_ms: 41.108
  timestamp: 1602800203
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     50 |          1293.77 | 8089600 |  249.519 |              296.394 |              136.242 |            807.916 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3267.188231792022
    time_step_min: 2960
  date: 2020-10-15_22-17-09
  done: false
  episode_len_mean: 807.809373146134
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 249.80069330840533
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 10114
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4684673274556796
        entropy_coeff: 0.0005000000000000001
        kl: 0.005773888862070938
        model: {}
        policy_loss: -0.009907488614165535
        total_loss: 6.344697276751201
        vf_explained_var: 0.9859089851379395
        vf_loss: 6.354694485664368
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.836666666666666
    gpu_util_percent0: 0.3436666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333326
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14906995111676552
    mean_env_wait_ms: 1.2004527712162953
    mean_inference_ms: 4.444810831823476
    mean_raw_obs_processing_ms: 0.3864083378739507
  time_since_restore: 1319.3802440166473
  time_this_iter_s: 25.606556177139282
  time_total_s: 1319.3802440166473
  timers:
    learn_throughput: 8667.556
    learn_time_ms: 18666.392
    sample_throughput: 23579.937
    sample_time_ms: 6861.426
    update_time_ms: 42.742
  timestamp: 1602800229
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     51 |          1319.38 | 8251392 |  249.801 |              296.394 |              136.242 |            807.809 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3264.671492421298
    time_step_min: 2960
  date: 2020-10-15_22-17-34
  done: false
  episode_len_mean: 807.5570294345468
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 250.17332406168575
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 10328
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4603114575147629
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055629881874968605
        model: {}
        policy_loss: -0.009810302134913703
        total_loss: 6.591102838516235
        vf_explained_var: 0.9873318672180176
        vf_loss: 6.601004242897034
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.593103448275862
    gpu_util_percent0: 0.3617241379310344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14902465647537105
    mean_env_wait_ms: 1.200477049034925
    mean_inference_ms: 4.441848956966613
    mean_raw_obs_processing_ms: 0.38625565263967393
  time_since_restore: 1344.9457256793976
  time_this_iter_s: 25.565481662750244
  time_total_s: 1344.9457256793976
  timers:
    learn_throughput: 8672.931
    learn_time_ms: 18654.824
    sample_throughput: 23665.688
    sample_time_ms: 6836.564
    update_time_ms: 43.136
  timestamp: 1602800254
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     52 |          1344.95 | 8413184 |  250.173 |              296.394 |              136.242 |            807.557 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3262.337414578588
    time_step_min: 2960
  date: 2020-10-15_22-18-00
  done: false
  episode_len_mean: 807.3285092697693
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 250.56256950893723
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 244
  episodes_total: 10572
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.436347837249438
        entropy_coeff: 0.0005000000000000001
        kl: 0.005471786794563134
        model: {}
        policy_loss: -0.006899951782543212
        total_loss: 6.8829309940338135
        vf_explained_var: 0.9876399636268616
        vf_loss: 6.889912207921346
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.339999999999996
    gpu_util_percent0: 0.29933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1489747788518227
    mean_env_wait_ms: 1.2004940499424852
    mean_inference_ms: 4.438687472138031
    mean_raw_obs_processing_ms: 0.3860974126877535
  time_since_restore: 1370.4279208183289
  time_this_iter_s: 25.482195138931274
  time_total_s: 1370.4279208183289
  timers:
    learn_throughput: 8676.377
    learn_time_ms: 18647.414
    sample_throughput: 23719.259
    sample_time_ms: 6821.124
    update_time_ms: 43.113
  timestamp: 1602800280
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     53 |          1370.43 | 8574976 |  250.563 |              296.394 |              136.242 |            807.329 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3260.346750093388
    time_step_min: 2960
  date: 2020-10-15_22-18-26
  done: false
  episode_len_mean: 807.1897803425168
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 250.84764435118115
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 172
  episodes_total: 10744
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.44254865994056064
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053455687981719775
        model: {}
        policy_loss: -0.011176405164102713
        total_loss: 5.371111472447713
        vf_explained_var: 0.9878227710723877
        vf_loss: 5.382375359535217
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.734482758620693
    gpu_util_percent0: 0.37344827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14894378296465738
    mean_env_wait_ms: 1.2005095589506638
    mean_inference_ms: 4.436538595192676
    mean_raw_obs_processing_ms: 0.3859919689955475
  time_since_restore: 1395.7970342636108
  time_this_iter_s: 25.369113445281982
  time_total_s: 1395.7970342636108
  timers:
    learn_throughput: 8680.594
    learn_time_ms: 18638.356
    sample_throughput: 23805.988
    sample_time_ms: 6796.273
    update_time_ms: 34.664
  timestamp: 1602800306
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     54 |           1395.8 | 8736768 |  250.848 |              296.394 |              136.242 |             807.19 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3258.4320602276903
    time_step_min: 2960
  date: 2020-10-15_22-18-52
  done: false
  episode_len_mean: 806.9568081991215
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 251.12959204046314
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 10928
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4463835855325063
        entropy_coeff: 0.0005000000000000001
        kl: 0.005551510878528158
        model: {}
        policy_loss: -0.011200966138858348
        total_loss: 5.981295307477315
        vf_explained_var: 0.9874823689460754
        vf_loss: 5.992580413818359
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.726666666666674
    gpu_util_percent0: 0.3483333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14890977750993048
    mean_env_wait_ms: 1.2005155579055158
    mean_inference_ms: 4.434168753535173
    mean_raw_obs_processing_ms: 0.3858722462555988
  time_since_restore: 1421.5252249240875
  time_this_iter_s: 25.728190660476685
  time_total_s: 1421.5252249240875
  timers:
    learn_throughput: 8671.386
    learn_time_ms: 18658.148
    sample_throughput: 23832.329
    sample_time_ms: 6788.762
    update_time_ms: 37.487
  timestamp: 1602800332
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     55 |          1421.53 | 8898560 |   251.13 |              296.394 |              136.242 |            806.957 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3256.056419009972
    time_step_min: 2960
  date: 2020-10-15_22-19-18
  done: false
  episode_len_mean: 806.6324885824304
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 251.48377389006023
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 239
  episodes_total: 11167
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42752107481161755
        entropy_coeff: 0.0005000000000000001
        kl: 0.005104644185242553
        model: {}
        policy_loss: -0.010616361068969127
        total_loss: 7.0546257098515825
        vf_explained_var: 0.9876191020011902
        vf_loss: 7.065328319867452
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.03793103448276
    gpu_util_percent0: 0.3644827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14886370002742805
    mean_env_wait_ms: 1.2005408313891133
    mean_inference_ms: 4.431345794328618
    mean_raw_obs_processing_ms: 0.3857282101671706
  time_since_restore: 1447.1090614795685
  time_this_iter_s: 25.583836555480957
  time_total_s: 1447.1090614795685
  timers:
    learn_throughput: 8673.254
    learn_time_ms: 18654.128
    sample_throughput: 23852.858
    sample_time_ms: 6782.919
    update_time_ms: 33.831
  timestamp: 1602800358
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     56 |          1447.11 | 9060352 |  251.484 |              296.394 |              136.242 |            806.632 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3253.932074805928
    time_step_min: 2960
  date: 2020-10-15_22-19-44
  done: false
  episode_len_mean: 806.4540098487513
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 251.82604802865086
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 205
  episodes_total: 11372
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.41268759469191235
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642210831865668
        model: {}
        policy_loss: -0.011997847468592227
        total_loss: 4.267285346984863
        vf_explained_var: 0.9909687638282776
        vf_loss: 4.279348532358806
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97666666666667
    gpu_util_percent0: 0.35533333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.79
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488308475245483
    mean_env_wait_ms: 1.2005511495357153
    mean_inference_ms: 4.429024161112019
    mean_raw_obs_processing_ms: 0.38561881035854
  time_since_restore: 1472.7310166358948
  time_this_iter_s: 25.621955156326294
  time_total_s: 1472.7310166358948
  timers:
    learn_throughput: 8665.432
    learn_time_ms: 18670.969
    sample_throughput: 23858.227
    sample_time_ms: 6781.392
    update_time_ms: 33.677
  timestamp: 1602800384
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     57 |          1472.73 | 9222144 |  251.826 |              296.394 |              136.242 |            806.454 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3252.293227853603
    time_step_min: 2960
  date: 2020-10-15_22-20-09
  done: false
  episode_len_mean: 806.2417887165266
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 252.07888924779468
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 167
  episodes_total: 11539
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42186784744262695
        entropy_coeff: 0.0005000000000000001
        kl: 0.006096464271346728
        model: {}
        policy_loss: -0.010270674087223597
        total_loss: 4.845337271690369
        vf_explained_var: 0.9890034198760986
        vf_loss: 4.855666438738505
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666676
    gpu_util_percent0: 0.402
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14880267883023068
    mean_env_wait_ms: 1.200558085390027
    mean_inference_ms: 4.427137660722556
    mean_raw_obs_processing_ms: 0.38552445771862914
  time_since_restore: 1498.4419918060303
  time_this_iter_s: 25.710975170135498
  time_total_s: 1498.4419918060303
  timers:
    learn_throughput: 8661.769
    learn_time_ms: 18678.864
    sample_throughput: 23869.638
    sample_time_ms: 6778.15
    update_time_ms: 33.911
  timestamp: 1602800409
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     58 |          1498.44 | 9383936 |  252.079 |              296.394 |              136.242 |            806.242 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3250.10774611841
    time_step_min: 2960
  date: 2020-10-15_22-20-35
  done: false
  episode_len_mean: 805.9806939955774
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 252.39672537588845
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 219
  episodes_total: 11758
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4129921222726504
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053271743236109614
        model: {}
        policy_loss: -0.011034174482726181
        total_loss: 5.47857944170634
        vf_explained_var: 0.9897108674049377
        vf_loss: 5.489686965942383
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.741379310344833
    gpu_util_percent0: 0.3472413793103449
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14876414148834563
    mean_env_wait_ms: 1.2005704210041013
    mean_inference_ms: 4.424666951939168
    mean_raw_obs_processing_ms: 0.3853980838420706
  time_since_restore: 1524.1446039676666
  time_this_iter_s: 25.702612161636353
  time_total_s: 1524.1446039676666
  timers:
    learn_throughput: 8657.767
    learn_time_ms: 18687.497
    sample_throughput: 23954.7
    sample_time_ms: 6754.082
    update_time_ms: 33.334
  timestamp: 1602800435
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     59 |          1524.14 | 9545728 |  252.397 |              296.394 |              136.242 |            805.981 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3247.9415405201976
    time_step_min: 2960
  date: 2020-10-15_22-21-01
  done: false
  episode_len_mean: 805.7597765363129
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 252.7291248177598
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 235
  episodes_total: 11993
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3986821745832761
        entropy_coeff: 0.0005000000000000001
        kl: 0.005131128787373503
        model: {}
        policy_loss: -0.010990443173795938
        total_loss: 4.98269510269165
        vf_explained_var: 0.9906366467475891
        vf_loss: 4.993756691614787
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.67000000000001
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14872691099198931
    mean_env_wait_ms: 1.200567117505912
    mean_inference_ms: 4.422230358320126
    mean_raw_obs_processing_ms: 0.38527883594420737
  time_since_restore: 1549.7161300182343
  time_this_iter_s: 25.571526050567627
  time_total_s: 1549.7161300182343
  timers:
    learn_throughput: 8653.785
    learn_time_ms: 18696.097
    sample_throughput: 23920.885
    sample_time_ms: 6763.629
    update_time_ms: 33.556
  timestamp: 1602800461
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     60 |          1549.72 | 9707520 |  252.729 |              296.394 |              136.242 |             805.76 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3246.39505358615
    time_step_min: 2960
  date: 2020-10-15_22-21-27
  done: false
  episode_len_mean: 805.6408844320237
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 252.97985443785205
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 173
  episodes_total: 12166
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3998661811153094
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056130079707751674
        model: {}
        policy_loss: -0.008932064840337262
        total_loss: 5.2044451634089155
        vf_explained_var: 0.988438069820404
        vf_loss: 5.213436643282573
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.70689655172414
    gpu_util_percent0: 0.2837931034482758
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7862068965517235
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14870067684310187
    mean_env_wait_ms: 1.2005760544375403
    mean_inference_ms: 4.42047020885464
    mean_raw_obs_processing_ms: 0.3851925411454854
  time_since_restore: 1575.3453161716461
  time_this_iter_s: 25.629186153411865
  time_total_s: 1575.3453161716461
  timers:
    learn_throughput: 8649.883
    learn_time_ms: 18704.529
    sample_throughput: 23939.955
    sample_time_ms: 6758.241
    update_time_ms: 31.947
  timestamp: 1602800487
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     61 |          1575.35 | 9869312 |   252.98 |              296.394 |              136.242 |            805.641 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3244.4254161591557
    time_step_min: 2960
  date: 2020-10-15_22-21-53
  done: false
  episode_len_mean: 805.5567970204842
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 253.26912800582946
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 12351
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.40204547842343646
        entropy_coeff: 0.0005000000000000001
        kl: 0.005185766688858469
        model: {}
        policy_loss: -0.009803345756760487
        total_loss: 4.718565662701924
        vf_explained_var: 0.9900150299072266
        vf_loss: 4.728440443674724
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.613333333333333
    gpu_util_percent0: 0.37466666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14867282230295473
    mean_env_wait_ms: 1.2005754618441058
    mean_inference_ms: 4.418557730184404
    mean_raw_obs_processing_ms: 0.38509623553350775
  time_since_restore: 1600.9797809123993
  time_this_iter_s: 25.634464740753174
  time_total_s: 1600.9797809123993
  timers:
    learn_throughput: 8650.557
    learn_time_ms: 18703.074
    sample_throughput: 23906.384
    sample_time_ms: 6767.732
    update_time_ms: 30.206
  timestamp: 1602800513
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     62 |          1600.98 | 10031104 |  253.269 |              296.394 |              136.242 |            805.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3242.267564122989
    time_step_min: 2960
  date: 2020-10-15_22-22-19
  done: false
  episode_len_mean: 805.4836378077839
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 253.58693768503136
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 239
  episodes_total: 12590
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3832032655676206
        entropy_coeff: 0.0005000000000000001
        kl: 0.004957521761146684
        model: {}
        policy_loss: -0.010417382038819293
        total_loss: 5.678230047225952
        vf_explained_var: 0.9898428320884705
        vf_loss: 5.688715140024821
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.31333333333333
    gpu_util_percent0: 0.3456666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486343228796063
    mean_env_wait_ms: 1.2005659721483044
    mean_inference_ms: 4.416216529679939
    mean_raw_obs_processing_ms: 0.3849743574124984
  time_since_restore: 1626.5679080486298
  time_this_iter_s: 25.58812713623047
  time_total_s: 1626.5679080486298
  timers:
    learn_throughput: 8651.331
    learn_time_ms: 18701.4
    sample_throughput: 23866.665
    sample_time_ms: 6778.995
    update_time_ms: 29.619
  timestamp: 1602800539
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     63 |          1626.57 | 10192896 |  253.587 |              296.394 |              136.242 |            805.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3240.7313479623826
    time_step_min: 2960
  date: 2020-10-15_22-22-45
  done: false
  episode_len_mean: 805.4840575179744
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 253.83679558953077
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 12796
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3708825930953026
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047113581870992976
        model: {}
        policy_loss: -0.01054179862209518
        total_loss: 5.086221973101298
        vf_explained_var: 0.9899683594703674
        vf_loss: 5.096890250841777
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49
    gpu_util_percent0: 0.3793333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14860667984505951
    mean_env_wait_ms: 1.200556423130197
    mean_inference_ms: 4.414315519416472
    mean_raw_obs_processing_ms: 0.3848835210972314
  time_since_restore: 1652.3516561985016
  time_this_iter_s: 25.783748149871826
  time_total_s: 1652.3516561985016
  timers:
    learn_throughput: 8639.023
    learn_time_ms: 18728.043
    sample_throughput: 23830.079
    sample_time_ms: 6789.403
    update_time_ms: 33.248
  timestamp: 1602800565
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     64 |          1652.35 | 10354688 |  253.837 |              296.394 |              136.242 |            805.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3239.3298259187623
    time_step_min: 2960
  date: 2020-10-15_22-23-11
  done: false
  episode_len_mean: 805.4105393102384
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 254.04562405164208
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 165
  episodes_total: 12961
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.37956344336271286
        entropy_coeff: 0.0005000000000000001
        kl: 0.005822447400229673
        model: {}
        policy_loss: -0.009251177175125727
        total_loss: 4.42275853951772
        vf_explained_var: 0.9900858998298645
        vf_loss: 4.432163159052531
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.42
    gpu_util_percent0: 0.314
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14858294728438387
    mean_env_wait_ms: 1.200544560181868
    mean_inference_ms: 4.412793447623649
    mean_raw_obs_processing_ms: 0.3848068345820885
  time_since_restore: 1678.1730346679688
  time_this_iter_s: 25.821378469467163
  time_total_s: 1678.1730346679688
  timers:
    learn_throughput: 8636.796
    learn_time_ms: 18732.872
    sample_throughput: 23813.166
    sample_time_ms: 6794.225
    update_time_ms: 32.325
  timestamp: 1602800591
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     65 |          1678.17 | 10516480 |  254.046 |              296.394 |              136.242 |            805.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3237.417313841937
    time_step_min: 2960
  date: 2020-10-15_22-23-37
  done: false
  episode_len_mean: 805.2220956719818
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 254.32208186650095
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 209
  episodes_total: 13170
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3738606820503871
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052025926221782965
        model: {}
        policy_loss: -0.009261313234067833
        total_loss: 5.5259010791778564
        vf_explained_var: 0.9895394444465637
        vf_loss: 5.535316904385884
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.649999999999995
    gpu_util_percent0: 0.3393333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14855399678207853
    mean_env_wait_ms: 1.200533142829761
    mean_inference_ms: 4.410875077952207
    mean_raw_obs_processing_ms: 0.3847081606361737
  time_since_restore: 1704.0297923088074
  time_this_iter_s: 25.856757640838623
  time_total_s: 1704.0297923088074
  timers:
    learn_throughput: 8633.306
    learn_time_ms: 18740.446
    sample_throughput: 23742.984
    sample_time_ms: 6814.308
    update_time_ms: 30.907
  timestamp: 1602800617
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     66 |          1704.03 | 10678272 |  254.322 |              296.394 |              136.242 |            805.222 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3235.4910300493348
    time_step_min: 2960
  date: 2020-10-15_22-24-03
  done: false
  episode_len_mean: 805.0401073505293
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 254.61971888257855
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 244
  episodes_total: 13414
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.35225145022074383
        entropy_coeff: 0.0005000000000000001
        kl: 0.004920171923004091
        model: {}
        policy_loss: -0.01008309266762808
        total_loss: 5.4109346469243365
        vf_explained_var: 0.9899579882621765
        vf_loss: 5.421163002649943
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.386206896551727
    gpu_util_percent0: 0.3137931034482758
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14852020299452098
    mean_env_wait_ms: 1.2005162133077467
    mean_inference_ms: 4.408800143798725
    mean_raw_obs_processing_ms: 0.3846034841157543
  time_since_restore: 1729.444274187088
  time_this_iter_s: 25.41448187828064
  time_total_s: 1729.444274187088
  timers:
    learn_throughput: 8641.511
    learn_time_ms: 18722.651
    sample_throughput: 23755.977
    sample_time_ms: 6810.581
    update_time_ms: 31.108
  timestamp: 1602800643
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     67 |          1729.44 | 10840064 |   254.62 |              296.394 |              136.242 |             805.04 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3233.98686540732
    time_step_min: 2960
  date: 2020-10-15_22-24-29
  done: false
  episode_len_mean: 804.9040329702678
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 254.83588014379882
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 174
  episodes_total: 13588
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3531215637922287
        entropy_coeff: 0.0005000000000000001
        kl: 0.005108428847355147
        model: {}
        policy_loss: -0.011081730267809084
        total_loss: 4.278166015942891
        vf_explained_var: 0.9902045726776123
        vf_loss: 4.289408286412557
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.026666666666664
    gpu_util_percent0: 0.377
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14849878067995015
    mean_env_wait_ms: 1.200498330818975
    mean_inference_ms: 4.407329609563752
    mean_raw_obs_processing_ms: 0.3845292195436141
  time_since_restore: 1755.0901443958282
  time_this_iter_s: 25.645870208740234
  time_total_s: 1755.0901443958282
  timers:
    learn_throughput: 8647.533
    learn_time_ms: 18709.614
    sample_throughput: 23705.138
    sample_time_ms: 6825.187
    update_time_ms: 29.196
  timestamp: 1602800669
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     68 |          1755.09 | 11001856 |  254.836 |              296.394 |              136.242 |            804.904 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3232.6620302942033
    time_step_min: 2959
  date: 2020-10-15_22-24-55
  done: false
  episode_len_mean: 804.7653253922139
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 255.04309069779723
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 13768
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.35613494863112766
        entropy_coeff: 0.0005000000000000001
        kl: 0.005562700525236626
        model: {}
        policy_loss: -0.009131224146888902
        total_loss: 5.495855371157329
        vf_explained_var: 0.9884891510009766
        vf_loss: 5.5051471789677935
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.29655172413793
    gpu_util_percent0: 0.3220689655172413
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7931034482758617
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14847500676404465
    mean_env_wait_ms: 1.200475743033192
    mean_inference_ms: 4.405767553497236
    mean_raw_obs_processing_ms: 0.384446478815639
  time_since_restore: 1780.7362957000732
  time_this_iter_s: 25.646151304244995
  time_total_s: 1780.7362957000732
  timers:
    learn_throughput: 8657.762
    learn_time_ms: 18687.508
    sample_throughput: 23653.786
    sample_time_ms: 6840.004
    update_time_ms: 29.618
  timestamp: 1602800695
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     69 |          1780.74 | 11163648 |  255.043 |              296.394 |              136.242 |            804.765 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3230.70392900594
    time_step_min: 2959
  date: 2020-10-15_22-25-21
  done: false
  episode_len_mean: 804.658219715897
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 255.34220209086365
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 241
  episodes_total: 14009
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3406384338935216
        entropy_coeff: 0.0005000000000000001
        kl: 0.005524209622914593
        model: {}
        policy_loss: -0.009801516970280014
        total_loss: 5.314028779665629
        vf_explained_var: 0.9903772473335266
        vf_loss: 5.323983470598857
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.213333333333335
    gpu_util_percent0: 0.361
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8533333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484434354603085
    mean_env_wait_ms: 1.2004574440599733
    mean_inference_ms: 4.403848205268749
    mean_raw_obs_processing_ms: 0.38434825050667376
  time_since_restore: 1806.2695395946503
  time_this_iter_s: 25.533243894577026
  time_total_s: 1806.2695395946503
  timers:
    learn_throughput: 8657.678
    learn_time_ms: 18687.689
    sample_throughput: 23678.085
    sample_time_ms: 6832.985
    update_time_ms: 31.228
  timestamp: 1602800721
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     70 |          1806.27 | 11325440 |  255.342 |              297.909 |              136.242 |            804.658 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3229.0798843686102
    time_step_min: 2959
  date: 2020-10-15_22-25-47
  done: false
  episode_len_mean: 804.5398410577396
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 255.60027236284353
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 14219
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3335581570863724
        entropy_coeff: 0.0005000000000000001
        kl: 0.005118343203018109
        model: {}
        policy_loss: -0.008664251750208981
        total_loss: 3.489299555619558
        vf_explained_var: 0.992800235748291
        vf_loss: 3.4981147050857544
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.576666666666668
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14841997518768102
    mean_env_wait_ms: 1.200436568028414
    mean_inference_ms: 4.402255383287084
    mean_raw_obs_processing_ms: 0.38426784782647705
  time_since_restore: 1831.9736437797546
  time_this_iter_s: 25.70410418510437
  time_total_s: 1831.9736437797546
  timers:
    learn_throughput: 8659.186
    learn_time_ms: 18684.435
    sample_throughput: 23680.798
    sample_time_ms: 6832.202
    update_time_ms: 39.585
  timestamp: 1602800747
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     71 |          1831.97 | 11487232 |    255.6 |              297.909 |              136.242 |             804.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3227.9840384749423
    time_step_min: 2959
  date: 2020-10-15_22-26-13
  done: false
  episode_len_mean: 804.4161162483488
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 255.77989377189817
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 164
  episodes_total: 14383
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.34026538332303363
        entropy_coeff: 0.0005000000000000001
        kl: 0.005363866764431198
        model: {}
        policy_loss: -0.009909541860300427
        total_loss: 5.410627643267314
        vf_explained_var: 0.9876885414123535
        vf_loss: 5.420690417289734
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.973333333333336
    gpu_util_percent0: 0.356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14840027018597188
    mean_env_wait_ms: 1.2004164044034467
    mean_inference_ms: 4.4009917657865
    mean_raw_obs_processing_ms: 0.3842014870505365
  time_since_restore: 1857.6441118717194
  time_this_iter_s: 25.67046809196472
  time_total_s: 1857.6441118717194
  timers:
    learn_throughput: 8656.071
    learn_time_ms: 18691.158
    sample_throughput: 23702.778
    sample_time_ms: 6825.866
    update_time_ms: 41.915
  timestamp: 1602800773
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     72 |          1857.64 | 11649024 |   255.78 |              297.909 |              136.242 |            804.416 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3226.49241227769
    time_step_min: 2959
  date: 2020-10-15_22-26-39
  done: false
  episode_len_mean: 804.2507706007261
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 256.00546322184783
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 216
  episodes_total: 14599
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3373646264274915
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051965160528197885
        model: {}
        policy_loss: -0.010510957625228912
        total_loss: 6.452934543291728
        vf_explained_var: 0.9878680109977722
        vf_loss: 6.463597814242045
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.15862068965517
    gpu_util_percent0: 0.3993103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483737126900023
    mean_env_wait_ms: 1.200397038026557
    mean_inference_ms: 4.399335516684801
    mean_raw_obs_processing_ms: 0.3841123397294377
  time_since_restore: 1883.3637013435364
  time_this_iter_s: 25.719589471817017
  time_total_s: 1883.3637013435364
  timers:
    learn_throughput: 8649.162
    learn_time_ms: 18706.09
    sample_throughput: 23711.208
    sample_time_ms: 6823.44
    update_time_ms: 42.155
  timestamp: 1602800799
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     73 |          1883.36 | 11810816 |  256.005 |              297.909 |              136.242 |            804.251 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3224.846081081081
    time_step_min: 2959
  date: 2020-10-15_22-27-05
  done: false
  episode_len_mean: 804.0986114855756
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 256.2684461220454
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 237
  episodes_total: 14836
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3218430131673813
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052020570340876775
        model: {}
        policy_loss: -0.009475928905885667
        total_loss: 4.777470310529073
        vf_explained_var: 0.9910323619842529
        vf_loss: 4.787090698877971
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84666666666667
    gpu_util_percent0: 0.312
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14834613507344324
    mean_env_wait_ms: 1.200365273508334
    mean_inference_ms: 4.397616567154901
    mean_raw_obs_processing_ms: 0.3840229511467465
  time_since_restore: 1909.0494089126587
  time_this_iter_s: 25.685707569122314
  time_total_s: 1909.0494089126587
  timers:
    learn_throughput: 8657.241
    learn_time_ms: 18688.632
    sample_throughput: 23679.334
    sample_time_ms: 6832.625
    update_time_ms: 40.14
  timestamp: 1602800825
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     74 |          1909.05 | 11972608 |  256.268 |              297.909 |              136.242 |            804.099 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3223.6562040870845
    time_step_min: 2959
  date: 2020-10-15_22-27-31
  done: false
  episode_len_mean: 804.0159227181879
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 256.4541820604445
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 174
  episodes_total: 15010
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3240286832054456
        entropy_coeff: 0.0005000000000000001
        kl: 0.005266964125136535
        model: {}
        policy_loss: -0.009812670527026057
        total_loss: 4.1972536245981855
        vf_explained_var: 0.9906252026557922
        vf_loss: 4.207211852073669
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.4
    gpu_util_percent0: 0.30900000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14832823264917172
    mean_env_wait_ms: 1.2003477944602918
    mean_inference_ms: 4.396410815564579
    mean_raw_obs_processing_ms: 0.38396066937515216
  time_since_restore: 1934.8591713905334
  time_this_iter_s: 25.809762477874756
  time_total_s: 1934.8591713905334
  timers:
    learn_throughput: 8663.545
    learn_time_ms: 18675.035
    sample_throughput: 23636.61
    sample_time_ms: 6844.975
    update_time_ms: 38.6
  timestamp: 1602800851
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     75 |          1934.86 | 12134400 |  256.454 |              297.909 |              136.242 |            804.016 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3222.457783641161
    time_step_min: 2959
  date: 2020-10-15_22-27-57
  done: false
  episode_len_mean: 803.9075414582785
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 256.63038518908485
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 15196
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.32788879921038944
        entropy_coeff: 0.0005000000000000001
        kl: 0.005106039112433791
        model: {}
        policy_loss: -0.010778997612457411
        total_loss: 4.95270053545634
        vf_explained_var: 0.9898748993873596
        vf_loss: 4.963627576828003
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.773333333333333
    gpu_util_percent0: 0.30433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483080190042377
    mean_env_wait_ms: 1.2003259765837986
    mean_inference_ms: 4.395079566780296
    mean_raw_obs_processing_ms: 0.3838894730093395
  time_since_restore: 1960.7519445419312
  time_this_iter_s: 25.892773151397705
  time_total_s: 1960.7519445419312
  timers:
    learn_throughput: 8657.669
    learn_time_ms: 18687.708
    sample_throughput: 23679.121
    sample_time_ms: 6832.686
    update_time_ms: 40.137
  timestamp: 1602800877
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     76 |          1960.75 | 12296192 |   256.63 |              297.909 |              136.242 |            803.908 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3220.8537567374506
    time_step_min: 2938
  date: 2020-10-15_22-28-23
  done: false
  episode_len_mean: 803.8274700356333
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 256.8621982703615
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 239
  episodes_total: 15435
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.31383611261844635
        entropy_coeff: 0.0005000000000000001
        kl: 0.004971800837665796
        model: {}
        policy_loss: -0.007888384411974888
        total_loss: 5.696129163106282
        vf_explained_var: 0.9900109171867371
        vf_loss: 5.7041590213775635
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.134482758620692
    gpu_util_percent0: 0.33793103448275863
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14828101798167845
    mean_env_wait_ms: 1.2002998408169048
    mean_inference_ms: 4.393463084755596
    mean_raw_obs_processing_ms: 0.38380163276073137
  time_since_restore: 1986.4104251861572
  time_this_iter_s: 25.658480644226074
  time_total_s: 1986.4104251861572
  timers:
    learn_throughput: 8651.687
    learn_time_ms: 18700.63
    sample_throughput: 23644.433
    sample_time_ms: 6842.71
    update_time_ms: 40.924
  timestamp: 1602800903
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     77 |          1986.41 | 12457984 |  256.862 |              299.576 |              136.242 |            803.827 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3219.6433835309194
    time_step_min: 2938
  date: 2020-10-15_22-28-49
  done: false
  episode_len_mean: 803.7618438718752
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.043397984706
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 15641
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.30528302987416583
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049380083025122685
        model: {}
        policy_loss: -0.009833232945917795
        total_loss: 5.408826470375061
        vf_explained_var: 0.9891956448554993
        vf_loss: 5.4188045263290405
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.746666666666666
    gpu_util_percent0: 0.3846666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826078497123746
    mean_env_wait_ms: 1.2002723425725024
    mean_inference_ms: 4.392117551092245
    mean_raw_obs_processing_ms: 0.38373072176962164
  time_since_restore: 2011.7906937599182
  time_this_iter_s: 25.380268573760986
  time_total_s: 2011.7906937599182
  timers:
    learn_throughput: 8669.135
    learn_time_ms: 18662.993
    sample_throughput: 23640.375
    sample_time_ms: 6843.885
    update_time_ms: 48.418
  timestamp: 1602800929
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     78 |          2011.79 | 12619776 |  257.043 |              299.576 |              136.242 |            803.762 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3218.7050168072556
    time_step_min: 2938
  date: 2020-10-15_22-29-15
  done: false
  episode_len_mean: 803.7270138581282
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.19698983123646
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 162
  episodes_total: 15803
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3131253769000371
        entropy_coeff: 0.0005000000000000001
        kl: 0.004959821739854912
        model: {}
        policy_loss: -0.011568373551805658
        total_loss: 4.384406864643097
        vf_explained_var: 0.9902186393737793
        vf_loss: 4.396127879619598
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.883333333333336
    gpu_util_percent0: 0.284
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14824401007602858
    mean_env_wait_ms: 1.2002506499533838
    mean_inference_ms: 4.391060218242706
    mean_raw_obs_processing_ms: 0.38367333818223204
  time_since_restore: 2037.601080417633
  time_this_iter_s: 25.810386657714844
  time_total_s: 2037.601080417633
  timers:
    learn_throughput: 8663.425
    learn_time_ms: 18675.294
    sample_throughput: 23631.418
    sample_time_ms: 6846.479
    update_time_ms: 48.649
  timestamp: 1602800955
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     79 |           2037.6 | 12781568 |  257.197 |              299.576 |              136.242 |            803.727 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3217.264409537518
    time_step_min: 2938
  date: 2020-10-15_22-29-41
  done: false
  episode_len_mean: 803.6711208242273
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.4079792618662
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 212
  episodes_total: 16015
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.3097456494967143
        entropy_coeff: 0.0005000000000000001
        kl: 0.00532760863037159
        model: {}
        policy_loss: -0.009199318175281709
        total_loss: 3.949576715628306
        vf_explained_var: 0.9924254417419434
        vf_loss: 3.958928942680359
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.972413793103446
    gpu_util_percent0: 0.3417241379310344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14822237321666118
    mean_env_wait_ms: 1.20022617449816
    mean_inference_ms: 4.389668466459365
    mean_raw_obs_processing_ms: 0.383597309267663
  time_since_restore: 2063.138697862625
  time_this_iter_s: 25.537617444992065
  time_total_s: 2063.138697862625
  timers:
    learn_throughput: 8665.918
    learn_time_ms: 18669.92
    sample_throughput: 23608.554
    sample_time_ms: 6853.109
    update_time_ms: 48.136
  timestamp: 1602800981
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     80 |          2063.14 | 12943360 |  257.408 |              299.576 |              136.242 |            803.671 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3215.9624468150705
    time_step_min: 2938
  date: 2020-10-15_22-30-07
  done: false
  episode_len_mean: 803.6356364978773
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.60361816652954
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 238
  episodes_total: 16253
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29288844267527264
        entropy_coeff: 0.0005000000000000001
        kl: 0.005010667955502868
        model: {}
        policy_loss: -0.009869467292446643
        total_loss: 4.766925930976868
        vf_explained_var: 0.9914345741271973
        vf_loss: 4.776940027872722
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57666666666667
    gpu_util_percent0: 0.342
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14819794864506985
    mean_env_wait_ms: 1.2001912071946097
    mean_inference_ms: 4.38821827667998
    mean_raw_obs_processing_ms: 0.38351938699663324
  time_since_restore: 2088.7591445446014
  time_this_iter_s: 25.62044668197632
  time_total_s: 2088.7591445446014
  timers:
    learn_throughput: 8667.836
    learn_time_ms: 18665.789
    sample_throughput: 23595.763
    sample_time_ms: 6856.824
    update_time_ms: 41.64
  timestamp: 1602801007
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     81 |          2088.76 | 13105152 |  257.604 |              299.576 |              136.242 |            803.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3214.9252256647965
    time_step_min: 2938
  date: 2020-10-15_22-30-33
  done: false
  episode_len_mean: 803.6209225900682
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.759493670886
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 179
  episodes_total: 16432
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29277368386586505
        entropy_coeff: 0.0005000000000000001
        kl: 0.005134436922768752
        model: {}
        policy_loss: -0.00982299484894611
        total_loss: 3.6141475240389505
        vf_explained_var: 0.9921143054962158
        vf_loss: 3.6241148511568704
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.32333333333334
    gpu_util_percent0: 0.32833333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14818189308492707
    mean_env_wait_ms: 1.2001682648165855
    mean_inference_ms: 4.387146285403761
    mean_raw_obs_processing_ms: 0.38346130651584065
  time_since_restore: 2114.459316968918
  time_this_iter_s: 25.700172424316406
  time_total_s: 2114.459316968918
  timers:
    learn_throughput: 8667.873
    learn_time_ms: 18665.709
    sample_throughput: 23588.839
    sample_time_ms: 6858.837
    update_time_ms: 41.539
  timestamp: 1602801033
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     82 |          2114.46 | 13266944 |  257.759 |              299.576 |              136.242 |            803.621 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3213.8907324725474
    time_step_min: 2938
  date: 2020-10-15_22-30-59
  done: false
  episode_len_mean: 803.6116797110175
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.9272709028879
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 178
  episodes_total: 16610
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.30325743556022644
        entropy_coeff: 0.0005000000000000001
        kl: 0.005625793749156098
        model: {}
        policy_loss: -0.01004063745494932
        total_loss: 4.162868936856587
        vf_explained_var: 0.9911107420921326
        vf_loss: 4.173058927059174
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.765517241379303
    gpu_util_percent0: 0.29827586206896545
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14816453282507935
    mean_env_wait_ms: 1.200139432421046
    mean_inference_ms: 4.386025721063597
    mean_raw_obs_processing_ms: 0.3833996511766475
  time_since_restore: 2139.9278049468994
  time_this_iter_s: 25.468487977981567
  time_total_s: 2139.9278049468994
  timers:
    learn_throughput: 8674.781
    learn_time_ms: 18650.846
    sample_throughput: 23620.719
    sample_time_ms: 6849.58
    update_time_ms: 39.993
  timestamp: 1602801059
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     83 |          2139.93 | 13428736 |  257.927 |              299.576 |              136.242 |            803.612 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3212.459305092813
    time_step_min: 2938
  date: 2020-10-15_22-31-25
  done: false
  episode_len_mean: 803.5714200902398
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.1452239085223
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 234
  episodes_total: 16844
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29385703553756076
        entropy_coeff: 0.0005000000000000001
        kl: 0.005531733234723409
        model: {}
        policy_loss: -0.007117972457005332
        total_loss: 4.653864701588948
        vf_explained_var: 0.9917592406272888
        vf_loss: 4.66112740834554
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95
    gpu_util_percent0: 0.3333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481411811813741
    mean_env_wait_ms: 1.200112999603195
    mean_inference_ms: 4.384688048354782
    mean_raw_obs_processing_ms: 0.383325507101449
  time_since_restore: 2165.520044565201
  time_this_iter_s: 25.59223961830139
  time_total_s: 2165.520044565201
  timers:
    learn_throughput: 8676.85
    learn_time_ms: 18646.398
    sample_throughput: 23642.766
    sample_time_ms: 6843.193
    update_time_ms: 40.03
  timestamp: 1602801085
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     84 |          2165.52 | 13590528 |  258.145 |              299.576 |              136.242 |            803.571 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3211.1603712624096
    time_step_min: 2938
  date: 2020-10-15_22-31-51
  done: false
  episode_len_mean: 803.5214256404244
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.35177911952627
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 17059
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.28310780475536984
        entropy_coeff: 0.0005000000000000001
        kl: 0.006362205099624892
        model: {}
        policy_loss: -0.009414531959919259
        total_loss: 3.906277616818746
        vf_explained_var: 0.9923078417778015
        vf_loss: 3.9158312678337097
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14812365404640077
    mean_env_wait_ms: 1.2000792764154113
    mean_inference_ms: 4.383466155231759
    mean_raw_obs_processing_ms: 0.38325944871585876
  time_since_restore: 2191.4591705799103
  time_this_iter_s: 25.939126014709473
  time_total_s: 2191.4591705799103
  timers:
    learn_throughput: 8665.181
    learn_time_ms: 18671.508
    sample_throughput: 23692.562
    sample_time_ms: 6828.81
    update_time_ms: 41.294
  timestamp: 1602801111
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     85 |          2191.46 | 13752320 |  258.352 |              299.576 |              136.242 |            803.521 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3210.3102565594277
    time_step_min: 2938
  date: 2020-10-15_22-32-17
  done: false
  episode_len_mean: 803.5009579100146
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.4939174033513
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 166
  episodes_total: 17225
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.28725717465082806
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050820055960988
        model: {}
        policy_loss: -0.01056079539800218
        total_loss: 3.478187382221222
        vf_explained_var: 0.9922289848327637
        vf_loss: 3.4888898134231567
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.993333333333336
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14810872666804178
    mean_env_wait_ms: 1.2000554400043286
    mean_inference_ms: 4.382538711701011
    mean_raw_obs_processing_ms: 0.3832078477228082
  time_since_restore: 2217.200093269348
  time_this_iter_s: 25.740922689437866
  time_total_s: 2217.200093269348
  timers:
    learn_throughput: 8669.115
    learn_time_ms: 18663.036
    sample_throughput: 23745.142
    sample_time_ms: 6813.688
    update_time_ms: 41.406
  timestamp: 1602801137
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     86 |           2217.2 | 13914112 |  258.494 |              299.576 |              136.242 |            803.501 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3208.9796963073736
    time_step_min: 2938
  date: 2020-10-15_22-32-43
  done: false
  episode_len_mean: 803.4781884973023
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.67730281810174
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 197
  episodes_total: 17422
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.2891300842165947
        entropy_coeff: 0.0005000000000000001
        kl: 0.005348453375821312
        model: {}
        policy_loss: -0.009791116928681731
        total_loss: 4.792139728864034
        vf_explained_var: 0.9905940890312195
        vf_loss: 4.8020734786987305
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.1
    gpu_util_percent0: 0.37034482758620685
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480909301504251
    mean_env_wait_ms: 1.2000243590748214
    mean_inference_ms: 4.3813956719264056
    mean_raw_obs_processing_ms: 0.3831443688775192
  time_since_restore: 2242.6392316818237
  time_this_iter_s: 25.439138412475586
  time_total_s: 2242.6392316818237
  timers:
    learn_throughput: 8673.805
    learn_time_ms: 18652.944
    sample_throughput: 23787.124
    sample_time_ms: 6801.663
    update_time_ms: 40.652
  timestamp: 1602801163
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     87 |          2242.64 | 14075904 |  258.677 |              299.576 |              136.242 |            803.478 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3207.562911277513
    time_step_min: 2938
  date: 2020-10-15_22-33-09
  done: false
  episode_len_mean: 803.4110620471015
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.8833974939613
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 242
  episodes_total: 17664
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.27116962025562924
        entropy_coeff: 0.0005000000000000001
        kl: 0.004562675875301163
        model: {}
        policy_loss: -0.00924643718462903
        total_loss: 4.395590861638387
        vf_explained_var: 0.9922053217887878
        vf_loss: 4.40497100353241
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.12
    gpu_util_percent0: 0.3426666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14807012923721513
    mean_env_wait_ms: 1.1999911300826422
    mean_inference_ms: 4.38014236765393
    mean_raw_obs_processing_ms: 0.3830754887568488
  time_since_restore: 2268.195256471634
  time_this_iter_s: 25.55602478981018
  time_total_s: 2268.195256471634
  timers:
    learn_throughput: 8660.83
    learn_time_ms: 18680.888
    sample_throughput: 23796.265
    sample_time_ms: 6799.05
    update_time_ms: 33.062
  timestamp: 1602801189
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     88 |           2268.2 | 14237696 |  258.883 |              299.576 |              136.242 |            803.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3206.521495117297
    time_step_min: 2938
  date: 2020-10-15_22-33-35
  done: false
  episode_len_mean: 803.3562787050521
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 259.0520229742252
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 190
  episodes_total: 17854
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.2679477706551552
        entropy_coeff: 0.0005000000000000001
        kl: 0.005150679304885368
        model: {}
        policy_loss: -0.011429781695672622
        total_loss: 3.701389968395233
        vf_explained_var: 0.992232084274292
        vf_loss: 3.7129526138305664
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.800000000000004
    gpu_util_percent0: 0.241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14805488490582672
    mean_env_wait_ms: 1.1999634379506976
    mean_inference_ms: 4.379142661692369
    mean_raw_obs_processing_ms: 0.3830204345462016
  time_since_restore: 2294.20529961586
  time_this_iter_s: 26.010043144226074
  time_total_s: 2294.20529961586
  timers:
    learn_throughput: 8643.808
    learn_time_ms: 18717.677
    sample_throughput: 23850.737
    sample_time_ms: 6783.522
    update_time_ms: 31.539
  timestamp: 1602801215
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     89 |          2294.21 | 14399488 |  259.052 |              299.576 |              136.242 |            803.356 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3205.331999110518
    time_step_min: 2938
  date: 2020-10-15_22-34-01
  done: false
  episode_len_mean: 803.2800710164225
  episode_reward_max: 301.0909090909089
  episode_reward_mean: 259.23077983563996
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 170
  episodes_total: 18024
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.27451568841934204
        entropy_coeff: 0.0005000000000000001
        kl: 0.005116381294404467
        model: {}
        policy_loss: -0.009435072677054753
        total_loss: 3.6329266826311746
        vf_explained_var: 0.9919072985649109
        vf_loss: 3.642497976620992
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.34666666666667
    gpu_util_percent0: 0.3306666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14804111017047153
    mean_env_wait_ms: 1.1999373487360672
    mean_inference_ms: 4.378232591766728
    mean_raw_obs_processing_ms: 0.38297001498875866
  time_since_restore: 2319.979466199875
  time_this_iter_s: 25.774166584014893
  time_total_s: 2319.979466199875
  timers:
    learn_throughput: 8632.773
    learn_time_ms: 18741.603
    sample_throughput: 23853.175
    sample_time_ms: 6782.829
    update_time_ms: 30.446
  timestamp: 1602801241
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     90 |          2319.98 | 14561280 |  259.231 |              301.091 |              136.242 |             803.28 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3203.874499259178
    time_step_min: 2938
  date: 2020-10-15_22-34-27
  done: false
  episode_len_mean: 803.1628785804261
  episode_reward_max: 301.0909090909089
  episode_reward_mean: 259.44230740506543
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 235
  episodes_total: 18259
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.2648405184348424
        entropy_coeff: 0.0005000000000000001
        kl: 0.00434200547169894
        model: {}
        policy_loss: -0.009165082970866933
        total_loss: 4.123057206471761
        vf_explained_var: 0.9925705790519714
        vf_loss: 4.132353901863098
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.11
    gpu_util_percent0: 0.2853333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14802098458613172
    mean_env_wait_ms: 1.1999061325383826
    mean_inference_ms: 4.377032052470133
    mean_raw_obs_processing_ms: 0.38290226791778653
  time_since_restore: 2345.6950154304504
  time_this_iter_s: 25.71554923057556
  time_total_s: 2345.6950154304504
  timers:
    learn_throughput: 8628.22
    learn_time_ms: 18751.493
    sample_throughput: 23863.371
    sample_time_ms: 6779.931
    update_time_ms: 30.911
  timestamp: 1602801267
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     91 |           2345.7 | 14723072 |  259.442 |              301.091 |              136.242 |            803.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3202.5689963671853
    time_step_min: 2938
  date: 2020-10-15_22-34-53
  done: false
  episode_len_mean: 803.095297364576
  episode_reward_max: 301.0909090909089
  episode_reward_mean: 259.6464684728118
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 18479
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.24779311940073967
        entropy_coeff: 0.0005000000000000001
        kl: 0.004587585067686935
        model: {}
        policy_loss: -0.00810739710383738
        total_loss: 3.9748273293177285
        vf_explained_var: 0.9924125075340271
        vf_loss: 3.983058234055837
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.027586206896554
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480048888747225
    mean_env_wait_ms: 1.1998733764578315
    mean_inference_ms: 4.375958147543475
    mean_raw_obs_processing_ms: 0.382844489330088
  time_since_restore: 2371.1051745414734
  time_this_iter_s: 25.41015911102295
  time_total_s: 2371.1051745414734
  timers:
    learn_throughput: 8636.807
    learn_time_ms: 18732.85
    sample_throughput: 23895.003
    sample_time_ms: 6770.956
    update_time_ms: 28.952
  timestamp: 1602801293
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     92 |          2371.11 | 14884864 |  259.646 |              301.091 |              136.242 |            803.095 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3201.5148030734513
    time_step_min: 2938
  date: 2020-10-15_22-35-19
  done: false
  episode_len_mean: 803.0435458786936
  episode_reward_max: 301.0909090909089
  episode_reward_mean: 259.7961764911408
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 168
  episodes_total: 18647
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2504790226618449
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047872335417196155
        model: {}
        policy_loss: -0.00997145293149515
        total_loss: 3.077739735444387
        vf_explained_var: 0.9931519031524658
        vf_loss: 3.0878362456957498
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.060000000000006
    gpu_util_percent0: 0.3656666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14799168011930425
    mean_env_wait_ms: 1.1998450149781397
    mean_inference_ms: 4.375122917552949
    mean_raw_obs_processing_ms: 0.38279670561091605
  time_since_restore: 2396.7868106365204
  time_this_iter_s: 25.681636095046997
  time_total_s: 2396.7868106365204
  timers:
    learn_throughput: 8632.235
    learn_time_ms: 18742.77
    sample_throughput: 23867.353
    sample_time_ms: 6778.8
    update_time_ms: 30.569
  timestamp: 1602801319
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     93 |          2396.79 | 15046656 |  259.796 |              301.091 |              136.242 |            803.044 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3200.1766081871347
    time_step_min: 2918
  date: 2020-10-15_22-35-45
  done: false
  episode_len_mean: 802.9568608723337
  episode_reward_max: 302.6060606060604
  episode_reward_mean: 259.99529037590156
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 18846
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.25324588765700656
        entropy_coeff: 0.0005000000000000001
        kl: 0.004991384611154596
        model: {}
        policy_loss: -0.01023354880453553
        total_loss: 3.584429462750753
        vf_explained_var: 0.9929077625274658
        vf_loss: 3.5947894851366677
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95
    gpu_util_percent0: 0.30533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479762372261276
    mean_env_wait_ms: 1.1998171480761126
    mean_inference_ms: 4.37412719297063
    mean_raw_obs_processing_ms: 0.3827404246058223
  time_since_restore: 2422.44163441658
  time_this_iter_s: 25.654823780059814
  time_total_s: 2422.44163441658
  timers:
    learn_throughput: 8630.749
    learn_time_ms: 18745.998
    sample_throughput: 23887.386
    sample_time_ms: 6773.114
    update_time_ms: 30.989
  timestamp: 1602801345
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     94 |          2422.44 | 15208448 |  259.995 |              302.606 |              136.242 |            802.957 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3198.641012179756
    time_step_min: 2918
  date: 2020-10-15_22-36-11
  done: false
  episode_len_mean: 802.8586774261162
  episode_reward_max: 302.6060606060604
  episode_reward_mean: 260.22600877777984
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 238
  episodes_total: 19084
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2388700358569622
        entropy_coeff: 0.0005000000000000001
        kl: 0.005095000029541552
        model: {}
        policy_loss: -0.008878010413657952
        total_loss: 3.757145663102468
        vf_explained_var: 0.9932150840759277
        vf_loss: 3.766142984231313
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.933333333333334
    gpu_util_percent0: 0.3653333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479578222667531
    mean_env_wait_ms: 1.199776876620589
    mean_inference_ms: 4.373026551747013
    mean_raw_obs_processing_ms: 0.3826791288099577
  time_since_restore: 2448.112291097641
  time_this_iter_s: 25.67065668106079
  time_total_s: 2448.112291097641
  timers:
    learn_throughput: 8640.199
    learn_time_ms: 18725.496
    sample_throughput: 23912.571
    sample_time_ms: 6765.981
    update_time_ms: 31.499
  timestamp: 1602801371
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     95 |          2448.11 | 15370240 |  260.226 |              302.606 |              136.242 |            802.859 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3197.4089704277326
    time_step_min: 2918
  date: 2020-10-15_22-36-38
  done: false
  episode_len_mean: 802.7863775483737
  episode_reward_max: 302.6060606060604
  episode_reward_mean: 260.4168918525924
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 193
  episodes_total: 19277
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2320684939622879
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047873918665573
        model: {}
        policy_loss: -0.00876318987381334
        total_loss: 3.3747625946998596
        vf_explained_var: 0.9928036332130432
        vf_loss: 3.3836416999499
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.913333333333334
    gpu_util_percent0: 0.36566666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14794487946401408
    mean_env_wait_ms: 1.1997455298900463
    mean_inference_ms: 4.372134042576178
    mean_raw_obs_processing_ms: 0.382629290589774
  time_since_restore: 2473.974905729294
  time_this_iter_s: 25.862614631652832
  time_total_s: 2473.974905729294
  timers:
    learn_throughput: 8640.381
    learn_time_ms: 18725.1
    sample_throughput: 23866.731
    sample_time_ms: 6778.976
    update_time_ms: 38.143
  timestamp: 1602801398
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     96 |          2473.97 | 15532032 |  260.417 |              302.606 |              136.242 |            802.786 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3196.3131729431766
    time_step_min: 2918
  date: 2020-10-15_22-37-04
  done: false
  episode_len_mean: 802.7279271867126
  episode_reward_max: 302.6060606060604
  episode_reward_mean: 260.57517946991896
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 170
  episodes_total: 19447
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.24688522890210152
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048849678520734114
        model: {}
        policy_loss: -0.008778546199512979
        total_loss: 4.057907541592916
        vf_explained_var: 0.9914454817771912
        vf_loss: 4.066809415817261
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.153333333333332
    gpu_util_percent0: 0.3473333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14793293122156823
    mean_env_wait_ms: 1.1997161612286602
    mean_inference_ms: 4.371338496209768
    mean_raw_obs_processing_ms: 0.3825848703520259
  time_since_restore: 2499.838309764862
  time_this_iter_s: 25.863404035568237
  time_total_s: 2499.838309764862
  timers:
    learn_throughput: 8621.876
    learn_time_ms: 18765.29
    sample_throughput: 23868.403
    sample_time_ms: 6778.501
    update_time_ms: 39.837
  timestamp: 1602801424
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     97 |          2499.84 | 15693824 |  260.575 |              302.606 |              136.242 |            802.728 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3194.8206903573973
    time_step_min: 2918
  date: 2020-10-15_22-37-30
  done: false
  episode_len_mean: 802.6548429718467
  episode_reward_max: 303.060606060606
  episode_reward_mean: 260.79570940628963
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 231
  episodes_total: 19678
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2386202352742354
        entropy_coeff: 0.0005000000000000001
        kl: 0.004923707262302439
        model: {}
        policy_loss: -0.00832096382509917
        total_loss: 3.9133440057436624
        vf_explained_var: 0.992855966091156
        vf_loss: 3.921784241994222
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479161826158186
    mean_env_wait_ms: 1.1996869825584782
    mean_inference_ms: 4.370326209270356
    mean_raw_obs_processing_ms: 0.3825261899007314
  time_since_restore: 2525.76931476593
  time_this_iter_s: 25.931005001068115
  time_total_s: 2525.76931476593
  timers:
    learn_throughput: 8600.91
    learn_time_ms: 18811.033
    sample_throughput: 23905.71
    sample_time_ms: 6767.923
    update_time_ms: 41.322
  timestamp: 1602801450
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     98 |          2525.77 | 15855616 |  260.796 |              303.061 |              136.242 |            802.655 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3193.3644648071695
    time_step_min: 2918
  date: 2020-10-15_22-37-56
  done: false
  episode_len_mean: 802.6171474520053
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.01054468699454
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 19898
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.22250954930981
        entropy_coeff: 0.0005000000000000001
        kl: 0.004387328478818138
        model: {}
        policy_loss: -0.009187571772296602
        total_loss: 3.756307045618693
        vf_explained_var: 0.9926998615264893
        vf_loss: 3.7656057874361673
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.693333333333335
    gpu_util_percent0: 0.31799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14790127876463724
    mean_env_wait_ms: 1.1996452821457522
    mean_inference_ms: 4.369351674059287
    mean_raw_obs_processing_ms: 0.3824722128251545
  time_since_restore: 2551.542248725891
  time_this_iter_s: 25.772933959960938
  time_total_s: 2551.542248725891
  timers:
    learn_throughput: 8618.703
    learn_time_ms: 18772.197
    sample_throughput: 23854.392
    sample_time_ms: 6782.483
    update_time_ms: 40.797
  timestamp: 1602801476
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |     99 |          2551.54 | 16017408 |  261.011 |              303.061 |              136.242 |            802.617 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3192.304347826087
    time_step_min: 2918
  date: 2020-10-15_22-38-22
  done: false
  episode_len_mean: 802.5968907269919
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.1705660924356
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 171
  episodes_total: 20069
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.22806943828860918
        entropy_coeff: 0.0005000000000000001
        kl: 0.004183990609211226
        model: {}
        policy_loss: -0.011238508105937703
        total_loss: 2.9428491592407227
        vf_explained_var: 0.993543803691864
        vf_loss: 2.9542016983032227
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.743333333333336
    gpu_util_percent0: 0.3793333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788954876151827
    mean_env_wait_ms: 1.1996126801896587
    mean_inference_ms: 4.368600843879751
    mean_raw_obs_processing_ms: 0.38242990657702336
  time_since_restore: 2577.229606628418
  time_this_iter_s: 25.687357902526855
  time_total_s: 2577.229606628418
  timers:
    learn_throughput: 8618.027
    learn_time_ms: 18773.671
    sample_throughput: 23894.774
    sample_time_ms: 6771.02
    update_time_ms: 41.258
  timestamp: 1602801502
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    100 |          2577.23 | 16179200 |  261.171 |              303.061 |              136.242 |            802.597 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3190.9797350731515
    time_step_min: 2918
  date: 2020-10-15_22-38-49
  done: false
  episode_len_mean: 802.558219853957
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.3628858149284
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 20268
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2327657255033652
        entropy_coeff: 0.0005000000000000001
        kl: 0.004338842933066189
        model: {}
        policy_loss: -0.008990125914957995
        total_loss: 3.1982950965563455
        vf_explained_var: 0.9936439990997314
        vf_loss: 3.2074015935262046
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94
    gpu_util_percent0: 0.3113333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787636139939117
    mean_env_wait_ms: 1.1995769077204521
    mean_inference_ms: 4.367723004288025
    mean_raw_obs_processing_ms: 0.38237927097574637
  time_since_restore: 2603.115787744522
  time_this_iter_s: 25.886181116104126
  time_total_s: 2603.115787744522
  timers:
    learn_throughput: 8616.895
    learn_time_ms: 18776.136
    sample_throughput: 23839.773
    sample_time_ms: 6786.642
    update_time_ms: 40.56
  timestamp: 1602801529
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    101 |          2603.12 | 16340992 |  261.363 |              303.061 |              136.242 |            802.558 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3189.4686309000294
    time_step_min: 2918
  date: 2020-10-15_22-39-15
  done: false
  episode_len_mean: 802.5222905082431
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.59390510312363
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 234
  episodes_total: 20502
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2161901444196701
        entropy_coeff: 0.0005000000000000001
        kl: 0.004484265538242956
        model: {}
        policy_loss: -0.009577901513694087
        total_loss: 3.2329789996147156
        vf_explained_var: 0.9940950870513916
        vf_loss: 3.2426650722821555
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.344827586206897
    gpu_util_percent0: 0.3341379310344827
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147860440092526
    mean_env_wait_ms: 1.1995325817681888
    mean_inference_ms: 4.3667644310026725
    mean_raw_obs_processing_ms: 0.3823243678819519
  time_since_restore: 2628.581580877304
  time_this_iter_s: 25.465793132781982
  time_total_s: 2628.581580877304
  timers:
    learn_throughput: 8616.507
    learn_time_ms: 18776.982
    sample_throughput: 23826.863
    sample_time_ms: 6790.319
    update_time_ms: 40.336
  timestamp: 1602801555
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    102 |          2628.58 | 16502784 |  261.594 |              303.061 |              136.242 |            802.522 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3188.174645432983
    time_step_min: 2918
  date: 2020-10-15_22-39-40
  done: false
  episode_len_mean: 802.4784247402754
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.7829881321062
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 193
  episodes_total: 20695
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.2085278940697511
        entropy_coeff: 0.0005000000000000001
        kl: 0.004920714107962946
        model: {}
        policy_loss: -0.009173751110211015
        total_loss: 2.8402439753214517
        vf_explained_var: 0.9939279556274414
        vf_loss: 2.8495219945907593
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.943333333333335
    gpu_util_percent0: 0.33833333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14784891325616534
    mean_env_wait_ms: 1.1994970600842618
    mean_inference_ms: 4.365978768400035
    mean_raw_obs_processing_ms: 0.3822799082516733
  time_since_restore: 2654.02717256546
  time_this_iter_s: 25.445591688156128
  time_total_s: 2654.02717256546
  timers:
    learn_throughput: 8623.268
    learn_time_ms: 18762.261
    sample_throughput: 23851.767
    sample_time_ms: 6783.229
    update_time_ms: 38.988
  timestamp: 1602801580
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    103 |          2654.03 | 16664576 |  261.783 |              303.061 |              136.242 |            802.478 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3187.109105745692
    time_step_min: 2918
  date: 2020-10-15_22-40-06
  done: false
  episode_len_mean: 802.4490871627773
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.95087682614627
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 174
  episodes_total: 20869
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.21678480009237924
        entropy_coeff: 0.0005000000000000001
        kl: 0.004601365653797984
        model: {}
        policy_loss: -0.008773865294642746
        total_loss: 2.8860333363215127
        vf_explained_var: 0.9937410950660706
        vf_loss: 2.894915541013082
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.730000000000004
    gpu_util_percent0: 0.3626666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14783736611940443
    mean_env_wait_ms: 1.1994619049825979
    mean_inference_ms: 4.3652640106000495
    mean_raw_obs_processing_ms: 0.38223917951709724
  time_since_restore: 2679.6110606193542
  time_this_iter_s: 25.583888053894043
  time_total_s: 2679.6110606193542
  timers:
    learn_throughput: 8629.14
    learn_time_ms: 18749.494
    sample_throughput: 23810.636
    sample_time_ms: 6794.947
    update_time_ms: 39.474
  timestamp: 1602801606
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    104 |          2679.61 | 16826368 |  261.951 |              303.061 |              136.242 |            802.449 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3185.767542401064
    time_step_min: 2918
  date: 2020-10-15_22-40-33
  done: false
  episode_len_mean: 802.4271282902538
  episode_reward_max: 303.96969696969666
  episode_reward_mean: 262.1531607275026
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 216
  episodes_total: 21085
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.21538700411717096
        entropy_coeff: 0.0005000000000000001
        kl: 0.00448287259011219
        model: {}
        policy_loss: -0.00921534831286408
        total_loss: 3.572128971417745
        vf_explained_var: 0.993457555770874
        vf_loss: 3.5814519921938577
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.943333333333335
    gpu_util_percent0: 0.35333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782395644165744
    mean_env_wait_ms: 1.1994199547906998
    mean_inference_ms: 4.364391367696856
    mean_raw_obs_processing_ms: 0.3821876301961597
  time_since_restore: 2705.396441936493
  time_this_iter_s: 25.785381317138672
  time_total_s: 2705.396441936493
  timers:
    learn_throughput: 8628.07
    learn_time_ms: 18751.818
    sample_throughput: 23808.341
    sample_time_ms: 6795.601
    update_time_ms: 46.209
  timestamp: 1602801633
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    105 |           2705.4 | 16988160 |  262.153 |               303.97 |              136.242 |            802.427 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3184.1904314315257
    time_step_min: 2918
  date: 2020-10-15_22-40-59
  done: false
  episode_len_mean: 802.385145913484
  episode_reward_max: 303.96969696969666
  episode_reward_mean: 262.3873041193581
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 229
  episodes_total: 21314
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.20282008747259775
        entropy_coeff: 0.0005000000000000001
        kl: 0.005946500886542101
        model: {}
        policy_loss: -0.008542639310083663
        total_loss: 2.7954265077908835
        vf_explained_var: 0.9945156574249268
        vf_loss: 2.804070512453715
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.756666666666664
    gpu_util_percent0: 0.3606666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478100416529979
    mean_env_wait_ms: 1.1993707385088959
    mean_inference_ms: 4.363517830888617
    mean_raw_obs_processing_ms: 0.38213702742935696
  time_since_restore: 2730.9963929653168
  time_this_iter_s: 25.599951028823853
  time_total_s: 2730.9963929653168
  timers:
    learn_throughput: 8635.838
    learn_time_ms: 18734.95
    sample_throughput: 23818.533
    sample_time_ms: 6792.694
    update_time_ms: 38.748
  timestamp: 1602801659
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    106 |             2731 | 17149952 |  262.387 |               303.97 |              136.242 |            802.385 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3183.0718314454853
    time_step_min: 2918
  date: 2020-10-15_22-41-25
  done: false
  episode_len_mean: 802.3614407371214
  episode_reward_max: 303.96969696969666
  episode_reward_mean: 262.55677534806387
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 175
  episodes_total: 21489
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.20489118744929632
        entropy_coeff: 0.0005000000000000001
        kl: 0.00440191892751803
        model: {}
        policy_loss: -0.010943801268391931
        total_loss: 2.446466783682505
        vf_explained_var: 0.9945385456085205
        vf_loss: 2.4575130343437195
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.160000000000007
    gpu_util_percent0: 0.32966666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477997203404794
    mean_env_wait_ms: 1.1993337884006319
    mean_inference_ms: 4.362842156714362
    mean_raw_obs_processing_ms: 0.38209788295043273
  time_since_restore: 2757.0455429553986
  time_this_iter_s: 26.049149990081787
  time_total_s: 2757.0455429553986
  timers:
    learn_throughput: 8641.468
    learn_time_ms: 18722.744
    sample_throughput: 23752.312
    sample_time_ms: 6811.632
    update_time_ms: 38.878
  timestamp: 1602801685
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    107 |          2757.05 | 17311744 |  262.557 |               303.97 |              136.242 |            802.361 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3181.844966970019
    time_step_min: 2918
  date: 2020-10-15_22-41-51
  done: false
  episode_len_mean: 802.3012498270534
  episode_reward_max: 304.27272727272697
  episode_reward_mean: 262.74305523528403
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 194
  episodes_total: 21683
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.21210420752565065
        entropy_coeff: 0.0005000000000000001
        kl: 0.004266667800645034
        model: {}
        policy_loss: -0.009544812882571327
        total_loss: 3.534186601638794
        vf_explained_var: 0.9929177761077881
        vf_loss: 3.5438374876976013
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.35
    gpu_util_percent0: 0.314
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477880778821139
    mean_env_wait_ms: 1.1992948135290178
    mean_inference_ms: 4.362086843955307
    mean_raw_obs_processing_ms: 0.3820537695428742
  time_since_restore: 2782.7988369464874
  time_this_iter_s: 25.753293991088867
  time_total_s: 2782.7988369464874
  timers:
    learn_throughput: 8659.365
    learn_time_ms: 18684.049
    sample_throughput: 23680.729
    sample_time_ms: 6832.222
    update_time_ms: 38.125
  timestamp: 1602801711
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    108 |           2782.8 | 17473536 |  262.743 |              304.273 |              136.242 |            802.301 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3180.3410409907233
    time_step_min: 2918
  date: 2020-10-15_22-42-18
  done: false
  episode_len_mean: 802.232492358228
  episode_reward_max: 304.27272727272697
  episode_reward_mean: 262.97783575063556
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 236
  episodes_total: 21919
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.19921182716886202
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045283739843095345
        model: {}
        policy_loss: -0.007905332871208278
        total_loss: 2.8254926999409995
        vf_explained_var: 0.9947331547737122
        vf_loss: 2.833497623602549
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.6
    gpu_util_percent0: 0.33166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14777408063244643
    mean_env_wait_ms: 1.1992432229518672
    mean_inference_ms: 4.361235855470853
    mean_raw_obs_processing_ms: 0.3820030528911552
  time_since_restore: 2808.5911564826965
  time_this_iter_s: 25.792319536209106
  time_total_s: 2808.5911564826965
  timers:
    learn_throughput: 8657.089
    learn_time_ms: 18688.961
    sample_throughput: 23730.64
    sample_time_ms: 6817.852
    update_time_ms: 40.069
  timestamp: 1602801738
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    109 |          2808.59 | 17635328 |  262.978 |              304.273 |              136.242 |            802.232 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3178.937867946744
    time_step_min: 2918
  date: 2020-10-15_22-42-44
  done: false
  episode_len_mean: 802.1858215028484
  episode_reward_max: 304.27272727272697
  episode_reward_mean: 263.19224161316566
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 22118
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.1900289791325728
        entropy_coeff: 0.0005000000000000001
        kl: 0.005051623952264587
        model: {}
        policy_loss: -0.009107721309798459
        total_loss: 2.154778629541397
        vf_explained_var: 0.9953210353851318
        vf_loss: 2.1639813085397086
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.020689655172415
    gpu_util_percent0: 0.3531034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14776324230319687
    mean_env_wait_ms: 1.1991984869708574
    mean_inference_ms: 4.360510855115161
    mean_raw_obs_processing_ms: 0.38196175739072
  time_since_restore: 2834.0922060012817
  time_this_iter_s: 25.501049518585205
  time_total_s: 2834.0922060012817
  timers:
    learn_throughput: 8667.998
    learn_time_ms: 18665.441
    sample_throughput: 23720.364
    sample_time_ms: 6820.806
    update_time_ms: 40.953
  timestamp: 1602801764
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    110 |          2834.09 | 17797120 |  263.192 |              304.273 |              136.242 |            802.186 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3177.7709316435216
    time_step_min: 2918
  date: 2020-10-15_22-43-10
  done: false
  episode_len_mean: 802.1407547000493
  episode_reward_max: 304.27272727272697
  episode_reward_mean: 263.3683272352002
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 169
  episodes_total: 22287
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.19913217052817345
        entropy_coeff: 0.0005000000000000001
        kl: 0.004601351063077648
        model: {}
        policy_loss: -0.008912130123159537
        total_loss: 2.6408274372418723
        vf_explained_var: 0.9941586852073669
        vf_loss: 2.6498391032218933
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.750000000000004
    gpu_util_percent0: 0.33433333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14775333814667332
    mean_env_wait_ms: 1.1991557183214694
    mean_inference_ms: 4.359885502347997
    mean_raw_obs_processing_ms: 0.38192482413369694
  time_since_restore: 2859.795315504074
  time_this_iter_s: 25.70310950279236
  time_total_s: 2859.795315504074
  timers:
    learn_throughput: 8671.273
    learn_time_ms: 18658.39
    sample_throughput: 23765.206
    sample_time_ms: 6807.936
    update_time_ms: 41.066
  timestamp: 1602801790
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    111 |           2859.8 | 17958912 |  263.368 |              304.273 |              136.242 |            802.141 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3176.2522249911
    time_step_min: 2918
  date: 2020-10-15_22-43-36
  done: false
  episode_len_mean: 802.0701528345477
  episode_reward_max: 304.57575757575717
  episode_reward_mean: 263.5940594859201
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 221
  episodes_total: 22508
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.19686042765776315
        entropy_coeff: 0.0005000000000000001
        kl: 0.00463165482506156
        model: {}
        policy_loss: -0.007648821745533496
        total_loss: 2.591915726661682
        vf_explained_var: 0.9950248599052429
        vf_loss: 2.599662959575653
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.063333333333336
    gpu_util_percent0: 0.3436666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14774095489465586
    mean_env_wait_ms: 1.199107669696955
    mean_inference_ms: 4.359091711035756
    mean_raw_obs_processing_ms: 0.38187612542435617
  time_since_restore: 2885.4802606105804
  time_this_iter_s: 25.684945106506348
  time_total_s: 2885.4802606105804
  timers:
    learn_throughput: 8667.054
    learn_time_ms: 18667.473
    sample_throughput: 23732.936
    sample_time_ms: 6817.193
    update_time_ms: 43.197
  timestamp: 1602801816
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    112 |          2885.48 | 18120704 |  263.594 |              304.576 |              136.242 |             802.07 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3174.638121668649
    time_step_min: 2896
  date: 2020-10-15_22-44-02
  done: false
  episode_len_mean: 802.019395698641
  episode_reward_max: 305.9393939393938
  episode_reward_mean: 263.8269713895785
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 229
  episodes_total: 22737
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.17844737445314726
        entropy_coeff: 0.0005000000000000001
        kl: 0.004137515226223816
        model: {}
        policy_loss: -0.008909039684416106
        total_loss: 2.859405517578125
        vf_explained_var: 0.994447648525238
        vf_loss: 2.8684038519859314
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45
    gpu_util_percent0: 0.3393333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14772816873267583
    mean_env_wait_ms: 1.1990526052878696
    mean_inference_ms: 4.358319515178652
    mean_raw_obs_processing_ms: 0.38183118810474526
  time_since_restore: 2911.3174917697906
  time_this_iter_s: 25.837231159210205
  time_total_s: 2911.3174917697906
  timers:
    learn_throughput: 8655.286
    learn_time_ms: 18692.854
    sample_throughput: 23695.4
    sample_time_ms: 6827.992
    update_time_ms: 44.838
  timestamp: 1602801842
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    113 |          2911.32 | 18282496 |  263.827 |              305.939 |              136.242 |            802.019 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3173.51374863388
    time_step_min: 2896
  date: 2020-10-15_22-44-28
  done: false
  episode_len_mean: 801.98694950024
  episode_reward_max: 305.9393939393938
  episode_reward_mean: 264.004793251356
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 174
  episodes_total: 22911
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.17948132505019507
        entropy_coeff: 0.0005000000000000001
        kl: 0.004640274525930484
        model: {}
        policy_loss: -0.008332632889505476
        total_loss: 2.306395868460337
        vf_explained_var: 0.994772732257843
        vf_loss: 2.314818243185679
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.916666666666664
    gpu_util_percent0: 0.31133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771905429110482
    mean_env_wait_ms: 1.1990089373005266
    mean_inference_ms: 4.357713006371397
    mean_raw_obs_processing_ms: 0.38179548628696525
  time_since_restore: 2937.1659500598907
  time_this_iter_s: 25.848458290100098
  time_total_s: 2937.1659500598907
  timers:
    learn_throughput: 8647.514
    learn_time_ms: 18709.654
    sample_throughput: 23665.219
    sample_time_ms: 6836.7
    update_time_ms: 44.601
  timestamp: 1602801868
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    114 |          2937.17 | 18444288 |  264.005 |              305.939 |              136.242 |            801.987 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3172.1408084571726
    time_step_min: 2896
  date: 2020-10-15_22-44-55
  done: false
  episode_len_mean: 801.9254228489856
  episode_reward_max: 305.9393939393938
  episode_reward_mean: 264.21694384691307
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 23117
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.1882436399658521
        entropy_coeff: 0.0005000000000000001
        kl: 0.004217337370694925
        model: {}
        policy_loss: -0.00838335547450697
        total_loss: 2.469933728377024
        vf_explained_var: 0.9949347972869873
        vf_loss: 2.4784111976623535
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89666666666667
    gpu_util_percent0: 0.31833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477083580443816
    mean_env_wait_ms: 1.198956967096539
    mean_inference_ms: 4.356993155538509
    mean_raw_obs_processing_ms: 0.381752569469621
  time_since_restore: 2963.052619457245
  time_this_iter_s: 25.886669397354126
  time_total_s: 2963.052619457245
  timers:
    learn_throughput: 8644.073
    learn_time_ms: 18717.101
    sample_throughput: 23632.94
    sample_time_ms: 6846.038
    update_time_ms: 37.89
  timestamp: 1602801895
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    115 |          2963.05 | 18606080 |  264.217 |              305.939 |              136.242 |            801.925 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3170.632310662948
    time_step_min: 2896
  date: 2020-10-15_22-45-21
  done: false
  episode_len_mean: 801.8785399083158
  episode_reward_max: 305.9393939393938
  episode_reward_mean: 264.44285319239253
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 224
  episodes_total: 23341
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.1726313754916191
        entropy_coeff: 0.0005000000000000001
        kl: 0.003985262029649069
        model: {}
        policy_loss: -0.007977147310157306
        total_loss: 2.51504514614741
        vf_explained_var: 0.9953476786613464
        vf_loss: 2.5231085618336997
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.006666666666668
    gpu_util_percent0: 0.3123333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769613146983818
    mean_env_wait_ms: 1.1989028995778523
    mean_inference_ms: 4.356274155123206
    mean_raw_obs_processing_ms: 0.38170856984372853
  time_since_restore: 2988.733035326004
  time_this_iter_s: 25.680415868759155
  time_total_s: 2988.733035326004
  timers:
    learn_throughput: 8643.364
    learn_time_ms: 18718.638
    sample_throughput: 23610.312
    sample_time_ms: 6852.599
    update_time_ms: 37.121
  timestamp: 1602801921
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    116 |          2988.73 | 18767872 |  264.443 |              305.939 |              136.242 |            801.879 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3169.4152091578367
    time_step_min: 2896
  date: 2020-10-15_22-45-47
  done: false
  episode_len_mean: 801.8584661142978
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 264.629571688845
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 194
  episodes_total: 23535
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.16437324633200964
        entropy_coeff: 0.0005000000000000001
        kl: 0.003923273393108199
        model: {}
        policy_loss: -0.005855827068444341
        total_loss: 2.5756904681523642
        vf_explained_var: 0.9947084784507751
        vf_loss: 2.5816285212834678
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74666666666667
    gpu_util_percent0: 0.31099999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476864600535127
    mean_env_wait_ms: 1.1988536895359039
    mean_inference_ms: 4.3556375225166235
    mean_raw_obs_processing_ms: 0.38167115644913246
  time_since_restore: 3014.456431388855
  time_this_iter_s: 25.723396062850952
  time_total_s: 3014.456431388855
  timers:
    learn_throughput: 8647.635
    learn_time_ms: 18709.393
    sample_throughput: 23657.413
    sample_time_ms: 6838.956
    update_time_ms: 37.074
  timestamp: 1602801947
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    117 |          3014.46 | 18929664 |   264.63 |                  307 |              136.242 |            801.858 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3168.2917001055966
    time_step_min: 2896
  date: 2020-10-15_22-46-13
  done: false
  episode_len_mean: 801.8232465944077
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 264.8048610605229
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 176
  episodes_total: 23711
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.17207426329453787
        entropy_coeff: 0.0005000000000000001
        kl: 0.004950701802348097
        model: {}
        policy_loss: -0.006777488432514171
        total_loss: 2.1098340153694153
        vf_explained_var: 0.995398223400116
        vf_loss: 2.1166975696881614
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.51
    gpu_util_percent0: 0.32233333333333325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476772957160344
    mean_env_wait_ms: 1.1988038269270203
    mean_inference_ms: 4.355065569679013
    mean_raw_obs_processing_ms: 0.38163655053944445
  time_since_restore: 3040.142678976059
  time_this_iter_s: 25.68624758720398
  time_total_s: 3040.142678976059
  timers:
    learn_throughput: 8640.803
    learn_time_ms: 18724.186
    sample_throughput: 23745.744
    sample_time_ms: 6813.516
    update_time_ms: 39.88
  timestamp: 1602801973
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    118 |          3040.14 | 19091456 |  264.805 |                  307 |              136.242 |            801.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3166.895505523937
    time_step_min: 2896
  date: 2020-10-15_22-46-39
  done: false
  episode_len_mean: 801.7650426207588
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 265.02321223263885
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 221
  episodes_total: 23932
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.17282702773809433
        entropy_coeff: 0.0005000000000000001
        kl: 0.003968695527873933
        model: {}
        policy_loss: -0.011567316275128784
        total_loss: 2.639465550581614
        vf_explained_var: 0.9949531555175781
        vf_loss: 2.651119291782379
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.168965517241382
    gpu_util_percent0: 0.31793103448275856
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476656992558074
    mean_env_wait_ms: 1.1987490392577058
    mean_inference_ms: 4.354355619510152
    mean_raw_obs_processing_ms: 0.38159185283200103
  time_since_restore: 3065.6451110839844
  time_this_iter_s: 25.502432107925415
  time_total_s: 3065.6451110839844
  timers:
    learn_throughput: 8649.837
    learn_time_ms: 18704.63
    sample_throughput: 23745.577
    sample_time_ms: 6813.564
    update_time_ms: 37.904
  timestamp: 1602801999
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    119 |          3065.65 | 19253248 |  265.023 |                  307 |              136.242 |            801.765 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3165.3603648424546
    time_step_min: 2896
  date: 2020-10-15_22-47-05
  done: false
  episode_len_mean: 801.6945686371915
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 265.25369943849813
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 224
  episodes_total: 24156
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.15604355682929358
        entropy_coeff: 0.0005000000000000001
        kl: 0.003540336872295787
        model: {}
        policy_loss: -0.008982419360108906
        total_loss: 2.360484302043915
        vf_explained_var: 0.9954218864440918
        vf_loss: 2.369544724623362
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.216666666666672
    gpu_util_percent0: 0.37533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14765440306439653
    mean_env_wait_ms: 1.1986918140102005
    mean_inference_ms: 4.3536740018680335
    mean_raw_obs_processing_ms: 0.3815507135482686
  time_since_restore: 3091.291177034378
  time_this_iter_s: 25.646065950393677
  time_total_s: 3091.291177034378
  timers:
    learn_throughput: 8644.134
    learn_time_ms: 18716.97
    sample_throughput: 23741.782
    sample_time_ms: 6814.653
    update_time_ms: 38.046
  timestamp: 1602802025
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    120 |          3091.29 | 19415040 |  265.254 |                  307 |              136.242 |            801.695 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3164.1995308062724
    time_step_min: 2896
  date: 2020-10-15_22-47-31
  done: false
  episode_len_mean: 801.6312415238565
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 265.4239696932335
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 177
  episodes_total: 24333
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.15993553648392358
        entropy_coeff: 0.0005000000000000001
        kl: 0.003991852553250889
        model: {}
        policy_loss: -0.009676661349658389
        total_loss: 2.419548451900482
        vf_explained_var: 0.9946107864379883
        vf_loss: 2.429305096467336
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94
    gpu_util_percent0: 0.3313333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476460791107815
    mean_env_wait_ms: 1.1986412035329574
    mean_inference_ms: 4.353118906562531
    mean_raw_obs_processing_ms: 0.38151697713208166
  time_since_restore: 3116.8847551345825
  time_this_iter_s: 25.593578100204468
  time_total_s: 3116.8847551345825
  timers:
    learn_throughput: 8648.469
    learn_time_ms: 18707.588
    sample_throughput: 23743.62
    sample_time_ms: 6814.125
    update_time_ms: 36.613
  timestamp: 1602802051
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    121 |          3116.88 | 19576832 |  265.424 |                  307 |              136.242 |            801.631 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3162.7913316736726
    time_step_min: 2896
  date: 2020-10-15_22-47-57
  done: false
  episode_len_mean: 801.5469660540365
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 265.62996689252844
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 24539
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.17219198991854986
        entropy_coeff: 0.0005000000000000001
        kl: 0.0064288977688799305
        model: {}
        policy_loss: -0.010138675029641794
        total_loss: 2.83526748418808
        vf_explained_var: 0.9942933917045593
        vf_loss: 2.8454922835032144
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90333333333334
    gpu_util_percent0: 0.3566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476361427293749
    mean_env_wait_ms: 1.1985844689973424
    mean_inference_ms: 4.352479461912706
    mean_raw_obs_processing_ms: 0.381477568069032
  time_since_restore: 3142.4637818336487
  time_this_iter_s: 25.579026699066162
  time_total_s: 3142.4637818336487
  timers:
    learn_throughput: 8654.768
    learn_time_ms: 18693.974
    sample_throughput: 23736.166
    sample_time_ms: 6816.265
    update_time_ms: 36.066
  timestamp: 1602802077
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    122 |          3142.46 | 19738624 |   265.63 |                  307 |              136.242 |            801.547 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3161.3080934670115
    time_step_min: 2896
  date: 2020-10-15_22-48-24
  done: false
  episode_len_mean: 801.4363394154691
  episode_reward_max: 308.36363636363603
  episode_reward_mean: 265.8587799519496
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 233
  episodes_total: 24772
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.16056357572476068
        entropy_coeff: 0.0005000000000000001
        kl: 0.004299966036342084
        model: {}
        policy_loss: -0.008023323675539965
        total_loss: 2.824439525604248
        vf_explained_var: 0.9946678280830383
        vf_loss: 2.832543134689331
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58666666666667
    gpu_util_percent0: 0.32099999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762431308085297
    mean_env_wait_ms: 1.1985264531356603
    mean_inference_ms: 4.351801346783403
    mean_raw_obs_processing_ms: 0.3814344339423685
  time_since_restore: 3168.4609236717224
  time_this_iter_s: 25.99714183807373
  time_total_s: 3168.4609236717224
  timers:
    learn_throughput: 8646.498
    learn_time_ms: 18711.853
    sample_throughput: 23740.294
    sample_time_ms: 6815.08
    update_time_ms: 34.015
  timestamp: 1602802104
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    123 |          3168.46 | 19900416 |  265.859 |              308.364 |              136.242 |            801.436 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3160.1154663991974
    time_step_min: 2896
  date: 2020-10-15_22-48-50
  done: false
  episode_len_mean: 801.3312367292978
  episode_reward_max: 308.36363636363603
  episode_reward_mean: 266.0337593312232
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 24961
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.14547821134328842
        entropy_coeff: 0.0005000000000000001
        kl: 0.003972978583381821
        model: {}
        policy_loss: -0.009451755729969591
        total_loss: 2.3672469655672708
        vf_explained_var: 0.9947699904441833
        vf_loss: 2.3767714699109397
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.996666666666666
    gpu_util_percent0: 0.39033333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14761626146416082
    mean_env_wait_ms: 1.1984733250419968
    mean_inference_ms: 4.351238778786493
    mean_raw_obs_processing_ms: 0.38140085703729815
  time_since_restore: 3194.0400726795197
  time_this_iter_s: 25.57914900779724
  time_total_s: 3194.0400726795197
  timers:
    learn_throughput: 8654.44
    learn_time_ms: 18694.682
    sample_throughput: 23776.608
    sample_time_ms: 6804.671
    update_time_ms: 34.074
  timestamp: 1602802130
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    124 |          3194.04 | 20062208 |  266.034 |              308.364 |              136.242 |            801.331 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3158.922651053491
    time_step_min: 2880
  date: 2020-10-15_22-49-16
  done: false
  episode_len_mean: 801.2262259873523
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 266.2103953266104
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 25143
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.15984883656104407
        entropy_coeff: 0.0005000000000000001
        kl: 0.003537013428285718
        model: {}
        policy_loss: -0.009445018314484818
        total_loss: 2.951190233230591
        vf_explained_var: 0.9936904311180115
        vf_loss: 2.960715174674988
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53666666666667
    gpu_util_percent0: 0.3546666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476073690528548
    mean_env_wait_ms: 1.1984213010869154
    mean_inference_ms: 4.350691684494465
    mean_raw_obs_processing_ms: 0.38136604149385517
  time_since_restore: 3219.8721675872803
  time_this_iter_s: 25.83209490776062
  time_total_s: 3219.8721675872803
  timers:
    learn_throughput: 8659.65
    learn_time_ms: 18683.435
    sample_throughput: 23759.51
    sample_time_ms: 6809.568
    update_time_ms: 33.964
  timestamp: 1602802156
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    125 |          3219.87 | 20224000 |   266.21 |              308.364 |              136.242 |            801.226 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3157.376045777427
    time_step_min: 2880
  date: 2020-10-15_22-49-42
  done: false
  episode_len_mean: 801.109749369483
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 266.4451330773051
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 233
  episodes_total: 25376
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.1553135278324286
        entropy_coeff: 0.0005000000000000001
        kl: 0.004857060712917398
        model: {}
        policy_loss: -0.007093027389297883
        total_loss: 2.5144524574279785
        vf_explained_var: 0.9952011704444885
        vf_loss: 2.5216230948766074
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.21
    gpu_util_percent0: 0.32733333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475971782812326
    mean_env_wait_ms: 1.1983656198490555
    mean_inference_ms: 4.350061471095765
    mean_raw_obs_processing_ms: 0.3813261756893843
  time_since_restore: 3245.53857254982
  time_this_iter_s: 25.666404962539673
  time_total_s: 3245.53857254982
  timers:
    learn_throughput: 8661.685
    learn_time_ms: 18679.045
    sample_throughput: 23760.913
    sample_time_ms: 6809.166
    update_time_ms: 35.558
  timestamp: 1602802182
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    126 |          3245.54 | 20385792 |  266.445 |              308.364 |              136.242 |             801.11 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3155.990059097491
    time_step_min: 2880
  date: 2020-10-15_22-50-09
  done: false
  episode_len_mean: 801.043420486966
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 266.65076370457996
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 211
  episodes_total: 25587
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.13924898703893027
        entropy_coeff: 0.0005000000000000001
        kl: 0.003963715261003624
        model: {}
        policy_loss: -0.008442982827546075
        total_loss: 2.1421040892601013
        vf_explained_var: 0.9955690503120422
        vf_loss: 2.150616725285848
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.786666666666665
    gpu_util_percent0: 0.33566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14758750725335987
    mean_env_wait_ms: 1.1983069304695873
    mean_inference_ms: 4.3494615329955675
    mean_raw_obs_processing_ms: 0.38128925786132123
  time_since_restore: 3271.3389580249786
  time_this_iter_s: 25.80038547515869
  time_total_s: 3271.3389580249786
  timers:
    learn_throughput: 8664.214
    learn_time_ms: 18673.592
    sample_throughput: 23717.755
    sample_time_ms: 6821.556
    update_time_ms: 35.029
  timestamp: 1602802209
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    127 |          3271.34 | 20547584 |  266.651 |              308.364 |              136.242 |            801.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3154.8632741126617
    time_step_min: 2880
  date: 2020-10-15_22-50-35
  done: false
  episode_len_mean: 800.9954190768275
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 266.82218630263964
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 172
  episodes_total: 25759
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.14556952317555746
        entropy_coeff: 0.0005000000000000001
        kl: 0.004224637368073066
        model: {}
        policy_loss: -0.006321212160401046
        total_loss: 1.7349521120389302
        vf_explained_var: 0.9960880875587463
        vf_loss: 1.7413460910320282
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.919354838709676
    gpu_util_percent0: 0.32129032258064527
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475795317372962
    mean_env_wait_ms: 1.1982577265011105
    mean_inference_ms: 4.348978090497965
    mean_raw_obs_processing_ms: 0.38125935535719246
  time_since_restore: 3297.3618273735046
  time_this_iter_s: 26.022869348526
  time_total_s: 3297.3618273735046
  timers:
    learn_throughput: 8658.841
    learn_time_ms: 18685.179
    sample_throughput: 23635.879
    sample_time_ms: 6845.187
    update_time_ms: 33.178
  timestamp: 1602802235
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    128 |          3297.36 | 20709376 |  266.822 |              308.364 |              136.242 |            800.995 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3153.5584981303728
    time_step_min: 2880
  date: 2020-10-15_22-51-02
  done: false
  episode_len_mean: 800.9069561535205
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 267.0282592643141
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 218
  episodes_total: 25977
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.1587205578883489
        entropy_coeff: 0.0005000000000000001
        kl: 0.003996756800916046
        model: {}
        policy_loss: -0.00969479835475795
        total_loss: 2.647255778312683
        vf_explained_var: 0.9948437213897705
        vf_loss: 2.657030006249746
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.924137931034487
    gpu_util_percent0: 0.39
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756999824632605
    mean_env_wait_ms: 1.1981993409434482
    mean_inference_ms: 4.3483762636096746
    mean_raw_obs_processing_ms: 0.38122071169111316
  time_since_restore: 3323.010533094406
  time_this_iter_s: 25.64870572090149
  time_total_s: 3323.010533094406
  timers:
    learn_throughput: 8655.492
    learn_time_ms: 18692.41
    sample_throughput: 23616.924
    sample_time_ms: 6850.681
    update_time_ms: 34.904
  timestamp: 1602802262
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    129 |          3323.01 | 20871168 |  267.028 |              308.364 |              136.242 |            800.907 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3152.1497841121854
    time_step_min: 2880
  date: 2020-10-15_22-51-28
  done: false
  episode_len_mean: 800.7645285610714
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 267.2463198012096
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 230
  episodes_total: 26207
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.14385775104165077
        entropy_coeff: 0.0005000000000000001
        kl: 0.003811001059754441
        model: {}
        policy_loss: -0.007713697210419923
        total_loss: 2.4613351225852966
        vf_explained_var: 0.9952512383460999
        vf_loss: 2.469120721022288
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.154838709677424
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147559155166119
    mean_env_wait_ms: 1.1981390559426455
    mean_inference_ms: 4.347773694256476
    mean_raw_obs_processing_ms: 0.3811821177652105
  time_since_restore: 3348.9769475460052
  time_this_iter_s: 25.96641445159912
  time_total_s: 3348.9769475460052
  timers:
    learn_throughput: 8642.156
    learn_time_ms: 18721.255
    sample_throughput: 23610.904
    sample_time_ms: 6852.427
    update_time_ms: 35.699
  timestamp: 1602802288
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    130 |          3348.98 | 21032960 |  267.246 |              308.364 |              136.242 |            800.765 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3151.0065652157414
    time_step_min: 2878
  date: 2020-10-15_22-51-54
  done: false
  episode_len_mean: 800.6537310039034
  episode_reward_max: 308.66666666666634
  episode_reward_mean: 267.4155570178611
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 26387
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.14070829252401987
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007642167600958298
        total_loss: .inf
        vf_explained_var: 0.9948969483375549
        vf_loss: 2.270488361517588
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.127586206896552
    gpu_util_percent0: 0.27827586206896554
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475520360793881
    mean_env_wait_ms: 1.1980887940681368
    mean_inference_ms: 4.347289534970617
    mean_raw_obs_processing_ms: 0.3811515713451837
  time_since_restore: 3374.591307401657
  time_this_iter_s: 25.614359855651855
  time_total_s: 3374.591307401657
  timers:
    learn_throughput: 8640.91
    learn_time_ms: 18723.954
    sample_throughput: 23624.315
    sample_time_ms: 6848.537
    update_time_ms: 37.647
  timestamp: 1602802314
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    131 |          3374.59 | 21194752 |  267.416 |              308.667 |              136.242 |            800.654 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3149.736885708906
    time_step_min: 2878
  date: 2020-10-15_22-52-21
  done: false
  episode_len_mean: 800.5151367003873
  episode_reward_max: 308.66666666666634
  episode_reward_mean: 267.6011432439546
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 204
  episodes_total: 26591
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.263256414560601e-15
        cur_lr: 5.0e-05
        entropy: 0.15472138424714407
        entropy_coeff: 0.0005000000000000001
        kl: 0.003816704615019262
        model: {}
        policy_loss: -0.008889230859495001
        total_loss: 2.7518296043078103
        vf_explained_var: 0.9945402145385742
        vf_loss: 2.760796229044596
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.740000000000006
    gpu_util_percent0: 0.30833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754340922216308
    mean_env_wait_ms: 1.1980331862119082
    mean_inference_ms: 4.346742807470893
    mean_raw_obs_processing_ms: 0.3811170925082078
  time_since_restore: 3400.357658624649
  time_this_iter_s: 25.766351222991943
  time_total_s: 3400.357658624649
  timers:
    learn_throughput: 8627.884
    learn_time_ms: 18752.222
    sample_throughput: 23656.797
    sample_time_ms: 6839.134
    update_time_ms: 38.046
  timestamp: 1602802341
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    132 |          3400.36 | 21356544 |  267.601 |              308.667 |              136.242 |            800.515 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3148.358044046286
    time_step_min: 2878
  date: 2020-10-15_22-52-47
  done: false
  episode_len_mean: 800.3646835159919
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 267.8082050656417
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 235
  episodes_total: 26826
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1316282072803005e-15
        cur_lr: 5.0e-05
        entropy: 0.14557435611883798
        entropy_coeff: 0.0005000000000000001
        kl: 0.003975215038129439
        model: {}
        policy_loss: -0.011097151092447652
        total_loss: 2.651736537615458
        vf_explained_var: 0.9950762391090393
        vf_loss: 2.662906507651011
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94666666666667
    gpu_util_percent0: 0.3393333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753329554385958
    mean_env_wait_ms: 1.1979762016401085
    mean_inference_ms: 4.3461566124414155
    mean_raw_obs_processing_ms: 0.3810780011800145
  time_since_restore: 3426.0371816158295
  time_this_iter_s: 25.67952299118042
  time_total_s: 3426.0371816158295
  timers:
    learn_throughput: 8640.966
    learn_time_ms: 18723.833
    sample_throughput: 23677.637
    sample_time_ms: 6833.114
    update_time_ms: 39.968
  timestamp: 1602802367
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    133 |          3426.04 | 21518336 |  267.808 |              308.667 |              136.242 |            800.365 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3147.277888963012
    time_step_min: 2878
  date: 2020-10-15_22-53-13
  done: false
  episode_len_mean: 800.22044562884
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 267.9751355437563
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 192
  episodes_total: 27018
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0658141036401502e-15
        cur_lr: 5.0e-05
        entropy: 0.13573087751865387
        entropy_coeff: 0.0005000000000000001
        kl: 0.003934694066022833
        model: {}
        policy_loss: -0.01063305014395155
        total_loss: 2.2533395489056907
        vf_explained_var: 0.9950329661369324
        vf_loss: 2.2640404105186462
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.913333333333334
    gpu_util_percent0: 0.33966666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475254820369995
    mean_env_wait_ms: 1.1979232569305962
    mean_inference_ms: 4.345664174170695
    mean_raw_obs_processing_ms: 0.3810472674761987
  time_since_restore: 3451.5781111717224
  time_this_iter_s: 25.540929555892944
  time_total_s: 3451.5781111717224
  timers:
    learn_throughput: 8638.872
    learn_time_ms: 18728.371
    sample_throughput: 23705.168
    sample_time_ms: 6825.178
    update_time_ms: 38.802
  timestamp: 1602802393
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    134 |          3451.58 | 21680128 |  267.975 |              308.667 |              136.242 |             800.22 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3146.12739720985
    time_step_min: 2878
  date: 2020-10-15_22-53-39
  done: false
  episode_len_mean: 800.0861302062273
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.13753607835133
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 27203
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.329070518200751e-16
        cur_lr: 5.0e-05
        entropy: 0.15098275244235992
        entropy_coeff: 0.0005000000000000001
        kl: 0.004871759602489571
        model: {}
        policy_loss: -0.007839497440727428
        total_loss: 3.0314738949139914
        vf_explained_var: 0.9937071800231934
        vf_loss: 3.03938889503479
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.663333333333338
    gpu_util_percent0: 0.37966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475184195050727
    mean_env_wait_ms: 1.1978738084185516
    mean_inference_ms: 4.345195356721433
    mean_raw_obs_processing_ms: 0.38101706371409255
  time_since_restore: 3477.404782772064
  time_this_iter_s: 25.826671600341797
  time_total_s: 3477.404782772064
  timers:
    learn_throughput: 8634.895
    learn_time_ms: 18736.997
    sample_throughput: 23739.474
    sample_time_ms: 6815.315
    update_time_ms: 38.788
  timestamp: 1602802419
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    135 |           3477.4 | 21841920 |  268.138 |              308.667 |              136.242 |            800.086 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3144.6848135444793
    time_step_min: 2878
  date: 2020-10-15_22-54-06
  done: false
  episode_len_mean: 799.9055098024925
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.35270973712034
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 239
  episodes_total: 27442
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6645352591003756e-16
        cur_lr: 5.0e-05
        entropy: 0.15152055894335112
        entropy_coeff: 0.0005000000000000001
        kl: 0.004096088116057217
        model: {}
        policy_loss: -0.009226406992335493
        total_loss: 2.275760034720103
        vf_explained_var: 0.9956778883934021
        vf_loss: 2.285062253475189
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.064516129032263
    gpu_util_percent0: 0.39354838709677414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750833228293572
    mean_env_wait_ms: 1.1978146742512132
    mean_inference_ms: 4.344614286245976
    mean_raw_obs_processing_ms: 0.38097857734340906
  time_since_restore: 3503.3933415412903
  time_this_iter_s: 25.988558769226074
  time_total_s: 3503.3933415412903
  timers:
    learn_throughput: 8623.416
    learn_time_ms: 18761.938
    sample_throughput: 23747.641
    sample_time_ms: 6812.971
    update_time_ms: 46.071
  timestamp: 1602802446
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    136 |          3503.39 | 22003712 |  268.353 |              308.667 |              136.242 |            799.906 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3143.5800130396988
    time_step_min: 2878
  date: 2020-10-15_22-54-32
  done: false
  episode_len_mean: 799.7616119230213
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.52779604758325
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 202
  episodes_total: 27644
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3322676295501878e-16
        cur_lr: 5.0e-05
        entropy: 0.13440921530127525
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033904482261277735
        model: {}
        policy_loss: -0.0074828818324021995
        total_loss: 2.5545804301897683
        vf_explained_var: 0.9946722984313965
        vf_loss: 2.5621305108070374
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.644827586206894
    gpu_util_percent0: 0.323103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750026174019787
    mean_env_wait_ms: 1.1977628854815325
    mean_inference_ms: 4.344112583341069
    mean_raw_obs_processing_ms: 0.38094670075233966
  time_since_restore: 3529.0144987106323
  time_this_iter_s: 25.62115716934204
  time_total_s: 3529.0144987106323
  timers:
    learn_throughput: 8625.694
    learn_time_ms: 18756.983
    sample_throughput: 23795.669
    sample_time_ms: 6799.221
    update_time_ms: 46.523
  timestamp: 1602802472
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    137 |          3529.01 | 22165504 |  268.528 |              308.667 |              136.242 |            799.762 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3142.5631679320645
    time_step_min: 2878
  date: 2020-10-15_22-54-58
  done: false
  episode_len_mean: 799.648327164265
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.68401955371434
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 183
  episodes_total: 27827
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.661338147750939e-17
        cur_lr: 5.0e-05
        entropy: 0.14186163991689682
        entropy_coeff: 0.0005000000000000001
        kl: 0.003602027155769368
        model: {}
        policy_loss: -0.008131344140565488
        total_loss: 2.159025381008784
        vf_explained_var: 0.9954721331596375
        vf_loss: 2.167227655649185
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77096774193549
    gpu_util_percent0: 0.3403225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749328074901322
    mean_env_wait_ms: 1.1977168302256236
    mean_inference_ms: 4.343678955587113
    mean_raw_obs_processing_ms: 0.38091882262284377
  time_since_restore: 3554.8225281238556
  time_this_iter_s: 25.808029413223267
  time_total_s: 3554.8225281238556
  timers:
    learn_throughput: 8625.372
    learn_time_ms: 18757.684
    sample_throughput: 23877.281
    sample_time_ms: 6775.981
    update_time_ms: 46.421
  timestamp: 1602802498
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    138 |          3554.82 | 22327296 |  268.684 |              308.667 |              136.242 |            799.648 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3141.2774030053183
    time_step_min: 2878
  date: 2020-10-15_22-55-24
  done: false
  episode_len_mean: 799.5094998752362
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.87612408979095
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 226
  episodes_total: 28053
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3306690738754695e-17
        cur_lr: 5.0e-05
        entropy: 0.14673407251636186
        entropy_coeff: 0.0005000000000000001
        kl: 0.003811852657236159
        model: {}
        policy_loss: -0.010056069717393257
        total_loss: 2.2437128722667694
        vf_explained_var: 0.9957773089408875
        vf_loss: 2.253842294216156
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.310000000000002
    gpu_util_percent0: 0.2736666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748412397116592
    mean_env_wait_ms: 1.1976586179719593
    mean_inference_ms: 4.343133617765104
    mean_raw_obs_processing_ms: 0.3808826872061162
  time_since_restore: 3580.4809815883636
  time_this_iter_s: 25.658453464508057
  time_total_s: 3580.4809815883636
  timers:
    learn_throughput: 8621.932
    learn_time_ms: 18765.169
    sample_throughput: 23900.688
    sample_time_ms: 6769.345
    update_time_ms: 44.705
  timestamp: 1602802524
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    139 |          3580.48 | 22489088 |  268.876 |              308.667 |              136.242 |            799.509 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3140.0775660551108
    time_step_min: 2878
  date: 2020-10-15_22-55-51
  done: false
  episode_len_mean: 799.365546515741
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 269.067535989538
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 217
  episodes_total: 28270
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6653345369377347e-17
        cur_lr: 5.0e-05
        entropy: 0.12911246965328851
        entropy_coeff: 0.0005000000000000001
        kl: 0.004554758818509678
        model: {}
        policy_loss: -0.009683790704002604
        total_loss: 2.1207492550214133
        vf_explained_var: 0.9956687092781067
        vf_loss: 2.1304975152015686
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.08
    gpu_util_percent0: 0.3403333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747561313545096
    mean_env_wait_ms: 1.1976055289176661
    mean_inference_ms: 4.342625664903536
    mean_raw_obs_processing_ms: 0.3808497365996931
  time_since_restore: 3606.010644674301
  time_this_iter_s: 25.5296630859375
  time_total_s: 3606.010644674301
  timers:
    learn_throughput: 8639.443
    learn_time_ms: 18727.133
    sample_throughput: 23920.411
    sample_time_ms: 6763.763
    update_time_ms: 43.48
  timestamp: 1602802551
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    140 |          3606.01 | 22650880 |  269.068 |              308.667 |              136.242 |            799.366 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3139.1251627890606
    time_step_min: 2878
  date: 2020-10-15_22-56-17
  done: false
  episode_len_mean: 799.2591485921187
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 269.20810949868485
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 177
  episodes_total: 28447
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.326672684688674e-18
        cur_lr: 5.0e-05
        entropy: 0.13276775802175203
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034511150248969593
        model: {}
        policy_loss: -0.009753992451199641
        total_loss: 2.0188810427983603
        vf_explained_var: 0.9956676959991455
        vf_loss: 2.028701434532801
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.06
    gpu_util_percent0: 0.32200000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746898961867325
    mean_env_wait_ms: 1.1975616733722028
    mean_inference_ms: 4.34221894660133
    mean_raw_obs_processing_ms: 0.38082295298536173
  time_since_restore: 3631.7818987369537
  time_this_iter_s: 25.771254062652588
  time_total_s: 3631.7818987369537
  timers:
    learn_throughput: 8640.996
    learn_time_ms: 18723.767
    sample_throughput: 23853.292
    sample_time_ms: 6782.795
    update_time_ms: 42.817
  timestamp: 1602802577
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    141 |          3631.78 | 22812672 |  269.208 |              308.667 |              136.242 |            799.259 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3137.948622122874
    time_step_min: 2878
  date: 2020-10-15_22-56-43
  done: false
  episode_len_mean: 799.1329751979628
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 269.3918326531085
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 28667
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.163336342344337e-18
        cur_lr: 5.0e-05
        entropy: 0.1421886645257473
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009860145218074953
        total_loss: .inf
        vf_explained_var: 0.9957287311553955
        vf_loss: 2.2697707613309226
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09
    gpu_util_percent0: 0.3043333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746097187204266
    mean_env_wait_ms: 1.1975072735968952
    mean_inference_ms: 4.341703704347577
    mean_raw_obs_processing_ms: 0.38079014316975296
  time_since_restore: 3657.459527015686
  time_this_iter_s: 25.6776282787323
  time_total_s: 3657.459527015686
  timers:
    learn_throughput: 8646.353
    learn_time_ms: 18712.166
    sample_throughput: 23872.303
    sample_time_ms: 6777.394
    update_time_ms: 40.875
  timestamp: 1602802603
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    142 |          3657.46 | 22974464 |  269.392 |              308.667 |              136.242 |            799.133 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3136.658917682927
    time_step_min: 2878
  date: 2020-10-15_22-57-10
  done: false
  episode_len_mean: 799.0103806228374
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 269.5848117856767
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 233
  episodes_total: 28900
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.245004513516507e-18
        cur_lr: 5.0e-05
        entropy: 0.12895207727948824
        entropy_coeff: 0.0005000000000000001
        kl: 0.004916979931294918
        model: {}
        policy_loss: -0.007742893346176061
        total_loss: 2.367744286855062
        vf_explained_var: 0.9955448508262634
        vf_loss: 2.375551621119181
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.569999999999997
    gpu_util_percent0: 0.3853333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745182576941887
    mean_env_wait_ms: 1.197450034055561
    mean_inference_ms: 4.341187667220271
    mean_raw_obs_processing_ms: 0.3807551880337533
  time_since_restore: 3683.4313263893127
  time_this_iter_s: 25.97179937362671
  time_total_s: 3683.4313263893127
  timers:
    learn_throughput: 8644.625
    learn_time_ms: 18715.908
    sample_throughput: 23793.544
    sample_time_ms: 6799.828
    update_time_ms: 40.6
  timestamp: 1602802630
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    143 |          3683.43 | 23136256 |  269.585 |              308.667 |              136.242 |             799.01 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3135.699517906336
    time_step_min: 2878
  date: 2020-10-15_22-57-36
  done: false
  episode_len_mean: 798.8973036181043
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 269.7250424175722
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 176
  episodes_total: 29076
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1225022567582536e-18
        cur_lr: 5.0e-05
        entropy: 0.12522268171111742
        entropy_coeff: 0.0005000000000000001
        kl: 0.00382148912952592
        model: {}
        policy_loss: -0.007524712515684466
        total_loss: 2.388529340426127
        vf_explained_var: 0.9947655200958252
        vf_loss: 2.396116634209951
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65333333333334
    gpu_util_percent0: 0.3206666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474455478021854
    mean_env_wait_ms: 1.1974055681728397
    mean_inference_ms: 4.340788644511594
    mean_raw_obs_processing_ms: 0.3807296732570833
  time_since_restore: 3709.0697073936462
  time_this_iter_s: 25.638381004333496
  time_total_s: 3709.0697073936462
  timers:
    learn_throughput: 8648.435
    learn_time_ms: 18707.663
    sample_throughput: 23762.921
    sample_time_ms: 6808.591
    update_time_ms: 45.287
  timestamp: 1602802656
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    144 |          3709.07 | 23298048 |  269.725 |              308.667 |              136.242 |            798.897 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3134.578911378556
    time_step_min: 2878
  date: 2020-10-15_22-58-03
  done: false
  episode_len_mean: 798.7644105996449
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 269.8984956103859
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 208
  episodes_total: 29284
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5612511283791268e-18
        cur_lr: 5.0e-05
        entropy: 0.13686399285991988
        entropy_coeff: 0.0005000000000000001
        kl: 0.005437889990086357
        model: {}
        policy_loss: -0.007267611421411857
        total_loss: 2.070864826440811
        vf_explained_var: 0.995894730091095
        vf_loss: 2.078200876712799
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46
    gpu_util_percent0: 0.2976666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743849845099583
    mean_env_wait_ms: 1.1973552613421146
    mean_inference_ms: 4.340332854184658
    mean_raw_obs_processing_ms: 0.38070056126328233
  time_since_restore: 3734.9484388828278
  time_this_iter_s: 25.87873148918152
  time_total_s: 3734.9484388828278
  timers:
    learn_throughput: 8652.038
    learn_time_ms: 18699.873
    sample_throughput: 23752.471
    sample_time_ms: 6811.586
    update_time_ms: 45.254
  timestamp: 1602802683
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    145 |          3734.95 | 23459840 |  269.898 |              308.667 |              136.242 |            798.764 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3133.2767299864313
    time_step_min: 2878
  date: 2020-10-15_22-58-29
  done: false
  episode_len_mean: 798.6439219406424
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 270.08844817602767
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 232
  episodes_total: 29516
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5612511283791268e-18
        cur_lr: 5.0e-05
        entropy: 0.12984738995631537
        entropy_coeff: 0.0005000000000000001
        kl: 0.004120348816892753
        model: {}
        policy_loss: -0.009564112076380601
        total_loss: 2.0620259841283164
        vf_explained_var: 0.9961709380149841
        vf_loss: 2.0716550250848136
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05862068965518
    gpu_util_percent0: 0.3010344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474293288455965
    mean_env_wait_ms: 1.1973011193981922
    mean_inference_ms: 4.339831411530093
    mean_raw_obs_processing_ms: 0.3806657400042824
  time_since_restore: 3760.5637760162354
  time_this_iter_s: 25.615337133407593
  time_total_s: 3760.5637760162354
  timers:
    learn_throughput: 8661.624
    learn_time_ms: 18679.175
    sample_throughput: 23778.484
    sample_time_ms: 6804.134
    update_time_ms: 37.597
  timestamp: 1602802709
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    146 |          3760.56 | 23621632 |  270.088 |              308.667 |              136.242 |            798.644 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3132.2472614513467
    time_step_min: 2878
  date: 2020-10-15_22-58-55
  done: false
  episode_len_mean: 798.5355327385962
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 270.24553054531157
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 29705
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.806255641895634e-19
        cur_lr: 5.0e-05
        entropy: 0.12073200071851413
        entropy_coeff: 0.0005000000000000001
        kl: 0.004599387951505681
        model: {}
        policy_loss: -0.01025845367500248
        total_loss: 1.813588410615921
        vf_explained_var: 0.9960399270057678
        vf_loss: 1.823907236258189
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09666666666667
    gpu_util_percent0: 0.37799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474233039466081
    mean_env_wait_ms: 1.1972543688435429
    mean_inference_ms: 4.339426484278492
    mean_raw_obs_processing_ms: 0.38064024296258936
  time_since_restore: 3786.1339089870453
  time_this_iter_s: 25.570132970809937
  time_total_s: 3786.1339089870453
  timers:
    learn_throughput: 8661.954
    learn_time_ms: 18678.464
    sample_throughput: 23792.083
    sample_time_ms: 6800.245
    update_time_ms: 35.671
  timestamp: 1602802735
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    147 |          3786.13 | 23783424 |  270.246 |              308.667 |              136.242 |            798.536 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3131.2123556002007
    time_step_min: 2878
  date: 2020-10-15_22-59-21
  done: false
  episode_len_mean: 798.4258386007157
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 270.4011277620186
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 196
  episodes_total: 29901
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.903127820947817e-19
        cur_lr: 5.0e-05
        entropy: 0.1336955800652504
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052194724169870215
        model: {}
        policy_loss: -0.008139124838635325
        total_loss: 2.065008540948232
        vf_explained_var: 0.9957743287086487
        vf_loss: 2.0732144812742868
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.386666666666667
    gpu_util_percent0: 0.3203333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474168614541044
    mean_env_wait_ms: 1.1972068653448757
    mean_inference_ms: 4.339017133560326
    mean_raw_obs_processing_ms: 0.38061355816469294
  time_since_restore: 3811.795963048935
  time_this_iter_s: 25.66205406188965
  time_total_s: 3811.795963048935
  timers:
    learn_throughput: 8670.828
    learn_time_ms: 18659.349
    sample_throughput: 23779.108
    sample_time_ms: 6803.956
    update_time_ms: 36.107
  timestamp: 1602802761
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    148 |           3811.8 | 23945216 |  270.401 |              308.667 |              136.242 |            798.426 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3130.005615364168
    time_step_min: 2878
  date: 2020-10-15_22-59-48
  done: false
  episode_len_mean: 798.2918823841763
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 270.59020310633196
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 231
  episodes_total: 30132
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.903127820947817e-19
        cur_lr: 5.0e-05
        entropy: 0.12926226233442625
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011422331435217833
        total_loss: .inf
        vf_explained_var: 0.9965452551841736
        vf_loss: 1.887120525042216
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.756666666666668
    gpu_util_percent0: 0.3493333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740873589838718
    mean_env_wait_ms: 1.1971547662247064
    mean_inference_ms: 4.338547164766457
    mean_raw_obs_processing_ms: 0.3805818854503616
  time_since_restore: 3837.543597459793
  time_this_iter_s: 25.747634410858154
  time_total_s: 3837.543597459793
  timers:
    learn_throughput: 8672.752
    learn_time_ms: 18655.209
    sample_throughput: 23742.665
    sample_time_ms: 6814.399
    update_time_ms: 38.335
  timestamp: 1602802788
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    149 |          3837.54 | 24107008 |   270.59 |              308.667 |              136.242 |            798.292 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3128.9617415989965
    time_step_min: 2878
  date: 2020-10-15_23-00-14
  done: false
  episode_len_mean: 798.183811407847
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 270.7507468353165
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 198
  episodes_total: 30330
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.854691731421723e-19
        cur_lr: 5.0e-05
        entropy: 0.11167993334432443
        entropy_coeff: 0.0005000000000000001
        kl: 0.003925101792750259
        model: {}
        policy_loss: -0.010190843924647197
        total_loss: 1.667695015668869
        vf_explained_var: 0.9965181946754456
        vf_loss: 1.6779416998227437
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.819999999999997
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740215972701717
    mean_env_wait_ms: 1.1971064989753992
    mean_inference_ms: 4.338125361185287
    mean_raw_obs_processing_ms: 0.3805547098927467
  time_since_restore: 3863.1427273750305
  time_this_iter_s: 25.599129915237427
  time_total_s: 3863.1427273750305
  timers:
    learn_throughput: 8673.749
    learn_time_ms: 18653.064
    sample_throughput: 23713.018
    sample_time_ms: 6822.919
    update_time_ms: 37.494
  timestamp: 1602802814
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    150 |          3863.14 | 24268800 |  270.751 |              308.667 |              136.242 |            798.184 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3127.9146465474823
    time_step_min: 2878
  date: 2020-10-15_23-00-40
  done: false
  episode_len_mean: 798.0771272238786
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 270.9096032240095
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 191
  episodes_total: 30521
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9273458657108617e-19
        cur_lr: 5.0e-05
        entropy: 0.11956185661256313
        entropy_coeff: 0.0005000000000000001
        kl: 0.003713660795862476
        model: {}
        policy_loss: -0.006202390127630982
        total_loss: 1.438343306382497
        vf_explained_var: 0.9969591498374939
        vf_loss: 1.4446054796377819
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.673333333333336
    gpu_util_percent0: 0.35866666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473960068456626
    mean_env_wait_ms: 1.1970606669104566
    mean_inference_ms: 4.337736339114887
    mean_raw_obs_processing_ms: 0.3805298265630595
  time_since_restore: 3888.907180786133
  time_this_iter_s: 25.764453411102295
  time_total_s: 3888.907180786133
  timers:
    learn_throughput: 8672.165
    learn_time_ms: 18656.472
    sample_throughput: 23728.458
    sample_time_ms: 6818.479
    update_time_ms: 36.766
  timestamp: 1602802840
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    151 |          3888.91 | 24430592 |   270.91 |              308.667 |              136.242 |            798.077 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3126.6580703982286
    time_step_min: 2878
  date: 2020-10-15_23-01-06
  done: false
  episode_len_mean: 797.9378150713891
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 271.09357996000574
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 226
  episodes_total: 30747
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4636729328554308e-19
        cur_lr: 5.0e-05
        entropy: 0.12471852575739224
        entropy_coeff: 0.0005000000000000001
        kl: 0.004232874023728073
        model: {}
        policy_loss: -0.007228552974993363
        total_loss: 1.7467515269915264
        vf_explained_var: 0.9966629147529602
        vf_loss: 1.7540424664815266
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.129999999999995
    gpu_util_percent0: 0.39133333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738847703526844
    mean_env_wait_ms: 1.1970068672142191
    mean_inference_ms: 4.33728353353535
    mean_raw_obs_processing_ms: 0.38049921268674564
  time_since_restore: 3914.381557703018
  time_this_iter_s: 25.474376916885376
  time_total_s: 3914.381557703018
  timers:
    learn_throughput: 8681.45
    learn_time_ms: 18636.518
    sample_throughput: 23708.508
    sample_time_ms: 6824.217
    update_time_ms: 38.578
  timestamp: 1602802866
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    152 |          3914.38 | 24592384 |  271.094 |              308.667 |              136.242 |            797.938 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3125.526290259992
    time_step_min: 2878
  date: 2020-10-15_23-01-33
  done: false
  episode_len_mean: 797.8228359173127
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 271.2637224962805
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 213
  episodes_total: 30960
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.318364664277154e-20
        cur_lr: 5.0e-05
        entropy: 0.11069825291633606
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033093879465013742
        model: {}
        policy_loss: -0.008189565285647404
        total_loss: 1.6310593883196514
        vf_explained_var: 0.9966135621070862
        vf_loss: 1.6393042902151744
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.963333333333335
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738114103578603
    mean_env_wait_ms: 1.1969588971823233
    mean_inference_ms: 4.336856398230412
    mean_raw_obs_processing_ms: 0.38047122134633377
  time_since_restore: 3940.1388103961945
  time_this_iter_s: 25.75725269317627
  time_total_s: 3940.1388103961945
  timers:
    learn_throughput: 8684.125
    learn_time_ms: 18630.776
    sample_throughput: 23759.821
    sample_time_ms: 6809.479
    update_time_ms: 38.758
  timestamp: 1602802893
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    153 |          3940.14 | 24754176 |  271.264 |              308.667 |              136.242 |            797.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3124.546953865938
    time_step_min: 2878
  date: 2020-10-15_23-01-59
  done: false
  episode_len_mean: 797.7187951575094
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 271.41588259850346
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 181
  episodes_total: 31141
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.659182332138577e-20
        cur_lr: 5.0e-05
        entropy: 0.11106523064275582
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043731861126919585
        model: {}
        policy_loss: -0.007723290781238272
        total_loss: 1.2422289649645488
        vf_explained_var: 0.9972507357597351
        vf_loss: 1.2500077784061432
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09
    gpu_util_percent0: 0.2993333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737546086172684
    mean_env_wait_ms: 1.196915610316182
    mean_inference_ms: 4.3364983738985865
    mean_raw_obs_processing_ms: 0.3804475295287644
  time_since_restore: 3965.945571422577
  time_this_iter_s: 25.806761026382446
  time_total_s: 3965.945571422577
  timers:
    learn_throughput: 8670.027
    learn_time_ms: 18661.072
    sample_throughput: 23777.085
    sample_time_ms: 6804.535
    update_time_ms: 32.798
  timestamp: 1602802919
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    154 |          3965.95 | 24915968 |  271.416 |              308.667 |              136.242 |            797.719 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3123.360220888662
    time_step_min: 2878
  date: 2020-10-15_23-02-25
  done: false
  episode_len_mean: 797.6058538451728
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 271.59427040459417
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 223
  episodes_total: 31364
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8295911660692886e-20
        cur_lr: 5.0e-05
        entropy: 0.12110077900191148
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01205759044144846
        total_loss: .inf
        vf_explained_var: 0.9966743588447571
        vf_loss: 1.766980618238449
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.790000000000003
    gpu_util_percent0: 0.35400000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473680040776899
    mean_env_wait_ms: 1.1968615441761798
    mean_inference_ms: 4.336057943543245
    mean_raw_obs_processing_ms: 0.3804184348800185
  time_since_restore: 3991.8044486045837
  time_this_iter_s: 25.858877182006836
  time_total_s: 3991.8044486045837
  timers:
    learn_throughput: 8667.964
    learn_time_ms: 18665.514
    sample_throughput: 23766.537
    sample_time_ms: 6807.555
    update_time_ms: 31.374
  timestamp: 1602802945
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    155 |           3991.8 | 25077760 |  271.594 |              308.667 |              136.242 |            797.606 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3122.195087163233
    time_step_min: 2878
  date: 2020-10-15_23-02-52
  done: false
  episode_len_mean: 797.5203887798392
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 271.7711203083835
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 222
  episodes_total: 31586
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.744386749103933e-20
        cur_lr: 5.0e-05
        entropy: 0.10826947974661986
        entropy_coeff: 0.0005000000000000001
        kl: 0.003178546806642165
        model: {}
        policy_loss: -0.007975032727699727
        total_loss: 1.9207045336564381
        vf_explained_var: 0.9962170124053955
        vf_loss: 1.9287337462107341
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.335483870967742
    gpu_util_percent0: 0.3664516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473608730160878
    mean_env_wait_ms: 1.1968128312033346
    mean_inference_ms: 4.335642077118087
    mean_raw_obs_processing_ms: 0.3803905321766032
  time_since_restore: 4017.5837144851685
  time_this_iter_s: 25.779265880584717
  time_total_s: 4017.5837144851685
  timers:
    learn_throughput: 8661.197
    learn_time_ms: 18680.097
    sample_throughput: 23771.446
    sample_time_ms: 6806.149
    update_time_ms: 31.986
  timestamp: 1602802972
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    156 |          4017.58 | 25239552 |  271.771 |              308.667 |              136.242 |             797.52 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3121.2731112301826
    time_step_min: 2878
  date: 2020-10-15_23-03-18
  done: false
  episode_len_mean: 797.4482259232441
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 271.9067668785578
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 177
  episodes_total: 31763
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3721933745519665e-20
        cur_lr: 5.0e-05
        entropy: 0.10728933724264304
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01060265973986437
        total_loss: .inf
        vf_explained_var: 0.9968308806419373
        vf_loss: 1.4554479916890461
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.52068965517241
    gpu_util_percent0: 0.3037931034482758
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473558870563934
    mean_env_wait_ms: 1.1967699678824868
    mean_inference_ms: 4.335300780794726
    mean_raw_obs_processing_ms: 0.3803684671306957
  time_since_restore: 4043.0765953063965
  time_this_iter_s: 25.492880821228027
  time_total_s: 4043.0765953063965
  timers:
    learn_throughput: 8667.771
    learn_time_ms: 18665.929
    sample_throughput: 23754.643
    sample_time_ms: 6810.963
    update_time_ms: 33.209
  timestamp: 1602802998
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    157 |          4043.08 | 25401344 |  271.907 |              308.667 |              136.242 |            797.448 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3120.150953439584
    time_step_min: 2878
  date: 2020-10-15_23-03-44
  done: false
  episode_len_mean: 797.3857629875207
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 272.0745515392247
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 31973
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0582900618279505e-20
        cur_lr: 5.0e-05
        entropy: 0.11127381895979245
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037489926520114145
        model: {}
        policy_loss: -0.007955584490749365
        total_loss: 1.4508507351080577
        vf_explained_var: 0.9971900582313538
        vf_loss: 1.4588619669278462
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.487096774193546
    gpu_util_percent0: 0.3693548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734940368997212
    mean_env_wait_ms: 1.196718891439992
    mean_inference_ms: 4.334903176800818
    mean_raw_obs_processing_ms: 0.38034223200737677
  time_since_restore: 4068.8806693553925
  time_this_iter_s: 25.80407404899597
  time_total_s: 4068.8806693553925
  timers:
    learn_throughput: 8664.025
    learn_time_ms: 18673.999
    sample_throughput: 23766.374
    sample_time_ms: 6807.601
    update_time_ms: 32.897
  timestamp: 1602803024
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    158 |          4068.88 | 25563136 |  272.075 |              308.667 |              136.242 |            797.386 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3118.980225725212
    time_step_min: 2878
  date: 2020-10-15_23-04-11
  done: false
  episode_len_mean: 797.3129910866797
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 272.2541392683942
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 226
  episodes_total: 32199
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0291450309139752e-20
        cur_lr: 5.0e-05
        entropy: 0.10616942433019479
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00773928167958123
        total_loss: .inf
        vf_explained_var: 0.996859610080719
        vf_loss: 1.6326012810071309
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.976666666666667
    gpu_util_percent0: 0.293
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734198372427318
    mean_env_wait_ms: 1.1966696454997288
    mean_inference_ms: 4.334494727420472
    mean_raw_obs_processing_ms: 0.38031349064846026
  time_since_restore: 4094.750011444092
  time_this_iter_s: 25.86934208869934
  time_total_s: 4094.750011444092
  timers:
    learn_throughput: 8664.067
    learn_time_ms: 18673.91
    sample_throughput: 23749.9
    sample_time_ms: 6812.323
    update_time_ms: 30.502
  timestamp: 1602803051
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    159 |          4094.75 | 25724928 |  272.254 |              308.667 |              136.242 |            797.313 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3117.9371599901087
    time_step_min: 2878
  date: 2020-10-15_23-04-37
  done: false
  episode_len_mean: 797.2663332098308
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 272.4115768653558
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 32388
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5437175463709622e-20
        cur_lr: 5.0e-05
        entropy: 0.0968475230038166
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030310306077202163
        model: {}
        policy_loss: -0.00669493567935812
        total_loss: 1.3176035583019257
        vf_explained_var: 0.997101366519928
        vf_loss: 1.3243469595909119
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85666666666667
    gpu_util_percent0: 0.31799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733692117951674
    mean_env_wait_ms: 1.1966238516696286
    mean_inference_ms: 4.334150930591374
    mean_raw_obs_processing_ms: 0.3802916375456588
  time_since_restore: 4120.516840696335
  time_this_iter_s: 25.766829252243042
  time_total_s: 4120.516840696335
  timers:
    learn_throughput: 8653.92
    learn_time_ms: 18695.805
    sample_throughput: 23773.479
    sample_time_ms: 6805.567
    update_time_ms: 30.908
  timestamp: 1602803077
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    160 |          4120.52 | 25886720 |  272.412 |              308.667 |              136.242 |            797.266 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3116.8590168970813
    time_step_min: 2878
  date: 2020-10-15_23-05-04
  done: false
  episode_len_mean: 797.211992880378
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 272.5737172870296
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 198
  episodes_total: 32586
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.718587731854811e-21
        cur_lr: 5.0e-05
        entropy: 0.10587653331458569
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035427333205007017
        model: {}
        policy_loss: -0.0077968636590715805
        total_loss: 1.4247685174147289
        vf_explained_var: 0.9969996809959412
        vf_loss: 1.4326183398564656
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.11666666666667
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473312673447962
    mean_env_wait_ms: 1.1965767136766658
    mean_inference_ms: 4.3337971564348665
    mean_raw_obs_processing_ms: 0.38026886698167767
  time_since_restore: 4146.204780101776
  time_this_iter_s: 25.687939405441284
  time_total_s: 4146.204780101776
  timers:
    learn_throughput: 8653.741
    learn_time_ms: 18696.192
    sample_throughput: 23807.758
    sample_time_ms: 6795.768
    update_time_ms: 31.573
  timestamp: 1602803104
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    161 |           4146.2 | 26048512 |  272.574 |              308.667 |              136.242 |            797.212 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3115.6089450241016
    time_step_min: 2878
  date: 2020-10-15_23-05-29
  done: false
  episode_len_mean: 797.1438105686598
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 272.7615928899526
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 228
  episodes_total: 32814
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8592938659274055e-21
        cur_lr: 5.0e-05
        entropy: 0.10316222595671813
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007686797151109204
        total_loss: .inf
        vf_explained_var: 0.9971780776977539
        vf_loss: 1.4478188753128052
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.358620689655176
    gpu_util_percent0: 0.3503448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732440559746715
    mean_env_wait_ms: 1.1965225280970564
    mean_inference_ms: 4.333389053752377
    mean_raw_obs_processing_ms: 0.38024074486752135
  time_since_restore: 4171.41414308548
  time_this_iter_s: 25.209362983703613
  time_total_s: 4171.41414308548
  timers:
    learn_throughput: 8666.685
    learn_time_ms: 18668.268
    sample_throughput: 23804.104
    sample_time_ms: 6796.811
    update_time_ms: 31.279
  timestamp: 1602803129
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    162 |          4171.41 | 26210304 |  272.762 |              308.667 |              136.242 |            797.144 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3114.5621152854847
    time_step_min: 2878
  date: 2020-10-15_23-05-56
  done: false
  episode_len_mean: 797.0864455550508
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 272.92034382902153
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 33015
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.788940798891108e-21
        cur_lr: 5.0e-05
        entropy: 0.09567353005210559
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038264618488028646
        model: {}
        policy_loss: -0.008314995415275916
        total_loss: 1.6691997647285461
        vf_explained_var: 0.9964999556541443
        vf_loss: 1.6775625745455425
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44
    gpu_util_percent0: 0.35200000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731835424563183
    mean_env_wait_ms: 1.1964744171395143
    mean_inference_ms: 4.333025927971471
    mean_raw_obs_processing_ms: 0.38021657305716366
  time_since_restore: 4197.085546731949
  time_this_iter_s: 25.671403646469116
  time_total_s: 4197.085546731949
  timers:
    learn_throughput: 8665.933
    learn_time_ms: 18669.889
    sample_throughput: 23837.966
    sample_time_ms: 6787.156
    update_time_ms: 31.064
  timestamp: 1602803156
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    163 |          4197.09 | 26372096 |   272.92 |              308.667 |              136.242 |            797.086 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3113.617950419205
    time_step_min: 2878
  date: 2020-10-15_23-06-22
  done: false
  episode_len_mean: 797.0382298005663
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 273.0653394826738
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 179
  episodes_total: 33194
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.894470399445554e-21
        cur_lr: 5.0e-05
        entropy: 0.1007075769205888
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008836542891610103
        total_loss: .inf
        vf_explained_var: 0.9972937107086182
        vf_loss: 1.222872257232666
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97333333333334
    gpu_util_percent0: 0.3353333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473135828070497
    mean_env_wait_ms: 1.196430739635615
    mean_inference_ms: 4.332710163355092
    mean_raw_obs_processing_ms: 0.3801958232547494
  time_since_restore: 4222.728698968887
  time_this_iter_s: 25.643152236938477
  time_total_s: 4222.728698968887
  timers:
    learn_throughput: 8670.456
    learn_time_ms: 18660.149
    sample_throughput: 23862.431
    sample_time_ms: 6780.198
    update_time_ms: 31.178
  timestamp: 1602803182
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    164 |          4222.73 | 26533888 |  273.065 |              308.667 |              136.242 |            797.038 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3112.5081932836047
    time_step_min: 2878
  date: 2020-10-15_23-06-48
  done: false
  episode_len_mean: 796.9895262890145
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 273.2405181177062
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 223
  episodes_total: 33417
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3417055991683326e-21
        cur_lr: 5.0e-05
        entropy: 0.10690222432216008
        entropy_coeff: 0.0005000000000000001
        kl: 0.003693336194070677
        model: {}
        policy_loss: -0.008693390603487691
        total_loss: 1.2319352328777313
        vf_explained_var: 0.9975783824920654
        vf_loss: 1.2406820853551228
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89677419354839
    gpu_util_percent0: 0.33483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730719599319078
    mean_env_wait_ms: 1.196375097949085
    mean_inference_ms: 4.332327058622629
    mean_raw_obs_processing_ms: 0.38017041013690467
  time_since_restore: 4248.651180267334
  time_this_iter_s: 25.922481298446655
  time_total_s: 4248.651180267334
  timers:
    learn_throughput: 8664.449
    learn_time_ms: 18673.087
    sample_throughput: 23891.555
    sample_time_ms: 6771.933
    update_time_ms: 31.34
  timestamp: 1602803208
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    165 |          4248.65 | 26695680 |  273.241 |              308.667 |              136.242 |             796.99 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3111.425191225929
    time_step_min: 2878
  date: 2020-10-15_23-07-15
  done: false
  episode_len_mean: 796.9322431990486
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 273.40635431166106
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 218
  episodes_total: 33635
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1708527995841663e-21
        cur_lr: 5.0e-05
        entropy: 0.10117162764072418
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007817333835798005
        total_loss: .inf
        vf_explained_var: 0.9976744651794434
        vf_loss: 1.1472369233767192
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.3
    gpu_util_percent0: 0.32566666666666677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730081247789278
    mean_env_wait_ms: 1.1963263736267122
    mean_inference_ms: 4.331959505833696
    mean_raw_obs_processing_ms: 0.38014488577081174
  time_since_restore: 4274.368007183075
  time_this_iter_s: 25.716826915740967
  time_total_s: 4274.368007183075
  timers:
    learn_throughput: 8671.758
    learn_time_ms: 18657.348
    sample_throughput: 23855.261
    sample_time_ms: 6782.235
    update_time_ms: 30.936
  timestamp: 1602803235
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    166 |          4274.37 | 26857472 |  273.406 |              308.667 |              136.242 |            796.932 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3110.490586702978
    time_step_min: 2878
  date: 2020-10-15_23-07-41
  done: false
  episode_len_mean: 796.8936956650305
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 273.54711405258433
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 183
  episodes_total: 33818
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2562791993762487e-21
        cur_lr: 5.0e-05
        entropy: 0.09785908025999863
        entropy_coeff: 0.0005000000000000001
        kl: 0.005811448480623464
        model: {}
        policy_loss: -0.010078524986359602
        total_loss: 1.0186588913202286
        vf_explained_var: 0.9976829886436462
        vf_loss: 1.0287863214810689
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89333333333333
    gpu_util_percent0: 0.33733333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729615786906605
    mean_env_wait_ms: 1.1962801886061585
    mean_inference_ms: 4.331657639519536
    mean_raw_obs_processing_ms: 0.38012530370966047
  time_since_restore: 4299.8423528671265
  time_this_iter_s: 25.474345684051514
  time_total_s: 4299.8423528671265
  timers:
    learn_throughput: 8672.631
    learn_time_ms: 18655.47
    sample_throughput: 23855.807
    sample_time_ms: 6782.08
    update_time_ms: 29.795
  timestamp: 1602803261
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    167 |          4299.84 | 27019264 |  273.547 |              308.667 |              136.242 |            796.894 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3109.3982641953517
    time_step_min: 2878
  date: 2020-10-15_23-08-07
  done: false
  episode_len_mean: 796.8527008758009
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 273.7132531450992
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 208
  episodes_total: 34026
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2562791993762487e-21
        cur_lr: 5.0e-05
        entropy: 0.10132051693896453
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00593658735548767
        total_loss: .inf
        vf_explained_var: 0.9978976845741272
        vf_loss: 1.0256401350100834
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.213333333333335
    gpu_util_percent0: 0.31666666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729045487315776
    mean_env_wait_ms: 1.196228651007455
    mean_inference_ms: 4.331305411034874
    mean_raw_obs_processing_ms: 0.3801021611338228
  time_since_restore: 4325.488522768021
  time_this_iter_s: 25.646169900894165
  time_total_s: 4325.488522768021
  timers:
    learn_throughput: 8680.032
    learn_time_ms: 18639.564
    sample_throughput: 23823.619
    sample_time_ms: 6791.244
    update_time_ms: 28.041
  timestamp: 1602803287
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    168 |          4325.49 | 27181056 |  273.713 |              308.667 |              136.242 |            796.853 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3108.2465513210195
    time_step_min: 2878
  date: 2020-10-15_23-08-34
  done: false
  episode_len_mean: 796.8062886838725
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 273.88583104193856
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 226
  episodes_total: 34252
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.8844187990643736e-21
        cur_lr: 5.0e-05
        entropy: 0.10110222175717354
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010486648961280784
        total_loss: .inf
        vf_explained_var: 0.9975947737693787
        vf_loss: 1.2215636769930522
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45806451612903
    gpu_util_percent0: 0.35161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472840318275607
    mean_env_wait_ms: 1.1961744520050783
    mean_inference_ms: 4.330936964849377
    mean_raw_obs_processing_ms: 0.38007624697050446
  time_since_restore: 4351.377153873444
  time_this_iter_s: 25.888631105422974
  time_total_s: 4351.377153873444
  timers:
    learn_throughput: 8677.015
    learn_time_ms: 18646.044
    sample_throughput: 23848.953
    sample_time_ms: 6784.029
    update_time_ms: 28.732
  timestamp: 1602803314
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    169 |          4351.38 | 27342848 |  273.886 |              308.667 |              136.242 |            796.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3107.303240808022
    time_step_min: 2878
  date: 2020-10-15_23-09-00
  done: false
  episode_len_mean: 796.7775616271305
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 274.02195586127505
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 34441
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.326628198596561e-21
        cur_lr: 5.0e-05
        entropy: 0.0926384466389815
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007744602798387253
        total_loss: .inf
        vf_explained_var: 0.9971820712089539
        vf_loss: 1.277089774608612
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.23666666666667
    gpu_util_percent0: 0.39300000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727941005224085
    mean_env_wait_ms: 1.19612759787508
    mean_inference_ms: 4.330635931619472
    mean_raw_obs_processing_ms: 0.3800564510808291
  time_since_restore: 4377.361395359039
  time_this_iter_s: 25.984241485595703
  time_total_s: 4377.361395359039
  timers:
    learn_throughput: 8676.227
    learn_time_ms: 18647.737
    sample_throughput: 23782.409
    sample_time_ms: 6803.011
    update_time_ms: 29.572
  timestamp: 1602803340
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    170 |          4377.36 | 27504640 |  274.022 |              308.667 |              136.242 |            796.778 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3106.3962825923572
    time_step_min: 2878
  date: 2020-10-15_23-09-27
  done: false
  episode_len_mean: 796.7515737799596
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 274.1608344490237
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 34630
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0989942297894839e-20
        cur_lr: 5.0e-05
        entropy: 0.09580996632575989
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00659530430129962
        total_loss: .inf
        vf_explained_var: 0.9979896545410156
        vf_loss: 0.9439964046080908
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.551612903225806
    gpu_util_percent0: 0.33096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147274511964716
    mean_env_wait_ms: 1.196082846974759
    mean_inference_ms: 4.330335687326753
    mean_raw_obs_processing_ms: 0.3800369230693947
  time_since_restore: 4403.317297458649
  time_this_iter_s: 25.955902099609375
  time_total_s: 4403.317297458649
  timers:
    learn_throughput: 8668.412
    learn_time_ms: 18664.55
    sample_throughput: 23779.384
    sample_time_ms: 6803.877
    update_time_ms: 29.726
  timestamp: 1602803367
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    171 |          4403.32 | 27666432 |  274.161 |              308.667 |              136.242 |            796.752 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3105.24546289915
    time_step_min: 2878
  date: 2020-10-15_23-09-54
  done: false
  episode_len_mean: 796.714314400459
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 274.33052121907525
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 230
  episodes_total: 34860
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.648491344684226e-20
        cur_lr: 5.0e-05
        entropy: 0.09942664081851642
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035025918623432517
        model: {}
        policy_loss: -0.008159063378116116
        total_loss: 1.4070898195107777
        vf_explained_var: 0.9972860217094421
        vf_loss: 1.4152985711892445
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.760000000000005
    gpu_util_percent0: 0.34233333333333327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726824294996743
    mean_env_wait_ms: 1.1960223514932038
    mean_inference_ms: 4.329964363005992
    mean_raw_obs_processing_ms: 0.38001129317879595
  time_since_restore: 4429.2875163555145
  time_this_iter_s: 25.970218896865845
  time_total_s: 4429.2875163555145
  timers:
    learn_throughput: 8629.524
    learn_time_ms: 18748.658
    sample_throughput: 23810.762
    sample_time_ms: 6794.911
    update_time_ms: 29.486
  timestamp: 1602803394
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    172 |          4429.29 | 27828224 |  274.331 |              308.667 |              136.242 |            796.714 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3104.2591640972937
    time_step_min: 2878
  date: 2020-10-15_23-10-20
  done: false
  episode_len_mean: 796.6915639972622
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 274.4813890098796
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 204
  episodes_total: 35064
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.24245672342113e-21
        cur_lr: 5.0e-05
        entropy: 0.09221001155674458
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007707047264072268
        total_loss: .inf
        vf_explained_var: 0.9981579780578613
        vf_loss: 0.8679824421803156
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.38709677419355
    gpu_util_percent0: 0.2919354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726291968617547
    mean_env_wait_ms: 1.1959731428833453
    mean_inference_ms: 4.329647508434334
    mean_raw_obs_processing_ms: 0.37998975897597487
  time_since_restore: 4455.155145406723
  time_this_iter_s: 25.867629051208496
  time_total_s: 4455.155145406723
  timers:
    learn_throughput: 8625.245
    learn_time_ms: 18757.961
    sample_throughput: 23784.35
    sample_time_ms: 6802.456
    update_time_ms: 29.225
  timestamp: 1602803420
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    173 |          4455.16 | 27990016 |  274.481 |              308.667 |              136.242 |            796.692 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3103.3648318563964
    time_step_min: 2878
  date: 2020-10-15_23-10-47
  done: false
  episode_len_mean: 796.6655033480877
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 274.6150292506266
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 35244
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2363685085131698e-20
        cur_lr: 5.0e-05
        entropy: 0.08844067653020223
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011096715486322259
        total_loss: .inf
        vf_explained_var: 0.9983188509941101
        vf_loss: 0.7760318319002787
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.586666666666666
    gpu_util_percent0: 0.34099999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14725862431126446
    mean_env_wait_ms: 1.1959260234641884
    mean_inference_ms: 4.3293656382262515
    mean_raw_obs_processing_ms: 0.37997087642668553
  time_since_restore: 4481.297832489014
  time_this_iter_s: 26.14268708229065
  time_total_s: 4481.297832489014
  timers:
    learn_throughput: 8604.294
    learn_time_ms: 18803.634
    sample_throughput: 23781.344
    sample_time_ms: 6803.316
    update_time_ms: 30.997
  timestamp: 1602803447
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    174 |           4481.3 | 28151808 |  274.615 |              308.667 |              136.242 |            796.666 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3102.3235277511153
    time_step_min: 2878
  date: 2020-10-15_23-11-14
  done: false
  episode_len_mean: 796.6562975915167
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 274.7721059657434
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 35458
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.854552762769754e-20
        cur_lr: 5.0e-05
        entropy: 0.08911866260071595
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006620506300047661
        total_loss: .inf
        vf_explained_var: 0.9981357455253601
        vf_loss: 0.968013400832812
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.653333333333332
    gpu_util_percent0: 0.3276666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14725325272218434
    mean_env_wait_ms: 1.1958709613215688
    mean_inference_ms: 4.3290419110282015
    mean_raw_obs_processing_ms: 0.3799500719822978
  time_since_restore: 4507.017686367035
  time_this_iter_s: 25.71985387802124
  time_total_s: 4507.017686367035
  timers:
    learn_throughput: 8612.773
    learn_time_ms: 18785.122
    sample_throughput: 23789.26
    sample_time_ms: 6801.052
    update_time_ms: 31.655
  timestamp: 1602803474
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    175 |          4507.02 | 28313600 |  274.772 |              308.667 |              136.242 |            796.656 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3101.224862529458
    time_step_min: 2878
  date: 2020-10-15_23-11-40
  done: false
  episode_len_mean: 796.653783632287
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 274.93789492458194
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 222
  episodes_total: 35680
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7818291441546316e-20
        cur_lr: 5.0e-05
        entropy: 0.09040277451276779
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007517613870732021
        total_loss: .inf
        vf_explained_var: 0.9984381794929504
        vf_loss: 0.7853374828894933
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.723333333333336
    gpu_util_percent0: 0.30999999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724687483420978
    mean_env_wait_ms: 1.1958153264899687
    mean_inference_ms: 4.328699406351791
    mean_raw_obs_processing_ms: 0.37992438935423917
  time_since_restore: 4532.654752969742
  time_this_iter_s: 25.63706660270691
  time_total_s: 4532.654752969742
  timers:
    learn_throughput: 8617.691
    learn_time_ms: 18774.403
    sample_throughput: 23777.844
    sample_time_ms: 6804.317
    update_time_ms: 29.979
  timestamp: 1602803500
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:11:41,184	WARNING util.py:136 -- The `process_trial` operation took 0.5132884979248047 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    176 |          4532.65 | 28475392 |  274.938 |              308.667 |              136.242 |            796.654 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3100.3897401546233
    time_step_min: 2878
  date: 2020-10-15_23-12-07
  done: false
  episode_len_mean: 796.6446396208003
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.0658868061626
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 35865
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.172743716231948e-20
        cur_lr: 5.0e-05
        entropy: 0.08845291286706924
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033800359621333578
        model: {}
        policy_loss: -0.0095252685326462
        total_loss: 1.2843176225821178
        vf_explained_var: 0.9972023367881775
        vf_loss: 1.2938871284325917
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490322580645167
    gpu_util_percent0: 0.34516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724299550483017
    mean_env_wait_ms: 1.1957678927303448
    mean_inference_ms: 4.32843350222489
    mean_raw_obs_processing_ms: 0.37990732924098886
  time_since_restore: 4558.676270723343
  time_this_iter_s: 26.021517753601074
  time_total_s: 4558.676270723343
  timers:
    learn_throughput: 8601.161
    learn_time_ms: 18810.483
    sample_throughput: 23718.646
    sample_time_ms: 6821.3
    update_time_ms: 30.138
  timestamp: 1602803527
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    177 |          4558.68 | 28637184 |  275.066 |              308.667 |              136.242 |            796.645 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3099.4815524277506
    time_step_min: 2878
  date: 2020-10-15_23-12-33
  done: false
  episode_len_mean: 796.6431206145825
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.19675665045474
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 192
  episodes_total: 36057
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.086371858115974e-20
        cur_lr: 5.0e-05
        entropy: 0.08787758958836396
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006260714745925118
        total_loss: .inf
        vf_explained_var: 0.9969832301139832
        vf_loss: 1.4927639067173004
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.203333333333337
    gpu_util_percent0: 0.341
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14723848619123878
    mean_env_wait_ms: 1.1957169940279098
    mean_inference_ms: 4.3281467811402665
    mean_raw_obs_processing_ms: 0.3798884879381944
  time_since_restore: 4584.508479833603
  time_this_iter_s: 25.83220911026001
  time_total_s: 4584.508479833603
  timers:
    learn_throughput: 8591.082
    learn_time_ms: 18832.553
    sample_throughput: 23739.993
    sample_time_ms: 6815.166
    update_time_ms: 32.277
  timestamp: 1602803553
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:12:34,411	WARNING util.py:136 -- The `process_trial` operation took 0.5240333080291748 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    178 |          4584.51 | 28798976 |  275.197 |              308.667 |              136.242 |            796.643 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3098.3958839108363
    time_step_min: 2878
  date: 2020-10-15_23-13-00
  done: false
  episode_len_mean: 796.6577554845111
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.36263082818016
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 227
  episodes_total: 36284
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.12955778717396e-20
        cur_lr: 5.0e-05
        entropy: 0.08336165795723598
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008370342974861463
        total_loss: .inf
        vf_explained_var: 0.9984498023986816
        vf_loss: 0.7991427580515543
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.474193548387102
    gpu_util_percent0: 0.30709677419354836
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14723242808074932
    mean_env_wait_ms: 1.1956562123260508
    mean_inference_ms: 4.327805870308728
    mean_raw_obs_processing_ms: 0.37986451400076815
  time_since_restore: 4610.2867159843445
  time_this_iter_s: 25.778236150741577
  time_total_s: 4610.2867159843445
  timers:
    learn_throughput: 8597.031
    learn_time_ms: 18819.519
    sample_throughput: 23739.067
    sample_time_ms: 6815.432
    update_time_ms: 33.811
  timestamp: 1602803580
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    179 |          4610.29 | 28960768 |  275.363 |              308.667 |              136.242 |            796.658 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.540010424382
    time_step_min: 2878
  date: 2020-10-15_23-13-26
  done: false
  episode_len_mean: 796.6537586669955
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.4740025428998
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 205
  episodes_total: 36489
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6943366807609416e-20
        cur_lr: 5.0e-05
        entropy: 0.1215192408611377
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055158498386542005
        model: {}
        policy_loss: -0.010049348813481629
        total_loss: 2.8464152018229165
        vf_explained_var: 0.9940297603607178
        vf_loss: 2.856525242328644
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85666666666667
    gpu_util_percent0: 0.339
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722765340628757
    mean_env_wait_ms: 1.195603597353258
    mean_inference_ms: 4.327515191643572
    mean_raw_obs_processing_ms: 0.37984388771689703
  time_since_restore: 4635.975170373917
  time_this_iter_s: 25.688454389572144
  time_total_s: 4635.975170373917
  timers:
    learn_throughput: 8612.013
    learn_time_ms: 18786.781
    sample_throughput: 23763.613
    sample_time_ms: 6808.392
    update_time_ms: 32.613
  timestamp: 1602803606
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    180 |          4635.98 | 29122560 |  275.474 |              308.667 |              136.242 |            796.654 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.6557663436606
    time_step_min: 2878
  date: 2020-10-15_23-13-53
  done: false
  episode_len_mean: 796.575359275722
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.4494353146692
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 36671
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6943366807609416e-20
        cur_lr: 5.0e-05
        entropy: 0.1764087143043677
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01086607575416565
        total_loss: .inf
        vf_explained_var: 0.9862727522850037
        vf_loss: 6.77450168132782
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87666666666667
    gpu_util_percent0: 0.369
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722333906762042
    mean_env_wait_ms: 1.195556577542284
    mean_inference_ms: 4.3272510602355005
    mean_raw_obs_processing_ms: 0.37982575844298766
  time_since_restore: 4661.720217704773
  time_this_iter_s: 25.745047330856323
  time_total_s: 4661.720217704773
  timers:
    learn_throughput: 8620.597
    learn_time_ms: 18768.074
    sample_throughput: 23747.625
    sample_time_ms: 6812.976
    update_time_ms: 33.01
  timestamp: 1602803633
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:13:53,698	WARNING util.py:136 -- The `process_trial` operation took 0.5063080787658691 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    181 |          4661.72 | 29284352 |  275.449 |              308.667 |              136.242 |            796.575 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.8733754781997
    time_step_min: 2878
  date: 2020-10-15_23-14-19
  done: false
  episode_len_mean: 796.4963272165452
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.42522068323694
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 222
  episodes_total: 36893
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.04150502114141e-20
        cur_lr: 5.0e-05
        entropy: 0.1795498157540957
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009575341556531688
        total_loss: .inf
        vf_explained_var: 0.9863112568855286
        vf_loss: 7.587931513786316
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.103333333333335
    gpu_util_percent0: 0.30866666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472179736479484
    mean_env_wait_ms: 1.1954999938039654
    mean_inference_ms: 4.32693462918893
    mean_raw_obs_processing_ms: 0.37980494486347516
  time_since_restore: 4687.648468971252
  time_this_iter_s: 25.928251266479492
  time_total_s: 4687.648468971252
  timers:
    learn_throughput: 8629.475
    learn_time_ms: 18748.765
    sample_throughput: 23702.266
    sample_time_ms: 6826.014
    update_time_ms: 33.9
  timestamp: 1602803659
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    182 |          4687.65 | 29446144 |  275.425 |              308.667 |              136.242 |            796.496 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.8876122663646
    time_step_min: 2878
  date: 2020-10-15_23-14-46
  done: false
  episode_len_mean: 796.4053566135856
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.42517814144986
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 37113
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0562257531712114e-19
        cur_lr: 5.0e-05
        entropy: 0.16530410572886467
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012764033356991908
        total_loss: .inf
        vf_explained_var: 0.9896092414855957
        vf_loss: 5.3135303656260175
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.779999999999998
    gpu_util_percent0: 0.3353333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721255320522775
    mean_env_wait_ms: 1.1954461581686964
    mean_inference_ms: 4.3266246459784945
    mean_raw_obs_processing_ms: 0.37978240790839213
  time_since_restore: 4713.297853946686
  time_this_iter_s: 25.64938497543335
  time_total_s: 4713.297853946686
  timers:
    learn_throughput: 8637.151
    learn_time_ms: 18732.103
    sample_throughput: 23725.504
    sample_time_ms: 6819.328
    update_time_ms: 35.052
  timestamp: 1602803686
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:14:46,918	WARNING util.py:136 -- The `process_trial` operation took 0.5230975151062012 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    183 |           4713.3 | 29607936 |  275.425 |              308.667 |              136.242 |            796.405 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.878979011219
    time_step_min: 2878
  date: 2020-10-15_23-15-12
  done: false
  episode_len_mean: 796.3412345149354
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.4276372346838
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 181
  episodes_total: 37294
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5843386297568174e-19
        cur_lr: 5.0e-05
        entropy: 0.16516077145934105
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011450072537021091
        total_loss: .inf
        vf_explained_var: 0.9870348572731018
        vf_loss: 6.085322539011638
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.212903225806453
    gpu_util_percent0: 0.3632258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14720885597793085
    mean_env_wait_ms: 1.1954015917349607
    mean_inference_ms: 4.32637810171812
    mean_raw_obs_processing_ms: 0.37976560140125365
  time_since_restore: 4738.897903680801
  time_this_iter_s: 25.6000497341156
  time_total_s: 4738.897903680801
  timers:
    learn_throughput: 8662.099
    learn_time_ms: 18678.152
    sample_throughput: 23723.755
    sample_time_ms: 6819.831
    update_time_ms: 33.391
  timestamp: 1602803712
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:15:13,414	WARNING util.py:136 -- The `process_trial` operation took 0.5556156635284424 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    184 |           4738.9 | 29769728 |  275.428 |              308.667 |              136.242 |            796.341 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.8489151031517
    time_step_min: 2878
  date: 2020-10-15_23-15-39
  done: false
  episode_len_mean: 796.2538594854019
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.44226022388915
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 211
  episodes_total: 37505
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3765079446352257e-19
        cur_lr: 5.0e-05
        entropy: 0.16653640444080034
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012244668641263464
        total_loss: .inf
        vf_explained_var: 0.9887943267822266
        vf_loss: 5.838082750638326
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.953333333333337
    gpu_util_percent0: 0.343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472038536727769
    mean_env_wait_ms: 1.1953511068107885
    mean_inference_ms: 4.326093471671564
    mean_raw_obs_processing_ms: 0.37974651932793924
  time_since_restore: 4764.809495925903
  time_this_iter_s: 25.91159224510193
  time_total_s: 4764.809495925903
  timers:
    learn_throughput: 8656.092
    learn_time_ms: 18691.114
    sample_throughput: 23703.62
    sample_time_ms: 6825.624
    update_time_ms: 32.242
  timestamp: 1602803739
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:15:40,064	WARNING util.py:136 -- The `process_trial` operation took 0.5523622035980225 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    185 |          4764.81 | 29931520 |  275.442 |              308.667 |              136.242 |            796.254 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.658372901138
    time_step_min: 2878
  date: 2020-10-15_23-16-05
  done: false
  episode_len_mean: 796.1904862859415
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.47200774138605
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 230
  episodes_total: 37735
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5647619169528383e-19
        cur_lr: 5.0e-05
        entropy: 0.15372013176480928
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009687046753242612
        total_loss: .inf
        vf_explained_var: 0.9920430183410645
        vf_loss: 4.213468670845032
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60666666666667
    gpu_util_percent0: 0.35433333333333344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719796181997133
    mean_env_wait_ms: 1.195295464408992
    mean_inference_ms: 4.325767626381704
    mean_raw_obs_processing_ms: 0.3797232587976135
  time_since_restore: 4790.49915266037
  time_this_iter_s: 25.689656734466553
  time_total_s: 4790.49915266037
  timers:
    learn_throughput: 8650.966
    learn_time_ms: 18702.189
    sample_throughput: 23735.648
    sample_time_ms: 6816.414
    update_time_ms: 34.412
  timestamp: 1602803765
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:16:06,458	WARNING util.py:136 -- The `process_trial` operation took 0.5246827602386475 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    186 |           4790.5 | 30093312 |  275.472 |              308.667 |              136.242 |             796.19 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.483067176983
    time_step_min: 2878
  date: 2020-10-15_23-16-32
  done: false
  episode_len_mean: 796.1417420426676
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.50085065203314
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 37921
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.34714287542926e-19
        cur_lr: 5.0e-05
        entropy: 0.15077397599816322
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011901797033109082
        total_loss: .inf
        vf_explained_var: 0.9898365139961243
        vf_loss: 4.745938579241435
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.766666666666662
    gpu_util_percent0: 0.375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719451407506087
    mean_env_wait_ms: 1.1952537957810565
    mean_inference_ms: 4.325530022739563
    mean_raw_obs_processing_ms: 0.3797075491911032
  time_since_restore: 4816.050887107849
  time_this_iter_s: 25.551734447479248
  time_total_s: 4816.050887107849
  timers:
    learn_throughput: 8665.332
    learn_time_ms: 18671.183
    sample_throughput: 23799.033
    sample_time_ms: 6798.259
    update_time_ms: 35.864
  timestamp: 1602803792
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:16:32,706	WARNING util.py:136 -- The `process_trial` operation took 0.5233118534088135 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    187 |          4816.05 | 30255104 |  275.501 |              308.667 |              136.242 |            796.142 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.2734744249556
    time_step_min: 2878
  date: 2020-10-15_23-16-58
  done: false
  episode_len_mean: 796.0964060860441
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.5338126808481
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 38120
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.020714313143888e-19
        cur_lr: 5.0e-05
        entropy: 0.15971124172210693
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011097070596103245
        total_loss: .inf
        vf_explained_var: 0.9930381774902344
        vf_loss: 3.4501381317774453
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.966666666666665
    gpu_util_percent0: 0.33666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718985467596615
    mean_env_wait_ms: 1.1952043116847662
    mean_inference_ms: 4.325260510415839
    mean_raw_obs_processing_ms: 0.3796895861375227
  time_since_restore: 4841.64742231369
  time_this_iter_s: 25.596535205841064
  time_total_s: 4841.64742231369
  timers:
    learn_throughput: 8680.673
    learn_time_ms: 18638.187
    sample_throughput: 23763.245
    sample_time_ms: 6808.498
    update_time_ms: 34.013
  timestamp: 1602803818
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:16:59,022	WARNING util.py:136 -- The `process_trial` operation took 0.5377871990203857 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    188 |          4841.65 | 30416896 |  275.534 |              308.667 |              136.242 |            796.096 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3097.0227094753327
    time_step_min: 2878
  date: 2020-10-15_23-17-24
  done: false
  episode_len_mean: 796.0610493923747
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.57851318694674
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 226
  episodes_total: 38346
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2031071469715833e-18
        cur_lr: 5.0e-05
        entropy: 0.15382454668482146
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012321851002828529
        total_loss: .inf
        vf_explained_var: 0.9940665364265442
        vf_loss: 3.2117744088172913
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.149999999999995
    gpu_util_percent0: 0.4063333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718456756980172
    mean_env_wait_ms: 1.1951543514548004
    mean_inference_ms: 4.324966927009049
    mean_raw_obs_processing_ms: 0.37966850648706746
  time_since_restore: 4867.555958032608
  time_this_iter_s: 25.908535718917847
  time_total_s: 4867.555958032608
  timers:
    learn_throughput: 8673.036
    learn_time_ms: 18654.598
    sample_throughput: 23745.62
    sample_time_ms: 6813.551
    update_time_ms: 33.78
  timestamp: 1602803844
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:17:25,699	WARNING util.py:136 -- The `process_trial` operation took 0.5311577320098877 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    189 |          4867.56 | 30578688 |  275.579 |              308.667 |              136.242 |            796.061 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3096.747065544822
    time_step_min: 2878
  date: 2020-10-15_23-17-51
  done: false
  episode_len_mean: 796.0275529265256
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.62036460495347
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 198
  episodes_total: 38544
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8046607204573752e-18
        cur_lr: 5.0e-05
        entropy: 0.14233806605140367
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010294362475785116
        total_loss: .inf
        vf_explained_var: 0.9944920539855957
        vf_loss: 2.661381800969442
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7741935483871
    gpu_util_percent0: 0.3216129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718033927937418
    mean_env_wait_ms: 1.1951078739059087
    mean_inference_ms: 4.32470569215186
    mean_raw_obs_processing_ms: 0.37965126298312846
  time_since_restore: 4893.329012393951
  time_this_iter_s: 25.773054361343384
  time_total_s: 4893.329012393951
  timers:
    learn_throughput: 8661.466
    learn_time_ms: 18679.517
    sample_throughput: 23777.596
    sample_time_ms: 6804.389
    update_time_ms: 35.096
  timestamp: 1602803871
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:17:52,158	WARNING util.py:136 -- The `process_trial` operation took 0.5045123100280762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    190 |          4893.33 | 30740480 |   275.62 |              308.667 |              136.242 |            796.028 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3096.5097441199277
    time_step_min: 2878
  date: 2020-10-15_23-18-17
  done: false
  episode_len_mean: 796.0198316376594
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.66155617007735
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 38726
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.706991080686062e-18
        cur_lr: 5.0e-05
        entropy: 0.14669796576102576
        entropy_coeff: 0.0005000000000000001
        kl: 0.004598261594461898
        model: {}
        policy_loss: -0.010499564775576195
        total_loss: 2.8801243702570596
        vf_explained_var: 0.9939906597137451
        vf_loss: 2.8906973203023276
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05
    gpu_util_percent0: 0.365
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471764419891624
    mean_env_wait_ms: 1.1950650632637707
    mean_inference_ms: 4.324460677469915
    mean_raw_obs_processing_ms: 0.37963453894695504
  time_since_restore: 4919.010349988937
  time_this_iter_s: 25.681337594985962
  time_total_s: 4919.010349988937
  timers:
    learn_throughput: 8661.04
    learn_time_ms: 18680.435
    sample_throughput: 23799.294
    sample_time_ms: 6798.185
    update_time_ms: 32.909
  timestamp: 1602803897
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: 0f5d2_00000
  
2020-10-15 23:18:18,567	WARNING util.py:136 -- The `process_trial` operation took 0.5398435592651367 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    191 |          4919.01 | 30902272 |  275.662 |              308.667 |              136.242 |             796.02 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3096.1872944078946
    time_step_min: 2878
  date: 2020-10-15_23-18-44
  done: false
  episode_len_mean: 795.9997175721475
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.7077058455562
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 222
  episodes_total: 38948
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.353495540343031e-18
        cur_lr: 5.0e-05
        entropy: 0.14566079527139664
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011244935332797468
        total_loss: .inf
        vf_explained_var: 0.9938663840293884
        vf_loss: 3.3823436498641968
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.206666666666667
    gpu_util_percent0: 0.3193333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717174551638673
    mean_env_wait_ms: 1.1950153742423335
    mean_inference_ms: 4.324191261907887
    mean_raw_obs_processing_ms: 0.37961625162522944
  time_since_restore: 4944.639577150345
  time_this_iter_s: 25.62922716140747
  time_total_s: 4944.639577150345
  timers:
    learn_throughput: 8671.037
    learn_time_ms: 18658.898
    sample_throughput: 23830.138
    sample_time_ms: 6789.386
    update_time_ms: 32.943
  timestamp: 1602803924
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:18:44,927	WARNING util.py:136 -- The `process_trial` operation took 0.5520076751708984 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    192 |          4944.64 | 31064064 |  275.708 |              308.667 |              136.242 |                796 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3095.798558761148
    time_step_min: 2878
  date: 2020-10-15_23-19-10
  done: false
  episode_len_mean: 795.9931323240318
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.76642165224956
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 221
  episodes_total: 39169
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.030243310514547e-18
        cur_lr: 5.0e-05
        entropy: 0.13864916935563087
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011593570310651558
        total_loss: .inf
        vf_explained_var: 0.9962298274040222
        vf_loss: 1.9282147387663524
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05806451612903
    gpu_util_percent0: 0.2967741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716657779207837
    mean_env_wait_ms: 1.1949630273001484
    mean_inference_ms: 4.323900608681289
    mean_raw_obs_processing_ms: 0.3795953129059306
  time_since_restore: 4970.397428750992
  time_this_iter_s: 25.757851600646973
  time_total_s: 4970.397428750992
  timers:
    learn_throughput: 8665.23
    learn_time_ms: 18671.404
    sample_throughput: 23831.78
    sample_time_ms: 6788.918
    update_time_ms: 30.28
  timestamp: 1602803950
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:19:11,623	WARNING util.py:136 -- The `process_trial` operation took 0.5989553928375244 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    193 |           4970.4 | 31225856 |  275.766 |              308.667 |              136.242 |            795.993 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3095.5181887560416
    time_step_min: 2878
  date: 2020-10-15_23-19-36
  done: false
  episode_len_mean: 795.9866568393229
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.7983954319793
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 177
  episodes_total: 39346
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0453649657718204e-18
        cur_lr: 5.0e-05
        entropy: 0.16154726718862852
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010305531318105446
        total_loss: .inf
        vf_explained_var: 0.9913549423217773
        vf_loss: 3.826459765434265
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.200000000000003
    gpu_util_percent0: 0.36103448275862066
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716303913914366
    mean_env_wait_ms: 1.1949223322818432
    mean_inference_ms: 4.32368199005452
    mean_raw_obs_processing_ms: 0.37958077850830846
  time_since_restore: 4995.76945567131
  time_this_iter_s: 25.372026920318604
  time_total_s: 4995.76945567131
  timers:
    learn_throughput: 8678.104
    learn_time_ms: 18643.704
    sample_throughput: 23823.248
    sample_time_ms: 6791.349
    update_time_ms: 31.902
  timestamp: 1602803976
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:19:37,776	WARNING util.py:136 -- The `process_trial` operation took 0.536576509475708 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    194 |          4995.77 | 31387648 |  275.798 |              308.667 |              136.242 |            795.987 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3095.7097917141195
    time_step_min: 2878
  date: 2020-10-15_23-20-03
  done: false
  episode_len_mean: 795.955472957597
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.7522957711835
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 203
  episodes_total: 39549
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.56804744865773e-18
        cur_lr: 5.0e-05
        entropy: 0.16170183445016542
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011224135091955153
        total_loss: .inf
        vf_explained_var: 0.9879506230354309
        vf_loss: 6.5325154066085815
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.206666666666663
    gpu_util_percent0: 0.352
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715888111752543
    mean_env_wait_ms: 1.1948755957075519
    mean_inference_ms: 4.323428212716264
    mean_raw_obs_processing_ms: 0.37956394188775816
  time_since_restore: 5021.302660942078
  time_this_iter_s: 25.533205270767212
  time_total_s: 5021.302660942078
  timers:
    learn_throughput: 8696.025
    learn_time_ms: 18605.281
    sample_throughput: 23826.952
    sample_time_ms: 6790.294
    update_time_ms: 33.182
  timestamp: 1602804003
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:20:04,130	WARNING util.py:136 -- The `process_trial` operation took 0.5799863338470459 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    195 |           5021.3 | 31549440 |  275.752 |              308.667 |              136.242 |            795.955 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3095.496917696198
    time_step_min: 2878
  date: 2020-10-15_23-20-29
  done: false
  episode_len_mean: 795.942306241987
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.7997740546822
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 230
  episodes_total: 39779
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.852071172986594e-18
        cur_lr: 5.0e-05
        entropy: 0.14044371247291565
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012690583079044396
        total_loss: .inf
        vf_explained_var: 0.9952113628387451
        vf_loss: 2.5706986586252847
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.416129032258066
    gpu_util_percent0: 0.3103225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715369960663505
    mean_env_wait_ms: 1.194823818045834
    mean_inference_ms: 4.323140889255506
    mean_raw_obs_processing_ms: 0.37954250162244546
  time_since_restore: 5047.12704873085
  time_this_iter_s: 25.824387788772583
  time_total_s: 5047.12704873085
  timers:
    learn_throughput: 8688.713
    learn_time_ms: 18620.94
    sample_throughput: 23834.111
    sample_time_ms: 6788.254
    update_time_ms: 32.469
  timestamp: 1602804029
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:20:30,716	WARNING util.py:136 -- The `process_trial` operation took 0.5761415958404541 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    196 |          5047.13 | 31711232 |    275.8 |              308.667 |              136.242 |            795.942 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3095.130378161783
    time_step_min: 2878
  date: 2020-10-15_23-20-56
  done: false
  episode_len_mean: 795.9542361006855
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.8566304085744
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 39966
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0278106759479891e-17
        cur_lr: 5.0e-05
        entropy: 0.1332553761700789
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010098111650828892
        total_loss: .inf
        vf_explained_var: 0.9964447617530823
        vf_loss: 1.6559051473935444
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.19333333333333
    gpu_util_percent0: 0.3726666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715015391137248
    mean_env_wait_ms: 1.1947835815753067
    mean_inference_ms: 4.322924811536532
    mean_raw_obs_processing_ms: 0.37952851997901066
  time_since_restore: 5073.069035053253
  time_this_iter_s: 25.941986322402954
  time_total_s: 5073.069035053253
  timers:
    learn_throughput: 8678.0
    learn_time_ms: 18643.927
    sample_throughput: 23782.979
    sample_time_ms: 6802.848
    update_time_ms: 32.944
  timestamp: 1602804056
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:20:57,386	WARNING util.py:136 -- The `process_trial` operation took 0.5442962646484375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    197 |          5073.07 | 31873024 |  275.857 |              308.667 |              136.242 |            795.954 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3094.822106995064
    time_step_min: 2878
  date: 2020-10-15_23-21-22
  done: false
  episode_len_mean: 795.9578331257783
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.90789086380596
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 40150
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5417160139219836e-17
        cur_lr: 5.0e-05
        entropy: 0.13797963658968607
        entropy_coeff: 0.0005000000000000001
        kl: 0.00457917603974541
        model: {}
        policy_loss: -0.012236171115849478
        total_loss: 1.9869009653727214
        vf_explained_var: 0.9958085417747498
        vf_loss: 1.9992060661315918
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.156666666666673
    gpu_util_percent0: 0.276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714644709797448
    mean_env_wait_ms: 1.1947420099038606
    mean_inference_ms: 4.3226988912659055
    mean_raw_obs_processing_ms: 0.379512920082197
  time_since_restore: 5098.604620933533
  time_this_iter_s: 25.53558588027954
  time_total_s: 5098.604620933533
  timers:
    learn_throughput: 8674.734
    learn_time_ms: 18650.947
    sample_throughput: 23835.359
    sample_time_ms: 6787.899
    update_time_ms: 33.083
  timestamp: 1602804082
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: 0f5d2_00000
  
2020-10-15 23:21:23,701	WARNING util.py:136 -- The `process_trial` operation took 0.5353090763092041 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    198 |           5098.6 | 32034816 |  275.908 |              308.667 |              136.242 |            795.958 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3094.4088204670534
    time_step_min: 2878
  date: 2020-10-15_23-21-49
  done: false
  episode_len_mean: 795.9631445980086
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 275.97065468175566
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 224
  episodes_total: 40374
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.708580069609918e-18
        cur_lr: 5.0e-05
        entropy: 0.1368178017437458
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00990507450963681
        total_loss: .inf
        vf_explained_var: 0.9960086941719055
        vf_loss: 2.1646995743115744
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90666666666667
    gpu_util_percent0: 0.302
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714216933483404
    mean_env_wait_ms: 1.1946930168218612
    mean_inference_ms: 4.32244444494892
    mean_raw_obs_processing_ms: 0.37949584299730144
  time_since_restore: 5124.144926548004
  time_this_iter_s: 25.540305614471436
  time_total_s: 5124.144926548004
  timers:
    learn_throughput: 8688.058
    learn_time_ms: 18622.344
    sample_throughput: 23860.473
    sample_time_ms: 6780.754
    update_time_ms: 31.712
  timestamp: 1602804109
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:21:50,127	WARNING util.py:136 -- The `process_trial` operation took 0.6073992252349854 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    199 |          5124.14 | 32196608 |  275.971 |              308.667 |              136.242 |            795.963 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3093.9959559095505
    time_step_min: 2878
  date: 2020-10-15_23-22-15
  done: false
  episode_len_mean: 795.9866712656138
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.02871878259276
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 40589
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1562870104414877e-17
        cur_lr: 5.0e-05
        entropy: 0.12435295060276985
        entropy_coeff: 0.0005000000000000001
        kl: 0.003951283249383171
        model: {}
        policy_loss: -0.008352366149968779
        total_loss: 2.3058287103970847
        vf_explained_var: 0.9955997467041016
        vf_loss: 2.314243217309316
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.22903225806452
    gpu_util_percent0: 0.2925806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713738380947564
    mean_env_wait_ms: 1.1946431386879584
    mean_inference_ms: 4.322177466687285
    mean_raw_obs_processing_ms: 0.37947682170890457
  time_since_restore: 5149.755882740021
  time_this_iter_s: 25.6109561920166
  time_total_s: 5149.755882740021
  timers:
    learn_throughput: 8694.02
    learn_time_ms: 18609.574
    sample_throughput: 23869.442
    sample_time_ms: 6778.206
    update_time_ms: 31.26
  timestamp: 1602804135
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: 0f5d2_00000
  
2020-10-15 23:22:16,678	WARNING util.py:136 -- The `process_trial` operation took 0.5756490230560303 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    200 |          5149.76 | 32358400 |  276.029 |              308.667 |              136.242 |            795.987 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3093.631589285276
    time_step_min: 2878
  date: 2020-10-15_23-22-42
  done: false
  episode_len_mean: 796.0103274868146
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.0817286070565
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 176
  episodes_total: 40765
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.7814350522074385e-18
        cur_lr: 5.0e-05
        entropy: 0.1245158767948548
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00993769922448943
        total_loss: .inf
        vf_explained_var: 0.9965621829032898
        vf_loss: 1.6316760281721752
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.020000000000003
    gpu_util_percent0: 0.316
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713422096463508
    mean_env_wait_ms: 1.1946023176420149
    mean_inference_ms: 4.321977120488663
    mean_raw_obs_processing_ms: 0.3794631739017203
  time_since_restore: 5175.503031492233
  time_this_iter_s: 25.747148752212524
  time_total_s: 5175.503031492233
  timers:
    learn_throughput: 8692.842
    learn_time_ms: 18612.095
    sample_throughput: 23857.605
    sample_time_ms: 6781.569
    update_time_ms: 31.269
  timestamp: 1602804162
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:22:43,202	WARNING util.py:136 -- The `process_trial` operation took 0.5855896472930908 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    201 |           5175.5 | 32520192 |  276.082 |              308.667 |              136.242 |             796.01 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3093.2185247623934
    time_step_min: 2878
  date: 2020-10-15_23-23-08
  done: false
  episode_len_mean: 796.0509703405346
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.1413401684363
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 200
  episodes_total: 40965
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.672152578311157e-18
        cur_lr: 5.0e-05
        entropy: 0.12911183262864748
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008888039349888762
        total_loss: .inf
        vf_explained_var: 0.9968311786651611
        vf_loss: 1.6553969482580821
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.91
    gpu_util_percent0: 0.358
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713035601583319
    mean_env_wait_ms: 1.1945566829163736
    mean_inference_ms: 4.321744202353877
    mean_raw_obs_processing_ms: 0.3794477285895358
  time_since_restore: 5201.081145763397
  time_this_iter_s: 25.57811427116394
  time_total_s: 5201.081145763397
  timers:
    learn_throughput: 8695.793
    learn_time_ms: 18605.779
    sample_throughput: 23852.304
    sample_time_ms: 6783.076
    update_time_ms: 29.02
  timestamp: 1602804188
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:23:09,502	WARNING util.py:136 -- The `process_trial` operation took 0.529310941696167 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    202 |          5201.08 | 32681984 |  276.141 |              308.667 |              136.242 |            796.051 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3092.706721423017
    time_step_min: 2878
  date: 2020-10-15_23-23-34
  done: false
  episode_len_mean: 796.103816645625
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.21978599238946
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 223
  episodes_total: 41188
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3008228867466733e-17
        cur_lr: 5.0e-05
        entropy: 0.11946260494490464
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010241461995368203
        total_loss: .inf
        vf_explained_var: 0.9968448281288147
        vf_loss: 1.7420690457026164
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793333333333337
    gpu_util_percent0: 0.36833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712588491813566
    mean_env_wait_ms: 1.1945054339881473
    mean_inference_ms: 4.321493075875852
    mean_raw_obs_processing_ms: 0.3794295823751803
  time_since_restore: 5226.325080871582
  time_this_iter_s: 25.243935108184814
  time_total_s: 5226.325080871582
  timers:
    learn_throughput: 8725.174
    learn_time_ms: 18543.126
    sample_throughput: 23849.391
    sample_time_ms: 6783.905
    update_time_ms: 31.564
  timestamp: 1602804214
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:23:35,502	WARNING util.py:136 -- The `process_trial` operation took 0.5614032745361328 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    203 |          5226.33 | 32843776 |   276.22 |              308.667 |              136.242 |            796.104 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3092.2189170233864
    time_step_min: 2878
  date: 2020-10-15_23-24-01
  done: false
  episode_len_mean: 796.1570134106561
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.29546278295805
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 197
  episodes_total: 41385
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.951234330120011e-17
        cur_lr: 5.0e-05
        entropy: 0.10960035460690658
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009312246169429272
        total_loss: .inf
        vf_explained_var: 0.996421754360199
        vf_loss: 1.7765213251113892
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94666666666667
    gpu_util_percent0: 0.3666666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471220469450726
    mean_env_wait_ms: 1.194458732267013
    mean_inference_ms: 4.321265010962232
    mean_raw_obs_processing_ms: 0.37941390696605237
  time_since_restore: 5252.302016019821
  time_this_iter_s: 25.976935148239136
  time_total_s: 5252.302016019821
  timers:
    learn_throughput: 8706.199
    learn_time_ms: 18583.54
    sample_throughput: 23781.115
    sample_time_ms: 6803.382
    update_time_ms: 31.286
  timestamp: 1602804241
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:24:02,322	WARNING util.py:136 -- The `process_trial` operation took 0.5725500583648682 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    204 |           5252.3 | 33005568 |  276.295 |              308.667 |              136.242 |            796.157 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3091.730633082091
    time_step_min: 2878
  date: 2020-10-15_23-24-28
  done: false
  episode_len_mean: 796.2036907826673
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.37030240328824
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 178
  episodes_total: 41563
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9268514951800156e-17
        cur_lr: 5.0e-05
        entropy: 0.10380984904865424
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008878714642681492
        total_loss: .inf
        vf_explained_var: 0.9969117641448975
        vf_loss: 1.5097226401170094
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.080000000000005
    gpu_util_percent0: 0.3496666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711882965255546
    mean_env_wait_ms: 1.1944163589391803
    mean_inference_ms: 4.321064479006313
    mean_raw_obs_processing_ms: 0.3794003593896544
  time_since_restore: 5278.057080745697
  time_this_iter_s: 25.755064725875854
  time_total_s: 5278.057080745697
  timers:
    learn_throughput: 8695.641
    learn_time_ms: 18606.104
    sample_throughput: 23784.412
    sample_time_ms: 6802.439
    update_time_ms: 30.562
  timestamp: 1602804268
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:24:28,937	WARNING util.py:136 -- The `process_trial` operation took 0.5753695964813232 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    205 |          5278.06 | 33167360 |   276.37 |              308.667 |              136.242 |            796.204 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3091.108752425905
    time_step_min: 2878
  date: 2020-10-15_23-24-54
  done: false
  episode_len_mean: 796.2560744978814
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.46916559848336
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 41773
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3902772427700235e-17
        cur_lr: 5.0e-05
        entropy: 0.10327217976252238
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007523590446605037
        total_loss: .inf
        vf_explained_var: 0.997353732585907
        vf_loss: 1.3866507013638814
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.274193548387103
    gpu_util_percent0: 0.35129032258064513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711492651677965
    mean_env_wait_ms: 1.1943703908678152
    mean_inference_ms: 4.32083585131416
    mean_raw_obs_processing_ms: 0.3793844406390598
  time_since_restore: 5303.733270645142
  time_this_iter_s: 25.67618989944458
  time_total_s: 5303.733270645142
  timers:
    learn_throughput: 8706.681
    learn_time_ms: 18582.512
    sample_throughput: 23779.444
    sample_time_ms: 6803.86
    update_time_ms: 29.09
  timestamp: 1602804294
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:24:55,445	WARNING util.py:136 -- The `process_trial` operation took 0.5526776313781738 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    206 |          5303.73 | 33329152 |  276.469 |              308.667 |              136.242 |            796.256 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3090.4045808804253
    time_step_min: 2878
  date: 2020-10-15_23-25-21
  done: false
  episode_len_mean: 796.3156716595623
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.5735761154995
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 41993
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.585415864155036e-17
        cur_lr: 5.0e-05
        entropy: 0.09733342503507932
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0072333251203720765
        total_loss: .inf
        vf_explained_var: 0.9975937008857727
        vf_loss: 1.3004344701766968
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.776666666666664
    gpu_util_percent0: 0.34966666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711062370033753
    mean_env_wait_ms: 1.194316151795664
    mean_inference_ms: 4.320593689802345
    mean_raw_obs_processing_ms: 0.3793669637504514
  time_since_restore: 5329.420734167099
  time_this_iter_s: 25.687463521957397
  time_total_s: 5329.420734167099
  timers:
    learn_throughput: 8714.204
    learn_time_ms: 18566.468
    sample_throughput: 23815.463
    sample_time_ms: 6793.569
    update_time_ms: 28.76
  timestamp: 1602804321
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:25:21,929	WARNING util.py:136 -- The `process_trial` operation took 0.5983262062072754 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    207 |          5329.42 | 33490944 |  276.574 |              308.667 |              136.242 |            796.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3089.8111667418075
    time_step_min: 2878
  date: 2020-10-15_23-25-47
  done: false
  episode_len_mean: 796.3624315417625
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.66433820650354
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 42179
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.878123796232553e-17
        cur_lr: 5.0e-05
        entropy: 0.09381239116191864
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009472529597890874
        total_loss: .inf
        vf_explained_var: 0.9979057312011719
        vf_loss: 0.9911322693030039
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84666666666667
    gpu_util_percent0: 0.2936666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710733467265638
    mean_env_wait_ms: 1.1942709776968912
    mean_inference_ms: 4.320392638359854
    mean_raw_obs_processing_ms: 0.37935333628092544
  time_since_restore: 5355.207667589188
  time_this_iter_s: 25.786933422088623
  time_total_s: 5355.207667589188
  timers:
    learn_throughput: 8705.461
    learn_time_ms: 18585.115
    sample_throughput: 23799.187
    sample_time_ms: 6798.215
    update_time_ms: 30.145
  timestamp: 1602804347
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:25:48,507	WARNING util.py:136 -- The `process_trial` operation took 0.5906574726104736 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    208 |          5355.21 | 33652736 |  276.664 |              308.667 |              136.242 |            796.362 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3089.2167914766956
    time_step_min: 2878
  date: 2020-10-15_23-26-14
  done: false
  episode_len_mean: 796.4016333467085
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.7510140468101
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 42367
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.481718569434883e-16
        cur_lr: 5.0e-05
        entropy: 0.09832791797816753
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008509648323524743
        total_loss: .inf
        vf_explained_var: 0.9972901940345764
        vf_loss: 1.3657210568586986
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.85483870967742
    gpu_util_percent0: 0.3370967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710397521588064
    mean_env_wait_ms: 1.1942244745072974
    mean_inference_ms: 4.320188905681243
    mean_raw_obs_processing_ms: 0.3793396609871352
  time_since_restore: 5381.039036035538
  time_this_iter_s: 25.831368446350098
  time_total_s: 5381.039036035538
  timers:
    learn_throughput: 8693.202
    learn_time_ms: 18611.324
    sample_throughput: 23797.343
    sample_time_ms: 6798.742
    update_time_ms: 31.312
  timestamp: 1602804374
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:26:15,302	WARNING util.py:136 -- The `process_trial` operation took 0.5976307392120361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    209 |          5381.04 | 33814528 |  276.751 |              308.667 |              136.242 |            796.402 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3088.569964747356
    time_step_min: 2878
  date: 2020-10-15_23-26-41
  done: false
  episode_len_mean: 796.4320668764383
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.85116391928466
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 219
  episodes_total: 42586
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2225778541523246e-16
        cur_lr: 5.0e-05
        entropy: 0.10064264573156834
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039174747265254455
        model: {}
        policy_loss: -0.007902012238143167
        total_loss: 1.4536522030830383
        vf_explained_var: 0.9972909092903137
        vf_loss: 1.4616045554478962
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.75333333333333
    gpu_util_percent0: 0.3356666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710016242054544
    mean_env_wait_ms: 1.1941749132451884
    mean_inference_ms: 4.319965729232062
    mean_raw_obs_processing_ms: 0.3793238516863294
  time_since_restore: 5406.89480471611
  time_this_iter_s: 25.85576868057251
  time_total_s: 5406.89480471611
  timers:
    learn_throughput: 8693.177
    learn_time_ms: 18611.377
    sample_throughput: 23721.773
    sample_time_ms: 6820.401
    update_time_ms: 31.968
  timestamp: 1602804401
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: 0f5d2_00000
  
2020-10-15 23:26:41,991	WARNING util.py:136 -- The `process_trial` operation took 0.6325252056121826 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    210 |          5406.89 | 33976320 |  276.851 |              308.667 |              136.242 |            796.432 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3087.9010850247873
    time_step_min: 2878
  date: 2020-10-15_23-27-07
  done: false
  episode_len_mean: 796.4631542056075
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 276.9553667516282
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 42800
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1112889270761623e-16
        cur_lr: 5.0e-05
        entropy: 0.09108650870621204
        entropy_coeff: 0.0005000000000000001
        kl: 0.003860309526013831
        model: {}
        policy_loss: -0.006886885993784138
        total_loss: 1.3675079544385274
        vf_explained_var: 0.9974039196968079
        vf_loss: 1.374440352121989
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94193548387097
    gpu_util_percent0: 0.3132258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709578373863597
    mean_env_wait_ms: 1.1941206909242243
    mean_inference_ms: 4.319727542377684
    mean_raw_obs_processing_ms: 0.37930644655548257
  time_since_restore: 5432.887850999832
  time_this_iter_s: 25.993046283721924
  time_total_s: 5432.887850999832
  timers:
    learn_throughput: 8688.576
    learn_time_ms: 18621.234
    sample_throughput: 23706.137
    sample_time_ms: 6824.899
    update_time_ms: 32.492
  timestamp: 1602804427
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: 0f5d2_00000
  
2020-10-15 23:27:08,813	WARNING util.py:136 -- The `process_trial` operation took 0.631140947341919 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    211 |          5432.89 | 34138112 |  276.955 |              308.667 |              136.242 |            796.463 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3087.3340785765854
    time_step_min: 2878
  date: 2020-10-15_23-27-34
  done: false
  episode_len_mean: 796.4810005817336
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.0391912140601
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 175
  episodes_total: 42975
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.556444635380811e-17
        cur_lr: 5.0e-05
        entropy: 0.09141827933490276
        entropy_coeff: 0.0005000000000000001
        kl: 0.005441232351586223
        model: {}
        policy_loss: -0.008524830705331018
        total_loss: 1.144716814160347
        vf_explained_var: 0.9975759387016296
        vf_loss: 1.1532873461643856
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.983333333333338
    gpu_util_percent0: 0.2723333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470931551828947
    mean_env_wait_ms: 1.194078873833593
    mean_inference_ms: 4.319554095786791
    mean_raw_obs_processing_ms: 0.37929467524079397
  time_since_restore: 5458.76460146904
  time_this_iter_s: 25.876750469207764
  time_total_s: 5458.76460146904
  timers:
    learn_throughput: 8675.608
    learn_time_ms: 18649.068
    sample_throughput: 23708.925
    sample_time_ms: 6824.097
    update_time_ms: 34.371
  timestamp: 1602804454
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: 0f5d2_00000
  
2020-10-15 23:27:35,455	WARNING util.py:136 -- The `process_trial` operation took 0.5609252452850342 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    212 |          5458.76 | 34299904 |  277.039 |              308.667 |              136.242 |            796.481 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3086.6954566527584
    time_step_min: 2878
  date: 2020-10-15_23-28-01
  done: false
  episode_len_mean: 796.4909208819714
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.13834846519643
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 43176
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.556444635380811e-17
        cur_lr: 5.0e-05
        entropy: 0.09886193399628003
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011065201671347799
        total_loss: .inf
        vf_explained_var: 0.9978331923484802
        vf_loss: 1.1059586703777313
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.403225806451612
    gpu_util_percent0: 0.3196774193548388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708934009356692
    mean_env_wait_ms: 1.1940282037333785
    mean_inference_ms: 4.319333358298925
    mean_raw_obs_processing_ms: 0.37927957392484324
  time_since_restore: 5484.722225904465
  time_this_iter_s: 25.957624435424805
  time_total_s: 5484.722225904465
  timers:
    learn_throughput: 8644.264
    learn_time_ms: 18716.688
    sample_throughput: 23695.109
    sample_time_ms: 6828.076
    update_time_ms: 40.705
  timestamp: 1602804481
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:28:02,256	WARNING util.py:136 -- The `process_trial` operation took 0.6449909210205078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    213 |          5484.72 | 34461696 |  277.138 |              308.667 |              136.242 |            796.491 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3086.0262417562144
    time_step_min: 2878
  date: 2020-10-15_23-28-27
  done: false
  episode_len_mean: 796.5012441822957
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.242221067874
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 226
  episodes_total: 43402
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.334666953071218e-17
        cur_lr: 5.0e-05
        entropy: 0.09895688605805238
        entropy_coeff: 0.0005000000000000001
        kl: 0.004025765461847186
        model: {}
        policy_loss: -0.009346980504536381
        total_loss: 1.1765869855880737
        vf_explained_var: 0.997799813747406
        vf_loss: 1.1859834790229797
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03666666666667
    gpu_util_percent0: 0.3506666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708542169326522
    mean_env_wait_ms: 1.1939741789559808
    mean_inference_ms: 4.319111128763013
    mean_raw_obs_processing_ms: 0.37926352507858907
  time_since_restore: 5510.298595666885
  time_this_iter_s: 25.576369762420654
  time_total_s: 5510.298595666885
  timers:
    learn_throughput: 8654.511
    learn_time_ms: 18694.529
    sample_throughput: 23764.679
    sample_time_ms: 6808.087
    update_time_ms: 40.771
  timestamp: 1602804507
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: 0f5d2_00000
  
2020-10-15 23:28:28,620	WARNING util.py:136 -- The `process_trial` operation took 0.5860745906829834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    214 |           5510.3 | 34623488 |  277.242 |              308.667 |              136.242 |            796.501 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3085.4225507299607
    time_step_min: 2878
  date: 2020-10-15_23-28-54
  done: false
  episode_len_mean: 796.5079587155964
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.33182165693614
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 198
  episodes_total: 43600
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.167333476535609e-17
        cur_lr: 5.0e-05
        entropy: 0.08555463887751102
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009070495492778718
        total_loss: .inf
        vf_explained_var: 0.9975411891937256
        vf_loss: 1.2181368966897328
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583870967741937
    gpu_util_percent0: 0.2832258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708195916638572
    mean_env_wait_ms: 1.1939247629667413
    mean_inference_ms: 4.3189119726895
    mean_raw_obs_processing_ms: 0.37924950085293146
  time_since_restore: 5536.049882888794
  time_this_iter_s: 25.75128722190857
  time_total_s: 5536.049882888794
  timers:
    learn_throughput: 8653.634
    learn_time_ms: 18696.424
    sample_throughput: 23777.875
    sample_time_ms: 6804.309
    update_time_ms: 41.955
  timestamp: 1602804534
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:28:55,350	WARNING util.py:136 -- The `process_trial` operation took 0.6086711883544922 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    215 |          5536.05 | 34785280 |  277.332 |              308.667 |              136.242 |            796.508 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3084.8556536052306
    time_step_min: 2878
  date: 2020-10-15_23-29-21
  done: false
  episode_len_mean: 796.5275937685595
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.42067622176324
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 178
  episodes_total: 43778
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.251000214803413e-17
        cur_lr: 5.0e-05
        entropy: 0.08820806816220284
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007429434102959931
        total_loss: .inf
        vf_explained_var: 0.9981115460395813
        vf_loss: 0.906253382563591
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.890000000000004
    gpu_util_percent0: 0.32999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707918545680013
    mean_env_wait_ms: 1.1938806640509585
    mean_inference_ms: 4.318730024452453
    mean_raw_obs_processing_ms: 0.37923743026546325
  time_since_restore: 5561.877443313599
  time_this_iter_s: 25.827560424804688
  time_total_s: 5561.877443313599
  timers:
    learn_throughput: 8648.89
    learn_time_ms: 18706.678
    sample_throughput: 23735.855
    sample_time_ms: 6816.355
    update_time_ms: 41.987
  timestamp: 1602804561
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:29:21,963	WARNING util.py:136 -- The `process_trial` operation took 0.5843594074249268 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    216 |          5561.88 | 34947072 |  277.421 |              308.667 |              136.242 |            796.528 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3084.1249516617
    time_step_min: 2878
  date: 2020-10-15_23-29-47
  done: false
  episode_len_mean: 796.5545378093962
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.5294506994621
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 219
  episodes_total: 43997
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.376500322205118e-17
        cur_lr: 5.0e-05
        entropy: 0.0918287963916858
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038153197577533624
        model: {}
        policy_loss: -0.009646903602212356
        total_loss: 0.9301093220710754
        vf_explained_var: 0.9982696175575256
        vf_loss: 0.9398021548986435
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.40645161290323
    gpu_util_percent0: 0.2903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707539712028503
    mean_env_wait_ms: 1.1938286088833117
    mean_inference_ms: 4.318517393907352
    mean_raw_obs_processing_ms: 0.37922234114236114
  time_since_restore: 5587.523448944092
  time_this_iter_s: 25.646005630493164
  time_total_s: 5587.523448944092
  timers:
    learn_throughput: 8647.928
    learn_time_ms: 18708.759
    sample_throughput: 23751.254
    sample_time_ms: 6811.935
    update_time_ms: 40.176
  timestamp: 1602804587
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: 0f5d2_00000
  
2020-10-15 23:29:48,593	WARNING util.py:136 -- The `process_trial` operation took 0.6077866554260254 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    217 |          5587.52 | 35108864 |  277.529 |              308.667 |              136.242 |            796.555 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3083.373690047758
    time_step_min: 2878
  date: 2020-10-15_23-30-14
  done: false
  episode_len_mean: 796.57609064387
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.63986770479727
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 44217
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.688250161102559e-17
        cur_lr: 5.0e-05
        entropy: 0.08847542479634285
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008935319752708892
        total_loss: .inf
        vf_explained_var: 0.9979227185249329
        vf_loss: 1.0969585925340652
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37666666666667
    gpu_util_percent0: 0.3469999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470712411265272
    mean_env_wait_ms: 1.19377124216366
    mean_inference_ms: 4.3182899648925614
    mean_raw_obs_processing_ms: 0.3792055749635018
  time_since_restore: 5613.184462785721
  time_this_iter_s: 25.66101384162903
  time_total_s: 5613.184462785721
  timers:
    learn_throughput: 8655.085
    learn_time_ms: 18693.288
    sample_throughput: 23739.161
    sample_time_ms: 6815.405
    update_time_ms: 38.439
  timestamp: 1602804614
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:30:15,042	WARNING util.py:136 -- The `process_trial` operation took 0.5827634334564209 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    218 |          5613.18 | 35270656 |   277.64 |              308.667 |              136.242 |            796.576 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3082.7643995581507
    time_step_min: 2878
  date: 2020-10-15_23-30-40
  done: false
  episode_len_mean: 796.5952922626421
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.7312794574872
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 178
  episodes_total: 44395
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.032375241653838e-17
        cur_lr: 5.0e-05
        entropy: 0.07878054243822892
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009257508916562074
        total_loss: .inf
        vf_explained_var: 0.9985277056694031
        vf_loss: 0.6773161192735037
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34666666666667
    gpu_util_percent0: 0.339
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706851697567866
    mean_env_wait_ms: 1.1937266576982508
    mean_inference_ms: 4.3181222599852065
    mean_raw_obs_processing_ms: 0.37919415827619973
  time_since_restore: 5638.984290122986
  time_this_iter_s: 25.799827337265015
  time_total_s: 5638.984290122986
  timers:
    learn_throughput: 8656.625
    learn_time_ms: 18689.964
    sample_throughput: 23741.797
    sample_time_ms: 6814.648
    update_time_ms: 38.672
  timestamp: 1602804640
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:30:41,786	WARNING util.py:136 -- The `process_trial` operation took 0.6537182331085205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    219 |          5638.98 | 35432448 |  277.731 |              308.667 |              136.242 |            796.595 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3082.091062129646
    time_step_min: 2878
  date: 2020-10-15_23-31-07
  done: false
  episode_len_mean: 796.6106127209115
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.8361775555862
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 193
  episodes_total: 44588
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.054856286248076e-16
        cur_lr: 5.0e-05
        entropy: 0.08024562584857146
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00757034504689121
        total_loss: .inf
        vf_explained_var: 0.9985185265541077
        vf_loss: 0.7389393448829651
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39032258064516
    gpu_util_percent0: 0.26903225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470654322190561
    mean_env_wait_ms: 1.1936759429022892
    mean_inference_ms: 4.317925581820344
    mean_raw_obs_processing_ms: 0.3791809205729166
  time_since_restore: 5664.650234460831
  time_this_iter_s: 25.66594433784485
  time_total_s: 5664.650234460831
  timers:
    learn_throughput: 8657.856
    learn_time_ms: 18687.306
    sample_throughput: 23821.903
    sample_time_ms: 6791.733
    update_time_ms: 38.586
  timestamp: 1602804667
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:31:08,403	WARNING util.py:136 -- The `process_trial` operation took 0.5721499919891357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    220 |          5664.65 | 35594240 |  277.836 |              308.667 |              136.242 |            796.611 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3081.320275159691
    time_step_min: 2878
  date: 2020-10-15_23-31-34
  done: false
  episode_len_mean: 796.628297255077
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 277.9517187045638
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 222
  episodes_total: 44810
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5822844293721138e-16
        cur_lr: 5.0e-05
        entropy: 0.07993997198839982
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044726467070480185
        model: {}
        policy_loss: -0.008786524587776512
        total_loss: 0.6924781799316406
        vf_explained_var: 0.9986689686775208
        vf_loss: 0.7013046542803446
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.259999999999998
    gpu_util_percent0: 0.3543333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706193736732007
    mean_env_wait_ms: 1.1936216936635862
    mean_inference_ms: 4.317726818303131
    mean_raw_obs_processing_ms: 0.3791666322335339
  time_since_restore: 5690.343592643738
  time_this_iter_s: 25.693358182907104
  time_total_s: 5690.343592643738
  timers:
    learn_throughput: 8665.406
    learn_time_ms: 18671.024
    sample_throughput: 23845.216
    sample_time_ms: 6785.093
    update_time_ms: 39.529
  timestamp: 1602804694
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: 0f5d2_00000
  
2020-10-15 23:31:34,926	WARNING util.py:136 -- The `process_trial` operation took 0.6216623783111572 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    221 |          5690.34 | 35756032 |  277.952 |              308.667 |              136.242 |            796.628 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3080.567372890776
    time_step_min: 2878
  date: 2020-10-15_23-32-00
  done: false
  episode_len_mean: 796.665948419486
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.0664254110061
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 207
  episodes_total: 45017
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.911422146860569e-17
        cur_lr: 5.0e-05
        entropy: 0.07497759660085042
        entropy_coeff: 0.0005000000000000001
        kl: 0.007847621338441968
        model: {}
        policy_loss: -0.009476887605463466
        total_loss: 0.4386044467488925
        vf_explained_var: 0.9990926384925842
        vf_loss: 0.44811880091826123
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.060000000000002
    gpu_util_percent0: 0.346
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705807341002844
    mean_env_wait_ms: 1.1935681726660285
    mean_inference_ms: 4.3175260068030585
    mean_raw_obs_processing_ms: 0.37915183059113045
  time_since_restore: 5715.891671419144
  time_this_iter_s: 25.548078775405884
  time_total_s: 5715.891671419144
  timers:
    learn_throughput: 8677.541
    learn_time_ms: 18644.914
    sample_throughput: 23862.498
    sample_time_ms: 6780.179
    update_time_ms: 37.639
  timestamp: 1602804720
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: 0f5d2_00000
  
2020-10-15 23:32:01,410	WARNING util.py:136 -- The `process_trial` operation took 0.6402435302734375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    222 |          5715.89 | 35917824 |  278.066 |              308.667 |              136.242 |            796.666 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3079.977279270102
    time_step_min: 2878
  date: 2020-10-15_23-32-27
  done: false
  episode_len_mean: 796.683225278251
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.1513280750771
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 176
  episodes_total: 45193
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.911422146860569e-17
        cur_lr: 5.0e-05
        entropy: 0.08560142604013284
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010689208575058728
        total_loss: .inf
        vf_explained_var: 0.9978604316711426
        vf_loss: 0.9714475671450297
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.019354838709678
    gpu_util_percent0: 0.317741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705556079690713
    mean_env_wait_ms: 1.1935216267738225
    mean_inference_ms: 4.317358027777132
    mean_raw_obs_processing_ms: 0.379140138746489
  time_since_restore: 5741.587476968765
  time_this_iter_s: 25.695805549621582
  time_total_s: 5741.587476968765
  timers:
    learn_throughput: 8685.331
    learn_time_ms: 18628.189
    sample_throughput: 23905.557
    sample_time_ms: 6767.966
    update_time_ms: 31.124
  timestamp: 1602804747
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:32:27,976	WARNING util.py:136 -- The `process_trial` operation took 0.5932772159576416 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    223 |          5741.59 | 36079616 |  278.151 |              308.667 |              136.242 |            796.683 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3079.334405995151
    time_step_min: 2878
  date: 2020-10-15_23-32-53
  done: false
  episode_len_mean: 796.6992247720566
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.2536088542561
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 213
  episodes_total: 45406
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1867133220290852e-16
        cur_lr: 5.0e-05
        entropy: 0.08313706703484058
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042938631571208434
        model: {}
        policy_loss: -0.009392981339866916
        total_loss: 0.8469865222771963
        vf_explained_var: 0.9983339905738831
        vf_loss: 0.8564210683107376
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.24333333333334
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705184266965896
    mean_env_wait_ms: 1.1934674123728954
    mean_inference_ms: 4.317154945314153
    mean_raw_obs_processing_ms: 0.3791262870543432
  time_since_restore: 5767.424970149994
  time_this_iter_s: 25.837493181228638
  time_total_s: 5767.424970149994
  timers:
    learn_throughput: 8678.899
    learn_time_ms: 18641.995
    sample_throughput: 23856.571
    sample_time_ms: 6781.863
    update_time_ms: 29.948
  timestamp: 1602804773
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: 0f5d2_00000
  
2020-10-15 23:32:54,622	WARNING util.py:136 -- The `process_trial` operation took 0.5968623161315918 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    224 |          5767.42 | 36241408 |  278.254 |              308.667 |              136.242 |            796.699 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3078.5499232287784
    time_step_min: 2878
  date: 2020-10-15_23-33-20
  done: false
  episode_len_mean: 796.7391618813834
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.37226714167474
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 45626
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.933566610145426e-17
        cur_lr: 5.0e-05
        entropy: 0.07017117366194725
        entropy_coeff: 0.0005000000000000001
        kl: 0.003827252386448284
        model: {}
        policy_loss: -0.007271373401939248
        total_loss: 0.43193121751149494
        vf_explained_var: 0.9991483688354492
        vf_loss: 0.43923765669266385
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.167741935483868
    gpu_util_percent0: 0.2858064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704839168878353
    mean_env_wait_ms: 1.1934103918690944
    mean_inference_ms: 4.316951337055118
    mean_raw_obs_processing_ms: 0.37911068935496695
  time_since_restore: 5793.2016541957855
  time_this_iter_s: 25.776684045791626
  time_total_s: 5793.2016541957855
  timers:
    learn_throughput: 8684.201
    learn_time_ms: 18630.615
    sample_throughput: 23808.66
    sample_time_ms: 6795.511
    update_time_ms: 28.856
  timestamp: 1602804800
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: 0f5d2_00000
  
2020-10-15 23:33:21,387	WARNING util.py:136 -- The `process_trial` operation took 0.6044538021087646 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    225 |           5793.2 | 36403200 |  278.372 |              308.667 |              136.242 |            796.739 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3077.883675208074
    time_step_min: 2878
  date: 2020-10-15_23-33-47
  done: false
  episode_len_mean: 796.7835112304368
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.4721473129565
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 45813
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.966783305072713e-17
        cur_lr: 5.0e-05
        entropy: 0.06394876322398584
        entropy_coeff: 0.0005000000000000001
        kl: 0.003329339592407147
        model: {}
        policy_loss: -0.007063683997936702
        total_loss: 0.4015878637631734
        vf_explained_var: 0.9991348385810852
        vf_loss: 0.40868351608514786
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970000000000006
    gpu_util_percent0: 0.3443333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704550632311225
    mean_env_wait_ms: 1.1933610743117458
    mean_inference_ms: 4.316788866529846
    mean_raw_obs_processing_ms: 0.3790991909401901
  time_since_restore: 5819.004226446152
  time_this_iter_s: 25.80257225036621
  time_total_s: 5819.004226446152
  timers:
    learn_throughput: 8680.809
    learn_time_ms: 18637.894
    sample_throughput: 23850.3
    sample_time_ms: 6783.646
    update_time_ms: 30.327
  timestamp: 1602804827
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: 0f5d2_00000
  
2020-10-15 23:33:48,062	WARNING util.py:136 -- The `process_trial` operation took 0.6573143005371094 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    226 |             5819 | 36564992 |  278.472 |              308.667 |              136.242 |            796.784 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3077.231119040884
    time_step_min: 2878
  date: 2020-10-15_23-34-13
  done: false
  episode_len_mean: 796.8232199152081
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.5698544308174
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 45995
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4833916525363565e-17
        cur_lr: 5.0e-05
        entropy: 0.06396221152196328
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00690593225832951
        total_loss: .inf
        vf_explained_var: 0.999262273311615
        vf_loss: 0.3493092382947604
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01
    gpu_util_percent0: 0.36933333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470426732140306
    mean_env_wait_ms: 1.1933118623973265
    mean_inference_ms: 4.316615145969916
    mean_raw_obs_processing_ms: 0.3790872658611739
  time_since_restore: 5844.562479019165
  time_this_iter_s: 25.558252573013306
  time_total_s: 5844.562479019165
  timers:
    learn_throughput: 8684.115
    learn_time_ms: 18630.799
    sample_throughput: 23865.729
    sample_time_ms: 6779.261
    update_time_ms: 32.263
  timestamp: 1602804853
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:34:14,476	WARNING util.py:136 -- The `process_trial` operation took 0.6360363960266113 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    227 |          5844.56 | 36726784 |   278.57 |              308.667 |              136.242 |            796.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3076.428951927241
    time_step_min: 2878
  date: 2020-10-15_23-34-40
  done: false
  episode_len_mean: 796.8702613813398
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.68922542894734
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 221
  episodes_total: 46216
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2250874788045355e-17
        cur_lr: 5.0e-05
        entropy: 0.06505957183738549
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007735482620773837
        total_loss: .inf
        vf_explained_var: 0.9991098046302795
        vf_loss: 0.5004699056347212
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.599999999999998
    gpu_util_percent0: 0.30666666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703952295099676
    mean_env_wait_ms: 1.1932564185636119
    mean_inference_ms: 4.316433334114654
    mean_raw_obs_processing_ms: 0.37907383952456286
  time_since_restore: 5870.243329286575
  time_this_iter_s: 25.68085026741028
  time_total_s: 5870.243329286575
  timers:
    learn_throughput: 8684.761
    learn_time_ms: 18629.414
    sample_throughput: 23857.526
    sample_time_ms: 6781.592
    update_time_ms: 32.617
  timestamp: 1602804880
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:34:41,015	WARNING util.py:136 -- The `process_trial` operation took 0.6431787014007568 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    228 |          5870.24 | 36888576 |  278.689 |              308.667 |              136.242 |             796.87 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3075.713241792589
    time_step_min: 2878
  date: 2020-10-15_23-35-06
  done: false
  episode_len_mean: 796.9196157408404
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.79820911421035
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 211
  episodes_total: 46427
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.337631218206803e-17
        cur_lr: 5.0e-05
        entropy: 0.06412088498473167
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009851360014484575
        total_loss: .inf
        vf_explained_var: 0.9991405010223389
        vf_loss: 0.4432536760965983
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.803225806451614
    gpu_util_percent0: 0.40645161290322585
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703579819471543
    mean_env_wait_ms: 1.1931976389107957
    mean_inference_ms: 4.316230714630731
    mean_raw_obs_processing_ms: 0.37905858659069613
  time_since_restore: 5896.029959201813
  time_this_iter_s: 25.786629915237427
  time_total_s: 5896.029959201813
  timers:
    learn_throughput: 8683.176
    learn_time_ms: 18632.813
    sample_throughput: 23875.762
    sample_time_ms: 6776.412
    update_time_ms: 30.969
  timestamp: 1602804906
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:35:07,814	WARNING util.py:136 -- The `process_trial` operation took 0.6264786720275879 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    229 |          5896.03 | 37050368 |  278.798 |              308.667 |              136.242 |             796.92 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3075.1266023920466
    time_step_min: 2878
  date: 2020-10-15_23-35-33
  done: false
  episode_len_mean: 796.9601347437081
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.88932147661495
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 46607
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.006446827310205e-17
        cur_lr: 5.0e-05
        entropy: 0.065768467883269
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009633096512213038
        total_loss: .inf
        vf_explained_var: 0.9990065693855286
        vf_loss: 0.507683960100015
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.630000000000003
    gpu_util_percent0: 0.313
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703342744297257
    mean_env_wait_ms: 1.1931491251121222
    mean_inference_ms: 4.316083666635739
    mean_raw_obs_processing_ms: 0.37904822384966463
  time_since_restore: 5921.810645341873
  time_this_iter_s: 25.780686140060425
  time_total_s: 5921.810645341873
  timers:
    learn_throughput: 8678.358
    learn_time_ms: 18643.157
    sample_throughput: 23849.209
    sample_time_ms: 6783.957
    update_time_ms: 30.487
  timestamp: 1602804933
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:35:34,452	WARNING util.py:136 -- The `process_trial` operation took 0.6361596584320068 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    230 |          5921.81 | 37212160 |  278.889 |              308.667 |              136.242 |             796.96 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3074.574302942183
    time_step_min: 2878
  date: 2020-10-15_23-36-00
  done: false
  episode_len_mean: 796.9789761558841
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.96518751311055
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 197
  episodes_total: 46804
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.509670240965306e-17
        cur_lr: 5.0e-05
        entropy: 0.12269765697419643
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014647722147249928
        total_loss: .inf
        vf_explained_var: 0.9958893656730652
        vf_loss: 1.9759367803732555
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.261290322580646
    gpu_util_percent0: 0.32096774193548383
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703033856592918
    mean_env_wait_ms: 1.1930944964698886
    mean_inference_ms: 4.315899909444758
    mean_raw_obs_processing_ms: 0.37903535688039697
  time_since_restore: 5947.670435190201
  time_this_iter_s: 25.859789848327637
  time_total_s: 5947.670435190201
  timers:
    learn_throughput: 8670.154
    learn_time_ms: 18660.799
    sample_throughput: 23857.276
    sample_time_ms: 6781.663
    update_time_ms: 31.05
  timestamp: 1602804960
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:36:01,339	WARNING util.py:136 -- The `process_trial` operation took 0.6463251113891602 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    231 |          5947.67 | 37373952 |  278.965 |              308.667 |              136.242 |            796.979 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3074.44415125657
    time_step_min: 2878
  date: 2020-10-15_23-36-26
  done: false
  episode_len_mean: 796.9610453124668
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 278.9840014897317
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 225
  episodes_total: 47029
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1264505361447956e-16
        cur_lr: 5.0e-05
        entropy: 0.141737533112367
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01222635479643941
        total_loss: .inf
        vf_explained_var: 0.9938011169433594
        vf_loss: 3.3306966026624045
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.863333333333337
    gpu_util_percent0: 0.3473333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702709136907002
    mean_env_wait_ms: 1.1930373078961924
    mean_inference_ms: 4.315720973779955
    mean_raw_obs_processing_ms: 0.3790215610582711
  time_since_restore: 5973.282124519348
  time_this_iter_s: 25.61168932914734
  time_total_s: 5973.282124519348
  timers:
    learn_throughput: 8676.76
    learn_time_ms: 18646.592
    sample_throughput: 23823.601
    sample_time_ms: 6791.249
    update_time_ms: 39.477
  timestamp: 1602804986
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:36:27,846	WARNING util.py:136 -- The `process_trial` operation took 0.6762678623199463 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    232 |          5973.28 | 37535744 |  278.984 |              308.667 |              136.242 |            796.961 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3074.2883356287743
    time_step_min: 2878
  date: 2020-10-15_23-36-53
  done: false
  episode_len_mean: 796.9465181766213
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.01729346994085
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 202
  episodes_total: 47231
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6896758042171935e-16
        cur_lr: 5.0e-05
        entropy: 0.11844579502940178
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011850437008736966
        total_loss: .inf
        vf_explained_var: 0.993435800075531
        vf_loss: 3.1515262126922607
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89666666666667
    gpu_util_percent0: 0.33133333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470238458647269
    mean_env_wait_ms: 1.1929828584508284
    mean_inference_ms: 4.315544057896689
    mean_raw_obs_processing_ms: 0.37900880513957197
  time_since_restore: 5999.19862818718
  time_this_iter_s: 25.91650366783142
  time_total_s: 5999.19862818718
  timers:
    learn_throughput: 8669.738
    learn_time_ms: 18661.695
    sample_throughput: 23768.36
    sample_time_ms: 6807.032
    update_time_ms: 39.232
  timestamp: 1602805013
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:36:54,728	WARNING util.py:136 -- The `process_trial` operation took 0.6498396396636963 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    233 |           5999.2 | 37697536 |  279.017 |              308.667 |              136.242 |            796.947 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3073.978068601583
    time_step_min: 2878
  date: 2020-10-15_23-37-20
  done: false
  episode_len_mean: 796.9482609520998
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.0738794155298
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 47411
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.53451370632579e-16
        cur_lr: 5.0e-05
        entropy: 0.10544740284482639
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010624941865292689
        total_loss: .inf
        vf_explained_var: 0.9964583516120911
        vf_loss: 1.6928097009658813
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44516129032258
    gpu_util_percent0: 0.3129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470212286165245
    mean_env_wait_ms: 1.192933487356217
    mean_inference_ms: 4.315379312624393
    mean_raw_obs_processing_ms: 0.3789974859971432
  time_since_restore: 6024.939947128296
  time_this_iter_s: 25.741318941116333
  time_total_s: 6024.939947128296
  timers:
    learn_throughput: 8676.797
    learn_time_ms: 18646.513
    sample_throughput: 23783.173
    sample_time_ms: 6802.793
    update_time_ms: 45.137
  timestamp: 1602805040
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:37:21,318	WARNING util.py:136 -- The `process_trial` operation took 0.6305944919586182 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    234 |          6024.94 | 37859328 |  279.074 |              308.667 |              136.242 |            796.948 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3073.49099619676
    time_step_min: 2878
  date: 2020-10-15_23-37-46
  done: false
  episode_len_mean: 796.9686312385832
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.15075037014253
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 216
  episodes_total: 47627
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.801770559488685e-16
        cur_lr: 5.0e-05
        entropy: 0.10319283852974574
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007916703235726649
        total_loss: .inf
        vf_explained_var: 0.9969577193260193
        vf_loss: 1.5895549257596333
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.009999999999994
    gpu_util_percent0: 0.36733333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470181987136909
    mean_env_wait_ms: 1.1928786601248063
    mean_inference_ms: 4.315206486306816
    mean_raw_obs_processing_ms: 0.3789847382487406
  time_since_restore: 6050.4972631931305
  time_this_iter_s: 25.557316064834595
  time_total_s: 6050.4972631931305
  timers:
    learn_throughput: 8681.967
    learn_time_ms: 18635.409
    sample_throughput: 23823.095
    sample_time_ms: 6791.393
    update_time_ms: 44.278
  timestamp: 1602805066
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:37:47,749	WARNING util.py:136 -- The `process_trial` operation took 0.6510298252105713 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    235 |           6050.5 | 38021120 |  279.151 |              308.667 |              136.242 |            796.969 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3072.927672035139
    time_step_min: 2878
  date: 2020-10-15_23-38-13
  done: false
  episode_len_mean: 796.9897797099026
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.23696860761584
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 219
  episodes_total: 47846
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.702655839233027e-16
        cur_lr: 5.0e-05
        entropy: 0.09011961705982685
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008443545462796465
        total_loss: .inf
        vf_explained_var: 0.9974818825721741
        vf_loss: 1.3760757545630138
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.883333333333333
    gpu_util_percent0: 0.32133333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701463096768896
    mean_env_wait_ms: 1.192819879778537
    mean_inference_ms: 4.315016964048498
    mean_raw_obs_processing_ms: 0.3789702177831729
  time_since_restore: 6076.159105300903
  time_this_iter_s: 25.661842107772827
  time_total_s: 6076.159105300903
  timers:
    learn_throughput: 8689.318
    learn_time_ms: 18619.643
    sample_throughput: 23815.996
    sample_time_ms: 6793.417
    update_time_ms: 42.512
  timestamp: 1602805093
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:38:14,395	WARNING util.py:136 -- The `process_trial` operation took 0.6763718128204346 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    236 |          6076.16 | 38182912 |  279.237 |              308.667 |              136.242 |             796.99 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3072.444657442907
    time_step_min: 2878
  date: 2020-10-15_23-38-40
  done: false
  episode_len_mean: 797.008744898809
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.31011076871795
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 48028
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.55398375884954e-16
        cur_lr: 5.0e-05
        entropy: 0.08229140316446622
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008860179300730428
        total_loss: .inf
        vf_explained_var: 0.9982637763023376
        vf_loss: 0.8234609663486481
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.20322580645162
    gpu_util_percent0: 0.3732258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701231475615117
    mean_env_wait_ms: 1.1927715882562229
    mean_inference_ms: 4.3148742553990616
    mean_raw_obs_processing_ms: 0.37896004627730984
  time_since_restore: 6101.954828739166
  time_this_iter_s: 25.79572343826294
  time_total_s: 6101.954828739166
  timers:
    learn_throughput: 8680.429
    learn_time_ms: 18638.709
    sample_throughput: 23801.814
    sample_time_ms: 6797.465
    update_time_ms: 42.153
  timestamp: 1602805120
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:38:41,210	WARNING util.py:136 -- The `process_trial` operation took 0.6309881210327148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    237 |          6101.95 | 38344704 |   279.31 |              308.667 |              136.242 |            797.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3071.920513033372
    time_step_min: 2878
  date: 2020-10-15_23-39-06
  done: false
  episode_len_mean: 797.0383658233098
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.38878624486233
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 192
  episodes_total: 48220
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2830975638274312e-15
        cur_lr: 5.0e-05
        entropy: 0.08236468397080898
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007422845176733972
        total_loss: .inf
        vf_explained_var: 0.9985706210136414
        vf_loss: 0.7266560643911362
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.863333333333333
    gpu_util_percent0: 0.343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700975384885362
    mean_env_wait_ms: 1.1927197459648136
    mean_inference_ms: 4.314709488899705
    mean_raw_obs_processing_ms: 0.3789486137979933
  time_since_restore: 6127.641499996185
  time_this_iter_s: 25.686671257019043
  time_total_s: 6127.641499996185
  timers:
    learn_throughput: 8679.109
    learn_time_ms: 18641.544
    sample_throughput: 23813.63
    sample_time_ms: 6794.092
    update_time_ms: 42.031
  timestamp: 1602805146
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:39:07,778	WARNING util.py:136 -- The `process_trial` operation took 0.6556384563446045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    238 |          6127.64 | 38506496 |  279.389 |              308.667 |              136.242 |            797.038 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3071.3187058386015
    time_step_min: 2878
  date: 2020-10-15_23-39-33
  done: false
  episode_len_mean: 797.073846979644
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.47887333636106
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 218
  episodes_total: 48438
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9246463457411464e-15
        cur_lr: 5.0e-05
        entropy: 0.08305691306789716
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00840201739144201
        total_loss: .inf
        vf_explained_var: 0.9984703660011292
        vf_loss: 0.8379646688699722
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.30645161290322
    gpu_util_percent0: 0.35258064516129023
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700667901163753
    mean_env_wait_ms: 1.19266332344577
    mean_inference_ms: 4.314544485611825
    mean_raw_obs_processing_ms: 0.37893614728388036
  time_since_restore: 6153.411761522293
  time_this_iter_s: 25.770261526107788
  time_total_s: 6153.411761522293
  timers:
    learn_throughput: 8680.789
    learn_time_ms: 18637.938
    sample_throughput: 23813.741
    sample_time_ms: 6794.06
    update_time_ms: 43.236
  timestamp: 1602805173
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:39:34,615	WARNING util.py:136 -- The `process_trial` operation took 0.6510529518127441 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    239 |          6153.41 | 38668288 |  279.479 |              308.667 |              136.242 |            797.074 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3070.7548035383666
    time_step_min: 2878
  date: 2020-10-15_23-40-00
  done: false
  episode_len_mean: 797.1060929983965
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.5655041555627
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 208
  episodes_total: 48646
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8869695186117205e-15
        cur_lr: 5.0e-05
        entropy: 0.08317642907301585
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00906453984498512
        total_loss: .inf
        vf_explained_var: 0.9988076686859131
        vf_loss: 0.6707822581132253
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.666666666666668
    gpu_util_percent0: 0.3313333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700348570414418
    mean_env_wait_ms: 1.1926085550145176
    mean_inference_ms: 4.314372552181574
    mean_raw_obs_processing_ms: 0.37892326573881807
  time_since_restore: 6179.258316278458
  time_this_iter_s: 25.84655475616455
  time_total_s: 6179.258316278458
  timers:
    learn_throughput: 8684.909
    learn_time_ms: 18629.095
    sample_throughput: 23766.467
    sample_time_ms: 6807.575
    update_time_ms: 43.297
  timestamp: 1602805200
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:40:01,378	WARNING util.py:136 -- The `process_trial` operation took 0.6959657669067383 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    240 |          6179.26 | 38830080 |  279.566 |              308.667 |              136.242 |            797.106 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3070.2639529402118
    time_step_min: 2878
  date: 2020-10-15_23-40-26
  done: false
  episode_len_mean: 797.1352380952382
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.6394761749598
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 179
  episodes_total: 48825
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.330454277917579e-15
        cur_lr: 5.0e-05
        entropy: 0.08160929506023724
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007375586933145921
        total_loss: .inf
        vf_explained_var: 0.9984173774719238
        vf_loss: 0.771807923913002
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.429999999999996
    gpu_util_percent0: 0.36766666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700123063344994
    mean_env_wait_ms: 1.1925590484220834
    mean_inference_ms: 4.314228351207241
    mean_raw_obs_processing_ms: 0.3789130863229432
  time_since_restore: 6204.443932294846
  time_this_iter_s: 25.18561601638794
  time_total_s: 6204.443932294846
  timers:
    learn_throughput: 8719.647
    learn_time_ms: 18554.881
    sample_throughput: 23778.929
    sample_time_ms: 6804.007
    update_time_ms: 43.506
  timestamp: 1602805226
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:40:27,427	WARNING util.py:136 -- The `process_trial` operation took 0.6322145462036133 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    241 |          6204.44 | 38991872 |  279.639 |              308.667 |              136.242 |            797.135 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3069.745116848658
    time_step_min: 2878
  date: 2020-10-15_23-40-53
  done: false
  episode_len_mean: 797.1534743325651
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.7102334144815
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 49031
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.495681416876371e-15
        cur_lr: 5.0e-05
        entropy: 0.10814083181321621
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01074394429451786
        total_loss: .inf
        vf_explained_var: 0.9967920184135437
        vf_loss: 1.6058274507522583
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.666666666666664
    gpu_util_percent0: 0.32566666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469982573863786
    mean_env_wait_ms: 1.1925040194469845
    mean_inference_ms: 4.314061137187081
    mean_raw_obs_processing_ms: 0.378900888970836
  time_since_restore: 6230.3247611522675
  time_this_iter_s: 25.880828857421875
  time_total_s: 6230.3247611522675
  timers:
    learn_throughput: 8703.517
    learn_time_ms: 18589.267
    sample_throughput: 23778.433
    sample_time_ms: 6804.149
    update_time_ms: 36.685
  timestamp: 1602805253
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:40:54,227	WARNING util.py:136 -- The `process_trial` operation took 0.6936075687408447 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    242 |          6230.32 | 39153664 |   279.71 |              308.667 |              136.242 |            797.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3069.3801706623326
    time_step_min: 2878
  date: 2020-10-15_23-41-19
  done: false
  episode_len_mean: 797.1342577554003
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.76195547319855
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 225
  episodes_total: 49256
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.743522125314556e-15
        cur_lr: 5.0e-05
        entropy: 0.11754737980663776
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00949122648065289
        total_loss: .inf
        vf_explained_var: 0.9950020909309387
        vf_loss: 2.579376836617788
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50333333333334
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699526258900414
    mean_env_wait_ms: 1.1924454875392045
    mean_inference_ms: 4.313889858856161
    mean_raw_obs_processing_ms: 0.37888836044946445
  time_since_restore: 6255.857651472092
  time_this_iter_s: 25.53289031982422
  time_total_s: 6255.857651472092
  timers:
    learn_throughput: 8716.898
    learn_time_ms: 18560.731
    sample_throughput: 23813.839
    sample_time_ms: 6794.033
    update_time_ms: 35.125
  timestamp: 1602805279
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:41:20,744	WARNING util.py:136 -- The `process_trial` operation took 0.6685428619384766 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    243 |          6255.86 | 39315456 |  279.762 |              308.667 |              136.242 |            797.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3069.008135510898
    time_step_min: 2878
  date: 2020-10-15_23-41-46
  done: false
  episode_len_mean: 797.1324394831038
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.82334967707754
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 193
  episodes_total: 49449
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4615283187971833e-14
        cur_lr: 5.0e-05
        entropy: 0.09207823624213536
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01069632897269912
        total_loss: .inf
        vf_explained_var: 0.9972166419029236
        vf_loss: 1.2697040637334187
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.270967741935486
    gpu_util_percent0: 0.34290322580645166
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699249466059391
    mean_env_wait_ms: 1.1923931159291608
    mean_inference_ms: 4.313739368901823
    mean_raw_obs_processing_ms: 0.37887710888575826
  time_since_restore: 6281.807232379913
  time_this_iter_s: 25.949580907821655
  time_total_s: 6281.807232379913
  timers:
    learn_throughput: 8710.379
    learn_time_ms: 18574.622
    sample_throughput: 23767.123
    sample_time_ms: 6807.387
    update_time_ms: 30.564
  timestamp: 1602805306
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:41:47,581	WARNING util.py:136 -- The `process_trial` operation took 0.6594207286834717 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    244 |          6281.81 | 39477248 |  279.823 |              308.667 |              136.242 |            797.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3068.5540545989757
    time_step_min: 2878
  date: 2020-10-15_23-42-13
  done: false
  episode_len_mean: 797.1380303823992
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.89267437643525
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 49634
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1922924781957745e-14
        cur_lr: 5.0e-05
        entropy: 0.07954613243540128
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048834929475560784
        model: {}
        policy_loss: -0.013289068949234206
        total_loss: 0.9137436598539352
        vf_explained_var: 0.9981003403663635
        vf_loss: 0.9270725200573603
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.293548387096774
    gpu_util_percent0: 0.31677419354838704
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699012466174194
    mean_env_wait_ms: 1.1923431695834632
    mean_inference_ms: 4.313589934767176
    mean_raw_obs_processing_ms: 0.3788665564827765
  time_since_restore: 6307.5695786476135
  time_this_iter_s: 25.762346267700195
  time_total_s: 6307.5695786476135
  timers:
    learn_throughput: 8703.699
    learn_time_ms: 18588.879
    sample_throughput: 23747.365
    sample_time_ms: 6813.051
    update_time_ms: 30.69
  timestamp: 1602805333
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: 0f5d2_00000
  
2020-10-15 23:42:14,431	WARNING util.py:136 -- The `process_trial` operation took 0.6887679100036621 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    245 |          6307.57 | 39639040 |  279.893 |              308.667 |              136.242 |            797.138 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3067.939531428801
    time_step_min: 2878
  date: 2020-10-15_23-42-40
  done: false
  episode_len_mean: 797.1672116677032
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 279.98728655139246
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 213
  episodes_total: 49847
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0961462390978872e-14
        cur_lr: 5.0e-05
        entropy: 0.06551803462207317
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006869283075502608
        total_loss: .inf
        vf_explained_var: 0.9992486834526062
        vf_loss: 0.39153220504522324
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72333333333334
    gpu_util_percent0: 0.2946666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469872846879996
    mean_env_wait_ms: 1.1922879526200105
    mean_inference_ms: 4.3134342801777334
    mean_raw_obs_processing_ms: 0.37885513738117665
  time_since_restore: 6333.324771642685
  time_this_iter_s: 25.75519299507141
  time_total_s: 6333.324771642685
  timers:
    learn_throughput: 8702.049
    learn_time_ms: 18592.403
    sample_throughput: 23737.362
    sample_time_ms: 6815.922
    update_time_ms: 33.013
  timestamp: 1602805360
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:42:41,090	WARNING util.py:136 -- The `process_trial` operation took 0.6736018657684326 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    246 |          6333.32 | 39800832 |  279.987 |              308.667 |              136.242 |            797.167 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3067.263303082397
    time_step_min: 2878
  date: 2020-10-15_23-43-07
  done: false
  episode_len_mean: 797.2082617554232
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.090159111792
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 50062
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.644219358646831e-14
        cur_lr: 5.0e-05
        entropy: 0.06263122831781705
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006415361868372808
        total_loss: .inf
        vf_explained_var: 0.9994892477989197
        vf_loss: 0.26862959563732147
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73548387096774
    gpu_util_percent0: 0.364516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469841700585478
    mean_env_wait_ms: 1.1922309055297091
    mean_inference_ms: 4.313268936075769
    mean_raw_obs_processing_ms: 0.3788424171282073
  time_since_restore: 6359.4778254032135
  time_this_iter_s: 26.153053760528564
  time_total_s: 6359.4778254032135
  timers:
    learn_throughput: 8691.11
    learn_time_ms: 18615.804
    sample_throughput: 23724.689
    sample_time_ms: 6819.563
    update_time_ms: 40.476
  timestamp: 1602805387
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:43:08,165	WARNING util.py:136 -- The `process_trial` operation took 0.6901359558105469 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    247 |          6359.48 | 39962624 |   280.09 |              308.667 |              136.242 |            797.208 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3066.7669388568015
    time_step_min: 2878
  date: 2020-10-15_23-43-33
  done: false
  episode_len_mean: 797.2166142578513
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.1619070536594
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 50246
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4663290379702467e-14
        cur_lr: 5.0e-05
        entropy: 0.07931177256007989
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00820235321104216
        total_loss: .inf
        vf_explained_var: 0.9980766177177429
        vf_loss: 0.8841816435257593
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.633333333333336
    gpu_util_percent0: 0.3446666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698208261286835
    mean_env_wait_ms: 1.1921808908222948
    mean_inference_ms: 4.31313334782472
    mean_raw_obs_processing_ms: 0.3788328664857476
  time_since_restore: 6385.15048289299
  time_this_iter_s: 25.67265748977661
  time_total_s: 6385.15048289299
  timers:
    learn_throughput: 8688.953
    learn_time_ms: 18620.425
    sample_throughput: 23751.044
    sample_time_ms: 6811.995
    update_time_ms: 40.441
  timestamp: 1602805413
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:43:34,765	WARNING util.py:136 -- The `process_trial` operation took 0.6873800754547119 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    248 |          6385.15 | 40124416 |  280.162 |              308.667 |              136.242 |            797.217 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3066.3146910028768
    time_step_min: 2878
  date: 2020-10-15_23-44-00
  done: false
  episode_len_mean: 797.2283063380979
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.2330421440468
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 195
  episodes_total: 50441
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.69949355695537e-14
        cur_lr: 5.0e-05
        entropy: 0.08142081089317799
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00998994440305978
        total_loss: .inf
        vf_explained_var: 0.9981933236122131
        vf_loss: 0.8990984459718069
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.68064516129032
    gpu_util_percent0: 0.3635483870967741
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469796036766395
    mean_env_wait_ms: 1.1921292400563748
    mean_inference_ms: 4.312990066757171
    mean_raw_obs_processing_ms: 0.37882259657861295
  time_since_restore: 6411.122211933136
  time_this_iter_s: 25.971729040145874
  time_total_s: 6411.122211933136
  timers:
    learn_throughput: 8687.738
    learn_time_ms: 18623.029
    sample_throughput: 23732.286
    sample_time_ms: 6817.379
    update_time_ms: 41.138
  timestamp: 1602805440
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:44:01,633	WARNING util.py:136 -- The `process_trial` operation took 0.6560308933258057 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    249 |          6411.12 | 40286208 |  280.233 |              308.667 |              136.242 |            797.228 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3065.725550617284
    time_step_min: 2878
  date: 2020-10-15_23-44-27
  done: false
  episode_len_mean: 797.266141607943
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.3236133467078
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 50661
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.5492403354330547e-14
        cur_lr: 5.0e-05
        entropy: 0.06693010280529658
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009336781949969009
        total_loss: .inf
        vf_explained_var: 0.9990294575691223
        vf_loss: 0.5277869204680125
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98
    gpu_util_percent0: 0.34766666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697681784522776
    mean_env_wait_ms: 1.1920700309080696
    mean_inference_ms: 4.312832771868882
    mean_raw_obs_processing_ms: 0.37881097396661745
  time_since_restore: 6436.767950534821
  time_this_iter_s: 25.64573860168457
  time_total_s: 6436.767950534821
  timers:
    learn_throughput: 8690.552
    learn_time_ms: 18617.0
    sample_throughput: 23787.303
    sample_time_ms: 6801.612
    update_time_ms: 42.033
  timestamp: 1602805467
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:44:28,213	WARNING util.py:136 -- The `process_trial` operation took 0.7049381732940674 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    250 |          6436.77 | 40448000 |  280.324 |              308.667 |              136.242 |            797.266 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3065.14592924881
    time_step_min: 2878
  date: 2020-10-15_23-44-53
  done: false
  episode_len_mean: 797.3099956745704
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.41175110786975
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 50862
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.323860503149583e-14
        cur_lr: 5.0e-05
        entropy: 0.06040966541816791
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007576923516656582
        total_loss: .inf
        vf_explained_var: 0.9990928173065186
        vf_loss: 0.46425727754831314
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.845161290322586
    gpu_util_percent0: 0.36290322580645157
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697410898961452
    mean_env_wait_ms: 1.192016964255515
    mean_inference_ms: 4.312685854250133
    mean_raw_obs_processing_ms: 0.3787997966301949
  time_since_restore: 6462.543338298798
  time_this_iter_s: 25.77538776397705
  time_total_s: 6462.543338298798
  timers:
    learn_throughput: 8658.097
    learn_time_ms: 18686.785
    sample_throughput: 23794.063
    sample_time_ms: 6799.679
    update_time_ms: 41.465
  timestamp: 1602805493
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:44:55,096	WARNING util.py:136 -- The `process_trial` operation took 0.7050676345825195 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    251 |          6462.54 | 40609792 |  280.412 |              308.667 |              136.242 |             797.31 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3064.627394466992
    time_step_min: 2878
  date: 2020-10-15_23-45-20
  done: false
  episode_len_mean: 797.3467740355414
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.49008215345697
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 177
  episodes_total: 51039
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2485790754724373e-13
        cur_lr: 5.0e-05
        entropy: 0.05880640416095654
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00805057534792771
        total_loss: .inf
        vf_explained_var: 0.9992678165435791
        vf_loss: 0.34405697633822757
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.973333333333336
    gpu_util_percent0: 0.35400000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697193743225084
    mean_env_wait_ms: 1.1919677457628846
    mean_inference_ms: 4.312554044369166
    mean_raw_obs_processing_ms: 0.3787903516609248
  time_since_restore: 6488.277462720871
  time_this_iter_s: 25.734124422073364
  time_total_s: 6488.277462720871
  timers:
    learn_throughput: 8664.034
    learn_time_ms: 18673.981
    sample_throughput: 23801.738
    sample_time_ms: 6797.487
    update_time_ms: 40.07
  timestamp: 1602805520
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:45:21,756	WARNING util.py:136 -- The `process_trial` operation took 0.6759436130523682 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    252 |          6488.28 | 40771584 |   280.49 |              308.667 |              136.242 |            797.347 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3064.0459018313873
    time_step_min: 2878
  date: 2020-10-15_23-45-47
  done: false
  episode_len_mean: 797.3657665743161
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.57435517227896
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 51254
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8728686132086563e-13
        cur_lr: 5.0e-05
        entropy: 0.0912864829103152
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008898225244289884
        total_loss: .inf
        vf_explained_var: 0.9980438351631165
        vf_loss: 0.9799745579560598
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.90967741935484
    gpu_util_percent0: 0.27161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696911431222723
    mean_env_wait_ms: 1.1919108491175543
    mean_inference_ms: 4.31239783622425
    mean_raw_obs_processing_ms: 0.3787791238112191
  time_since_restore: 6514.170126438141
  time_this_iter_s: 25.892663717269897
  time_total_s: 6514.170126438141
  timers:
    learn_throughput: 8655.988
    learn_time_ms: 18691.339
    sample_throughput: 23780.636
    sample_time_ms: 6803.519
    update_time_ms: 41.866
  timestamp: 1602805547
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:45:48,547	WARNING util.py:136 -- The `process_trial` operation took 0.6547787189483643 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    253 |          6514.17 | 40933376 |  280.574 |              308.667 |              136.242 |            797.366 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3063.622232158479
    time_step_min: 2878
  date: 2020-10-15_23-46-14
  done: false
  episode_len_mean: 797.3320835356969
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.6365137533663
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 221
  episodes_total: 51475
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8093029198129844e-13
        cur_lr: 5.0e-05
        entropy: 0.10262433625757694
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008197558859440809
        total_loss: .inf
        vf_explained_var: 0.9959527850151062
        vf_loss: 2.085080772638321
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.493333333333336
    gpu_util_percent0: 0.3446666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469663383253282
    mean_env_wait_ms: 1.191850648058814
    mean_inference_ms: 4.3122474181816886
    mean_raw_obs_processing_ms: 0.37876740320196045
  time_since_restore: 6539.785147428513
  time_this_iter_s: 25.615020990371704
  time_total_s: 6539.785147428513
  timers:
    learn_throughput: 8662.839
    learn_time_ms: 18676.556
    sample_throughput: 23846.405
    sample_time_ms: 6784.754
    update_time_ms: 40.687
  timestamp: 1602805574
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:46:15,108	WARNING util.py:136 -- The `process_trial` operation took 0.7062468528747559 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    254 |          6539.79 | 41095168 |  280.637 |              308.667 |              136.242 |            797.332 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3063.224761757186
    time_step_min: 2878
  date: 2020-10-15_23-46-40
  done: false
  episode_len_mean: 797.3230876432332
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.6972436114002
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 51664
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.213954379719476e-13
        cur_lr: 5.0e-05
        entropy: 0.08843325016399224
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011213263521009745
        total_loss: .inf
        vf_explained_var: 0.9963898658752441
        vf_loss: 1.6645260552565257
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.725806451612904
    gpu_util_percent0: 0.37258064516129036
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469641163506682
    mean_env_wait_ms: 1.1917999927528198
    mean_inference_ms: 4.312112578626681
    mean_raw_obs_processing_ms: 0.37875762305087074
  time_since_restore: 6565.657966375351
  time_this_iter_s: 25.87281894683838
  time_total_s: 6565.657966375351
  timers:
    learn_throughput: 8665.752
    learn_time_ms: 18670.278
    sample_throughput: 23788.91
    sample_time_ms: 6801.152
    update_time_ms: 40.8
  timestamp: 1602805600
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:46:41,915	WARNING util.py:136 -- The `process_trial` operation took 0.701709508895874 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    255 |          6565.66 | 41256960 |  280.697 |              308.667 |              136.242 |            797.323 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3062.752807904589
    time_step_min: 2878
  date: 2020-10-15_23-47-08
  done: false
  episode_len_mean: 797.3371003201296
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.7703996418847
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 190
  episodes_total: 51854
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.320931569579213e-13
        cur_lr: 5.0e-05
        entropy: 0.07416727766394615
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0087309352044637
        total_loss: .inf
        vf_explained_var: 0.998390257358551
        vf_loss: 0.7859423110882441
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.025806451612905
    gpu_util_percent0: 0.3645161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696176596137145
    mean_env_wait_ms: 1.1917494654430922
    mean_inference_ms: 4.31197771712174
    mean_raw_obs_processing_ms: 0.37874781542862257
  time_since_restore: 6591.774289608002
  time_this_iter_s: 26.116323232650757
  time_total_s: 6591.774289608002
  timers:
    learn_throughput: 8650.626
    learn_time_ms: 18702.924
    sample_throughput: 23810.336
    sample_time_ms: 6795.032
    update_time_ms: 40.41
  timestamp: 1602805628
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:47:08,976	WARNING util.py:136 -- The `process_trial` operation took 0.6963920593261719 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    256 |          6591.77 | 41418752 |   280.77 |              308.667 |              136.242 |            797.337 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3062.13866200296
    time_step_min: 2878
  date: 2020-10-15_23-47-34
  done: false
  episode_len_mean: 797.3802262382608
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.86336428876103
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 52069
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.481397354368822e-13
        cur_lr: 5.0e-05
        entropy: 0.06358458784719308
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008654962060973048
        total_loss: .inf
        vf_explained_var: 0.9993786811828613
        vf_loss: 0.3311474323272705
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333333
    gpu_util_percent0: 0.3936666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695928880706496
    mean_env_wait_ms: 1.191692694127833
    mean_inference_ms: 4.311837011538073
    mean_raw_obs_processing_ms: 0.37873762753982787
  time_since_restore: 6617.603152036667
  time_this_iter_s: 25.82886242866516
  time_total_s: 6617.603152036667
  timers:
    learn_throughput: 8661.176
    learn_time_ms: 18680.142
    sample_throughput: 23825.838
    sample_time_ms: 6790.611
    update_time_ms: 33.127
  timestamp: 1602805654
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:47:35,874	WARNING util.py:136 -- The `process_trial` operation took 0.729564905166626 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    257 |           6617.6 | 41580544 |  280.863 |              308.667 |              136.242 |             797.38 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3061.5375456999022
    time_step_min: 2878
  date: 2020-10-15_23-48-01
  done: false
  episode_len_mean: 797.425562845502
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 280.9527082837013
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 52279
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4222096031553233e-12
        cur_lr: 5.0e-05
        entropy: 0.05917739123106003
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007397092194878496
        total_loss: .inf
        vf_explained_var: 0.9994425177574158
        vf_loss: 0.3005656376481056
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.038709677419355
    gpu_util_percent0: 0.34129032258064523
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695661463636397
    mean_env_wait_ms: 1.1916348399706818
    mean_inference_ms: 4.311684284475352
    mean_raw_obs_processing_ms: 0.3787257477610483
  time_since_restore: 6643.361683368683
  time_this_iter_s: 25.75853133201599
  time_total_s: 6643.361683368683
  timers:
    learn_throughput: 8660.59
    learn_time_ms: 18681.406
    sample_throughput: 23837.218
    sample_time_ms: 6787.369
    update_time_ms: 34.599
  timestamp: 1602805681
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:48:02,676	WARNING util.py:136 -- The `process_trial` operation took 0.7256464958190918 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    258 |          6643.36 | 41742336 |  280.953 |              308.667 |              136.242 |            797.426 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3061.023444355423
    time_step_min: 2878
  date: 2020-10-15_23-48-28
  done: false
  episode_len_mean: 797.4629036562584
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.0298414777996
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 179
  episodes_total: 52458
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1333144047329853e-12
        cur_lr: 5.0e-05
        entropy: 0.06006209241847197
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005958001808418582
        total_loss: .inf
        vf_explained_var: 0.9993709921836853
        vf_loss: 0.3244853417078654
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333333
    gpu_util_percent0: 0.33266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469545032862385
    mean_env_wait_ms: 1.1915853934876817
    mean_inference_ms: 4.311564797849898
    mean_raw_obs_processing_ms: 0.3787169497191573
  time_since_restore: 6669.17690372467
  time_this_iter_s: 25.81522035598755
  time_total_s: 6669.17690372467
  timers:
    learn_throughput: 8663.477
    learn_time_ms: 18675.181
    sample_throughput: 23828.36
    sample_time_ms: 6789.892
    update_time_ms: 32.812
  timestamp: 1602805708
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:48:29,447	WARNING util.py:136 -- The `process_trial` operation took 0.7160098552703857 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    259 |          6669.18 | 41904128 |   281.03 |              308.667 |              136.242 |            797.463 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3060.494185383651
    time_step_min: 2878
  date: 2020-10-15_23-48-55
  done: false
  episode_len_mean: 797.4974174926892
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.10726554596874
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 204
  episodes_total: 52662
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1999716070994782e-12
        cur_lr: 5.0e-05
        entropy: 0.07836122810840607
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01180441512648637
        total_loss: .inf
        vf_explained_var: 0.9981221556663513
        vf_loss: 0.9703450004259745
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.706451612903233
    gpu_util_percent0: 0.3048387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695216312622184
    mean_env_wait_ms: 1.1915301973565118
    mean_inference_ms: 4.311421247498017
    mean_raw_obs_processing_ms: 0.3787067326703891
  time_since_restore: 6694.944632768631
  time_this_iter_s: 25.76772904396057
  time_total_s: 6694.944632768631
  timers:
    learn_throughput: 8660.388
    learn_time_ms: 18681.842
    sample_throughput: 23841.506
    sample_time_ms: 6786.148
    update_time_ms: 32.575
  timestamp: 1602805735
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:48:56,196	WARNING util.py:136 -- The `process_trial` operation took 0.7370917797088623 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    260 |          6694.94 | 42065920 |  281.107 |              308.667 |              136.242 |            797.497 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3060.060977686936
    time_step_min: 2878
  date: 2020-10-15_23-49-21
  done: false
  episode_len_mean: 797.5249550827423
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.17563435776174
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 213
  episodes_total: 52875
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.7999574106492155e-12
        cur_lr: 5.0e-05
        entropy: 0.07788123811284701
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009702637287167212
        total_loss: .inf
        vf_explained_var: 0.998145580291748
        vf_loss: 0.9673544466495514
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01
    gpu_util_percent0: 0.31366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694947294586114
    mean_env_wait_ms: 1.191471552581136
    mean_inference_ms: 4.311284665072874
    mean_raw_obs_processing_ms: 0.3786963904166291
  time_since_restore: 6720.7415163517
  time_this_iter_s: 25.796883583068848
  time_total_s: 6720.7415163517
  timers:
    learn_throughput: 8662.957
    learn_time_ms: 18676.301
    sample_throughput: 23826.834
    sample_time_ms: 6790.327
    update_time_ms: 32.804
  timestamp: 1602805761
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:49:22,955	WARNING util.py:136 -- The `process_trial` operation took 0.7159714698791504 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    261 |          6720.74 | 42227712 |  281.176 |              308.667 |              136.242 |            797.525 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3059.549132730015
    time_step_min: 2878
  date: 2020-10-15_23-49-48
  done: false
  episode_len_mean: 797.5634938578642
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.2539200506075
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 53076
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.199936115973824e-12
        cur_lr: 5.0e-05
        entropy: 0.06173479681213697
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00982472568769784
        total_loss: .inf
        vf_explained_var: 0.9991909861564636
        vf_loss: 0.39843713740507763
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.016129032258068
    gpu_util_percent0: 0.29935483870967744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694710860600216
    mean_env_wait_ms: 1.1914172034693544
    mean_inference_ms: 4.311146589464827
    mean_raw_obs_processing_ms: 0.378685753867483
  time_since_restore: 6746.69150185585
  time_this_iter_s: 25.94998550415039
  time_total_s: 6746.69150185585
  timers:
    learn_throughput: 8661.478
    learn_time_ms: 18679.491
    sample_throughput: 23802.004
    sample_time_ms: 6797.411
    update_time_ms: 34.329
  timestamp: 1602805788
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:49:49,858	WARNING util.py:136 -- The `process_trial` operation took 0.6893386840820312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    262 |          6746.69 | 42389504 |  281.254 |              308.667 |              136.242 |            797.563 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3059.027637907711
    time_step_min: 2878
  date: 2020-10-15_23-50-15
  done: false
  episode_len_mean: 797.6094066841907
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.3314927343276
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 53260
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0799904173960736e-11
        cur_lr: 5.0e-05
        entropy: 0.058852843940258026
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009065371317168077
        total_loss: .inf
        vf_explained_var: 0.9994795918464661
        vf_loss: 0.2776776837805907
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26666666666667
    gpu_util_percent0: 0.30533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694506723508582
    mean_env_wait_ms: 1.1913652396278547
    mean_inference_ms: 4.311026956908214
    mean_raw_obs_processing_ms: 0.3786772240977529
  time_since_restore: 6772.360820531845
  time_this_iter_s: 25.669318675994873
  time_total_s: 6772.360820531845
  timers:
    learn_throughput: 8664.886
    learn_time_ms: 18672.145
    sample_throughput: 23814.811
    sample_time_ms: 6793.755
    update_time_ms: 32.045
  timestamp: 1602805815
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:50:16,463	WARNING util.py:136 -- The `process_trial` operation took 0.691133975982666 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    263 |          6772.36 | 42551296 |  281.331 |              308.667 |              136.242 |            797.609 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3058.4666841356593
    time_step_min: 2878
  date: 2020-10-15_23-50-42
  done: false
  episode_len_mean: 797.654833158761
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.41407755544344
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 204
  episodes_total: 53464
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6199856260941104e-11
        cur_lr: 5.0e-05
        entropy: 0.06134834854553143
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008000767576353004
        total_loss: .inf
        vf_explained_var: 0.9989581108093262
        vf_loss: 0.5463051026066145
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61935483870968
    gpu_util_percent0: 0.37709677419354837
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694261389589464
    mean_env_wait_ms: 1.1913105991635535
    mean_inference_ms: 4.310889926917734
    mean_raw_obs_processing_ms: 0.3786667797700719
  time_since_restore: 6798.094598531723
  time_this_iter_s: 25.73377799987793
  time_total_s: 6798.094598531723
  timers:
    learn_throughput: 8657.224
    learn_time_ms: 18688.67
    sample_throughput: 23836.679
    sample_time_ms: 6787.523
    update_time_ms: 33.1
  timestamp: 1602805842
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:50:43,372	WARNING util.py:136 -- The `process_trial` operation took 0.781064510345459 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    264 |          6798.09 | 42713088 |  281.414 |              308.667 |              136.242 |            797.655 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3057.883831040525
    time_step_min: 2878
  date: 2020-10-15_23-51-09
  done: false
  episode_len_mean: 797.7021534220037
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.50104882512363
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 218
  episodes_total: 53682
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.429978439141166e-11
        cur_lr: 5.0e-05
        entropy: 0.06201594074567159
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007661693729460239
        total_loss: .inf
        vf_explained_var: 0.9994683265686035
        vf_loss: 0.2964029411474864
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72666666666667
    gpu_util_percent0: 0.312
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469402448715698
    mean_env_wait_ms: 1.1912498089109793
    mean_inference_ms: 4.310758381898166
    mean_raw_obs_processing_ms: 0.3786565394292872
  time_since_restore: 6823.852509260178
  time_this_iter_s: 25.75791072845459
  time_total_s: 6823.852509260178
  timers:
    learn_throughput: 8652.421
    learn_time_ms: 18699.044
    sample_throughput: 23919.04
    sample_time_ms: 6764.151
    update_time_ms: 34.48
  timestamp: 1602805869
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:51:10,172	WARNING util.py:136 -- The `process_trial` operation took 0.7214064598083496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    265 |          6823.85 | 42874880 |  281.501 |              308.667 |              136.242 |            797.702 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3057.37361616762
    time_step_min: 2878
  date: 2020-10-15_23-51-36
  done: false
  episode_len_mean: 797.7433174933175
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.5774748899746
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 190
  episodes_total: 53872
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.644967658711749e-11
        cur_lr: 5.0e-05
        entropy: 0.05855567939579487
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008287421233641604
        total_loss: .inf
        vf_explained_var: 0.9994754791259766
        vf_loss: 0.26019972811142605
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.332258064516132
    gpu_util_percent0: 0.30612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693788418727857
    mean_env_wait_ms: 1.1911965238024198
    mean_inference_ms: 4.310627753241665
    mean_raw_obs_processing_ms: 0.37864665542426346
  time_since_restore: 6849.8086841106415
  time_this_iter_s: 25.956174850463867
  time_total_s: 6849.8086841106415
  timers:
    learn_throughput: 8659.465
    learn_time_ms: 18683.834
    sample_throughput: 23927.896
    sample_time_ms: 6761.648
    update_time_ms: 35.136
  timestamp: 1602805896
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:51:37,150	WARNING util.py:136 -- The `process_trial` operation took 0.716944694519043 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    266 |          6849.81 | 43036672 |  281.577 |              308.667 |              136.242 |            797.743 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3056.8968919494273
    time_step_min: 2878
  date: 2020-10-15_23-52-03
  done: false
  episode_len_mean: 797.7825073533493
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.6511762836195
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 54057
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.467451488067623e-11
        cur_lr: 5.0e-05
        entropy: 0.05958165352543195
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008524141812813468
        total_loss: .inf
        vf_explained_var: 0.9993526935577393
        vf_loss: 0.3288353234529495
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.326666666666668
    gpu_util_percent0: 0.3406666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693585475295556
    mean_env_wait_ms: 1.1911443317188855
    mean_inference_ms: 4.310508203746045
    mean_raw_obs_processing_ms: 0.3786381017895163
  time_since_restore: 6875.7912175655365
  time_this_iter_s: 25.98253345489502
  time_total_s: 6875.7912175655365
  timers:
    learn_throughput: 8658.178
    learn_time_ms: 18686.612
    sample_throughput: 23901.902
    sample_time_ms: 6769.001
    update_time_ms: 35.275
  timestamp: 1602805923
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:52:04,091	WARNING util.py:136 -- The `process_trial` operation took 0.7093868255615234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    267 |          6875.79 | 43198464 |  281.651 |              308.667 |              136.242 |            797.783 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3056.375246630156
    time_step_min: 2878
  date: 2020-10-15_23-52-29
  done: false
  episode_len_mean: 797.8236681592865
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.72899708567763
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 54267
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.201177232101434e-11
        cur_lr: 5.0e-05
        entropy: 0.062372306982676186
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046490961297725635
        model: {}
        policy_loss: -0.008398266499474024
        total_loss: 0.5922143633166949
        vf_explained_var: 0.9988744854927063
        vf_loss: 0.6006438235441843
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.929032258064517
    gpu_util_percent0: 0.31451612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469334931239071
    mean_env_wait_ms: 1.1910894972052315
    mean_inference_ms: 4.310386381797036
    mean_raw_obs_processing_ms: 0.3786290335592843
  time_since_restore: 6901.526211977005
  time_this_iter_s: 25.734994411468506
  time_total_s: 6901.526211977005
  timers:
    learn_throughput: 8658.554
    learn_time_ms: 18685.798
    sample_throughput: 23876.894
    sample_time_ms: 6776.091
    update_time_ms: 34.942
  timestamp: 1602805949
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: 0f5d2_00000
  
2020-10-15 23:52:30,975	WARNING util.py:136 -- The `process_trial` operation took 0.7345211505889893 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    268 |          6901.53 | 43360256 |  281.729 |              308.667 |              136.242 |            797.824 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3055.8296109907064
    time_step_min: 2878
  date: 2020-10-15_23-52-56
  done: false
  episode_len_mean: 797.8673873940016
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.8134051502133
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 54482
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.100588616050717e-11
        cur_lr: 5.0e-05
        entropy: 0.059755331836640835
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008813949903318038
        total_loss: .inf
        vf_explained_var: 0.9994152188301086
        vf_loss: 0.30281910051902133
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.920689655172417
    gpu_util_percent0: 0.3444827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693105733221343
    mean_env_wait_ms: 1.191028213389485
    mean_inference_ms: 4.31024907716188
    mean_raw_obs_processing_ms: 0.37861803405978117
  time_since_restore: 6926.723363637924
  time_this_iter_s: 25.19715166091919
  time_total_s: 6926.723363637924
  timers:
    learn_throughput: 8688.738
    learn_time_ms: 18620.887
    sample_throughput: 23873.164
    sample_time_ms: 6777.15
    update_time_ms: 35.638
  timestamp: 1602805976
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:52:57,228	WARNING util.py:136 -- The `process_trial` operation took 0.7196681499481201 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    269 |          6926.72 | 43522048 |  281.813 |              308.667 |              136.242 |            797.867 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3055.38606992495
    time_step_min: 2878
  date: 2020-10-15_23-53-23
  done: false
  episode_len_mean: 797.8948340833425
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.87574127844096
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 54666
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.150882924076077e-11
        cur_lr: 5.0e-05
        entropy: 0.07381075744827588
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009608551549414793
        total_loss: .inf
        vf_explained_var: 0.9984015822410583
        vf_loss: 0.7506989141305288
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16451612903226
    gpu_util_percent0: 0.33967741935483875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692911862850944
    mean_env_wait_ms: 1.1909763363079668
    mean_inference_ms: 4.310133109240968
    mean_raw_obs_processing_ms: 0.37860934661325624
  time_since_restore: 6952.524580001831
  time_this_iter_s: 25.80121636390686
  time_total_s: 6952.524580001831
  timers:
    learn_throughput: 8690.433
    learn_time_ms: 18617.255
    sample_throughput: 23813.087
    sample_time_ms: 6794.247
    update_time_ms: 33.926
  timestamp: 1602806003
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:53:24,117	WARNING util.py:136 -- The `process_trial` operation took 0.7845280170440674 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    270 |          6952.52 | 43683840 |  281.876 |              308.667 |              136.242 |            797.895 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3055.018969448244
    time_step_min: 2878
  date: 2020-10-15_23-53-49
  done: false
  episode_len_mean: 797.9137638759775
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 281.93181666282743
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 195
  episodes_total: 54861
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.226324386114114e-11
        cur_lr: 5.0e-05
        entropy: 0.07498376319805782
        entropy_coeff: 0.0005000000000000001
        kl: 0.005283771936471264
        model: {}
        policy_loss: -0.011780116129860593
        total_loss: 0.8595046748717626
        vf_explained_var: 0.9982455372810364
        vf_loss: 0.87132228910923
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.938709677419357
    gpu_util_percent0: 0.34774193548387095
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692686873201963
    mean_env_wait_ms: 1.1909215078593185
    mean_inference_ms: 4.310002003018307
    mean_raw_obs_processing_ms: 0.37859968985683107
  time_since_restore: 6978.324731349945
  time_this_iter_s: 25.800151348114014
  time_total_s: 6978.324731349945
  timers:
    learn_throughput: 8689.429
    learn_time_ms: 18619.406
    sample_throughput: 23816.108
    sample_time_ms: 6793.385
    update_time_ms: 33.799
  timestamp: 1602806029
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: 0f5d2_00000
  
2020-10-15 23:53:51,083	WARNING util.py:136 -- The `process_trial` operation took 0.7395639419555664 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    271 |          6978.32 | 43845632 |  281.932 |              308.667 |              136.242 |            797.914 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3054.5831894907064
    time_step_min: 2878
  date: 2020-10-15_23-54-16
  done: false
  episode_len_mean: 797.9468705173134
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.0009711627924
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 212
  episodes_total: 55073
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.226324386114114e-11
        cur_lr: 5.0e-05
        entropy: 0.06203584745526314
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045690870222946005
        model: {}
        policy_loss: -0.010194746530639046
        total_loss: 0.7735479027032852
        vf_explained_var: 0.9985714554786682
        vf_loss: 0.7837736656268438
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.563333333333336
    gpu_util_percent0: 0.3016666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692479486609983
    mean_env_wait_ms: 1.190863673935653
    mean_inference_ms: 4.309886318007278
    mean_raw_obs_processing_ms: 0.3785912295543708
  time_since_restore: 7004.236778974533
  time_this_iter_s: 25.912047624588013
  time_total_s: 7004.236778974533
  timers:
    learn_throughput: 8686.072
    learn_time_ms: 18626.602
    sample_throughput: 23827.469
    sample_time_ms: 6790.146
    update_time_ms: 34.27
  timestamp: 1602806056
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: 0f5d2_00000
  
2020-10-15 23:54:18,076	WARNING util.py:136 -- The `process_trial` operation took 0.7406861782073975 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    272 |          7004.24 | 44007424 |  282.001 |              308.667 |              136.242 |            797.947 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3054.078833517369
    time_step_min: 2878
  date: 2020-10-15_23-54-43
  done: false
  episode_len_mean: 797.9968885110078
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.0799086945722
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 55279
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.613162193057057e-11
        cur_lr: 5.0e-05
        entropy: 0.05315053928643465
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007627775126214449
        total_loss: .inf
        vf_explained_var: 0.999382495880127
        vf_loss: 0.3121332178513209
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83548387096775
    gpu_util_percent0: 0.27096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692222680251077
    mean_env_wait_ms: 1.190805961895054
    mean_inference_ms: 4.309757848597256
    mean_raw_obs_processing_ms: 0.3785808913178588
  time_since_restore: 7030.1100351810455
  time_this_iter_s: 25.87325620651245
  time_total_s: 7030.1100351810455
  timers:
    learn_throughput: 8679.207
    learn_time_ms: 18641.335
    sample_throughput: 23848.51
    sample_time_ms: 6784.156
    update_time_ms: 35.79
  timestamp: 1602806083
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:54:44,977	WARNING util.py:136 -- The `process_trial` operation took 0.7639517784118652 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    273 |          7030.11 | 44169216 |   282.08 |              308.667 |              136.242 |            797.997 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3053.625852549529
    time_step_min: 2878
  date: 2020-10-15_23-55-10
  done: false
  episode_len_mean: 798.0401204515128
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.14774216250976
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 179
  episodes_total: 55458
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.919743289585587e-11
        cur_lr: 5.0e-05
        entropy: 0.055322108479837574
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00735678576650874
        total_loss: .inf
        vf_explained_var: 0.99937504529953
        vf_loss: 0.30066803594430286
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.02
    gpu_util_percent0: 0.32433333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692041144523071
    mean_env_wait_ms: 1.190754300632437
    mean_inference_ms: 4.309648036418076
    mean_raw_obs_processing_ms: 0.3785726612928392
  time_since_restore: 7055.840481996536
  time_this_iter_s: 25.730446815490723
  time_total_s: 7055.840481996536
  timers:
    learn_throughput: 8680.41
    learn_time_ms: 18638.75
    sample_throughput: 23846.657
    sample_time_ms: 6784.683
    update_time_ms: 34.801
  timestamp: 1602806110
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:55:11,760	WARNING util.py:136 -- The `process_trial` operation took 0.7290487289428711 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    274 |          7055.84 | 44331008 |  282.148 |              308.667 |              136.242 |             798.04 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3053.1370990939163
    time_step_min: 2878
  date: 2020-10-15_23-55-37
  done: false
  episode_len_mean: 798.0860941430111
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.22069327845435
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 202
  episodes_total: 55660
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0379614934378378e-10
        cur_lr: 5.0e-05
        entropy: 0.061303588872154556
        entropy_coeff: 0.0005000000000000001
        kl: 0.004622042062692344
        model: {}
        policy_loss: -0.01075281624798663
        total_loss: 0.39979727069536847
        vf_explained_var: 0.9993221163749695
        vf_loss: 0.41058073192834854
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.066666666666663
    gpu_util_percent0: 0.3336666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691812548604688
    mean_env_wait_ms: 1.1906971695492523
    mean_inference_ms: 4.3095173443441235
    mean_raw_obs_processing_ms: 0.37856278174155117
  time_since_restore: 7081.4372363090515
  time_this_iter_s: 25.59675431251526
  time_total_s: 7081.4372363090515
  timers:
    learn_throughput: 8686.304
    learn_time_ms: 18626.104
    sample_throughput: 23867.001
    sample_time_ms: 6778.899
    update_time_ms: 33.122
  timestamp: 1602806137
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: 0f5d2_00000
  
2020-10-15 23:55:38,499	WARNING util.py:136 -- The `process_trial` operation took 0.7827401161193848 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    275 |          7081.44 | 44492800 |  282.221 |              308.667 |              136.242 |            798.086 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3052.636383173051
    time_step_min: 2878
  date: 2020-10-15_23-56-04
  done: false
  episode_len_mean: 798.1289127516778
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.29519083451936
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 55875
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.189807467189189e-11
        cur_lr: 5.0e-05
        entropy: 0.07083870532612006
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010558469550839314
        total_loss: .inf
        vf_explained_var: 0.9989461898803711
        vf_loss: 0.571654349565506
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380645161290325
    gpu_util_percent0: 0.32870967741935486
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691582984469906
    mean_env_wait_ms: 1.1906365469847322
    mean_inference_ms: 4.309396675888471
    mean_raw_obs_processing_ms: 0.37855397015433667
  time_since_restore: 7107.156701564789
  time_this_iter_s: 25.719465255737305
  time_total_s: 7107.156701564789
  timers:
    learn_throughput: 8694.424
    learn_time_ms: 18608.709
    sample_throughput: 23849.297
    sample_time_ms: 6783.932
    update_time_ms: 30.873
  timestamp: 1602806164
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:56:05,262	WARNING util.py:136 -- The `process_trial` operation took 0.7839152812957764 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    276 |          7107.16 | 44654592 |  282.295 |              308.667 |              136.242 |            798.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3052.19827263156
    time_step_min: 2878
  date: 2020-10-15_23-56-31
  done: false
  episode_len_mean: 798.1630316540347
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.3584593145
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 200
  episodes_total: 56075
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.784711200783784e-11
        cur_lr: 5.0e-05
        entropy: 0.07371996529400349
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011726730546797626
        total_loss: .inf
        vf_explained_var: 0.9985727667808533
        vf_loss: 0.7205009410778681
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.367741935483874
    gpu_util_percent0: 0.3667741935483872
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691377479556483
    mean_env_wait_ms: 1.1905809552361108
    mean_inference_ms: 4.309279548477475
    mean_raw_obs_processing_ms: 0.378544393442351
  time_since_restore: 7133.175038814545
  time_this_iter_s: 26.01833724975586
  time_total_s: 7133.175038814545
  timers:
    learn_throughput: 8693.02
    learn_time_ms: 18611.713
    sample_throughput: 23862.953
    sample_time_ms: 6780.05
    update_time_ms: 30.315
  timestamp: 1602806191
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:56:32,310	WARNING util.py:136 -- The `process_trial` operation took 0.7670426368713379 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    277 |          7133.18 | 44816384 |  282.358 |              308.667 |              136.242 |            798.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3051.8922682887787
    time_step_min: 2878
  date: 2020-10-15_23-56-58
  done: false
  episode_len_mean: 798.1869212037185
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.4084141150207
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 56259
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1677066801175674e-10
        cur_lr: 5.0e-05
        entropy: 0.07184266361097495
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010172345103152717
        total_loss: .inf
        vf_explained_var: 0.9986142516136169
        vf_loss: 0.6731013307968775
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35333333333334
    gpu_util_percent0: 0.33399999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691184275205735
    mean_env_wait_ms: 1.190527402057228
    mean_inference_ms: 4.309167923021702
    mean_raw_obs_processing_ms: 0.3785362503826318
  time_since_restore: 7158.926494836807
  time_this_iter_s: 25.751456022262573
  time_total_s: 7158.926494836807
  timers:
    learn_throughput: 8694.378
    learn_time_ms: 18608.806
    sample_throughput: 23854.987
    sample_time_ms: 6782.314
    update_time_ms: 30.766
  timestamp: 1602806218
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:56:59,096	WARNING util.py:136 -- The `process_trial` operation took 0.7757699489593506 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    278 |          7158.93 | 44978176 |  282.408 |              308.667 |              136.242 |            798.187 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3051.446320975402
    time_step_min: 2878
  date: 2020-10-15_23-57-24
  done: false
  episode_len_mean: 798.2307487956929
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.4794553998468
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 205
  episodes_total: 56464
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7515600201763515e-10
        cur_lr: 5.0e-05
        entropy: 0.061772466326753296
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007965044274897082
        total_loss: .inf
        vf_explained_var: 0.9991278648376465
        vf_loss: 0.45053767661253613
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.661290322580644
    gpu_util_percent0: 0.3535483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690952064175936
    mean_env_wait_ms: 1.1904697316009856
    mean_inference_ms: 4.309048538125003
    mean_raw_obs_processing_ms: 0.3785269709264974
  time_since_restore: 7184.721987247467
  time_this_iter_s: 25.79549241065979
  time_total_s: 7184.721987247467
  timers:
    learn_throughput: 8667.114
    learn_time_ms: 18667.344
    sample_throughput: 23852.328
    sample_time_ms: 6783.07
    update_time_ms: 30.848
  timestamp: 1602806244
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:57:26,083	WARNING util.py:136 -- The `process_trial` operation took 0.7588405609130859 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    279 |          7184.72 | 45139968 |  282.479 |              308.667 |              136.242 |            798.231 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3050.922874593984
    time_step_min: 2878
  date: 2020-10-15_23-57-51
  done: false
  episode_len_mean: 798.2803789429116
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.5597138201574
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 56684
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6273400302645277e-10
        cur_lr: 5.0e-05
        entropy: 0.05829969638337692
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036347604861172536
        model: {}
        policy_loss: -0.007836543411637345
        total_loss: 0.27577238778273266
        vf_explained_var: 0.999467670917511
        vf_loss: 0.2836380774776141
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05333333333333
    gpu_util_percent0: 0.36900000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469076019347853
    mean_env_wait_ms: 1.1904081137665286
    mean_inference_ms: 4.308932231894722
    mean_raw_obs_processing_ms: 0.3785179793769849
  time_since_restore: 7210.494336843491
  time_this_iter_s: 25.77234959602356
  time_total_s: 7210.494336843491
  timers:
    learn_throughput: 8665.786
    learn_time_ms: 18670.206
    sample_throughput: 23881.588
    sample_time_ms: 6774.759
    update_time_ms: 31.833
  timestamp: 1602806271
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: 0f5d2_00000
  
2020-10-15 23:57:52,947	WARNING util.py:136 -- The `process_trial` operation took 0.7337932586669922 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    280 |          7210.49 | 45301760 |   282.56 |              308.667 |              136.242 |             798.28 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3050.4670983179676
    time_step_min: 2878
  date: 2020-10-15_23-58-18
  done: false
  episode_len_mean: 798.3246237164158
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.62768812047864
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 56872
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3136700151322638e-10
        cur_lr: 5.0e-05
        entropy: 0.05704321091373762
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010984125509821752
        total_loss: .inf
        vf_explained_var: 0.9995636940002441
        vf_loss: 0.2202314684788386
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.306451612903228
    gpu_util_percent0: 0.3125806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690562262672524
    mean_env_wait_ms: 1.1903544751015338
    mean_inference_ms: 4.308818701071568
    mean_raw_obs_processing_ms: 0.3785090691529751
  time_since_restore: 7236.283254146576
  time_this_iter_s: 25.788917303085327
  time_total_s: 7236.283254146576
  timers:
    learn_throughput: 8668.589
    learn_time_ms: 18664.168
    sample_throughput: 23859.764
    sample_time_ms: 6780.956
    update_time_ms: 29.946
  timestamp: 1602806298
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:58:19,773	WARNING util.py:136 -- The `process_trial` operation took 0.7682914733886719 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    281 |          7236.28 | 45463552 |  282.628 |              308.667 |              136.242 |            798.325 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3050.0458100950577
    time_step_min: 2878
  date: 2020-10-15_23-58-45
  done: false
  episode_len_mean: 798.3648122831003
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.6920291356086
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 57054
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9705050226983952e-10
        cur_lr: 5.0e-05
        entropy: 0.059444078554709755
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006225006582099013
        total_loss: .inf
        vf_explained_var: 0.999208927154541
        vf_loss: 0.4041055738925934
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.206666666666667
    gpu_util_percent0: 0.31233333333333324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690379549383648
    mean_env_wait_ms: 1.1903015949814162
    mean_inference_ms: 4.308714182707075
    mean_raw_obs_processing_ms: 0.37850112196048447
  time_since_restore: 7261.956282615662
  time_this_iter_s: 25.673028469085693
  time_total_s: 7261.956282615662
  timers:
    learn_throughput: 8675.981
    learn_time_ms: 18648.266
    sample_throughput: 23889.005
    sample_time_ms: 6772.656
    update_time_ms: 29.44
  timestamp: 1602806325
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:58:46,608	WARNING util.py:136 -- The `process_trial` operation took 0.8016219139099121 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    282 |          7261.96 | 45625344 |  282.692 |              308.667 |              136.242 |            798.365 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3049.5588096527863
    time_step_min: 2878
  date: 2020-10-15_23-59-12
  done: false
  episode_len_mean: 798.4091996577197
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.76669635424815
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 209
  episodes_total: 57263
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9557575340475927e-10
        cur_lr: 5.0e-05
        entropy: 0.06350688356906176
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008935384277720004
        total_loss: .inf
        vf_explained_var: 0.9993464350700378
        vf_loss: 0.35613797356685
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.76129032258065
    gpu_util_percent0: 0.3319354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690157699447928
    mean_env_wait_ms: 1.1902441436701312
    mean_inference_ms: 4.30860099827649
    mean_raw_obs_processing_ms: 0.37849228166290033
  time_since_restore: 7287.935337781906
  time_this_iter_s: 25.979055166244507
  time_total_s: 7287.935337781906
  timers:
    learn_throughput: 8668.126
    learn_time_ms: 18665.165
    sample_throughput: 23885.137
    sample_time_ms: 6773.752
    update_time_ms: 29.541
  timestamp: 1602806352
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:59:13,614	WARNING util.py:136 -- The `process_trial` operation took 0.7540159225463867 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    283 |          7287.94 | 45787136 |  282.767 |              308.667 |              136.242 |            798.409 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3049.052866120076
    time_step_min: 2878
  date: 2020-10-15_23-59-39
  done: false
  episode_len_mean: 798.4542734373641
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.8415452473693
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 57483
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.433636301071389e-10
        cur_lr: 5.0e-05
        entropy: 0.06668374749521415
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009232022896564255
        total_loss: .inf
        vf_explained_var: 0.9992701411247253
        vf_loss: 0.4085746953884761
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.141935483870963
    gpu_util_percent0: 0.3483870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468994361956723
    mean_env_wait_ms: 1.1901805302411028
    mean_inference_ms: 4.30848389219362
    mean_raw_obs_processing_ms: 0.3784829522607725
  time_since_restore: 7313.670144557953
  time_this_iter_s: 25.734806776046753
  time_total_s: 7313.670144557953
  timers:
    learn_throughput: 8665.597
    learn_time_ms: 18670.613
    sample_throughput: 23901.087
    sample_time_ms: 6769.232
    update_time_ms: 30.11
  timestamp: 1602806379
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 23:59:40,570	WARNING util.py:136 -- The `process_trial` operation took 0.8058083057403564 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    284 |          7313.67 | 45948928 |  282.842 |              308.667 |              136.242 |            798.454 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3048.70460342524
    time_step_min: 2878
  date: 2020-10-16_00-00-06
  done: false
  episode_len_mean: 798.4808469315207
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.8940699764738
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 57667
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.650454451607084e-10
        cur_lr: 5.0e-05
        entropy: 0.07603630051016808
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009123859247968843
        total_loss: .inf
        vf_explained_var: 0.9975731372833252
        vf_loss: 1.182237262527148
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.843333333333337
    gpu_util_percent0: 0.37533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689753617736737
    mean_env_wait_ms: 1.190126767942703
    mean_inference_ms: 4.308373154646536
    mean_raw_obs_processing_ms: 0.37847442192090697
  time_since_restore: 7339.339819431305
  time_this_iter_s: 25.66967487335205
  time_total_s: 7339.339819431305
  timers:
    learn_throughput: 8665.493
    learn_time_ms: 18670.837
    sample_throughput: 23881.196
    sample_time_ms: 6774.87
    update_time_ms: 32.26
  timestamp: 1602806406
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:00:07,382	WARNING util.py:136 -- The `process_trial` operation took 0.7783064842224121 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    285 |          7339.34 | 46110720 |  282.894 |              308.667 |              136.242 |            798.481 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3048.4227183895123
    time_step_min: 2878
  date: 2020-10-16_00-00-33
  done: false
  episode_len_mean: 798.4895691100472
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 282.9345664676909
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 190
  episodes_total: 57857
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.975681677410626e-10
        cur_lr: 5.0e-05
        entropy: 0.0791918362180392
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012691389264849326
        total_loss: .inf
        vf_explained_var: 0.9978148341178894
        vf_loss: 1.0815058102210362
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10322580645162
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689554827928175
    mean_env_wait_ms: 1.1900709981112672
    mean_inference_ms: 4.308262373446603
    mean_raw_obs_processing_ms: 0.37846623912732474
  time_since_restore: 7365.3721759319305
  time_this_iter_s: 26.03235650062561
  time_total_s: 7365.3721759319305
  timers:
    learn_throughput: 8659.945
    learn_time_ms: 18682.798
    sample_throughput: 23831.559
    sample_time_ms: 6788.981
    update_time_ms: 34.096
  timestamp: 1602806433
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:00:34,434	WARNING util.py:136 -- The `process_trial` operation took 0.7552063465118408 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    286 |          7365.37 | 46272512 |  282.935 |              308.667 |              136.242 |             798.49 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3047.9914883352285
    time_step_min: 2878
  date: 2020-10-16_00-01-00
  done: false
  episode_len_mean: 798.5237627854117
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.0027879789732
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 217
  episodes_total: 58074
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.496352251611594e-09
        cur_lr: 5.0e-05
        entropy: 0.0633953536550204
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009275745326400889
        total_loss: .inf
        vf_explained_var: 0.9990836977958679
        vf_loss: 0.48462606718142826
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0
    gpu_util_percent0: 0.30548387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689338019030423
    mean_env_wait_ms: 1.1900114658228922
    mean_inference_ms: 4.30815439923122
    mean_raw_obs_processing_ms: 0.3784577055290036
  time_since_restore: 7391.40176486969
  time_this_iter_s: 26.0295889377594
  time_total_s: 7391.40176486969
  timers:
    learn_throughput: 8655.937
    learn_time_ms: 18691.449
    sample_throughput: 23833.315
    sample_time_ms: 6788.481
    update_time_ms: 35.747
  timestamp: 1602806460
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:01:01,490	WARNING util.py:136 -- The `process_trial` operation took 0.7583775520324707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    287 |           7391.4 | 46434304 |  283.003 |              308.667 |              136.242 |            798.524 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3047.5195893280224
    time_step_min: 2878
  date: 2020-10-16_00-01-27
  done: false
  episode_len_mean: 798.5652860231289
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.0748887592507
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 208
  episodes_total: 58282
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2445283774173905e-09
        cur_lr: 5.0e-05
        entropy: 0.05743660901983579
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0077552251241286285
        total_loss: .inf
        vf_explained_var: 0.9995432496070862
        vf_loss: 0.24025737370053926
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60666666666667
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146891303770243
    mean_env_wait_ms: 1.1899519192373726
    mean_inference_ms: 4.308041515313149
    mean_raw_obs_processing_ms: 0.3784481437866343
  time_since_restore: 7417.12703371048
  time_this_iter_s: 25.725268840789795
  time_total_s: 7417.12703371048
  timers:
    learn_throughput: 8654.908
    learn_time_ms: 18693.672
    sample_throughput: 23855.608
    sample_time_ms: 6782.137
    update_time_ms: 36.215
  timestamp: 1602806487
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:01:28,364	WARNING util.py:136 -- The `process_trial` operation took 0.7747893333435059 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    288 |          7417.13 | 46596096 |  283.075 |              308.667 |              136.242 |            798.565 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3047.108840393667
    time_step_min: 2878
  date: 2020-10-16_00-01-54
  done: false
  episode_len_mean: 798.6015292246112
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.13770744858107
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 179
  episodes_total: 58461
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3667925661260855e-09
        cur_lr: 5.0e-05
        entropy: 0.05495129432529211
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00767343383631669
        total_loss: .inf
        vf_explained_var: 0.9995911121368408
        vf_loss: 0.20615815619627634
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.558064516129033
    gpu_util_percent0: 0.2948387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688965331304252
    mean_env_wait_ms: 1.1899005334426185
    mean_inference_ms: 4.30794556219838
    mean_raw_obs_processing_ms: 0.3784410365253577
  time_since_restore: 7443.1183133125305
  time_this_iter_s: 25.99127960205078
  time_total_s: 7443.1183133125305
  timers:
    learn_throughput: 8646.488
    learn_time_ms: 18711.874
    sample_throughput: 23890.187
    sample_time_ms: 6772.32
    update_time_ms: 36.931
  timestamp: 1602806514
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:01:55,410	WARNING util.py:136 -- The `process_trial` operation took 0.7745003700256348 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    289 |          7443.12 | 46757888 |  283.138 |              308.667 |              136.242 |            798.602 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3046.6619984989084
    time_step_min: 2878
  date: 2020-10-16_00-02-21
  done: false
  episode_len_mean: 798.6420218206614
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.2068106912973
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 58660
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.050188849189129e-09
        cur_lr: 5.0e-05
        entropy: 0.05818154849112034
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0066415844291138155
        total_loss: .inf
        vf_explained_var: 0.9995809197425842
        vf_loss: 0.21856835732857385
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.363333333333337
    gpu_util_percent0: 0.3279999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688756273475234
    mean_env_wait_ms: 1.1898433505258443
    mean_inference_ms: 4.307831685020658
    mean_raw_obs_processing_ms: 0.37843236555126064
  time_since_restore: 7468.95400929451
  time_this_iter_s: 25.83569598197937
  time_total_s: 7468.95400929451
  timers:
    learn_throughput: 8641.117
    learn_time_ms: 18723.505
    sample_throughput: 23912.251
    sample_time_ms: 6766.072
    update_time_ms: 37.683
  timestamp: 1602806541
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:02:22,447	WARNING util.py:136 -- The `process_trial` operation took 0.832427978515625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    290 |          7468.95 | 46919680 |  283.207 |              308.667 |              136.242 |            798.642 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3046.1682897397986
    time_step_min: 2878
  date: 2020-10-16_00-02-48
  done: false
  episode_len_mean: 798.6876263269639
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.28192498230686
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 58875
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.575283273783693e-09
        cur_lr: 5.0e-05
        entropy: 0.06386660411953926
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009477394788215557
        total_loss: .inf
        vf_explained_var: 0.999619722366333
        vf_loss: 0.20363453775644302
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.690322580645166
    gpu_util_percent0: 0.3635483870967741
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688566571264014
    mean_env_wait_ms: 1.1897824074074823
    mean_inference_ms: 4.30772886165799
    mean_raw_obs_processing_ms: 0.37842415345151004
  time_since_restore: 7494.802872657776
  time_this_iter_s: 25.84886336326599
  time_total_s: 7494.802872657776
  timers:
    learn_throughput: 8638.098
    learn_time_ms: 18730.049
    sample_throughput: 23924.822
    sample_time_ms: 6762.516
    update_time_ms: 40.031
  timestamp: 1602806568
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:02:49,457	WARNING util.py:136 -- The `process_trial` operation took 0.7875218391418457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    291 |           7494.8 | 47081472 |  283.282 |              308.667 |              136.242 |            798.688 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3045.849745934959
    time_step_min: 2878
  date: 2020-10-16_00-03-15
  done: false
  episode_len_mean: 798.7071061006161
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.3234646895521
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 59076
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1362924910675543e-08
        cur_lr: 5.0e-05
        entropy: 0.0965523353467385
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011603139146851996
        total_loss: .inf
        vf_explained_var: 0.996610701084137
        vf_loss: 1.6660547355810802
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23225806451613
    gpu_util_percent0: 0.28516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688373007016514
    mean_env_wait_ms: 1.189724460305225
    mean_inference_ms: 4.307622239652754
    mean_raw_obs_processing_ms: 0.3784153578556013
  time_since_restore: 7520.633109092712
  time_this_iter_s: 25.830236434936523
  time_total_s: 7520.633109092712
  timers:
    learn_throughput: 8635.637
    learn_time_ms: 18735.386
    sample_throughput: 23885.541
    sample_time_ms: 6773.638
    update_time_ms: 38.501
  timestamp: 1602806595
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:03:16,392	WARNING util.py:136 -- The `process_trial` operation took 0.8258564472198486 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    292 |          7520.63 | 47243264 |  283.323 |              308.667 |              136.242 |            798.707 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3045.747728884536
    time_step_min: 2878
  date: 2020-10-16_00-03-42
  done: false
  episode_len_mean: 798.706419386412
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.3352729768231
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 59258
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7044387366013314e-08
        cur_lr: 5.0e-05
        entropy: 0.10478341517349084
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011884580162586644
        total_loss: .inf
        vf_explained_var: 0.9955108761787415
        vf_loss: 2.178488870461782
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.358064516129033
    gpu_util_percent0: 0.2661290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688200780164115
    mean_env_wait_ms: 1.1896722313209305
    mean_inference_ms: 4.307526973484322
    mean_raw_obs_processing_ms: 0.37840809148349625
  time_since_restore: 7546.502863645554
  time_this_iter_s: 25.869754552841187
  time_total_s: 7546.502863645554
  timers:
    learn_throughput: 8647.659
    learn_time_ms: 18709.341
    sample_throughput: 23862.441
    sample_time_ms: 6780.195
    update_time_ms: 38.791
  timestamp: 1602806622
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:03:43,383	WARNING util.py:136 -- The `process_trial` operation took 0.8355777263641357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    293 |           7546.5 | 47405056 |  283.335 |              308.667 |              136.242 |            798.706 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3045.477133530758
    time_step_min: 2878
  date: 2020-10-16_00-04-09
  done: false
  episode_len_mean: 798.731569919957
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.3826025099312
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 59468
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5566581049019972e-08
        cur_lr: 5.0e-05
        entropy: 0.07380057498812675
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010222053669470673
        total_loss: .inf
        vf_explained_var: 0.9983195662498474
        vf_loss: 0.8891342928012212
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34516129032258
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687992720564233
    mean_env_wait_ms: 1.189612375663878
    mean_inference_ms: 4.307415851833238
    mean_raw_obs_processing_ms: 0.3783992670824738
  time_since_restore: 7572.5403208732605
  time_this_iter_s: 26.03745722770691
  time_total_s: 7572.5403208732605
  timers:
    learn_throughput: 8644.238
    learn_time_ms: 18716.746
    sample_throughput: 23829.696
    sample_time_ms: 6789.512
    update_time_ms: 39.037
  timestamp: 1602806649
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:04:10,521	WARNING util.py:136 -- The `process_trial` operation took 0.7851653099060059 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    294 |          7572.54 | 47566848 |  283.383 |              308.667 |              136.242 |            798.732 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3045.0154901007527
    time_step_min: 2878
  date: 2020-10-16_00-04-36
  done: false
  episode_len_mean: 798.7715415417092
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.45238722608974
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 219
  episodes_total: 59687
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.834987157352995e-08
        cur_lr: 5.0e-05
        entropy: 0.05483874958008528
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040369023142072065
        model: {}
        policy_loss: -0.007360763071725766
        total_loss: 0.26704611132542294
        vf_explained_var: 0.9994792342185974
        vf_loss: 0.27443428213397664
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85
    gpu_util_percent0: 0.3449999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468780133914037
    mean_env_wait_ms: 1.1895508542926079
    mean_inference_ms: 4.3073067785395756
    mean_raw_obs_processing_ms: 0.3783905607836957
  time_since_restore: 7598.146280527115
  time_this_iter_s: 25.60595965385437
  time_total_s: 7598.146280527115
  timers:
    learn_throughput: 8646.8
    learn_time_ms: 18711.199
    sample_throughput: 23832.317
    sample_time_ms: 6788.765
    update_time_ms: 38.544
  timestamp: 1602806676
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: 0f5d2_00000
  
2020-10-16 00:04:37,213	WARNING util.py:136 -- The `process_trial` operation took 0.7969362735748291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    295 |          7598.15 | 47728640 |  283.452 |              308.667 |              136.242 |            798.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3044.603579665096
    time_step_min: 2878
  date: 2020-10-16_00-05-03
  done: false
  episode_len_mean: 798.8099508968835
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.5144632010046
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 59874
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9174935786764975e-08
        cur_lr: 5.0e-05
        entropy: 0.050692591816186905
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007869638081804927
        total_loss: .inf
        vf_explained_var: 0.999553918838501
        vf_loss: 0.21311862394213676
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380645161290325
    gpu_util_percent0: 0.2861290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8709677419354844
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687634682769335
    mean_env_wait_ms: 1.1894972950232883
    mean_inference_ms: 4.307209211056658
    mean_raw_obs_processing_ms: 0.3783828205817701
  time_since_restore: 7624.028343200684
  time_this_iter_s: 25.882062673568726
  time_total_s: 7624.028343200684
  timers:
    learn_throughput: 8649.967
    learn_time_ms: 18704.349
    sample_throughput: 23890.124
    sample_time_ms: 6772.338
    update_time_ms: 38.471
  timestamp: 1602806703
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:05:04,169	WARNING util.py:136 -- The `process_trial` operation took 0.7857875823974609 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    296 |          7624.03 | 47890432 |  283.514 |              308.667 |              136.242 |             798.81 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3044.2188437187606
    time_step_min: 2878
  date: 2020-10-16_00-05-30
  done: false
  episode_len_mean: 798.8443286266152
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.5734304548075
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 60056
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8762403680147468e-08
        cur_lr: 5.0e-05
        entropy: 0.04924075212329626
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0073873401221741615
        total_loss: .inf
        vf_explained_var: 0.9995300769805908
        vf_loss: 0.232844065874815
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.177419354838715
    gpu_util_percent0: 0.29354838709677417
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687464962780017
    mean_env_wait_ms: 1.1894442191755097
    mean_inference_ms: 4.3071131761514545
    mean_raw_obs_processing_ms: 0.37837519846296463
  time_since_restore: 7650.01998591423
  time_this_iter_s: 25.991642713546753
  time_total_s: 7650.01998591423
  timers:
    learn_throughput: 8656.926
    learn_time_ms: 18689.313
    sample_throughput: 23877.838
    sample_time_ms: 6775.823
    update_time_ms: 36.519
  timestamp: 1602806730
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:05:31,305	WARNING util.py:136 -- The `process_trial` operation took 0.7868301868438721 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    297 |          7650.02 | 48052224 |  283.573 |              308.667 |              136.242 |            798.844 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3043.7529962484646
    time_step_min: 2878
  date: 2020-10-16_00-05-56
  done: false
  episode_len_mean: 798.8852649391155
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.64540909945504
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 222
  episodes_total: 60278
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.31436055202212e-08
        cur_lr: 5.0e-05
        entropy: 0.05321617300311724
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008913883929684138
        total_loss: .inf
        vf_explained_var: 0.9993106722831726
        vf_loss: 0.3662925238410632
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.550000000000004
    gpu_util_percent0: 0.32300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146872515913004
    mean_env_wait_ms: 1.1893815543371646
    mean_inference_ms: 4.30700750390428
    mean_raw_obs_processing_ms: 0.3783671826754903
  time_since_restore: 7675.441203594208
  time_this_iter_s: 25.421217679977417
  time_total_s: 7675.441203594208
  timers:
    learn_throughput: 8678.541
    learn_time_ms: 18642.765
    sample_throughput: 23846.78
    sample_time_ms: 6784.648
    update_time_ms: 36.277
  timestamp: 1602806756
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:05:57,828	WARNING util.py:136 -- The `process_trial` operation took 0.7647218704223633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    298 |          7675.44 | 48214016 |  283.645 |              308.667 |              136.242 |            798.885 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3043.319735318445
    time_step_min: 2878
  date: 2020-10-16_00-06-23
  done: false
  episode_len_mean: 798.9204774658599
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.7121693073976
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 208
  episodes_total: 60486
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.471540828033179e-08
        cur_lr: 5.0e-05
        entropy: 0.05200602921346823
        entropy_coeff: 0.0005000000000000001
        kl: 0.005947431044963499
        model: {}
        policy_loss: -0.007025093267050882
        total_loss: 0.25529612228274345
        vf_explained_var: 0.9994871020317078
        vf_loss: 0.2623472201327483
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89333333333333
    gpu_util_percent0: 0.3206666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687061678411287
    mean_env_wait_ms: 1.1893227298945745
    mean_inference_ms: 4.306904294175909
    mean_raw_obs_processing_ms: 0.37835831450242885
  time_since_restore: 7701.1616294384
  time_this_iter_s: 25.720425844192505
  time_total_s: 7701.1616294384
  timers:
    learn_throughput: 8686.575
    learn_time_ms: 18625.523
    sample_throughput: 23851.437
    sample_time_ms: 6783.323
    update_time_ms: 36.394
  timestamp: 1602806783
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: 0f5d2_00000
  
2020-10-16 00:06:24,791	WARNING util.py:136 -- The `process_trial` operation took 0.8911552429199219 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    299 |          7701.16 | 48375808 |  283.712 |              308.667 |              136.242 |             798.92 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3042.924543550329
    time_step_min: 2878
  date: 2020-10-16_00-06-50
  done: false
  episode_len_mean: 798.9577364959533
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.77226548705244
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 181
  episodes_total: 60667
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.471540828033179e-08
        cur_lr: 5.0e-05
        entropy: 0.04929885050902764
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006631755008129403
        total_loss: .inf
        vf_explained_var: 0.9995689392089844
        vf_loss: 0.21472944815953574
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53548387096774
    gpu_util_percent0: 0.31677419354838704
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468690167194803
    mean_env_wait_ms: 1.1892706381668483
    mean_inference_ms: 4.306812644861092
    mean_raw_obs_processing_ms: 0.37835100960623635
  time_since_restore: 7727.074211835861
  time_this_iter_s: 25.912582397460938
  time_total_s: 7727.074211835861
  timers:
    learn_throughput: 8683.698
    learn_time_ms: 18631.693
    sample_throughput: 23849.899
    sample_time_ms: 6783.761
    update_time_ms: 36.096
  timestamp: 1602806810
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:06:51,774	WARNING util.py:136 -- The `process_trial` operation took 0.7864120006561279 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    300 |          7727.07 | 48537600 |  283.772 |              308.667 |              136.242 |            798.958 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3042.5131860182173
    time_step_min: 2878
  date: 2020-10-16_00-07-17
  done: false
  episode_len_mean: 798.9940188635841
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.8332840382526
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 191
  episodes_total: 60858
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.707311242049768e-08
        cur_lr: 5.0e-05
        entropy: 0.050573162734508514
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0078032346003359026
        total_loss: .inf
        vf_explained_var: 0.9992628693580627
        vf_loss: 0.3708868945638339
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.093548387096778
    gpu_util_percent0: 0.33677419354838706
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468673199377066
    mean_env_wait_ms: 1.1892143468282848
    mean_inference_ms: 4.306713583169279
    mean_raw_obs_processing_ms: 0.3783436531308398
  time_since_restore: 7753.018211841583
  time_this_iter_s: 25.944000005722046
  time_total_s: 7753.018211841583
  timers:
    learn_throughput: 8686.951
    learn_time_ms: 18624.716
    sample_throughput: 23790.954
    sample_time_ms: 6800.568
    update_time_ms: 34.493
  timestamp: 1602806837
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:07:18,837	WARNING util.py:136 -- The `process_trial` operation took 0.8260178565979004 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    301 |          7753.02 | 48699392 |  283.833 |              308.667 |              136.242 |            798.994 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3042.047771952818
    time_step_min: 2878
  date: 2020-10-16_00-07-44
  done: false
  episode_len_mean: 799.0309450520663
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.9030785290851
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 218
  episodes_total: 61076
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4560966863074653e-07
        cur_lr: 5.0e-05
        entropy: 0.05287955577174822
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006101237372301209
        total_loss: .inf
        vf_explained_var: 0.9995284676551819
        vf_loss: 0.25819779684146243
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.939999999999994
    gpu_util_percent0: 0.3613333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468651205045756
    mean_env_wait_ms: 1.1891538271543467
    mean_inference_ms: 4.306609878470145
    mean_raw_obs_processing_ms: 0.37833525671212115
  time_since_restore: 7778.669908046722
  time_this_iter_s: 25.65169620513916
  time_total_s: 7778.669908046722
  timers:
    learn_throughput: 8691.43
    learn_time_ms: 18615.118
    sample_throughput: 23836.118
    sample_time_ms: 6787.683
    update_time_ms: 36.309
  timestamp: 1602806864
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:07:45,759	WARNING util.py:136 -- The `process_trial` operation took 0.8840212821960449 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    302 |          7778.67 | 48861184 |  283.903 |              308.667 |              136.242 |            799.031 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3041.604757861738
    time_step_min: 2878
  date: 2020-10-16_00-08-11
  done: false
  episode_len_mean: 799.0727783035802
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 283.96911100496135
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 61282
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1841450294611982e-07
        cur_lr: 5.0e-05
        entropy: 0.05229502140233914
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007807342170660074
        total_loss: .inf
        vf_explained_var: 0.9995617270469666
        vf_loss: 0.24551788965861002
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.306451612903235
    gpu_util_percent0: 0.29903225806451617
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686343661614137
    mean_env_wait_ms: 1.1890953682428713
    mean_inference_ms: 4.306515308198786
    mean_raw_obs_processing_ms: 0.3783274512470573
  time_since_restore: 7804.646066427231
  time_this_iter_s: 25.976158380508423
  time_total_s: 7804.646066427231
  timers:
    learn_throughput: 8689.765
    learn_time_ms: 18618.684
    sample_throughput: 23784.811
    sample_time_ms: 6802.324
    update_time_ms: 36.373
  timestamp: 1602806891
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:08:12,928	WARNING util.py:136 -- The `process_trial` operation took 0.7947120666503906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    303 |          7804.65 | 49022976 |  283.969 |              308.667 |              136.242 |            799.073 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3041.21608113828
    time_step_min: 2878
  date: 2020-10-16_00-08-38
  done: false
  episode_len_mean: 799.1058052129771
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.0275080044528
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 61462
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.276217544191797e-07
        cur_lr: 5.0e-05
        entropy: 0.04931490061183771
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007995809783096774
        total_loss: .inf
        vf_explained_var: 0.9996364116668701
        vf_loss: 0.16979516173402467
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.432258064516137
    gpu_util_percent0: 0.3158064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468618707933503
    mean_env_wait_ms: 1.1890432614369624
    mean_inference_ms: 4.306426181320808
    mean_raw_obs_processing_ms: 0.37832011417710837
  time_since_restore: 7830.598338127136
  time_this_iter_s: 25.952271699905396
  time_total_s: 7830.598338127136
  timers:
    learn_throughput: 8693.248
    learn_time_ms: 18611.225
    sample_throughput: 23749.27
    sample_time_ms: 6812.504
    update_time_ms: 35.754
  timestamp: 1602806918
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:08:40,068	WARNING util.py:136 -- The `process_trial` operation took 0.8137524127960205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    304 |           7830.6 | 49184768 |  284.028 |              308.667 |              136.242 |            799.106 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3040.798993833171
    time_step_min: 2878
  date: 2020-10-16_00-09-06
  done: false
  episode_len_mean: 799.1422894770988
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.0901625244265
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 194
  episodes_total: 61656
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.914326316287697e-07
        cur_lr: 5.0e-05
        entropy: 0.055540236023565136
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0077578202860119445
        total_loss: .inf
        vf_explained_var: 0.9996216893196106
        vf_loss: 0.1933278702199459
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.851612903225814
    gpu_util_percent0: 0.334516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686034806027198
    mean_env_wait_ms: 1.1889883000410395
    mean_inference_ms: 4.306332077375158
    mean_raw_obs_processing_ms: 0.37831297479504483
  time_since_restore: 7856.535366296768
  time_this_iter_s: 25.937028169631958
  time_total_s: 7856.535366296768
  timers:
    learn_throughput: 8681.798
    learn_time_ms: 18635.772
    sample_throughput: 23751.691
    sample_time_ms: 6811.81
    update_time_ms: 43.629
  timestamp: 1602806946
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:09:07,158	WARNING util.py:136 -- The `process_trial` operation took 0.8588318824768066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    305 |          7856.54 | 49346560 |   284.09 |              308.667 |              136.242 |            799.142 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3040.3421078161887
    time_step_min: 2878
  date: 2020-10-16_00-09-32
  done: false
  episode_len_mean: 799.178145502731
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.1587052777864
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 226
  episodes_total: 61882
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.371489474431545e-07
        cur_lr: 5.0e-05
        entropy: 0.06233204994350672
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010977622031229354
        total_loss: .inf
        vf_explained_var: 0.9993054270744324
        vf_loss: 0.3728857586781184
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5
    gpu_util_percent0: 0.30677419354838714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146858253667706
    mean_env_wait_ms: 1.1889235648413223
    mean_inference_ms: 4.306226163780362
    mean_raw_obs_processing_ms: 0.3783041986752349
  time_since_restore: 7882.359574079514
  time_this_iter_s: 25.82420778274536
  time_total_s: 7882.359574079514
  timers:
    learn_throughput: 8683.154
    learn_time_ms: 18632.862
    sample_throughput: 23759.524
    sample_time_ms: 6809.564
    update_time_ms: 42.16
  timestamp: 1602806972
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:09:34,141	WARNING util.py:136 -- The `process_trial` operation took 0.8625326156616211 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    306 |          7882.36 | 49508352 |  284.159 |              308.667 |              136.242 |            799.178 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3039.960816865994
    time_step_min: 2878
  date: 2020-10-16_00-09-59
  done: false
  episode_len_mean: 799.2070781919521
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.21562901803856
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 196
  episodes_total: 62078
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1057234211647318e-06
        cur_lr: 5.0e-05
        entropy: 0.057269440653423466
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008105764592376849
        total_loss: .inf
        vf_explained_var: 0.9992366433143616
        vf_loss: 0.39140255252520245
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.47000000000001
    gpu_util_percent0: 0.3466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685653776949792
    mean_env_wait_ms: 1.1888680202997635
    mean_inference_ms: 4.306138118835844
    mean_raw_obs_processing_ms: 0.37829705377263495
  time_since_restore: 7908.114610671997
  time_this_iter_s: 25.75503659248352
  time_total_s: 7908.114610671997
  timers:
    learn_throughput: 8686.996
    learn_time_ms: 18624.619
    sample_throughput: 23793.761
    sample_time_ms: 6799.766
    update_time_ms: 43.153
  timestamp: 1602806999
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:10:01,151	WARNING util.py:136 -- The `process_trial` operation took 0.8582751750946045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    307 |          7908.11 | 49670144 |  284.216 |              308.667 |              136.242 |            799.207 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3039.5991514391785
    time_step_min: 2878
  date: 2020-10-16_00-10-26
  done: false
  episode_len_mean: 799.2364638044298
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.2700600180961
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 181
  episodes_total: 62259
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6585851317470977e-06
        cur_lr: 5.0e-05
        entropy: 0.057170987129211426
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00902517382868003
        total_loss: .inf
        vf_explained_var: 0.9993206858634949
        vf_loss: 0.3314601356784503
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14516129032258
    gpu_util_percent0: 0.33741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685504076004569
    mean_env_wait_ms: 1.1888163323719823
    mean_inference_ms: 4.306050735032826
    mean_raw_obs_processing_ms: 0.37829000526498063
  time_since_restore: 7933.9082498550415
  time_this_iter_s: 25.793639183044434
  time_total_s: 7933.9082498550415
  timers:
    learn_throughput: 8666.811
    learn_time_ms: 18667.997
    sample_throughput: 23795.762
    sample_time_ms: 6799.194
    update_time_ms: 41.533
  timestamp: 1602807026
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:10:28,077	WARNING util.py:136 -- The `process_trial` operation took 0.8375802040100098 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    308 |          7933.91 | 49831936 |   284.27 |              308.667 |              136.242 |            799.236 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3039.1854456333695
    time_step_min: 2878
  date: 2020-10-16_00-10-54
  done: false
  episode_len_mean: 799.2739658043033
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.3318667916353
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 205
  episodes_total: 62464
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4878776976206463e-06
        cur_lr: 5.0e-05
        entropy: 0.060631701412300266
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0070580829778919
        total_loss: .inf
        vf_explained_var: 0.9994992613792419
        vf_loss: 0.27353760475913685
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.758064516129032
    gpu_util_percent0: 0.38193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685324695842208
    mean_env_wait_ms: 1.188758667405852
    mean_inference_ms: 4.305958751843783
    mean_raw_obs_processing_ms: 0.37828272780690125
  time_since_restore: 7959.917623996735
  time_this_iter_s: 26.009374141693115
  time_total_s: 7959.917623996735
  timers:
    learn_throughput: 8654.465
    learn_time_ms: 18694.627
    sample_throughput: 23797.217
    sample_time_ms: 6798.778
    update_time_ms: 42.833
  timestamp: 1602807054
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:10:55,233	WARNING util.py:136 -- The `process_trial` operation took 0.8551986217498779 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    309 |          7959.92 | 49993728 |  284.332 |              308.667 |              136.242 |            799.274 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3038.7500798110077
    time_step_min: 2878
  date: 2020-10-16_00-11-20
  done: false
  episode_len_mean: 799.3094410056793
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.39529975267925
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 62684
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.73181654643097e-06
        cur_lr: 5.0e-05
        entropy: 0.0601249864945809
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009901141665371446
        total_loss: .inf
        vf_explained_var: 0.9993874430656433
        vf_loss: 0.3335751270254453
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.173333333333336
    gpu_util_percent0: 0.33133333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468514111864512
    mean_env_wait_ms: 1.1886948972666453
    mean_inference_ms: 4.305855634905523
    mean_raw_obs_processing_ms: 0.37827427596568064
  time_since_restore: 7985.553185224533
  time_this_iter_s: 25.635561227798462
  time_total_s: 7985.553185224533
  timers:
    learn_throughput: 8663.867
    learn_time_ms: 18674.34
    sample_throughput: 23818.832
    sample_time_ms: 6792.609
    update_time_ms: 41.024
  timestamp: 1602807080
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:11:22,068	WARNING util.py:136 -- The `process_trial` operation took 0.8450729846954346 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    310 |          7985.55 | 50155520 |  284.395 |              308.667 |              136.242 |            799.309 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3038.3674284622975
    time_step_min: 2878
  date: 2020-10-16_00-11-47
  done: false
  episode_len_mean: 799.3442341339271
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.4535019352099
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 62870
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.597724819646453e-06
        cur_lr: 5.0e-05
        entropy: 0.05699097861846288
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008010772939693803
        total_loss: .inf
        vf_explained_var: 0.9997034072875977
        vf_loss: 0.14451748505234718
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.987096774193553
    gpu_util_percent0: 0.3280645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684999174824132
    mean_env_wait_ms: 1.1886422236584886
    mean_inference_ms: 4.305773371800079
    mean_raw_obs_processing_ms: 0.37826760029297174
  time_since_restore: 8011.413791418076
  time_this_iter_s: 25.86060619354248
  time_total_s: 8011.413791418076
  timers:
    learn_throughput: 8659.217
    learn_time_ms: 18684.369
    sample_throughput: 23899.058
    sample_time_ms: 6769.807
    update_time_ms: 41.853
  timestamp: 1602807107
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:11:49,097	WARNING util.py:136 -- The `process_trial` operation took 0.8625991344451904 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    311 |          8011.41 | 50317312 |  284.454 |              308.667 |              136.242 |            799.344 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3038.001269478562
    time_step_min: 2878
  date: 2020-10-16_00-12-14
  done: false
  episode_len_mean: 799.3763440860215
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.50923691189143
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 63054
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.396587229469678e-06
        cur_lr: 5.0e-05
        entropy: 0.05748292524367571
        entropy_coeff: 0.0005000000000000001
        kl: 0.00368935971831282
        model: {}
        policy_loss: -0.009589665598468855
        total_loss: 0.1950813097258409
        vf_explained_var: 0.9995738863945007
        vf_loss: 0.20469968393445015
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.373333333333335
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684845860838558
    mean_env_wait_ms: 1.1885890811070585
    mean_inference_ms: 4.305688129420556
    mean_raw_obs_processing_ms: 0.3782606089661837
  time_since_restore: 8036.959864139557
  time_this_iter_s: 25.546072721481323
  time_total_s: 8036.959864139557
  timers:
    learn_throughput: 8661.262
    learn_time_ms: 18679.956
    sample_throughput: 23908.638
    sample_time_ms: 6767.094
    update_time_ms: 39.715
  timestamp: 1602807134
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: 0f5d2_00000
  
2020-10-16 00:12:15,881	WARNING util.py:136 -- The `process_trial` operation took 0.8534548282623291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    312 |          8036.96 | 50479104 |  284.509 |              308.667 |              136.242 |            799.376 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3037.5854117572635
    time_step_min: 2878
  date: 2020-10-16_00-12-41
  done: false
  episode_len_mean: 799.4142574883427
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.5721723677936
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 211
  episodes_total: 63265
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.198293614734839e-06
        cur_lr: 5.0e-05
        entropy: 0.06048204222073158
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01041702569151918
        total_loss: .inf
        vf_explained_var: 0.9996822476387024
        vf_loss: 0.1715968685845534
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.8741935483871
    gpu_util_percent0: 0.29096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684643326041016
    mean_env_wait_ms: 1.1885297257649976
    mean_inference_ms: 4.3055944838645965
    mean_raw_obs_processing_ms: 0.37825319505919575
  time_since_restore: 8062.903078317642
  time_this_iter_s: 25.943214178085327
  time_total_s: 8062.903078317642
  timers:
    learn_throughput: 8655.645
    learn_time_ms: 18692.078
    sample_throughput: 23968.079
    sample_time_ms: 6750.312
    update_time_ms: 39.347
  timestamp: 1602807161
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:12:42,942	WARNING util.py:136 -- The `process_trial` operation took 0.8277580738067627 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    313 |           8062.9 | 50640896 |  284.572 |              308.667 |              136.242 |            799.414 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3037.1698059675614
    time_step_min: 2878
  date: 2020-10-16_00-13-08
  done: false
  episode_len_mean: 799.452511854314
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.63492531770197
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 63479
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.297440422102259e-06
        cur_lr: 5.0e-05
        entropy: 0.058365286948780216
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007937750441972943
        total_loss: .inf
        vf_explained_var: 0.9995085597038269
        vf_loss: 0.26643622666597366
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.922580645161293
    gpu_util_percent0: 0.3151612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468449058693727
    mean_env_wait_ms: 1.1884688353725976
    mean_inference_ms: 4.305501089770242
    mean_raw_obs_processing_ms: 0.3782455565626772
  time_since_restore: 8088.700452804565
  time_this_iter_s: 25.797374486923218
  time_total_s: 8088.700452804565
  timers:
    learn_throughput: 8661.368
    learn_time_ms: 18679.728
    sample_throughput: 24014.978
    sample_time_ms: 6737.129
    update_time_ms: 40.261
  timestamp: 1602807188
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:13:09,946	WARNING util.py:136 -- The `process_trial` operation took 0.876685619354248 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    314 |           8088.7 | 50802688 |  284.635 |              308.667 |              136.242 |            799.453 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3036.837809803706
    time_step_min: 2878
  date: 2020-10-16_00-13-36
  done: false
  episode_len_mean: 799.4858870651065
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.68736211561907
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 63665
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.44616063315339e-06
        cur_lr: 5.0e-05
        entropy: 0.05656493113686641
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00973573370235196
        total_loss: .inf
        vf_explained_var: 0.999570906162262
        vf_loss: 0.20964563886324564
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03548387096774
    gpu_util_percent0: 0.3325806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684343125681104
    mean_env_wait_ms: 1.1884146606498596
    mean_inference_ms: 4.305414861453387
    mean_raw_obs_processing_ms: 0.3782385378612963
  time_since_restore: 8114.799021244049
  time_this_iter_s: 26.098568439483643
  time_total_s: 8114.799021244049
  timers:
    learn_throughput: 8659.315
    learn_time_ms: 18684.156
    sample_throughput: 23980.034
    sample_time_ms: 6746.946
    update_time_ms: 32.957
  timestamp: 1602807216
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:13:37,214	WARNING util.py:136 -- The `process_trial` operation took 0.8701207637786865 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    315 |           8114.8 | 50964480 |  284.687 |              308.667 |              136.242 |            799.486 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3036.513233565776
    time_step_min: 2878
  date: 2020-10-16_00-14-02
  done: false
  episode_len_mean: 799.5167655949007
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.7379154024779
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 63851
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4169240949730082e-05
        cur_lr: 5.0e-05
        entropy: 0.059028444811701775
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008149910242840027
        total_loss: .inf
        vf_explained_var: 0.9991303086280823
        vf_loss: 0.42321624358495075
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.300000000000004
    gpu_util_percent0: 0.3646666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684175119877454
    mean_env_wait_ms: 1.1883605881454873
    mean_inference_ms: 4.305327063446354
    mean_raw_obs_processing_ms: 0.3782313895744276
  time_since_restore: 8140.511559963226
  time_this_iter_s: 25.712538719177246
  time_total_s: 8140.511559963226
  timers:
    learn_throughput: 8660.143
    learn_time_ms: 18682.37
    sample_throughput: 23985.579
    sample_time_ms: 6745.387
    update_time_ms: 32.637
  timestamp: 1602807242
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:14:04,079	WARNING util.py:136 -- The `process_trial` operation took 0.8478138446807861 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    316 |          8140.51 | 51126272 |  284.738 |              308.667 |              136.242 |            799.517 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3036.117975948774
    time_step_min: 2878
  date: 2020-10-16_00-14-29
  done: false
  episode_len_mean: 799.5538819342553
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.79724554886076
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 64066
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1253861424595126e-05
        cur_lr: 5.0e-05
        entropy: 0.05976077231268088
        entropy_coeff: 0.0005000000000000001
        kl: 0.013274190326531729
        model: {}
        policy_loss: -0.009487816506104233
        total_loss: 0.3869638939698537
        vf_explained_var: 0.9992716312408447
        vf_loss: 0.39648130536079407
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14516129032258
    gpu_util_percent0: 0.30096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683994006530834
    mean_env_wait_ms: 1.1883010347614584
    mean_inference_ms: 4.305242157800196
    mean_raw_obs_processing_ms: 0.3782243116801932
  time_since_restore: 8166.430788755417
  time_this_iter_s: 25.91922879219055
  time_total_s: 8166.430788755417
  timers:
    learn_throughput: 8661.603
    learn_time_ms: 18679.221
    sample_throughput: 23916.473
    sample_time_ms: 6764.877
    update_time_ms: 31.788
  timestamp: 1602807269
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: 0f5d2_00000
  
2020-10-16 00:14:31,141	WARNING util.py:136 -- The `process_trial` operation took 0.8395910263061523 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    317 |          8166.43 | 51288064 |  284.797 |              308.667 |              136.242 |            799.554 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3035.7244326141777
    time_step_min: 2878
  date: 2020-10-16_00-14-56
  done: false
  episode_len_mean: 799.5919288092349
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.8563262608344
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 212
  episodes_total: 64278
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1253861424595126e-05
        cur_lr: 5.0e-05
        entropy: 0.056085116850833096
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006304914636226992
        total_loss: .inf
        vf_explained_var: 0.9994065165519714
        vf_loss: 0.3181939671436946
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65806451612903
    gpu_util_percent0: 0.34838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683836853751323
    mean_env_wait_ms: 1.188240762842715
    mean_inference_ms: 4.305151763240508
    mean_raw_obs_processing_ms: 0.37821685066174726
  time_since_restore: 8192.274976730347
  time_this_iter_s: 25.84418797492981
  time_total_s: 8192.274976730347
  timers:
    learn_throughput: 8661.861
    learn_time_ms: 18678.664
    sample_throughput: 23928.99
    sample_time_ms: 6761.338
    update_time_ms: 33.367
  timestamp: 1602807296
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:14:58,226	WARNING util.py:136 -- The `process_trial` operation took 0.8750343322753906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    318 |          8192.27 | 51449856 |  284.856 |              308.667 |              136.242 |            799.592 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3035.394890023749
    time_step_min: 2878
  date: 2020-10-16_00-15-24
  done: false
  episode_len_mean: 799.6206580927411
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.90582644264794
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 181
  episodes_total: 64459
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.188079213689268e-05
        cur_lr: 5.0e-05
        entropy: 0.0593639574944973
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009148456117448708
        total_loss: .inf
        vf_explained_var: 0.9994053840637207
        vf_loss: 0.3020966326196988
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.740000000000002
    gpu_util_percent0: 0.3353333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468369024924498
    mean_env_wait_ms: 1.1881884663466904
    mean_inference_ms: 4.305069457773687
    mean_raw_obs_processing_ms: 0.3782099316357699
  time_since_restore: 8218.179374933243
  time_this_iter_s: 25.904398202896118
  time_total_s: 8218.179374933243
  timers:
    learn_throughput: 8671.517
    learn_time_ms: 18657.866
    sample_throughput: 23893.557
    sample_time_ms: 6771.365
    update_time_ms: 32.23
  timestamp: 1602807324
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:15:25,371	WARNING util.py:136 -- The `process_trial` operation took 0.8824758529663086 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    319 |          8218.18 | 51611648 |  284.906 |              308.667 |              136.242 |            799.621 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3035.031617066716
    time_step_min: 2878
  date: 2020-10-16_00-15-51
  done: false
  episode_len_mean: 799.6488639351616
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 284.9592875532736
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 194
  episodes_total: 64653
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.782118820533903e-05
        cur_lr: 5.0e-05
        entropy: 0.05790729355067015
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008964156324509531
        total_loss: .inf
        vf_explained_var: 0.9990715980529785
        vf_loss: 0.4632120182116826
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35161290322581
    gpu_util_percent0: 0.3212903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683519177364363
    mean_env_wait_ms: 1.1881323446740821
    mean_inference_ms: 4.304979354401306
    mean_raw_obs_processing_ms: 0.37820280943754314
  time_since_restore: 8244.073569774628
  time_this_iter_s: 25.894194841384888
  time_total_s: 8244.073569774628
  timers:
    learn_throughput: 8664.175
    learn_time_ms: 18673.677
    sample_throughput: 23866.295
    sample_time_ms: 6779.1
    update_time_ms: 33.682
  timestamp: 1602807351
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:15:52,456	WARNING util.py:136 -- The `process_trial` operation took 0.8785045146942139 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    320 |          8244.07 | 51773440 |  284.959 |              308.667 |              136.242 |            799.649 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3034.6240534246363
    time_step_min: 2878
  date: 2020-10-16_00-16-18
  done: false
  episode_len_mean: 799.6815414258189
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.02005021311345
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 222
  episodes_total: 64875
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.173178230800854e-05
        cur_lr: 5.0e-05
        entropy: 0.05546328301231066
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007961469770331556
        total_loss: .inf
        vf_explained_var: 0.9996585249900818
        vf_loss: 0.18212834745645523
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.751612903225812
    gpu_util_percent0: 0.34677419354838723
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683353036435412
    mean_env_wait_ms: 1.1880707505272423
    mean_inference_ms: 4.30489756980309
    mean_raw_obs_processing_ms: 0.3781959171109635
  time_since_restore: 8269.956471681595
  time_this_iter_s: 25.882901906967163
  time_total_s: 8269.956471681595
  timers:
    learn_throughput: 8669.77
    learn_time_ms: 18661.626
    sample_throughput: 23831.532
    sample_time_ms: 6788.989
    update_time_ms: 39.556
  timestamp: 1602807378
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:16:19,582	WARNING util.py:136 -- The `process_trial` operation took 0.9050219058990479 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    321 |          8269.96 | 51935232 |   285.02 |              308.667 |              136.242 |            799.682 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3034.2413936253633
    time_step_min: 2878
  date: 2020-10-16_00-16-45
  done: false
  episode_len_mean: 799.7166653860929
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.0774165007738
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 200
  episodes_total: 65075
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00010759767346201281
        cur_lr: 5.0e-05
        entropy: 0.0560658760368824
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007174372808852543
        total_loss: .inf
        vf_explained_var: 0.9996495246887207
        vf_loss: 0.18508813778559366
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.364516129032264
    gpu_util_percent0: 0.3267741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683190977256433
    mean_env_wait_ms: 1.1880130158164999
    mean_inference_ms: 4.304807125247411
    mean_raw_obs_processing_ms: 0.3781883441713373
  time_since_restore: 8295.726206064224
  time_this_iter_s: 25.769734382629395
  time_total_s: 8295.726206064224
  timers:
    learn_throughput: 8666.589
    learn_time_ms: 18668.476
    sample_throughput: 23824.546
    sample_time_ms: 6790.979
    update_time_ms: 41.979
  timestamp: 1602807405
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:16:46,533	WARNING util.py:136 -- The `process_trial` operation took 0.8655750751495361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    322 |          8295.73 | 52097024 |  285.077 |              308.667 |              136.242 |            799.717 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3033.9132754760954
    time_step_min: 2878
  date: 2020-10-16_00-17-12
  done: false
  episode_len_mean: 799.7486131118399
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.1271530086159
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 179
  episodes_total: 65254
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00016139651019301922
        cur_lr: 5.0e-05
        entropy: 0.058687069763739906
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006582341040484607
        total_loss: .inf
        vf_explained_var: 0.9995859265327454
        vf_loss: 0.21227984999616942
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.580000000000002
    gpu_util_percent0: 0.35766666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468305360909118
    mean_env_wait_ms: 1.1879619220388824
    mean_inference_ms: 4.304731261967413
    mean_raw_obs_processing_ms: 0.3781815745168119
  time_since_restore: 8321.317289113998
  time_this_iter_s: 25.59108304977417
  time_total_s: 8321.317289113998
  timers:
    learn_throughput: 8680.764
    learn_time_ms: 18637.99
    sample_throughput: 23849.381
    sample_time_ms: 6783.908
    update_time_ms: 42.684
  timestamp: 1602807432
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:17:13,301	WARNING util.py:136 -- The `process_trial` operation took 0.866748571395874 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    323 |          8321.32 | 52258816 |  285.127 |              308.667 |              136.242 |            799.749 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3033.571426387636
    time_step_min: 2878
  date: 2020-10-16_00-17-39
  done: false
  episode_len_mean: 799.7823629169022
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.1797880412913
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 65453
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00024209476528952887
        cur_lr: 5.0e-05
        entropy: 0.05728571116924286
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049680593656376
        model: {}
        policy_loss: -0.010070575265369067
        total_loss: 0.23282487069567046
        vf_explained_var: 0.9995302557945251
        vf_loss: 0.24292288720607758
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.04193548387096
    gpu_util_percent0: 0.31258064516129036
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682900660553833
    mean_env_wait_ms: 1.1879048448234497
    mean_inference_ms: 4.304643268749859
    mean_raw_obs_processing_ms: 0.37817471788844004
  time_since_restore: 8347.17746925354
  time_this_iter_s: 25.860180139541626
  time_total_s: 8347.17746925354
  timers:
    learn_throughput: 8677.575
    learn_time_ms: 18644.84
    sample_throughput: 23858.463
    sample_time_ms: 6781.325
    update_time_ms: 42.501
  timestamp: 1602807459
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: 0f5d2_00000
  
2020-10-16 00:17:40,428	WARNING util.py:136 -- The `process_trial` operation took 0.8965649604797363 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    324 |          8347.18 | 52420608 |   285.18 |              308.667 |              136.242 |            799.782 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3033.174629776342
    time_step_min: 2878
  date: 2020-10-16_00-18-06
  done: false
  episode_len_mean: 799.8209282494822
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.238361812792
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 219
  episodes_total: 65672
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00012104738264476443
        cur_lr: 5.0e-05
        entropy: 0.0593166695907712
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0075862661469727755
        total_loss: .inf
        vf_explained_var: 0.999186098575592
        vf_loss: 0.43584397186835605
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.377419354838715
    gpu_util_percent0: 0.3306451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468273828756891
    mean_env_wait_ms: 1.1878434347432418
    mean_inference_ms: 4.304560452917972
    mean_raw_obs_processing_ms: 0.37816752768205886
  time_since_restore: 8373.340691804886
  time_this_iter_s: 26.163222551345825
  time_total_s: 8373.340691804886
  timers:
    learn_throughput: 8673.901
    learn_time_ms: 18652.739
    sample_throughput: 23866.709
    sample_time_ms: 6778.982
    update_time_ms: 41.537
  timestamp: 1602807486
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:18:07,819	WARNING util.py:136 -- The `process_trial` operation took 0.8784675598144531 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    325 |          8373.34 | 52582400 |  285.238 |              308.667 |              136.242 |            799.821 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3032.796749449381
    time_step_min: 2878
  date: 2020-10-16_00-18-33
  done: false
  episode_len_mean: 799.8579951723824
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.29293757357664
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 65871
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00018157107396714667
        cur_lr: 5.0e-05
        entropy: 0.05625675370295843
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006710322360352923
        total_loss: .inf
        vf_explained_var: 0.9996839165687561
        vf_loss: 0.15870019669334093
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.620000000000005
    gpu_util_percent0: 0.3936666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682574904280882
    mean_env_wait_ms: 1.1877866167244076
    mean_inference_ms: 4.304477781729577
    mean_raw_obs_processing_ms: 0.37816056347521687
  time_since_restore: 8398.927807807922
  time_this_iter_s: 25.5871160030365
  time_total_s: 8398.927807807922
  timers:
    learn_throughput: 8679.035
    learn_time_ms: 18641.704
    sample_throughput: 23876.911
    sample_time_ms: 6776.086
    update_time_ms: 41.586
  timestamp: 1602807513
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:18:34,608	WARNING util.py:136 -- The `process_trial` operation took 0.8836832046508789 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    326 |          8398.93 | 52744192 |  285.293 |              308.667 |              136.242 |            799.858 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3032.468401599612
    time_step_min: 2878
  date: 2020-10-16_00-19-00
  done: false
  episode_len_mean: 799.8916156967238
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.34295614658026
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 181
  episodes_total: 66052
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00027235661095072005
        cur_lr: 5.0e-05
        entropy: 0.06026191357523203
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010093563219318943
        total_loss: .inf
        vf_explained_var: 0.999494731426239
        vf_loss: 0.23955110584696135
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.816129032258072
    gpu_util_percent0: 0.33000000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468243853459271
    mean_env_wait_ms: 1.1877350480454063
    mean_inference_ms: 4.304399988456088
    mean_raw_obs_processing_ms: 0.3781536943289639
  time_since_restore: 8424.814373254776
  time_this_iter_s: 25.886565446853638
  time_total_s: 8424.814373254776
  timers:
    learn_throughput: 8677.011
    learn_time_ms: 18646.053
    sample_throughput: 23932.619
    sample_time_ms: 6760.313
    update_time_ms: 49.164
  timestamp: 1602807540
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:19:01,727	WARNING util.py:136 -- The `process_trial` operation took 0.8866970539093018 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    327 |          8424.81 | 52905984 |  285.343 |              308.667 |              136.242 |            799.892 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3032.1227534962395
    time_step_min: 2878
  date: 2020-10-16_00-19-27
  done: false
  episode_len_mean: 799.9253283018868
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.3926861063461
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 198
  episodes_total: 66250
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00040853491642608004
        cur_lr: 5.0e-05
        entropy: 0.06377055558065574
        entropy_coeff: 0.0005000000000000001
        kl: 0.00396488414844498
        model: {}
        policy_loss: -0.008611041092080995
        total_loss: 0.3807059774796168
        vf_explained_var: 0.9993900656700134
        vf_loss: 0.38934728999932605
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.038709677419355
    gpu_util_percent0: 0.3490322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468230153546996
    mean_env_wait_ms: 1.1876796274078958
    mean_inference_ms: 4.304321375142004
    mean_raw_obs_processing_ms: 0.3781471722577612
  time_since_restore: 8450.679094314575
  time_this_iter_s: 25.864721059799194
  time_total_s: 8450.679094314575
  timers:
    learn_throughput: 8677.291
    learn_time_ms: 18645.452
    sample_throughput: 23926.408
    sample_time_ms: 6762.068
    update_time_ms: 49.314
  timestamp: 1602807567
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: 0f5d2_00000
  
2020-10-16 00:19:28,879	WARNING util.py:136 -- The `process_trial` operation took 0.9286026954650879 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    328 |          8450.68 | 53067776 |  285.393 |              308.667 |              136.242 |            799.925 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3031.739847677534
    time_step_min: 2878
  date: 2020-10-16_00-19-54
  done: false
  episode_len_mean: 799.9682733098655
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.45199444576605
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 224
  episodes_total: 66474
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00020426745821304002
        cur_lr: 5.0e-05
        entropy: 0.05955292625973622
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005790869749034755
        total_loss: .inf
        vf_explained_var: 0.9991826415061951
        vf_loss: 0.45271720240513486
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60645161290323
    gpu_util_percent0: 0.28064516129032263
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682117413168513
    mean_env_wait_ms: 1.187615439361234
    mean_inference_ms: 4.304230434985447
    mean_raw_obs_processing_ms: 0.378139477858729
  time_since_restore: 8476.541040420532
  time_this_iter_s: 25.86194610595703
  time_total_s: 8476.541040420532
  timers:
    learn_throughput: 8684.204
    learn_time_ms: 18630.608
    sample_throughput: 23918.007
    sample_time_ms: 6764.443
    update_time_ms: 48.299
  timestamp: 1602807594
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:19:55,946	WARNING util.py:136 -- The `process_trial` operation took 0.8672399520874023 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    329 |          8476.54 | 53229568 |  285.452 |              308.667 |              136.242 |            799.968 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3031.4179324007923
    time_step_min: 2878
  date: 2020-10-16_00-20-21
  done: false
  episode_len_mean: 800.0022050882035
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.50111049896503
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 190
  episodes_total: 66664
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00030640118731956
        cur_lr: 5.0e-05
        entropy: 0.06296875172605117
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010292771912645549
        total_loss: .inf
        vf_explained_var: 0.9995129108428955
        vf_loss: 0.24589934448401132
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.023333333333333
    gpu_util_percent0: 0.32833333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681980869088126
    mean_env_wait_ms: 1.1875617294524061
    mean_inference_ms: 4.3041557006304405
    mean_raw_obs_processing_ms: 0.37813307528950324
  time_since_restore: 8502.35360789299
  time_this_iter_s: 25.812567472457886
  time_total_s: 8502.35360789299
  timers:
    learn_throughput: 8684.905
    learn_time_ms: 18629.103
    sample_throughput: 23943.385
    sample_time_ms: 6757.273
    update_time_ms: 47.475
  timestamp: 1602807621
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:20:23,031	WARNING util.py:136 -- The `process_trial` operation took 0.8722379207611084 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    330 |          8502.35 | 53391360 |  285.501 |              308.667 |              136.242 |            800.002 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3031.108571685151
    time_step_min: 2878
  date: 2020-10-16_00-20-48
  done: false
  episode_len_mean: 800.029289892145
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.547769577478
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 66849
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00045960178097934
        cur_lr: 5.0e-05
        entropy: 0.06540534272789955
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0075829013852247345
        total_loss: .inf
        vf_explained_var: 0.9992344975471497
        vf_loss: 0.3955036501089732
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.706451612903226
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681845192634435
    mean_env_wait_ms: 1.1875084157365792
    mean_inference_ms: 4.304077609940682
    mean_raw_obs_processing_ms: 0.37812642429449267
  time_since_restore: 8528.224754810333
  time_this_iter_s: 25.87114691734314
  time_total_s: 8528.224754810333
  timers:
    learn_throughput: 8682.716
    learn_time_ms: 18633.801
    sample_throughput: 23950.256
    sample_time_ms: 6755.335
    update_time_ms: 42.129
  timestamp: 1602807648
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:20:50,228	WARNING util.py:136 -- The `process_trial` operation took 0.9114387035369873 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    331 |          8528.22 | 53553152 |  285.548 |              308.667 |              136.242 |            800.029 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3030.764640933439
    time_step_min: 2878
  date: 2020-10-16_00-21-16
  done: false
  episode_len_mean: 800.0612016642558
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.60007609988935
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 208
  episodes_total: 67057
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00068940267146901
        cur_lr: 5.0e-05
        entropy: 0.06445901561528444
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00943937428140392
        total_loss: .inf
        vf_explained_var: 0.9993526339530945
        vf_loss: 0.35249796758095425
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.86774193548387
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681677709404384
    mean_env_wait_ms: 1.1874510457645946
    mean_inference_ms: 4.304000637905066
    mean_raw_obs_processing_ms: 0.3781195804001678
  time_since_restore: 8554.380460262299
  time_this_iter_s: 26.155705451965332
  time_total_s: 8554.380460262299
  timers:
    learn_throughput: 8671.871
    learn_time_ms: 18657.105
    sample_throughput: 23861.899
    sample_time_ms: 6780.349
    update_time_ms: 41.748
  timestamp: 1602807676
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:21:17,712	WARNING util.py:136 -- The `process_trial` operation took 0.921605110168457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    332 |          8554.38 | 53714944 |    285.6 |              308.667 |              136.242 |            800.061 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3030.424213579237
    time_step_min: 2878
  date: 2020-10-16_00-21-43
  done: false
  episode_len_mean: 800.0931605000669
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.65361407928003
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 67271
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.001034104007203515
        cur_lr: 5.0e-05
        entropy: 0.0684724220385154
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006896076932510671
        total_loss: .inf
        vf_explained_var: 0.999316394329071
        vf_loss: 0.3579763174057007
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9483870967742
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681509760918715
    mean_env_wait_ms: 1.1873901444744306
    mean_inference_ms: 4.303914375114292
    mean_raw_obs_processing_ms: 0.3781125474930524
  time_since_restore: 8580.378818750381
  time_this_iter_s: 25.998358488082886
  time_total_s: 8580.378818750381
  timers:
    learn_throughput: 8659.676
    learn_time_ms: 18683.378
    sample_throughput: 23810.045
    sample_time_ms: 6795.115
    update_time_ms: 40.712
  timestamp: 1602807703
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:21:45,028	WARNING util.py:136 -- The `process_trial` operation took 0.8802239894866943 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    333 |          8580.38 | 53876736 |  285.654 |              308.667 |              136.242 |            800.093 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3030.121484931182
    time_step_min: 2878
  date: 2020-10-16_00-22-10
  done: false
  episode_len_mean: 800.1167061962644
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.6970460609651
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 67460
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015511560108052724
        cur_lr: 5.0e-05
        entropy: 0.073812880863746
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007583027763757855
        total_loss: .inf
        vf_explained_var: 0.998593807220459
        vf_loss: 0.6890866955121359
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.832258064516132
    gpu_util_percent0: 0.3454838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681395765286145
    mean_env_wait_ms: 1.187336153524611
    mean_inference_ms: 4.30384131961605
    mean_raw_obs_processing_ms: 0.37810627533999824
  time_since_restore: 8606.329417467117
  time_this_iter_s: 25.95059871673584
  time_total_s: 8606.329417467117
  timers:
    learn_throughput: 8657.04
    learn_time_ms: 18689.067
    sample_throughput: 23764.679
    sample_time_ms: 6808.087
    update_time_ms: 39.513
  timestamp: 1602807730
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:22:12,186	WARNING util.py:136 -- The `process_trial` operation took 0.890418291091919 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    334 |          8606.33 | 54038528 |  285.697 |              308.667 |              136.242 |            800.117 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3029.8544128740255
    time_step_min: 2878
  date: 2020-10-16_00-22-38
  done: false
  episode_len_mean: 800.1391972799172
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.7369421915209
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 67645
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0023267340162079083
        cur_lr: 5.0e-05
        entropy: 0.07051179620126884
        entropy_coeff: 0.0005000000000000001
        kl: 0.004408355026195447
        model: {}
        policy_loss: -0.009551883325912058
        total_loss: 0.8721087922652563
        vf_explained_var: 0.9982315897941589
        vf_loss: 0.8816856940587362
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.422580645161286
    gpu_util_percent0: 0.3319354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681247487985036
    mean_env_wait_ms: 1.1872834861791057
    mean_inference_ms: 4.3037654749089755
    mean_raw_obs_processing_ms: 0.37809987589977584
  time_since_restore: 8632.306686162949
  time_this_iter_s: 25.9772686958313
  time_total_s: 8632.306686162949
  timers:
    learn_throughput: 8660.545
    learn_time_ms: 18681.503
    sample_throughput: 23777.072
    sample_time_ms: 6804.538
    update_time_ms: 40.315
  timestamp: 1602807758
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: 0f5d2_00000
  
2020-10-16 00:22:39,476	WARNING util.py:136 -- The `process_trial` operation took 0.9124891757965088 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    335 |          8632.31 | 54200320 |  285.737 |              308.667 |              136.242 |            800.139 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3029.5152891221655
    time_step_min: 2878
  date: 2020-10-16_00-23-05
  done: false
  episode_len_mean: 800.1727034275441
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.78872721199764
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 217
  episodes_total: 67862
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0011633670081039541
        cur_lr: 5.0e-05
        entropy: 0.06555462100853522
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009257237897448553
        total_loss: .inf
        vf_explained_var: 0.9989665150642395
        vf_loss: 0.5962252219518026
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19032258064517
    gpu_util_percent0: 0.32322580645161286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681091396824328
    mean_env_wait_ms: 1.187223303027943
    mean_inference_ms: 4.303690746881526
    mean_raw_obs_processing_ms: 0.3780931916984203
  time_since_restore: 8658.221257686615
  time_this_iter_s: 25.914571523666382
  time_total_s: 8658.221257686615
  timers:
    learn_throughput: 8649.154
    learn_time_ms: 18706.107
    sample_throughput: 23755.183
    sample_time_ms: 6810.808
    update_time_ms: 41.767
  timestamp: 1602807785
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:23:06,639	WARNING util.py:136 -- The `process_trial` operation took 0.9231412410736084 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    336 |          8658.22 | 54362112 |  285.789 |              308.667 |              136.242 |            800.173 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3029.179980892188
    time_step_min: 2878
  date: 2020-10-16_00-23-32
  done: false
  episode_len_mean: 800.2093402476826
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.8399144743252
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 209
  episodes_total: 68071
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.001745050512155931
        cur_lr: 5.0e-05
        entropy: 0.0627020209406813
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009377447114047754
        total_loss: .inf
        vf_explained_var: 0.9991788268089294
        vf_loss: 0.4240822767217954
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.067741935483873
    gpu_util_percent0: 0.34516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680946406969106
    mean_env_wait_ms: 1.1871642711673414
    mean_inference_ms: 4.303605770461139
    mean_raw_obs_processing_ms: 0.37808632555219096
  time_since_restore: 8684.011491060257
  time_this_iter_s: 25.790233373641968
  time_total_s: 8684.011491060257
  timers:
    learn_throughput: 8655.543
    learn_time_ms: 18692.299
    sample_throughput: 23743.368
    sample_time_ms: 6814.198
    update_time_ms: 34.519
  timestamp: 1602807812
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:23:33,650	WARNING util.py:136 -- The `process_trial` operation took 0.8943591117858887 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    337 |          8684.01 | 54523904 |   285.84 |              308.667 |              136.242 |            800.209 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3028.885437434033
    time_step_min: 2878
  date: 2020-10-16_00-23-59
  done: false
  episode_len_mean: 800.2472894567193
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.8857202985723
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 181
  episodes_total: 68252
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.002617575768233897
        cur_lr: 5.0e-05
        entropy: 0.06056345098962387
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006769284864276415
        total_loss: .inf
        vf_explained_var: 0.9994267821311951
        vf_loss: 0.2891746709744136
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61935483870968
    gpu_util_percent0: 0.29580645161290314
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680813378385213
    mean_env_wait_ms: 1.1871127640259094
    mean_inference_ms: 4.303535235198454
    mean_raw_obs_processing_ms: 0.3780801151830527
  time_since_restore: 8709.719226837158
  time_this_iter_s: 25.707735776901245
  time_total_s: 8709.719226837158
  timers:
    learn_throughput: 8656.228
    learn_time_ms: 18690.82
    sample_throughput: 23766.005
    sample_time_ms: 6807.707
    update_time_ms: 34.56
  timestamp: 1602807839
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:24:00,831	WARNING util.py:136 -- The `process_trial` operation took 0.9634096622467041 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    338 |          8709.72 | 54685696 |  285.886 |              308.667 |              136.242 |            800.247 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3028.5649084848837
    time_step_min: 2878
  date: 2020-10-16_00-24-26
  done: false
  episode_len_mean: 800.2840882524839
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.93213033313816
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 68440
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.003926363652350845
        cur_lr: 5.0e-05
        entropy: 0.05973708350211382
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009326786287905028
        total_loss: .inf
        vf_explained_var: 0.9994804859161377
        vf_loss: 0.2598383128643036
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.387096774193548
    gpu_util_percent0: 0.3048387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680680375089977
    mean_env_wait_ms: 1.1870593113521122
    mean_inference_ms: 4.303461807611893
    mean_raw_obs_processing_ms: 0.3780737694220259
  time_since_restore: 8735.769633054733
  time_this_iter_s: 26.050406217575073
  time_total_s: 8735.769633054733
  timers:
    learn_throughput: 8641.273
    learn_time_ms: 18723.167
    sample_throughput: 23788.158
    sample_time_ms: 6801.367
    update_time_ms: 35.212
  timestamp: 1602807866
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:24:28,299	WARNING util.py:136 -- The `process_trial` operation took 0.9259967803955078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    339 |          8735.77 | 54847488 |  285.932 |              308.667 |              136.242 |            800.284 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3028.2074231671595
    time_step_min: 2878
  date: 2020-10-16_00-24-53
  done: false
  episode_len_mean: 800.327531714706
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 285.98785257135916
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 219
  episodes_total: 68659
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.005889545478526269
        cur_lr: 5.0e-05
        entropy: 0.06223114766180515
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007792058206784229
        total_loss: .inf
        vf_explained_var: 0.9996379017829895
        vf_loss: 0.21225216488043466
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.120000000000005
    gpu_util_percent0: 0.35866666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468053136558464
    mean_env_wait_ms: 1.1869993088210054
    mean_inference_ms: 4.303391503455501
    mean_raw_obs_processing_ms: 0.37806758140533886
  time_since_restore: 8761.422535657883
  time_this_iter_s: 25.652902603149414
  time_total_s: 8761.422535657883
  timers:
    learn_throughput: 8648.592
    learn_time_ms: 18707.323
    sample_throughput: 23798.718
    sample_time_ms: 6798.349
    update_time_ms: 36.212
  timestamp: 1602807893
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:24:55,332	WARNING util.py:136 -- The `process_trial` operation took 0.9541802406311035 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    340 |          8761.42 | 55009280 |  285.988 |              308.667 |              136.242 |            800.328 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3027.8733345925293
    time_step_min: 2878
  date: 2020-10-16_00-25-21
  done: false
  episode_len_mean: 800.3665684039324
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.03999992959194
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 204
  episodes_total: 68863
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.008834318217789404
        cur_lr: 5.0e-05
        entropy: 0.059032004637022815
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0062942235322983224
        total_loss: .inf
        vf_explained_var: 0.999589741230011
        vf_loss: 0.2090842922528585
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.129032258064516
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680381092386344
    mean_env_wait_ms: 1.1869412130488626
    mean_inference_ms: 4.3033077695377235
    mean_raw_obs_processing_ms: 0.37806062606161356
  time_since_restore: 8787.14246749878
  time_this_iter_s: 25.719931840896606
  time_total_s: 8787.14246749878
  timers:
    learn_throughput: 8654.679
    learn_time_ms: 18694.165
    sample_throughput: 23805.433
    sample_time_ms: 6796.432
    update_time_ms: 35.535
  timestamp: 1602807921
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:25:22,338	WARNING util.py:136 -- The `process_trial` operation took 0.9649560451507568 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    341 |          8787.14 | 55171072 |   286.04 |              308.667 |              136.242 |            800.367 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3027.5790175336906
    time_step_min: 2878
  date: 2020-10-16_00-25-48
  done: false
  episode_len_mean: 800.3944326970426
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.08254619888856
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 183
  episodes_total: 69046
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.013251477326684103
        cur_lr: 5.0e-05
        entropy: 0.06559406593441963
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009026629539827505
        total_loss: .inf
        vf_explained_var: 0.9988186359405518
        vf_loss: 0.5707468862334887
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11290322580646
    gpu_util_percent0: 0.3116129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468025293995879
    mean_env_wait_ms: 1.1868892922812797
    mean_inference_ms: 4.303240941502923
    mean_raw_obs_processing_ms: 0.3780545231614142
  time_since_restore: 8813.043524980545
  time_this_iter_s: 25.901057481765747
  time_total_s: 8813.043524980545
  timers:
    learn_throughput: 8660.117
    learn_time_ms: 18682.428
    sample_throughput: 23860.548
    sample_time_ms: 6780.733
    update_time_ms: 36.523
  timestamp: 1602807948
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:25:49,495	WARNING util.py:136 -- The `process_trial` operation took 0.9194097518920898 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    342 |          8813.04 | 55332864 |  286.083 |              308.667 |              136.242 |            800.394 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3027.3062173994713
    time_step_min: 2878
  date: 2020-10-16_00-26-15
  done: false
  episode_len_mean: 800.406773052206
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.12688587076593
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 69245
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.01987721599002615
        cur_lr: 5.0e-05
        entropy: 0.06956179005404313
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009204348209702099
        total_loss: .inf
        vf_explained_var: 0.9988104701042175
        vf_loss: 0.6313881228367487
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.680645161290325
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680119813906006
    mean_env_wait_ms: 1.1868330886764753
    mean_inference_ms: 4.303165250089408
    mean_raw_obs_processing_ms: 0.3780480495391415
  time_since_restore: 8839.014675855637
  time_this_iter_s: 25.971150875091553
  time_total_s: 8839.014675855637
  timers:
    learn_throughput: 8663.57
    learn_time_ms: 18674.981
    sample_throughput: 23846.11
    sample_time_ms: 6784.838
    update_time_ms: 37.604
  timestamp: 1602807975
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:26:16,704	WARNING util.py:136 -- The `process_trial` operation took 0.9165959358215332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    343 |          8839.01 | 55494656 |  286.127 |              308.667 |              136.242 |            800.407 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3026.992913317777
    time_step_min: 2878
  date: 2020-10-16_00-26-42
  done: false
  episode_len_mean: 800.4284932768996
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.1774922935842
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 217
  episodes_total: 69462
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.029815823985039232
        cur_lr: 5.0e-05
        entropy: 0.06380583128581445
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009414034869526708
        total_loss: .inf
        vf_explained_var: 0.999407947063446
        vf_loss: 0.3417493949333827
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.270967741935483
    gpu_util_percent0: 0.2854838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679974710038776
    mean_env_wait_ms: 1.1867734302860395
    mean_inference_ms: 4.303093775359978
    mean_raw_obs_processing_ms: 0.3780420658016541
  time_since_restore: 8864.93172287941
  time_this_iter_s: 25.917047023773193
  time_total_s: 8864.93172287941
  timers:
    learn_throughput: 8661.138
    learn_time_ms: 18680.224
    sample_throughput: 23879.728
    sample_time_ms: 6775.287
    update_time_ms: 39.082
  timestamp: 1602808002
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:26:43,878	WARNING util.py:136 -- The `process_trial` operation took 0.9326903820037842 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    344 |          8864.93 | 55656448 |  286.177 |              308.667 |              136.242 |            800.428 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3026.689964234929
    time_step_min: 2878
  date: 2020-10-16_00-27-09
  done: false
  episode_len_mean: 800.4578434328208
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.22411591691025
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 195
  episodes_total: 69657
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.04472373597755885
        cur_lr: 5.0e-05
        entropy: 0.061064223758876324
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008983103490512198
        total_loss: .inf
        vf_explained_var: 0.9994816184043884
        vf_loss: 0.2693639472126961
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.309677419354838
    gpu_util_percent0: 0.36129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679835777600217
    mean_env_wait_ms: 1.1867185060786263
    mean_inference_ms: 4.303018528338356
    mean_raw_obs_processing_ms: 0.3780357014675748
  time_since_restore: 8890.907679080963
  time_this_iter_s: 25.975956201553345
  time_total_s: 8890.907679080963
  timers:
    learn_throughput: 8665.972
    learn_time_ms: 18669.803
    sample_throughput: 23843.578
    sample_time_ms: 6785.559
    update_time_ms: 37.219
  timestamp: 1602808029
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:27:11,107	WARNING util.py:136 -- The `process_trial` operation took 0.9148972034454346 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    345 |          8890.91 | 55818240 |  286.224 |              308.667 |              136.242 |            800.458 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3026.412709327145
    time_step_min: 2878
  date: 2020-10-16_00-27-37
  done: false
  episode_len_mean: 800.4809071775268
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.2653349351941
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 69843
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.06708560396633825
        cur_lr: 5.0e-05
        entropy: 0.06298885711779197
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01029325338701407
        total_loss: .inf
        vf_explained_var: 0.9991416931152344
        vf_loss: 0.4133503809571266
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380645161290325
    gpu_util_percent0: 0.2509677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8870967741935494
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467970165005901
    mean_env_wait_ms: 1.1866659553562235
    mean_inference_ms: 4.302947280018671
    mean_raw_obs_processing_ms: 0.37802926922878866
  time_since_restore: 8917.085688829422
  time_this_iter_s: 26.178009748458862
  time_total_s: 8917.085688829422
  timers:
    learn_throughput: 8656.991
    learn_time_ms: 18689.173
    sample_throughput: 23819.318
    sample_time_ms: 6792.47
    update_time_ms: 37.0
  timestamp: 1602808057
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:27:38,684	WARNING util.py:136 -- The `process_trial` operation took 0.9724905490875244 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    346 |          8917.09 | 55980032 |  286.265 |              308.667 |              136.242 |            800.481 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3026.0893897840247
    time_step_min: 2878
  date: 2020-10-16_00-28-04
  done: false
  episode_len_mean: 800.5151904517161
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.3137603549626
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 70044
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1006284059495074
        cur_lr: 5.0e-05
        entropy: 0.05734242592006922
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006662329018581659
        total_loss: .inf
        vf_explained_var: 0.9995665550231934
        vf_loss: 0.25926894694566727
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.932258064516137
    gpu_util_percent0: 0.3203225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679555294272711
    mean_env_wait_ms: 1.186609875170181
    mean_inference_ms: 4.302875448345731
    mean_raw_obs_processing_ms: 0.37802283803179765
  time_since_restore: 8942.922558307648
  time_this_iter_s: 25.836869478225708
  time_total_s: 8942.922558307648
  timers:
    learn_throughput: 8650.808
    learn_time_ms: 18702.531
    sample_throughput: 23823.923
    sample_time_ms: 6791.157
    update_time_ms: 35.731
  timestamp: 1602808084
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:28:05,845	WARNING util.py:136 -- The `process_trial` operation took 0.9849355220794678 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    347 |          8942.92 | 56141824 |  286.314 |              308.667 |              136.242 |            800.515 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3025.7429657686394
    time_step_min: 2878
  date: 2020-10-16_00-28-31
  done: false
  episode_len_mean: 800.5539678925196
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.36609419367267
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 70264
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.15094260892426112
        cur_lr: 5.0e-05
        entropy: 0.06090412319948276
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008014328084148778
        total_loss: .inf
        vf_explained_var: 0.9994399547576904
        vf_loss: 0.3126589109500249
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.76774193548387
    gpu_util_percent0: 0.31322580645161285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679428983650997
    mean_env_wait_ms: 1.186549127773664
    mean_inference_ms: 4.302801224446189
    mean_raw_obs_processing_ms: 0.3780172521971871
  time_since_restore: 8969.044728517532
  time_this_iter_s: 26.122170209884644
  time_total_s: 8969.044728517532
  timers:
    learn_throughput: 8646.885
    learn_time_ms: 18711.016
    sample_throughput: 23706.814
    sample_time_ms: 6824.704
    update_time_ms: 34.76
  timestamp: 1602808111
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:28:33,255	WARNING util.py:136 -- The `process_trial` operation took 0.9589104652404785 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    348 |          8969.04 | 56303616 |  286.366 |              308.667 |              136.242 |            800.554 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3025.4546048427183
    time_step_min: 2878
  date: 2020-10-16_00-28-59
  done: false
  episode_len_mean: 800.5866346822614
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.4098563239523
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 70451
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2264139133863917
        cur_lr: 5.0e-05
        entropy: 0.05868496155987183
        entropy_coeff: 0.0005000000000000001
        kl: 0.0016771718995490421
        model: {}
        policy_loss: -0.008531440738200521
        total_loss: 0.3233230436841647
        vf_explained_var: 0.9993430972099304
        vf_loss: 0.33150409907102585
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.293548387096774
    gpu_util_percent0: 0.32290322580645153
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679301208994305
    mean_env_wait_ms: 1.1864965010680668
    mean_inference_ms: 4.302732993279702
    mean_raw_obs_processing_ms: 0.37801116905741305
  time_since_restore: 8995.021963596344
  time_this_iter_s: 25.977235078811646
  time_total_s: 8995.021963596344
  timers:
    learn_throughput: 8653.685
    learn_time_ms: 18696.313
    sample_throughput: 23688.158
    sample_time_ms: 6830.079
    update_time_ms: 35.277
  timestamp: 1602808139
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: 0f5d2_00000
  
2020-10-16 00:29:00,500	WARNING util.py:136 -- The `process_trial` operation took 0.9359133243560791 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    349 |          8995.02 | 56465408 |   286.41 |              308.667 |              136.242 |            800.587 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3025.16986770913
    time_step_min: 2878
  date: 2020-10-16_00-29-26
  done: false
  episode_len_mean: 800.6162971771568
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.4523485942406
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 70638
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.11320695669319585
        cur_lr: 5.0e-05
        entropy: 0.0597645261635383
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010547318597673438
        total_loss: .inf
        vf_explained_var: 0.9992983341217041
        vf_loss: 0.3677864074707031
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49677419354839
    gpu_util_percent0: 0.2767741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679168404674664
    mean_env_wait_ms: 1.1864436800551796
    mean_inference_ms: 4.302664841731332
    mean_raw_obs_processing_ms: 0.3780049556810888
  time_since_restore: 9020.898116350174
  time_this_iter_s: 25.876152753829956
  time_total_s: 9020.898116350174
  timers:
    learn_throughput: 8645.807
    learn_time_ms: 18713.348
    sample_throughput: 23667.335
    sample_time_ms: 6836.089
    update_time_ms: 35.151
  timestamp: 1602808166
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:29:27,634	WARNING util.py:136 -- The `process_trial` operation took 0.921619176864624 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    350 |           9020.9 | 56627200 |  286.452 |              308.667 |              136.242 |            800.616 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3024.839617578941
    time_step_min: 2878
  date: 2020-10-16_00-29-53
  done: false
  episode_len_mean: 800.6494184733514
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.50151284183255
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 70848
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.16981043503979373
        cur_lr: 5.0e-05
        entropy: 0.058243486719826855
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0059975998107499135
        total_loss: .inf
        vf_explained_var: 0.9993259310722351
        vf_loss: 0.388956181704998
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06875
    gpu_util_percent0: 0.30875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146790355484158
    mean_env_wait_ms: 1.1863858885941085
    mean_inference_ms: 4.302595261217411
    mean_raw_obs_processing_ms: 0.37799871932906065
  time_since_restore: 9047.02557349205
  time_this_iter_s: 26.12745714187622
  time_total_s: 9047.02557349205
  timers:
    learn_throughput: 8628.914
    learn_time_ms: 18749.984
    sample_throughput: 23659.514
    sample_time_ms: 6838.348
    update_time_ms: 35.443
  timestamp: 1602808193
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:29:55,264	WARNING util.py:136 -- The `process_trial` operation took 0.9837594032287598 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    351 |          9047.03 | 56788992 |  286.502 |              308.667 |              136.242 |            800.649 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3024.5031466906953
    time_step_min: 2878
  date: 2020-10-16_00-30-20
  done: false
  episode_len_mean: 800.6857154918874
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.5517400479895
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 71063
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.25471565255969064
        cur_lr: 5.0e-05
        entropy: 0.05843887974818548
        entropy_coeff: 0.0005000000000000001
        kl: 0.001802959683118388
        model: {}
        policy_loss: -0.009591531756692953
        total_loss: 0.2817460584143798
        vf_explained_var: 0.9994474053382874
        vf_loss: 0.29090757047136623
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.40666666666667
    gpu_util_percent0: 0.32833333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678896541392925
    mean_env_wait_ms: 1.1863265648262056
    mean_inference_ms: 4.302521337277091
    mean_raw_obs_processing_ms: 0.37799280522408607
  time_since_restore: 9072.756202220917
  time_this_iter_s: 25.730628728866577
  time_total_s: 9072.756202220917
  timers:
    learn_throughput: 8633.404
    learn_time_ms: 18740.232
    sample_throughput: 23689.199
    sample_time_ms: 6829.779
    update_time_ms: 34.402
  timestamp: 1602808220
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: 0f5d2_00000
  
2020-10-16 00:30:22,353	WARNING util.py:136 -- The `process_trial` operation took 0.9310891628265381 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    352 |          9072.76 | 56950784 |  286.552 |              308.667 |              136.242 |            800.686 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3024.2111471542316
    time_step_min: 2878
  date: 2020-10-16_00-30-48
  done: false
  episode_len_mean: 800.7180232150125
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.59564910973353
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 71247
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.12735782627984532
        cur_lr: 5.0e-05
        entropy: 0.055542659945786
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00788233729933078
        total_loss: .inf
        vf_explained_var: 0.999275267124176
        vf_loss: 0.39269117762645084
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.445161290322584
    gpu_util_percent0: 0.3406451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678765424944393
    mean_env_wait_ms: 1.1862753138302309
    mean_inference_ms: 4.302458649823885
    mean_raw_obs_processing_ms: 0.3779869517516332
  time_since_restore: 9098.490232229233
  time_this_iter_s: 25.73403000831604
  time_total_s: 9098.490232229233
  timers:
    learn_throughput: 8639.638
    learn_time_ms: 18726.711
    sample_throughput: 23731.345
    sample_time_ms: 6817.65
    update_time_ms: 34.375
  timestamp: 1602808248
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:30:49,487	WARNING util.py:136 -- The `process_trial` operation took 0.969423770904541 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    353 |          9098.49 | 57112576 |  286.596 |              308.667 |              136.242 |            800.718 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3023.923780445104
    time_step_min: 2878
  date: 2020-10-16_00-31-15
  done: false
  episode_len_mean: 800.7549940505354
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.64025783134025
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 71435
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.19103673941976798
        cur_lr: 5.0e-05
        entropy: 0.055758244668444
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007515446554558973
        total_loss: .inf
        vf_explained_var: 0.9995715022087097
        vf_loss: 0.25956596806645393
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11935483870968
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146786404147891
    mean_env_wait_ms: 1.1862220631543416
    mean_inference_ms: 4.302390341912164
    mean_raw_obs_processing_ms: 0.37798112119041294
  time_since_restore: 9124.248691797256
  time_this_iter_s: 25.75845956802368
  time_total_s: 9124.248691797256
  timers:
    learn_throughput: 8649.026
    learn_time_ms: 18706.384
    sample_throughput: 23722.192
    sample_time_ms: 6820.28
    update_time_ms: 34.347
  timestamp: 1602808275
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:31:16,581	WARNING util.py:136 -- The `process_trial` operation took 1.0000226497650146 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    354 |          9124.25 | 57274368 |   286.64 |              308.667 |              136.242 |            800.755 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3023.59376047369
    time_step_min: 2878
  date: 2020-10-16_00-31-42
  done: false
  episode_len_mean: 800.7961448272011
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.6891025152983
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 209
  episodes_total: 71644
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.28655510912965193
        cur_lr: 5.0e-05
        entropy: 0.05766582489013672
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007847272066404306
        total_loss: .inf
        vf_explained_var: 0.9996328949928284
        vf_loss: 0.2046960989634196
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.84193548387097
    gpu_util_percent0: 0.35483870967741943
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678507151697576
    mean_env_wait_ms: 1.1861651611934851
    mean_inference_ms: 4.302322886440174
    mean_raw_obs_processing_ms: 0.3779748612625386
  time_since_restore: 9150.395564317703
  time_this_iter_s: 26.146872520446777
  time_total_s: 9150.395564317703
  timers:
    learn_throughput: 8647.093
    learn_time_ms: 18710.564
    sample_throughput: 23683.396
    sample_time_ms: 6831.453
    update_time_ms: 36.227
  timestamp: 1602808302
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:31:44,129	WARNING util.py:136 -- The `process_trial` operation took 0.9647936820983887 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    355 |           9150.4 | 57436160 |  286.689 |              308.667 |              136.242 |            800.796 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3023.25685403584
    time_step_min: 2878
  date: 2020-10-16_00-32-09
  done: false
  episode_len_mean: 800.8369633289263
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.7389819143347
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 211
  episodes_total: 71855
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.429832663694478
        cur_lr: 5.0e-05
        entropy: 0.05580807818720738
        entropy_coeff: 0.0005000000000000001
        kl: 0.001177039390313439
        model: {}
        policy_loss: -0.005118100006560174
        total_loss: 0.14661557351549467
        vf_explained_var: 0.9997171759605408
        vf_loss: 0.15125564858317375
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08709677419355
    gpu_util_percent0: 0.35451612903225815
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678383090937114
    mean_env_wait_ms: 1.1861061025537891
    mean_inference_ms: 4.30225021897935
    mean_raw_obs_processing_ms: 0.3779690821724836
  time_since_restore: 9175.876269578934
  time_this_iter_s: 25.48070526123047
  time_total_s: 9175.876269578934
  timers:
    learn_throughput: 8681.126
    learn_time_ms: 18637.214
    sample_throughput: 23709.011
    sample_time_ms: 6824.072
    update_time_ms: 36.846
  timestamp: 1602808329
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: 0f5d2_00000
  
2020-10-16 00:32:10,923	WARNING util.py:136 -- The `process_trial` operation took 0.9627957344055176 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    356 |          9175.88 | 57597952 |  286.739 |              308.667 |              136.242 |            800.837 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3022.974084773068
    time_step_min: 2878
  date: 2020-10-16_00-32-36
  done: false
  episode_len_mean: 800.8733897834536
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.78129784799654
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 72040
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.214916331847239
        cur_lr: 5.0e-05
        entropy: 0.05566776109238466
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007391215049816917
        total_loss: .inf
        vf_explained_var: 0.9996815323829651
        vf_loss: 0.1750904737661282
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56333333333334
    gpu_util_percent0: 0.3289999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678269370998606
    mean_env_wait_ms: 1.1860549131358438
    mean_inference_ms: 4.302189280395182
    mean_raw_obs_processing_ms: 0.3779634919579998
  time_since_restore: 9201.313159227371
  time_this_iter_s: 25.4368896484375
  time_total_s: 9201.313159227371
  timers:
    learn_throughput: 8699.172
    learn_time_ms: 18598.552
    sample_throughput: 23720.881
    sample_time_ms: 6820.657
    update_time_ms: 38.249
  timestamp: 1602808356
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:32:37,844	WARNING util.py:136 -- The `process_trial` operation took 1.0286903381347656 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    357 |          9201.31 | 57759744 |  286.781 |              308.667 |              136.242 |            800.873 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3022.68828956117
    time_step_min: 2878
  date: 2020-10-16_00-33-03
  done: false
  episode_len_mean: 800.9093011020657
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.8248979242496
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 72228
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.3223744977708584
        cur_lr: 5.0e-05
        entropy: 0.0552863177532951
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006698049740710606
        total_loss: .inf
        vf_explained_var: 0.999640941619873
        vf_loss: 0.2015108751753966
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.035483870967745
    gpu_util_percent0: 0.2954838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678129634553602
    mean_env_wait_ms: 1.186001971855367
    mean_inference_ms: 4.302123594890793
    mean_raw_obs_processing_ms: 0.37795764329779524
  time_since_restore: 9226.809718132019
  time_this_iter_s: 25.496558904647827
  time_total_s: 9226.809718132019
  timers:
    learn_throughput: 8722.803
    learn_time_ms: 18548.165
    sample_throughput: 23806.148
    sample_time_ms: 6796.228
    update_time_ms: 38.658
  timestamp: 1602808383
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:33:04,663	WARNING util.py:136 -- The `process_trial` operation took 0.9758946895599365 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    358 |          9226.81 | 57921536 |  286.825 |              308.667 |              136.242 |            800.909 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3022.365628970775
    time_step_min: 2878
  date: 2020-10-16_00-33-30
  done: false
  episode_len_mean: 800.9505521811154
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.87370948580184
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 212
  episodes_total: 72440
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.48356174665628776
        cur_lr: 5.0e-05
        entropy: 0.055171435388425984
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0070536671167549985
        total_loss: .inf
        vf_explained_var: 0.9996269345283508
        vf_loss: 0.20475062479575476
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.677419354838705
    gpu_util_percent0: 0.3103225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467798918018614
    mean_env_wait_ms: 1.1859440508934438
    mean_inference_ms: 4.30205749016877
    mean_raw_obs_processing_ms: 0.37795164599289793
  time_since_restore: 9252.886261701584
  time_this_iter_s: 26.07654356956482
  time_total_s: 9252.886261701584
  timers:
    learn_throughput: 8723.457
    learn_time_ms: 18546.777
    sample_throughput: 23788.97
    sample_time_ms: 6801.135
    update_time_ms: 36.307
  timestamp: 1602808410
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:33:32,134	WARNING util.py:136 -- The `process_trial` operation took 1.039872407913208 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    359 |          9252.89 | 58083328 |  286.874 |              308.667 |              136.242 |            800.951 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3022.050513675049
    time_step_min: 2878
  date: 2020-10-16_00-33-58
  done: false
  episode_len_mean: 800.9898692360633
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.9210890738072
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 72650
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.7253426199844314
        cur_lr: 5.0e-05
        entropy: 0.05543059421082338
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007890416784600044
        total_loss: .inf
        vf_explained_var: 0.9995782971382141
        vf_loss: 0.2281993292272091
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.59354838709677
    gpu_util_percent0: 0.33806451612903227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677889991363027
    mean_env_wait_ms: 1.1858855658115932
    mean_inference_ms: 4.301990495041037
    mean_raw_obs_processing_ms: 0.3779462693430962
  time_since_restore: 9279.047131538391
  time_this_iter_s: 26.16086983680725
  time_total_s: 9279.047131538391
  timers:
    learn_throughput: 8721.438
    learn_time_ms: 18551.07
    sample_throughput: 23750.859
    sample_time_ms: 6812.048
    update_time_ms: 37.032
  timestamp: 1602808438
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:33:59,625	WARNING util.py:136 -- The `process_trial` operation took 0.977363109588623 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    360 |          9279.05 | 58245120 |  286.921 |              308.667 |              136.242 |             800.99 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3021.7730263609765
    time_step_min: 2878
  date: 2020-10-16_00-34-25
  done: false
  episode_len_mean: 801.0255378743152
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 286.9626563716327
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 183
  episodes_total: 72833
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0880139299766471
        cur_lr: 5.0e-05
        entropy: 0.054133679096897445
        entropy_coeff: 0.0005000000000000001
        kl: 0.0007690407267849272
        model: {}
        policy_loss: -0.00762127002235502
        total_loss: 0.16486966982483864
        vf_explained_var: 0.9996609091758728
        vf_loss: 0.17168127869566283
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.267741935483873
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467777144650717
    mean_env_wait_ms: 1.1858349357521474
    mean_inference_ms: 4.301927889326197
    mean_raw_obs_processing_ms: 0.3779404745413436
  time_since_restore: 9305.13881444931
  time_this_iter_s: 26.09168291091919
  time_total_s: 9305.13881444931
  timers:
    learn_throughput: 8726.583
    learn_time_ms: 18540.131
    sample_throughput: 23721.075
    sample_time_ms: 6820.602
    update_time_ms: 35.811
  timestamp: 1602808465
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: 0f5d2_00000
  
2020-10-16 00:34:27,102	WARNING util.py:136 -- The `process_trial` operation took 0.9940216541290283 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    361 |          9305.14 | 58406912 |  286.963 |              308.667 |              136.242 |            801.026 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3021.5021099936976
    time_step_min: 2878
  date: 2020-10-16_00-34-53
  done: false
  episode_len_mean: 801.059995617759
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.0047059292215
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 73022
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.5440069649883236
        cur_lr: 5.0e-05
        entropy: 0.05512133582184712
        entropy_coeff: 0.0005000000000000001
        kl: 0.0010771802529537429
        model: {}
        policy_loss: -0.007578901733116557
        total_loss: 0.28614915410677594
        vf_explained_var: 0.9994299411773682
        vf_loss: 0.29316962758700055
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.045161290322586
    gpu_util_percent0: 0.3351612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677629655074448
    mean_env_wait_ms: 1.1857819487355947
    mean_inference_ms: 4.3018658332634265
    mean_raw_obs_processing_ms: 0.3779347644491734
  time_since_restore: 9331.163273334503
  time_this_iter_s: 26.02445888519287
  time_total_s: 9331.163273334503
  timers:
    learn_throughput: 8721.11
    learn_time_ms: 18551.766
    sample_throughput: 23662.526
    sample_time_ms: 6837.478
    update_time_ms: 35.455
  timestamp: 1602808493
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: 0f5d2_00000
  
2020-10-16 00:34:54,467	WARNING util.py:136 -- The `process_trial` operation took 0.9929766654968262 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    362 |          9331.16 | 58568704 |  287.005 |              308.667 |              136.242 |             801.06 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3021.182450340173
    time_step_min: 2878
  date: 2020-10-16_00-35-20
  done: false
  episode_len_mean: 801.0993254499276
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.0539863501053
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 212
  episodes_total: 73234
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2720034824941618
        cur_lr: 5.0e-05
        entropy: 0.05410508532077074
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008639783113418767
        total_loss: .inf
        vf_explained_var: 0.9996773600578308
        vf_loss: 0.17168333753943443
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.103225806451615
    gpu_util_percent0: 0.3203225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467749600768114
    mean_env_wait_ms: 1.1857240680929566
    mean_inference_ms: 4.301802266615829
    mean_raw_obs_processing_ms: 0.37792886233094286
  time_since_restore: 9357.072358846664
  time_this_iter_s: 25.909085512161255
  time_total_s: 9357.072358846664
  timers:
    learn_throughput: 8715.348
    learn_time_ms: 18564.033
    sample_throughput: 23644.845
    sample_time_ms: 6842.591
    update_time_ms: 35.088
  timestamp: 1602808520
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:35:21,719	WARNING util.py:136 -- The `process_trial` operation took 0.9970006942749023 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    363 |          9357.07 | 58730496 |  287.054 |              308.667 |              136.242 |            801.099 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3020.863015937883
    time_step_min: 2878
  date: 2020-10-16_00-35-47
  done: false
  episode_len_mean: 801.1364812243008
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.10123083626024
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 212
  episodes_total: 73446
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.40800522374124276
        cur_lr: 5.0e-05
        entropy: 0.052937519736588
        entropy_coeff: 0.0005000000000000001
        kl: 0.001112153321931449
        model: {}
        policy_loss: -0.005944426559532682
        total_loss: 0.3573666736483574
        vf_explained_var: 0.9993247985839844
        vf_loss: 0.3628837938110034
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0625
    gpu_util_percent0: 0.30218749999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677383495404459
    mean_env_wait_ms: 1.1856645989408128
    mean_inference_ms: 4.30173393264779
    mean_raw_obs_processing_ms: 0.3779233902565773
  time_since_restore: 9383.18014717102
  time_this_iter_s: 26.10778832435608
  time_total_s: 9383.18014717102
  timers:
    learn_throughput: 8701.513
    learn_time_ms: 18593.547
    sample_throughput: 23631.663
    sample_time_ms: 6846.408
    update_time_ms: 35.432
  timestamp: 1602808547
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: 0f5d2_00000
  
2020-10-16 00:35:49,491	WARNING util.py:136 -- The `process_trial` operation took 1.0001978874206543 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    364 |          9383.18 | 58892288 |  287.101 |              308.667 |              136.242 |            801.136 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3020.589539055281
    time_step_min: 2878
  date: 2020-10-16_00-36-15
  done: false
  episode_len_mean: 801.1719547973487
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.1427996964095
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 178
  episodes_total: 73624
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20400261187062138
        cur_lr: 5.0e-05
        entropy: 0.05172531275699536
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006500147822710763
        total_loss: .inf
        vf_explained_var: 0.9997879862785339
        vf_loss: 0.10774754484494527
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.067741935483877
    gpu_util_percent0: 0.33483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467728319445134
    mean_env_wait_ms: 1.1856155145951273
    mean_inference_ms: 4.301676194955922
    mean_raw_obs_processing_ms: 0.37791784729100225
  time_since_restore: 9409.282531738281
  time_this_iter_s: 26.102384567260742
  time_total_s: 9409.282531738281
  timers:
    learn_throughput: 8699.263
    learn_time_ms: 18598.358
    sample_throughput: 23671.456
    sample_time_ms: 6834.898
    update_time_ms: 35.326
  timestamp: 1602808575
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:36:16,952	WARNING util.py:136 -- The `process_trial` operation took 1.0088958740234375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    365 |          9409.28 | 59054080 |  287.143 |              308.667 |              136.242 |            801.172 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3020.294222089698
    time_step_min: 2878
  date: 2020-10-16_00-36-43
  done: false
  episode_len_mean: 801.2064700543235
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.1848551762524
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 193
  episodes_total: 73817
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.3060039178059321
        cur_lr: 5.0e-05
        entropy: 0.05412807408720255
        entropy_coeff: 0.0005000000000000001
        kl: 0.001809194072848186
        model: {}
        policy_loss: -0.008503640216076747
        total_loss: 0.21307156855861345
        vf_explained_var: 0.9995676875114441
        vf_loss: 0.2210486431916555
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.290625
    gpu_util_percent0: 0.2709375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677159953970195
    mean_env_wait_ms: 1.185561861715194
    mean_inference_ms: 4.301618425122911
    mean_raw_obs_processing_ms: 0.37791245722130506
  time_since_restore: 9435.423857688904
  time_this_iter_s: 26.14132595062256
  time_total_s: 9435.423857688904
  timers:
    learn_throughput: 8673.804
    learn_time_ms: 18652.946
    sample_throughput: 23631.092
    sample_time_ms: 6846.573
    update_time_ms: 34.864
  timestamp: 1602808603
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: 0f5d2_00000
  
2020-10-16 00:36:44,418	WARNING util.py:136 -- The `process_trial` operation took 0.9565255641937256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    366 |          9435.42 | 59215872 |  287.185 |              308.667 |              136.242 |            801.206 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3019.9718228012325
    time_step_min: 2878
  date: 2020-10-16_00-37-10
  done: false
  episode_len_mean: 801.246190836395
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.23402247021727
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 74032
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.15300195890296606
        cur_lr: 5.0e-05
        entropy: 0.05405070539563894
        entropy_coeff: 0.0005000000000000001
        kl: 0.0019494478474371135
        model: {}
        policy_loss: -0.00687956006731838
        total_loss: 0.11503552210827668
        vf_explained_var: 0.9997929930686951
        vf_loss: 0.12164384250839551
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89032258064517
    gpu_util_percent0: 0.31516129032258056
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467703347438964
    mean_env_wait_ms: 1.1855033637068855
    mean_inference_ms: 4.301555799794637
    mean_raw_obs_processing_ms: 0.377906604915015
  time_since_restore: 9461.65015077591
  time_this_iter_s: 26.226293087005615
  time_total_s: 9461.65015077591
  timers:
    learn_throughput: 8650.518
    learn_time_ms: 18703.157
    sample_throughput: 23560.853
    sample_time_ms: 6866.984
    update_time_ms: 42.235
  timestamp: 1602808630
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: 0f5d2_00000
  
2020-10-16 00:37:11,994	WARNING util.py:136 -- The `process_trial` operation took 1.0069293975830078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    367 |          9461.65 | 59377664 |  287.234 |              308.667 |              136.242 |            801.246 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3019.6598293823536
    time_step_min: 2878
  date: 2020-10-16_00-37-37
  done: false
  episode_len_mean: 801.2856796476151
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.28028374317915
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 205
  episodes_total: 74237
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.07650097945148303
        cur_lr: 5.0e-05
        entropy: 0.05398203805088997
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008724054467165843
        total_loss: .inf
        vf_explained_var: 0.9998266100883484
        vf_loss: 0.1026222538203001
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.225806451612907
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676910268179752
    mean_env_wait_ms: 1.1854456908570778
    mean_inference_ms: 4.30148988393054
    mean_raw_obs_processing_ms: 0.3779012190720175
  time_since_restore: 9487.476104974747
  time_this_iter_s: 25.82595419883728
  time_total_s: 9487.476104974747
  timers:
    learn_throughput: 8633.584
    learn_time_ms: 18739.842
    sample_throughput: 23539.992
    sample_time_ms: 6873.069
    update_time_ms: 42.52
  timestamp: 1602808657
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:37:39,178	WARNING util.py:136 -- The `process_trial` operation took 1.007079839706421 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    368 |          9487.48 | 59539456 |   287.28 |              308.667 |              136.242 |            801.286 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3019.3899786235734
    time_step_min: 2878
  date: 2020-10-16_00-38-05
  done: false
  episode_len_mean: 801.3163927597188
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.3186120310564
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 74417
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.11475146917722451
        cur_lr: 5.0e-05
        entropy: 0.05627954099327326
        entropy_coeff: 0.0005000000000000001
        kl: 0.0023993776994757354
        model: {}
        policy_loss: -0.008601355521629253
        total_loss: 0.29051460325717926
        vf_explained_var: 0.9993996620178223
        vf_loss: 0.2988687629501025
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.125806451612906
    gpu_util_percent0: 0.317741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676800467284074
    mean_env_wait_ms: 1.1853958905208486
    mean_inference_ms: 4.301433609049281
    mean_raw_obs_processing_ms: 0.3778956372893713
  time_since_restore: 9513.342722415924
  time_this_iter_s: 25.866617441177368
  time_total_s: 9513.342722415924
  timers:
    learn_throughput: 8643.531
    learn_time_ms: 18718.275
    sample_throughput: 23552.644
    sample_time_ms: 6869.377
    update_time_ms: 44.66
  timestamp: 1602808685
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: 0f5d2_00000
  
2020-10-16 00:38:06,408	WARNING util.py:136 -- The `process_trial` operation took 1.0188193321228027 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    369 |          9513.34 | 59701248 |  287.319 |              308.667 |              136.242 |            801.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3019.098986269041
    time_step_min: 2878
  date: 2020-10-16_00-38-32
  done: false
  episode_len_mean: 801.3457352704659
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.36013176855096
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 195
  episodes_total: 74612
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.057375734588612254
        cur_lr: 5.0e-05
        entropy: 0.0582743107030789
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009637965209549293
        total_loss: .inf
        vf_explained_var: 0.9994550347328186
        vf_loss: 0.2765308730304241
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.41612903225807
    gpu_util_percent0: 0.3661290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467668605581801
    mean_env_wait_ms: 1.1853427537556598
    mean_inference_ms: 4.301378582070436
    mean_raw_obs_processing_ms: 0.3778905381295665
  time_since_restore: 9539.30338191986
  time_this_iter_s: 25.960659503936768
  time_total_s: 9539.30338191986
  timers:
    learn_throughput: 8647.192
    learn_time_ms: 18710.352
    sample_throughput: 23552.987
    sample_time_ms: 6869.277
    update_time_ms: 42.628
  timestamp: 1602808712
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:38:33,714	WARNING util.py:136 -- The `process_trial` operation took 0.9936110973358154 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    370 |           9539.3 | 59863040 |   287.36 |              308.667 |              136.242 |            801.346 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3018.7824796438167
    time_step_min: 2878
  date: 2020-10-16_00-38-59
  done: false
  episode_len_mean: 801.3816434804688
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.4062940271492
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 217
  episodes_total: 74829
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0860636018829184
        cur_lr: 5.0e-05
        entropy: 0.05860571749508381
        entropy_coeff: 0.0005000000000000001
        kl: 0.0024978188739623874
        model: {}
        policy_loss: -0.009064738444673518
        total_loss: 0.2800467846294244
        vf_explained_var: 0.9994850158691406
        vf_loss: 0.28892585014303523
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.341935483870973
    gpu_util_percent0: 0.3151612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676547439294846
    mean_env_wait_ms: 1.1852822967985202
    mean_inference_ms: 4.301311685047255
    mean_raw_obs_processing_ms: 0.3778842335785959
  time_since_restore: 9565.10971045494
  time_this_iter_s: 25.806328535079956
  time_total_s: 9565.10971045494
  timers:
    learn_throughput: 8660.337
    learn_time_ms: 18681.952
    sample_throughput: 23591.239
    sample_time_ms: 6858.139
    update_time_ms: 51.231
  timestamp: 1602808739
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: 0f5d2_00000
  
2020-10-16 00:39:00,865	WARNING util.py:136 -- The `process_trial` operation took 0.9898390769958496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    371 |          9565.11 | 60024832 |  287.406 |              308.667 |              136.242 |            801.382 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3018.5219142099018
    time_step_min: 2878
  date: 2020-10-16_00-39-26
  done: false
  episode_len_mean: 801.4145376034545
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.44836231653994
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 204
  episodes_total: 75033
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0430318009414592
        cur_lr: 5.0e-05
        entropy: 0.05930084021141132
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009782494579364235
        total_loss: .inf
        vf_explained_var: 0.9995977282524109
        vf_loss: 0.20057436948021254
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116129032258062
    gpu_util_percent0: 0.3061290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467644239604136
    mean_env_wait_ms: 1.1852260376275237
    mean_inference_ms: 4.301252402497054
    mean_raw_obs_processing_ms: 0.37787902467759327
  time_since_restore: 9590.902510643005
  time_this_iter_s: 25.792800188064575
  time_total_s: 9590.902510643005
  timers:
    learn_throughput: 8671.831
    learn_time_ms: 18657.19
    sample_throughput: 23614.248
    sample_time_ms: 6851.457
    update_time_ms: 57.84
  timestamp: 1602808766
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:39:27,997	WARNING util.py:136 -- The `process_trial` operation took 0.9897923469543457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    372 |           9590.9 | 60186624 |  287.448 |              308.667 |              136.242 |            801.415 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3018.2685103892304
    time_step_min: 2878
  date: 2020-10-16_00-39-53
  done: false
  episode_len_mean: 801.4454726765058
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.48603506142354
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 177
  episodes_total: 75210
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0645477014121888
        cur_lr: 5.0e-05
        entropy: 0.06046681230266889
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007066984583313267
        total_loss: .inf
        vf_explained_var: 0.9993317723274231
        vf_loss: 0.3778415024280548
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.60967741935484
    gpu_util_percent0: 0.2858064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467634118074885
    mean_env_wait_ms: 1.1851771696890852
    mean_inference_ms: 4.30119908456493
    mean_raw_obs_processing_ms: 0.3778737980096374
  time_since_restore: 9616.666887283325
  time_this_iter_s: 25.764376640319824
  time_total_s: 9616.666887283325
  timers:
    learn_throughput: 8676.954
    learn_time_ms: 18646.175
    sample_throughput: 23641.052
    sample_time_ms: 6843.689
    update_time_ms: 55.889
  timestamp: 1602808793
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:39:55,210	WARNING util.py:136 -- The `process_trial` operation took 1.0162129402160645 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    373 |          9616.67 | 60348416 |  287.486 |              308.667 |              136.242 |            801.445 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3017.990885572139
    time_step_min: 2878
  date: 2020-10-16_00-40-21
  done: false
  episode_len_mean: 801.4795189030777
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.52684822526055
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 75411
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.09682155211828318
        cur_lr: 5.0e-05
        entropy: 0.06253336432079475
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008336394714812437
        total_loss: .inf
        vf_explained_var: 0.9995250701904297
        vf_loss: 0.26037709787487984
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.067741935483877
    gpu_util_percent0: 0.3174193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676220997605446
    mean_env_wait_ms: 1.1851227328791527
    mean_inference_ms: 4.301142704911291
    mean_raw_obs_processing_ms: 0.37786852266091037
  time_since_restore: 9642.700468063354
  time_this_iter_s: 26.033580780029297
  time_total_s: 9642.700468063354
  timers:
    learn_throughput: 8682.809
    learn_time_ms: 18633.602
    sample_throughput: 23658.867
    sample_time_ms: 6838.535
    update_time_ms: 55.646
  timestamp: 1602808821
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:40:22,660	WARNING util.py:136 -- The `process_trial` operation took 1.0594146251678467 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    374 |           9642.7 | 60510208 |  287.527 |              308.667 |              136.242 |             801.48 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3017.702468515187
    time_step_min: 2878
  date: 2020-10-16_00-40-48
  done: false
  episode_len_mean: 801.5138440789126
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.57071214605423
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 217
  episodes_total: 75628
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1452323281774248
        cur_lr: 5.0e-05
        entropy: 0.06287473905831575
        entropy_coeff: 0.0005000000000000001
        kl: 0.0019612691345779845
        model: {}
        policy_loss: -0.009767299149340639
        total_loss: 0.4113983338077863
        vf_explained_var: 0.9992738366127014
        vf_loss: 0.420912226041158
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81935483870968
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676104383405486
    mean_env_wait_ms: 1.1850625574879994
    mean_inference_ms: 4.301081941900723
    mean_raw_obs_processing_ms: 0.37786323275482914
  time_since_restore: 9668.80950140953
  time_this_iter_s: 26.109033346176147
  time_total_s: 9668.80950140953
  timers:
    learn_throughput: 8678.379
    learn_time_ms: 18643.113
    sample_throughput: 23692.066
    sample_time_ms: 6828.953
    update_time_ms: 57.086
  timestamp: 1602808848
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: 0f5d2_00000
  
2020-10-16 00:40:50,079	WARNING util.py:136 -- The `process_trial` operation took 0.9618372917175293 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    375 |          9668.81 | 60672000 |  287.571 |              308.667 |              136.242 |            801.514 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3017.434461920093
    time_step_min: 2878
  date: 2020-10-16_00-41-16
  done: false
  episode_len_mean: 801.5478080818738
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.61165170378575
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 196
  episodes_total: 75824
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0726161640887124
        cur_lr: 5.0e-05
        entropy: 0.06070092630883058
        entropy_coeff: 0.0005000000000000001
        kl: 0.002819754222097496
        model: {}
        policy_loss: -0.008653056536180278
        total_loss: 0.19154142836729685
        vf_explained_var: 0.9995977878570557
        vf_loss: 0.20002007856965065
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.087096774193554
    gpu_util_percent0: 0.3170967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675993221749964
    mean_env_wait_ms: 1.185008256306622
    mean_inference_ms: 4.301021583681714
    mean_raw_obs_processing_ms: 0.3778577215813043
  time_since_restore: 9694.84177160263
  time_this_iter_s: 26.032270193099976
  time_total_s: 9694.84177160263
  timers:
    learn_throughput: 8679.492
    learn_time_ms: 18640.722
    sample_throughput: 23699.765
    sample_time_ms: 6826.734
    update_time_ms: 58.027
  timestamp: 1602808876
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: 0f5d2_00000
  
2020-10-16 00:41:17,585	WARNING util.py:136 -- The `process_trial` operation took 1.0150799751281738 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    376 |          9694.84 | 60833792 |  287.612 |              308.667 |              136.242 |            801.548 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3017.179116264989
    time_step_min: 2878
  date: 2020-10-16_00-41-43
  done: false
  episode_len_mean: 801.575997579234
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.6497057565348
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 76009
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0363080820443562
        cur_lr: 5.0e-05
        entropy: 0.061301849161585174
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00869837840218679
        total_loss: .inf
        vf_explained_var: 0.9996253848075867
        vf_loss: 0.22856405998269716
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.593548387096778
    gpu_util_percent0: 0.3119354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675875946038527
    mean_env_wait_ms: 1.1849574140919406
    mean_inference_ms: 4.30096484086171
    mean_raw_obs_processing_ms: 0.3778522402668525
  time_since_restore: 9720.97694015503
  time_this_iter_s: 26.13516855239868
  time_total_s: 9720.97694015503
  timers:
    learn_throughput: 8679.347
    learn_time_ms: 18641.033
    sample_throughput: 23709.72
    sample_time_ms: 6823.868
    update_time_ms: 50.344
  timestamp: 1602808903
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:41:45,250	WARNING util.py:136 -- The `process_trial` operation took 1.0746698379516602 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    377 |          9720.98 | 60995584 |   287.65 |              308.667 |              136.242 |            801.576 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3016.905800336099
    time_step_min: 2878
  date: 2020-10-16_00-42-11
  done: false
  episode_len_mean: 801.6099942260249
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.69114323116696
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 195
  episodes_total: 76204
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0544621230665343
        cur_lr: 5.0e-05
        entropy: 0.0620507076382637
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010066486526435861
        total_loss: .inf
        vf_explained_var: 0.99953693151474
        vf_loss: 0.23833891501029333
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.621875
    gpu_util_percent0: 0.29593749999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675773332378222
    mean_env_wait_ms: 1.184904823223411
    mean_inference_ms: 4.300915132896262
    mean_raw_obs_processing_ms: 0.3778474934579683
  time_since_restore: 9747.01192688942
  time_this_iter_s: 26.03498673439026
  time_total_s: 9747.01192688942
  timers:
    learn_throughput: 8675.108
    learn_time_ms: 18650.141
    sample_throughput: 23713.213
    sample_time_ms: 6822.863
    update_time_ms: 50.755
  timestamp: 1602808931
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:42:12,768	WARNING util.py:136 -- The `process_trial` operation took 1.056030511856079 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    378 |          9747.01 | 61157376 |  287.691 |              308.667 |              136.242 |             801.61 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3016.6086893915594
    time_step_min: 2878
  date: 2020-10-16_00-42-39
  done: false
  episode_len_mean: 801.6443711728685
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.7357171970922
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 224
  episodes_total: 76428
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.08169318459980145
        cur_lr: 5.0e-05
        entropy: 0.06386269597957532
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006221027967209618
        total_loss: .inf
        vf_explained_var: 0.999286949634552
        vf_loss: 0.3855814461906751
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69677419354839
    gpu_util_percent0: 0.34387096774193554
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675668731423344
    mean_env_wait_ms: 1.1848429414882173
    mean_inference_ms: 4.300853279118864
    mean_raw_obs_processing_ms: 0.37784208081748005
  time_since_restore: 9773.27033495903
  time_this_iter_s: 26.258408069610596
  time_total_s: 9773.27033495903
  timers:
    learn_throughput: 8654.454
    learn_time_ms: 18694.651
    sample_throughput: 23707.606
    sample_time_ms: 6824.476
    update_time_ms: 50.653
  timestamp: 1602808959
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:42:40,434	WARNING util.py:136 -- The `process_trial` operation took 1.0290248394012451 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    379 |          9773.27 | 61319168 |  287.736 |              308.667 |              136.242 |            801.644 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3016.358696787673
    time_step_min: 2878
  date: 2020-10-16_00-43-06
  done: false
  episode_len_mean: 801.6667145243813
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.7732165288677
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 76616
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.12253977689970214
        cur_lr: 5.0e-05
        entropy: 0.06241455270598332
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010198701017846664
        total_loss: .inf
        vf_explained_var: 0.9992484450340271
        vf_loss: 0.37392406165599823
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28709677419355
    gpu_util_percent0: 0.30258064516129035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675542602566458
    mean_env_wait_ms: 1.1847911757688805
    mean_inference_ms: 4.30079708248644
    mean_raw_obs_processing_ms: 0.3778367233197767
  time_since_restore: 9798.943208694458
  time_this_iter_s: 25.672873735427856
  time_total_s: 9798.943208694458
  timers:
    learn_throughput: 8672.26
    learn_time_ms: 18656.266
    sample_throughput: 23718.845
    sample_time_ms: 6821.243
    update_time_ms: 52.625
  timestamp: 1602808986
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:43:07,499	WARNING util.py:136 -- The `process_trial` operation took 1.0296077728271484 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    380 |          9798.94 | 61480960 |  287.773 |              308.667 |              136.242 |            801.667 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3016.1199749889274
    time_step_min: 2878
  date: 2020-10-16_00-43-33
  done: false
  episode_len_mean: 801.6940444259264
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.8098944708664
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 76802
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.18380966534955326
        cur_lr: 5.0e-05
        entropy: 0.061360386510690056
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008399490853662428
        total_loss: .inf
        vf_explained_var: 0.9990530014038086
        vf_loss: 0.469102847079436
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.183870967741935
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675433204632385
    mean_env_wait_ms: 1.184740942645747
    mean_inference_ms: 4.300742731371984
    mean_raw_obs_processing_ms: 0.3778314715955668
  time_since_restore: 9825.046731233597
  time_this_iter_s: 26.103522539138794
  time_total_s: 9825.046731233597
  timers:
    learn_throughput: 8662.318
    learn_time_ms: 18677.679
    sample_throughput: 23657.118
    sample_time_ms: 6839.041
    update_time_ms: 44.31
  timestamp: 1602809013
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:43:35,079	WARNING util.py:136 -- The `process_trial` operation took 1.0423998832702637 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    381 |          9825.05 | 61642752 |   287.81 |              308.667 |              136.242 |            801.694 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3015.8509972065226
    time_step_min: 2878
  date: 2020-10-16_00-44-00
  done: false
  episode_len_mean: 801.7216529655459
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.8507229146568
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 77001
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2757144980243299
        cur_lr: 5.0e-05
        entropy: 0.060443781626721226
        entropy_coeff: 0.0005000000000000001
        kl: 0.001839918753830716
        model: {}
        policy_loss: -0.00733774520388882
        total_loss: 0.18257670477032661
        vf_explained_var: 0.9996585249900818
        vf_loss: 0.18943738068143526
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44193548387097
    gpu_util_percent0: 0.3835483870967743
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467533215127387
    mean_env_wait_ms: 1.1846865095665728
    mean_inference_ms: 4.30069219063093
    mean_raw_obs_processing_ms: 0.37782673891027013
  time_since_restore: 9850.775389432907
  time_this_iter_s: 25.728658199310303
  time_total_s: 9850.775389432907
  timers:
    learn_throughput: 8656.545
    learn_time_ms: 18690.136
    sample_throughput: 23691.047
    sample_time_ms: 6829.247
    update_time_ms: 36.105
  timestamp: 1602809040
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: 0f5d2_00000
  
2020-10-16 00:44:02,305	WARNING util.py:136 -- The `process_trial` operation took 1.0410475730895996 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    382 |          9850.78 | 61804544 |  287.851 |              308.667 |              136.242 |            801.722 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3015.545156567816
    time_step_min: 2878
  date: 2020-10-16_00-44-28
  done: false
  episode_len_mean: 801.7578312160884
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.89678102653465
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 222
  episodes_total: 77223
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.13785724901216495
        cur_lr: 5.0e-05
        entropy: 0.057615200988948345
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004931011178996414
        total_loss: .inf
        vf_explained_var: 0.9996137619018555
        vf_loss: 0.2631249117354552
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.877419354838715
    gpu_util_percent0: 0.31677419354838704
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675236022922794
    mean_env_wait_ms: 1.1846258056146137
    mean_inference_ms: 4.300633864676548
    mean_raw_obs_processing_ms: 0.37782171327295555
  time_since_restore: 9876.790083169937
  time_this_iter_s: 26.01469373703003
  time_total_s: 9876.790083169937
  timers:
    learn_throughput: 8647.027
    learn_time_ms: 18710.707
    sample_throughput: 23670.427
    sample_time_ms: 6835.196
    update_time_ms: 38.532
  timestamp: 1602809068
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:44:29,802	WARNING util.py:136 -- The `process_trial` operation took 1.0213572978973389 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    383 |          9876.79 | 61966336 |  287.897 |              308.667 |              136.242 |            801.758 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3015.29658924948
    time_step_min: 2878
  date: 2020-10-16_00-44-55
  done: false
  episode_len_mean: 801.7877249415443
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.9338805252067
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 77409
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20678587351824743
        cur_lr: 5.0e-05
        entropy: 0.05687315482646227
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005866348122557004
        total_loss: .inf
        vf_explained_var: 0.9996281266212463
        vf_loss: 0.19511126354336739
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.864516129032257
    gpu_util_percent0: 0.31967741935483873
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467511934959741
    mean_env_wait_ms: 1.1845745717946188
    mean_inference_ms: 4.300578317571178
    mean_raw_obs_processing_ms: 0.37781630334727795
  time_since_restore: 9902.845195293427
  time_this_iter_s: 26.05511212348938
  time_total_s: 9902.845195293427
  timers:
    learn_throughput: 8643.357
    learn_time_ms: 18718.652
    sample_throughput: 23660.412
    sample_time_ms: 6838.089
    update_time_ms: 38.801
  timestamp: 1602809095
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:44:57,313	WARNING util.py:136 -- The `process_trial` operation took 0.9937777519226074 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    384 |          9902.85 | 62128128 |  287.934 |              308.667 |              136.242 |            801.788 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3015.0490845796803
    time_step_min: 2878
  date: 2020-10-16_00-45-23
  done: false
  episode_len_mean: 801.8158539099953
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 287.9715335998259
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 77596
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.3101788102773711
        cur_lr: 5.0e-05
        entropy: 0.05825994350016117
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007169547481074308
        total_loss: .inf
        vf_explained_var: 0.9996423125267029
        vf_loss: 0.18066969017187753
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.928124999999998
    gpu_util_percent0: 0.34125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675005656312115
    mean_env_wait_ms: 1.1845241036255076
    mean_inference_ms: 4.300525055643357
    mean_raw_obs_processing_ms: 0.37781120630194975
  time_since_restore: 9928.895288467407
  time_this_iter_s: 26.050093173980713
  time_total_s: 9928.895288467407
  timers:
    learn_throughput: 8650.54
    learn_time_ms: 18703.11
    sample_throughput: 23659.048
    sample_time_ms: 6838.483
    update_time_ms: 37.314
  timestamp: 1602809123
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:45:24,756	WARNING util.py:136 -- The `process_trial` operation took 1.009451150894165 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    385 |           9928.9 | 62289920 |  287.972 |              308.667 |              136.242 |            801.816 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3014.775021539807
    time_step_min: 2878
  date: 2020-10-16_00-45-50
  done: false
  episode_len_mean: 801.8502937055746
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.01335531694497
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 203
  episodes_total: 77799
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4652682154160566
        cur_lr: 5.0e-05
        entropy: 0.05527229638149341
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007703270801963906
        total_loss: .inf
        vf_explained_var: 0.9997827410697937
        vf_loss: 0.11625742415587108
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.838709677419356
    gpu_util_percent0: 0.34032258064516124
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674895616778927
    mean_env_wait_ms: 1.1844680280635858
    mean_inference_ms: 4.300474108063546
    mean_raw_obs_processing_ms: 0.37780629677708233
  time_since_restore: 9954.88568162918
  time_this_iter_s: 25.99039316177368
  time_total_s: 9954.88568162918
  timers:
    learn_throughput: 8652.211
    learn_time_ms: 18699.498
    sample_throughput: 23697.69
    sample_time_ms: 6827.332
    update_time_ms: 36.665
  timestamp: 1602809150
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:45:52,189	WARNING util.py:136 -- The `process_trial` operation took 1.054018259048462 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    386 |          9954.89 | 62451712 |  288.013 |              308.667 |              136.242 |             801.85 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3014.4805334701205
    time_step_min: 2878
  date: 2020-10-16_00-46-18
  done: false
  episode_len_mean: 801.8897918375718
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.0588309779498
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 217
  episodes_total: 78016
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.697902323124085
        cur_lr: 5.0e-05
        entropy: 0.05352932494133711
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004926066894161825
        total_loss: .inf
        vf_explained_var: 0.9998635649681091
        vf_loss: 0.07922456103066604
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.06129032258065
    gpu_util_percent0: 0.3029032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674800986466058
    mean_env_wait_ms: 1.1844098892785968
    mean_inference_ms: 4.300419140916592
    mean_raw_obs_processing_ms: 0.37780161640357635
  time_since_restore: 9980.915889263153
  time_this_iter_s: 26.030207633972168
  time_total_s: 9980.915889263153
  timers:
    learn_throughput: 8649.727
    learn_time_ms: 18704.869
    sample_throughput: 23760.3
    sample_time_ms: 6809.342
    update_time_ms: 37.217
  timestamp: 1602809178
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:46:19,811	WARNING util.py:136 -- The `process_trial` operation took 1.044264316558838 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    387 |          9980.92 | 62613504 |  288.059 |              308.667 |              136.242 |             801.89 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3014.2227339602123
    time_step_min: 2878
  date: 2020-10-16_00-46-45
  done: false
  episode_len_mean: 801.9217657063209
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.0968122937275
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 78201
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.046853484686128
        cur_lr: 5.0e-05
        entropy: 0.05259000416845083
        entropy_coeff: 0.0005000000000000001
        kl: 0.0006719472439726815
        model: {}
        policy_loss: -0.006685095203768772
        total_loss: 0.13815326802432537
        vf_explained_var: 0.9997355937957764
        vf_loss: 0.14416122684876123
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77741935483871
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467470035805985
    mean_env_wait_ms: 1.1843591837988021
    mean_inference_ms: 4.300365877032045
    mean_raw_obs_processing_ms: 0.3777963322068646
  time_since_restore: 10006.946373462677
  time_this_iter_s: 26.030484199523926
  time_total_s: 10006.946373462677
  timers:
    learn_throughput: 8643.023
    learn_time_ms: 18719.376
    sample_throughput: 23809.168
    sample_time_ms: 6795.366
    update_time_ms: 37.037
  timestamp: 1602809205
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: 0f5d2_00000
  
2020-10-16 00:46:47,225	WARNING util.py:136 -- The `process_trial` operation took 1.008922815322876 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    388 |          10006.9 | 62775296 |  288.097 |              308.667 |              136.242 |            801.922 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3013.975099550745
    time_step_min: 2878
  date: 2020-10-16_00-47-13
  done: false
  episode_len_mean: 801.9546869418789
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.1339571919631
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 78388
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.523426742343064
        cur_lr: 5.0e-05
        entropy: 0.05351137463003397
        entropy_coeff: 0.0005000000000000001
        kl: 0.001044350157220227
        model: {}
        policy_loss: -0.007568209044014414
        total_loss: 0.09473006303111713
        vf_explained_var: 0.9997970461845398
        vf_loss: 0.10177839050690334
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.148387096774197
    gpu_util_percent0: 0.2796774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419363
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674587083270227
    mean_env_wait_ms: 1.18430865106352
    mean_inference_ms: 4.300316090480888
    mean_raw_obs_processing_ms: 0.3777915970239582
  time_since_restore: 10032.820548057556
  time_this_iter_s: 25.87417459487915
  time_total_s: 10032.820548057556
  timers:
    learn_throughput: 8658.937
    learn_time_ms: 18684.971
    sample_throughput: 23848.335
    sample_time_ms: 6784.205
    update_time_ms: 35.395
  timestamp: 1602809233
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: 0f5d2_00000
  
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    389 |          10032.8 | 62937088 |  288.134 |              308.667 |              136.242 |            801.955 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


2020-10-16 00:47:14,513	WARNING util.py:136 -- The `process_trial` operation took 0.987311601638794 seconds to complete, which may be a performance bottleneck.
Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3013.7115088407145
    time_step_min: 2878
  date: 2020-10-16_00-47-40
  done: false
  episode_len_mean: 801.9897319099665
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.17446653626683
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 205
  episodes_total: 78593
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.261713371171532
        cur_lr: 5.0e-05
        entropy: 0.05531713583817085
        entropy_coeff: 0.0005000000000000001
        kl: 0.0014900850558963914
        model: {}
        policy_loss: -0.00622705627635393
        total_loss: 0.20841211577256522
        vf_explained_var: 0.9996406435966492
        vf_loss: 0.2142768514653047
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.080645161290327
    gpu_util_percent0: 0.3003225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674479621381184
    mean_env_wait_ms: 1.1842520455805072
    mean_inference_ms: 4.300266954352079
    mean_raw_obs_processing_ms: 0.37778675950598956
  time_since_restore: 10058.925397396088
  time_this_iter_s: 26.104849338531494
  time_total_s: 10058.925397396088
  timers:
    learn_throughput: 8636.535
    learn_time_ms: 18733.438
    sample_throughput: 23870.024
    sample_time_ms: 6778.041
    update_time_ms: 35.275
  timestamp: 1602809260
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: 0f5d2_00000
  
2020-10-16 00:47:42,081	WARNING util.py:136 -- The `process_trial` operation took 1.0910649299621582 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    390 |          10058.9 | 63098880 |  288.174 |              308.667 |              136.242 |             801.99 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3013.448402371371
    time_step_min: 2878
  date: 2020-10-16_00-48-08
  done: false
  episode_len_mean: 802.0213681178545
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.2166661475747
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 216
  episodes_total: 78809
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.130856685585766
        cur_lr: 5.0e-05
        entropy: 0.05367735835413138
        entropy_coeff: 0.0005000000000000001
        kl: 0.0017430370401901503
        model: {}
        policy_loss: -0.007937646606781831
        total_loss: 0.33006764327486354
        vf_explained_var: 0.9993570446968079
        vf_loss: 0.33780404428641003
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.045161290322582
    gpu_util_percent0: 0.3029032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674379787501227
    mean_env_wait_ms: 1.1841941196640406
    mean_inference_ms: 4.300210404774992
    mean_raw_obs_processing_ms: 0.37778189590547023
  time_since_restore: 10084.966465234756
  time_this_iter_s: 26.041067838668823
  time_total_s: 10084.966465234756
  timers:
    learn_throughput: 8634.292
    learn_time_ms: 18738.306
    sample_throughput: 23918.421
    sample_time_ms: 6764.326
    update_time_ms: 36.124
  timestamp: 1602809288
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: 0f5d2_00000
  
2020-10-16 00:48:09,607	WARNING util.py:136 -- The `process_trial` operation took 1.0353734493255615 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    391 |            10085 | 63260672 |  288.217 |              308.667 |              136.242 |            802.021 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3013.199103332109
    time_step_min: 2878
  date: 2020-10-16_00-48-35
  done: false
  episode_len_mean: 802.053560351921
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.25408205736034
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 78995
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.065428342792883
        cur_lr: 5.0e-05
        entropy: 0.04991330920408169
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004254943603882566
        total_loss: .inf
        vf_explained_var: 0.9997744560241699
        vf_loss: 0.1224302351474762
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.141935483870967
    gpu_util_percent0: 0.312258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674290375957916
    mean_env_wait_ms: 1.1841438597360023
    mean_inference_ms: 4.3001599440506055
    mean_raw_obs_processing_ms: 0.3777768557084786
  time_since_restore: 10110.749333620071
  time_this_iter_s: 25.78286838531494
  time_total_s: 10110.749333620071
  timers:
    learn_throughput: 8634.693
    learn_time_ms: 18737.435
    sample_throughput: 23911.615
    sample_time_ms: 6766.252
    update_time_ms: 37.978
  timestamp: 1602809315
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:48:36,921	WARNING util.py:136 -- The `process_trial` operation took 1.0517418384552002 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    392 |          10110.7 | 63422464 |  288.254 |              308.667 |              136.242 |            802.054 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3012.9476543646633
    time_step_min: 2878
  date: 2020-10-16_00-49-03
  done: false
  episode_len_mean: 802.0837553515274
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.2912876539535
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 79183
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.09814251418932447
        cur_lr: 5.0e-05
        entropy: 0.051466358825564384
        entropy_coeff: 0.0005000000000000001
        kl: 0.0018793143778263282
        model: {}
        policy_loss: -0.008036915086753046
        total_loss: 0.21761437878012657
        vf_explained_var: 0.9995412230491638
        vf_loss: 0.22549258545041084
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.70625
    gpu_util_percent0: 0.27718750000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674177121196594
    mean_env_wait_ms: 1.1840931400072339
    mean_inference_ms: 4.300109947506264
    mean_raw_obs_processing_ms: 0.37777178301456515
  time_since_restore: 10136.849298000336
  time_this_iter_s: 26.099964380264282
  time_total_s: 10136.849298000336
  timers:
    learn_throughput: 8629.385
    learn_time_ms: 18748.962
    sample_throughput: 23924.612
    sample_time_ms: 6762.576
    update_time_ms: 37.898
  timestamp: 1602809343
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: 0f5d2_00000
  
2020-10-16 00:49:04,632	WARNING util.py:136 -- The `process_trial` operation took 1.0572290420532227 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    393 |          10136.8 | 63584256 |  288.291 |              308.667 |              136.242 |            802.084 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3012.6822512350036
    time_step_min: 2878
  date: 2020-10-16_00-49-30
  done: false
  episode_len_mean: 802.1148788229959
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.3317580246457
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 205
  episodes_total: 79388
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.049071257094662236
        cur_lr: 5.0e-05
        entropy: 0.05332607993235191
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008934174785584522
        total_loss: .inf
        vf_explained_var: 0.9997122287750244
        vf_loss: 0.16908922170599303
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.54516129032258
    gpu_util_percent0: 0.2687096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467406657737071
    mean_env_wait_ms: 1.1840360680222979
    mean_inference_ms: 4.300059674821148
    mean_raw_obs_processing_ms: 0.3777673170624212
  time_since_restore: 10162.628482818604
  time_this_iter_s: 25.779184818267822
  time_total_s: 10162.628482818604
  timers:
    learn_throughput: 8640.706
    learn_time_ms: 18724.397
    sample_throughput: 23935.567
    sample_time_ms: 6759.481
    update_time_ms: 36.229
  timestamp: 1602809370
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:49:31,980	WARNING util.py:136 -- The `process_trial` operation took 1.0281500816345215 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    394 |          10162.6 | 63746048 |  288.332 |              308.667 |              136.242 |            802.115 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3012.43288590604
    time_step_min: 2878
  date: 2020-10-16_00-49-57
  done: false
  episode_len_mean: 802.14624004422
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.3703458798427
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 79602
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.07360688564199336
        cur_lr: 5.0e-05
        entropy: 0.055106586776673794
        entropy_coeff: 0.0005000000000000001
        kl: 0.0026350859358596304
        model: {}
        policy_loss: -0.009244459118538847
        total_loss: 0.4823242897788684
        vf_explained_var: 0.9991154670715332
        vf_loss: 0.491402342915535
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.209677419354843
    gpu_util_percent0: 0.2803225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673964985907847
    mean_env_wait_ms: 1.183978728356434
    mean_inference_ms: 4.300006484060372
    mean_raw_obs_processing_ms: 0.3777626197058805
  time_since_restore: 10188.631773471832
  time_this_iter_s: 26.00329065322876
  time_total_s: 10188.631773471832
  timers:
    learn_throughput: 8643.029
    learn_time_ms: 18719.364
    sample_throughput: 23937.297
    sample_time_ms: 6758.992
    update_time_ms: 36.319
  timestamp: 1602809397
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: 0f5d2_00000
  
2020-10-16 00:49:59,431	WARNING util.py:136 -- The `process_trial` operation took 1.072817325592041 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    395 |          10188.6 | 63907840 |   288.37 |              308.667 |              136.242 |            802.146 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3012.2002658173883
    time_step_min: 2878
  date: 2020-10-16_00-50-25
  done: false
  episode_len_mean: 802.1771149266825
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.4070343743231
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 79790
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.03680344282099668
        cur_lr: 5.0e-05
        entropy: 0.05459484209616979
        entropy_coeff: 0.0005000000000000001
        kl: 0.004665358690544963
        model: {}
        policy_loss: -0.007697140574843313
        total_loss: 0.18512370685736337
        vf_explained_var: 0.9996203780174255
        vf_loss: 0.1926764448483785
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.241935483870968
    gpu_util_percent0: 0.3438709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673874316405314
    mean_env_wait_ms: 1.1839282045933253
    mean_inference_ms: 4.299956297434507
    mean_raw_obs_processing_ms: 0.3777576424422558
  time_since_restore: 10214.612789869308
  time_this_iter_s: 25.981016397476196
  time_total_s: 10214.612789869308
  timers:
    learn_throughput: 8640.86
    learn_time_ms: 18724.063
    sample_throughput: 23958.647
    sample_time_ms: 6752.969
    update_time_ms: 36.253
  timestamp: 1602809425
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: 0f5d2_00000
  
2020-10-16 00:50:26,830	WARNING util.py:136 -- The `process_trial` operation took 1.0437023639678955 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    396 |          10214.6 | 64069632 |  288.407 |              308.667 |              136.242 |            802.177 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3011.9804112879015
    time_step_min: 2878
  date: 2020-10-16_00-50-52
  done: false
  episode_len_mean: 802.1973618404601
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.4399906794876
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 190
  episodes_total: 79980
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.01840172141049834
        cur_lr: 5.0e-05
        entropy: 0.059758550487458706
        entropy_coeff: 0.0005000000000000001
        kl: 0.004112410650122911
        model: {}
        policy_loss: -0.00951069735068207
        total_loss: 0.3162601614991824
        vf_explained_var: 0.9993774890899658
        vf_loss: 0.32572505871454877
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.941935483870974
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467377274547539
    mean_env_wait_ms: 1.1838775164010917
    mean_inference_ms: 4.299910986662567
    mean_raw_obs_processing_ms: 0.3777528532064068
  time_since_restore: 10240.778281927109
  time_this_iter_s: 26.165492057800293
  time_total_s: 10240.778281927109
  timers:
    learn_throughput: 8643.47
    learn_time_ms: 18718.409
    sample_throughput: 23890.129
    sample_time_ms: 6772.337
    update_time_ms: 36.34
  timestamp: 1602809452
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: 0f5d2_00000
  
2020-10-16 00:50:54,542	WARNING util.py:136 -- The `process_trial` operation took 1.0618035793304443 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    397 |          10240.8 | 64231424 |   288.44 |              308.667 |              136.242 |            802.197 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3011.7270027822483
    time_step_min: 2878
  date: 2020-10-16_00-51-20
  done: false
  episode_len_mean: 802.2259219075411
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.4785212293532
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 207
  episodes_total: 80187
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00920086070524917
        cur_lr: 5.0e-05
        entropy: 0.05644703904787699
        entropy_coeff: 0.0005000000000000001
        kl: 0.003939458110835403
        model: {}
        policy_loss: -0.0073546638086554594
        total_loss: 0.28248023986816406
        vf_explained_var: 0.9994757175445557
        vf_loss: 0.2898268774151802
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.316129032258072
    gpu_util_percent0: 0.3364516129032259
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673666165345062
    mean_env_wait_ms: 1.1838205925416458
    mean_inference_ms: 4.299862600144722
    mean_raw_obs_processing_ms: 0.37774841501956435
  time_since_restore: 10266.459995508194
  time_this_iter_s: 25.681713581085205
  time_total_s: 10266.459995508194
  timers:
    learn_throughput: 8655.117
    learn_time_ms: 18693.22
    sample_throughput: 23888.205
    sample_time_ms: 6772.882
    update_time_ms: 34.383
  timestamp: 1602809480
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: 0f5d2_00000
  
2020-10-16 00:51:21,746	WARNING util.py:136 -- The `process_trial` operation took 1.0437901020050049 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    398 |          10266.5 | 64393216 |  288.479 |              308.667 |              136.242 |            802.226 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3011.477787456446
    time_step_min: 2878
  date: 2020-10-16_00-51-47
  done: false
  episode_len_mean: 802.2523011095079
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.5159524746441
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 209
  episodes_total: 80396
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.004600430352624585
        cur_lr: 5.0e-05
        entropy: 0.058964721548060574
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050215068040415645
        model: {}
        policy_loss: -0.007523144459507118
        total_loss: 0.22459889203310013
        vf_explained_var: 0.9995527863502502
        vf_loss: 0.2321284127732118
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.738709677419354
    gpu_util_percent0: 0.32161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673563903081596
    mean_env_wait_ms: 1.1837648078252478
    mean_inference_ms: 4.299809750019443
    mean_raw_obs_processing_ms: 0.3777438569009993
  time_since_restore: 10292.3309674263
  time_this_iter_s: 25.87097191810608
  time_total_s: 10292.3309674263
  timers:
    learn_throughput: 8654.983
    learn_time_ms: 18693.508
    sample_throughput: 23853.534
    sample_time_ms: 6782.727
    update_time_ms: 35.508
  timestamp: 1602809507
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: 0f5d2_00000
  
2020-10-16 00:51:49,148	WARNING util.py:136 -- The `process_trial` operation took 1.0513637065887451 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    399 |          10292.3 | 64555008 |  288.516 |              308.667 |              136.242 |            802.252 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3011.2561858293916
    time_step_min: 2878
  date: 2020-10-16_00-52-15
  done: false
  episode_len_mean: 802.274561632106
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.54887168847887
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 80583
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.004600430352624585
        cur_lr: 5.0e-05
        entropy: 0.0634268643334508
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008974760031075371
        total_loss: .inf
        vf_explained_var: 0.9990786910057068
        vf_loss: 0.4496566504240036
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.554838709677423
    gpu_util_percent0: 0.32967741935483863
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467347082992766
    mean_env_wait_ms: 1.1837139718757848
    mean_inference_ms: 4.299758952768116
    mean_raw_obs_processing_ms: 0.3777387918453062
  time_since_restore: 10318.347304105759
  time_this_iter_s: 26.016336679458618
  time_total_s: 10318.347304105759
  timers:
    learn_throughput: 8664.252
    learn_time_ms: 18673.509
    sample_throughput: 23786.074
    sample_time_ms: 6801.963
    update_time_ms: 35.055
  timestamp: 1602809535
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:52:16,728	WARNING util.py:136 -- The `process_trial` operation took 1.0781548023223877 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    400 |          10318.3 | 64716800 |  288.549 |              308.667 |              136.242 |            802.275 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3011.0827171961114
    time_step_min: 2878
  date: 2020-10-16_00-52-42
  done: false
  episode_len_mean: 802.2849927581981
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.57506471856345
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 198
  episodes_total: 80781
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006900645528936877
        cur_lr: 5.0e-05
        entropy: 0.07416905835270882
        entropy_coeff: 0.0005000000000000001
        kl: 0.007188402038688461
        model: {}
        policy_loss: -0.011072376859374344
        total_loss: 0.9639216065406799
        vf_explained_var: 0.9980846047401428
        vf_loss: 0.9749814718961716
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.587096774193547
    gpu_util_percent0: 0.33225806451612905
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467337973801182
    mean_env_wait_ms: 1.1836619748121269
    mean_inference_ms: 4.299712585838123
    mean_raw_obs_processing_ms: 0.3777340289494332
  time_since_restore: 10344.446706533432
  time_this_iter_s: 26.09940242767334
  time_total_s: 10344.446706533432
  timers:
    learn_throughput: 8662.083
    learn_time_ms: 18678.187
    sample_throughput: 23786.43
    sample_time_ms: 6801.861
    update_time_ms: 35.798
  timestamp: 1602809562
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: 0f5d2_00000
  
2020-10-16 00:52:44,394	WARNING util.py:136 -- The `process_trial` operation took 1.0857975482940674 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    401 |          10344.4 | 64878592 |  288.575 |              308.667 |              136.242 |            802.285 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3010.9087281980337
    time_step_min: 2878
  date: 2020-10-16_00-53-10
  done: false
  episode_len_mean: 802.3026471750296
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.602492726554
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 211
  episodes_total: 80992
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006900645528936877
        cur_lr: 5.0e-05
        entropy: 0.06570309028029442
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012308423989452422
        total_loss: .inf
        vf_explained_var: 0.9988579750061035
        vf_loss: 0.6269734799861908
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.234375
    gpu_util_percent0: 0.32625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673281483750006
    mean_env_wait_ms: 1.1836047060402377
    mean_inference_ms: 4.2996648050316395
    mean_raw_obs_processing_ms: 0.37772960000638445
  time_since_restore: 10370.494138479233
  time_this_iter_s: 26.04743194580078
  time_total_s: 10370.494138479233
  timers:
    learn_throughput: 8654.052
    learn_time_ms: 18695.52
    sample_throughput: 23787.085
    sample_time_ms: 6801.674
    update_time_ms: 35.927
  timestamp: 1602809590
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:53:11,962	WARNING util.py:136 -- The `process_trial` operation took 1.0764153003692627 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    402 |          10370.5 | 65040384 |  288.602 |              308.667 |              136.242 |            802.303 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3010.6819951697967
    time_step_min: 2878
  date: 2020-10-16_00-53-38
  done: false
  episode_len_mean: 802.3266454823136
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.6368869003361
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 200
  episodes_total: 81192
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.010350968293405316
        cur_lr: 5.0e-05
        entropy: 0.05773105782767137
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008647439807342986
        total_loss: .inf
        vf_explained_var: 0.9994235634803772
        vf_loss: 0.2870696571966012
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.28709677419355
    gpu_util_percent0: 0.2983870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467317525145937
    mean_env_wait_ms: 1.1835516329667117
    mean_inference_ms: 4.2996157148250695
    mean_raw_obs_processing_ms: 0.37772513436478344
  time_since_restore: 10396.608618974686
  time_this_iter_s: 26.11448049545288
  time_total_s: 10396.608618974686
  timers:
    learn_throughput: 8656.045
    learn_time_ms: 18691.216
    sample_throughput: 23797.074
    sample_time_ms: 6798.819
    update_time_ms: 35.383
  timestamp: 1602809618
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:53:39,602	WARNING util.py:136 -- The `process_trial` operation took 1.1271076202392578 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    403 |          10396.6 | 65202176 |  288.637 |              308.667 |              136.242 |            802.327 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3010.450039339103
    time_step_min: 2878
  date: 2020-10-16_00-54-05
  done: false
  episode_len_mean: 802.3507987220447
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.67112759444984
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 81380
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.015526452440107974
        cur_lr: 5.0e-05
        entropy: 0.057530707058807216
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0062731782187862946
        total_loss: .inf
        vf_explained_var: 0.9996079802513123
        vf_loss: 0.2028404859205087
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.803225806451614
    gpu_util_percent0: 0.2990322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673076833872659
    mean_env_wait_ms: 1.1835002760828273
    mean_inference_ms: 4.299566198650592
    mean_raw_obs_processing_ms: 0.37772005928830826
  time_since_restore: 10422.636167287827
  time_this_iter_s: 26.02754831314087
  time_total_s: 10422.636167287827
  timers:
    learn_throughput: 8654.207
    learn_time_ms: 18695.184
    sample_throughput: 23762.272
    sample_time_ms: 6808.776
    update_time_ms: 42.627
  timestamp: 1602809645
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:54:07,158	WARNING util.py:136 -- The `process_trial` operation took 1.1274826526641846 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    404 |          10422.6 | 65363968 |  288.671 |              308.667 |              136.242 |            802.351 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3010.213911678501
    time_step_min: 2878
  date: 2020-10-16_00-54-33
  done: false
  episode_len_mean: 802.3753662094412
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.7060243147835
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 81579
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.02328967866016196
        cur_lr: 5.0e-05
        entropy: 0.06304597482085228
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008012854913734676
        total_loss: .inf
        vf_explained_var: 0.9993228316307068
        vf_loss: 0.35672178616126377
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.200000000000003
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467298718975723
    mean_env_wait_ms: 1.1834479571807839
    mean_inference_ms: 4.299522582586665
    mean_raw_obs_processing_ms: 0.37771571438990487
  time_since_restore: 10448.626963853836
  time_this_iter_s: 25.99079656600952
  time_total_s: 10448.626963853836
  timers:
    learn_throughput: 8650.012
    learn_time_ms: 18704.251
    sample_throughput: 23763.668
    sample_time_ms: 6808.376
    update_time_ms: 41.014
  timestamp: 1602809673
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:54:34,830	WARNING util.py:136 -- The `process_trial` operation took 1.220738410949707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    405 |          10448.6 | 65525760 |  288.706 |              308.667 |              136.242 |            802.375 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3009.9712318211286
    time_step_min: 2878
  date: 2020-10-16_00-55-00
  done: false
  episode_len_mean: 802.4009756336117
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.7417971975815
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 81793
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.03493451799024294
        cur_lr: 5.0e-05
        entropy: 0.06438723765313625
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012351542905283472
        total_loss: .inf
        vf_explained_var: 0.9989423751831055
        vf_loss: 0.5694256871938705
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65483870967742
    gpu_util_percent0: 0.3325806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467288751780427
    mean_env_wait_ms: 1.1833908235518222
    mean_inference_ms: 4.299474175237247
    mean_raw_obs_processing_ms: 0.37771136157194485
  time_since_restore: 10474.467132806778
  time_this_iter_s: 25.840168952941895
  time_total_s: 10474.467132806778
  timers:
    learn_throughput: 8655.172
    learn_time_ms: 18693.1
    sample_throughput: 23743.069
    sample_time_ms: 6814.283
    update_time_ms: 41.266
  timestamp: 1602809700
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:55:02,252	WARNING util.py:136 -- The `process_trial` operation took 1.1104395389556885 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    406 |          10474.5 | 65687552 |  288.742 |              308.667 |              136.242 |            802.401 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3009.754234185865
    time_step_min: 2878
  date: 2020-10-16_00-55-28
  done: false
  episode_len_mean: 802.4282455969167
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.7758271350867
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 195
  episodes_total: 81988
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05240177698536441
        cur_lr: 5.0e-05
        entropy: 0.06114954656610886
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00713710820612808
        total_loss: .inf
        vf_explained_var: 0.9996903538703918
        vf_loss: 0.15528435384233794
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.01875
    gpu_util_percent0: 0.32375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672805026779803
    mean_env_wait_ms: 1.1833387345034185
    mean_inference_ms: 4.299423437190669
    mean_raw_obs_processing_ms: 0.37770658248315003
  time_since_restore: 10500.597691774368
  time_this_iter_s: 26.130558967590332
  time_total_s: 10500.597691774368
  timers:
    learn_throughput: 8653.267
    learn_time_ms: 18697.216
    sample_throughput: 23772.596
    sample_time_ms: 6805.82
    update_time_ms: 41.252
  timestamp: 1602809728
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:55:30,064	WARNING util.py:136 -- The `process_trial` operation took 1.1424715518951416 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    407 |          10500.6 | 65849344 |  288.776 |              308.667 |              136.242 |            802.428 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3009.5475286096907
    time_step_min: 2878
  date: 2020-10-16_00-55-56
  done: false
  episode_len_mean: 802.4527355919004
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.80682223815217
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 82176
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.07860266547804662
        cur_lr: 5.0e-05
        entropy: 0.0653278020521005
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032000585342757404
        model: {}
        policy_loss: -0.009043085388839245
        total_loss: 0.32301121453444165
        vf_explained_var: 0.9993516802787781
        vf_loss: 0.33183542142311734
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.748387096774195
    gpu_util_percent0: 0.32903225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146726920317853
    mean_env_wait_ms: 1.1832887731114365
    mean_inference_ms: 4.299377810610887
    mean_raw_obs_processing_ms: 0.3777015435119423
  time_since_restore: 10526.624994516373
  time_this_iter_s: 26.027302742004395
  time_total_s: 10526.624994516373
  timers:
    learn_throughput: 8639.638
    learn_time_ms: 18726.709
    sample_throughput: 23766.772
    sample_time_ms: 6807.487
    update_time_ms: 43.888
  timestamp: 1602809756
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: 0f5d2_00000
  
2020-10-16 00:55:57,629	WARNING util.py:136 -- The `process_trial` operation took 1.1301443576812744 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    408 |          10526.6 | 66011136 |  288.807 |              308.667 |              136.242 |            802.453 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3009.3498834045863
    time_step_min: 2878
  date: 2020-10-16_00-56-23
  done: false
  episode_len_mean: 802.4801995823824
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.83645626860516
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 196
  episodes_total: 82372
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.03930133273902331
        cur_lr: 5.0e-05
        entropy: 0.06365002195040385
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009932288909719015
        total_loss: .inf
        vf_explained_var: 0.9993662238121033
        vf_loss: 0.3499836102128029
    num_steps_sampled: 66172928
    num_steps_trained: 66172928
  iterations_since_restore: 409
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.990322580645163
    gpu_util_percent0: 0.31806451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467260190006916
    mean_env_wait_ms: 1.1832364939820106
    mean_inference_ms: 4.299334389353049
    mean_raw_obs_processing_ms: 0.3776974965829749
  time_since_restore: 10552.501017332077
  time_this_iter_s: 25.876022815704346
  time_total_s: 10552.501017332077
  timers:
    learn_throughput: 8639.107
    learn_time_ms: 18727.861
    sample_throughput: 23780.909
    sample_time_ms: 6803.44
    update_time_ms: 44.113
  timestamp: 1602809783
  timesteps_since_restore: 0
  timesteps_total: 66172928
  training_iteration: 409
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:56:25,091	WARNING util.py:136 -- The `process_trial` operation took 1.0993599891662598 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    409 |          10552.5 | 66172928 |  288.836 |              308.667 |              136.242 |             802.48 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3009.1169957601455
    time_step_min: 2878
  date: 2020-10-16_00-56-51
  done: false
  episode_len_mean: 802.511939069576
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.87428495107724
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 82586
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05895199910853497
        cur_lr: 5.0e-05
        entropy: 0.06186700922747453
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007960850217690071
        total_loss: .inf
        vf_explained_var: 0.9995656609535217
        vf_loss: 0.23278005172808966
    num_steps_sampled: 66334720
    num_steps_trained: 66334720
  iterations_since_restore: 410
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.484375
    gpu_util_percent0: 0.3303125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467250925669326
    mean_env_wait_ms: 1.1831796347396377
    mean_inference_ms: 4.299287031281401
    mean_raw_obs_processing_ms: 0.3776931350689684
  time_since_restore: 10578.548099040985
  time_this_iter_s: 26.04708170890808
  time_total_s: 10578.548099040985
  timers:
    learn_throughput: 8627.725
    learn_time_ms: 18752.569
    sample_throughput: 23861.364
    sample_time_ms: 6780.501
    update_time_ms: 44.77
  timestamp: 1602809811
  timesteps_since_restore: 0
  timesteps_total: 66334720
  training_iteration: 410
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:56:52,808	WARNING util.py:136 -- The `process_trial` operation took 1.1232311725616455 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    410 |          10578.5 | 66334720 |  288.874 |              308.667 |              136.242 |            802.512 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3008.8849086931823
    time_step_min: 2878
  date: 2020-10-16_00-57-18
  done: false
  episode_len_mean: 802.5402819555684
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.908850400134
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 193
  episodes_total: 82779
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.08842799866280245
        cur_lr: 5.0e-05
        entropy: 0.062295449897646904
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007891186726434777
        total_loss: .inf
        vf_explained_var: 0.9996836185455322
        vf_loss: 0.15675013760725656
    num_steps_sampled: 66496512
    num_steps_trained: 66496512
  iterations_since_restore: 411
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.72580645161291
    gpu_util_percent0: 0.2790322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467243000958848
    mean_env_wait_ms: 1.1831282308766762
    mean_inference_ms: 4.299238498307719
    mean_raw_obs_processing_ms: 0.3776886199310036
  time_since_restore: 10604.673247337341
  time_this_iter_s: 26.1251482963562
  time_total_s: 10604.673247337341
  timers:
    learn_throughput: 8630.165
    learn_time_ms: 18747.266
    sample_throughput: 23871.502
    sample_time_ms: 6777.621
    update_time_ms: 44.186
  timestamp: 1602809838
  timesteps_since_restore: 0
  timesteps_total: 66496512
  training_iteration: 411
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:57:20,484	WARNING util.py:136 -- The `process_trial` operation took 1.1421377658843994 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    411 |          10604.7 | 66496512 |  288.909 |              308.667 |              136.242 |             802.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3008.6721289731345
    time_step_min: 2878
  date: 2020-10-16_00-57-46
  done: false
  episode_len_mean: 802.5653625494166
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.9412979228205
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 82968
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1326419979942037
        cur_lr: 5.0e-05
        entropy: 0.06255510728806257
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008448350825347006
        total_loss: .inf
        vf_explained_var: 0.9994208812713623
        vf_loss: 0.2868826513489087
    num_steps_sampled: 66658304
    num_steps_trained: 66658304
  iterations_since_restore: 412
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83225806451613
    gpu_util_percent0: 0.326774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672308482431948
    mean_env_wait_ms: 1.183078420542073
    mean_inference_ms: 4.299192751421828
    mean_raw_obs_processing_ms: 0.3776834863381868
  time_since_restore: 10630.604630947113
  time_this_iter_s: 25.93138360977173
  time_total_s: 10630.604630947113
  timers:
    learn_throughput: 8635.544
    learn_time_ms: 18735.589
    sample_throughput: 23849.529
    sample_time_ms: 6783.865
    update_time_ms: 44.031
  timestamp: 1602809866
  timesteps_since_restore: 0
  timesteps_total: 66658304
  training_iteration: 412
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:57:47,985	WARNING util.py:136 -- The `process_trial` operation took 1.082935094833374 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    412 |          10630.6 | 66658304 |  288.941 |              308.667 |              136.242 |            802.565 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3008.443246884473
    time_step_min: 2878
  date: 2020-10-16_00-58-13
  done: false
  episode_len_mean: 802.5961186994998
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 288.9764193979032
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 200
  episodes_total: 83168
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1989629969913055
        cur_lr: 5.0e-05
        entropy: 0.0622854291771849
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006212111926288344
        total_loss: .inf
        vf_explained_var: 0.9996582865715027
        vf_loss: 0.1776958629488945
    num_steps_sampled: 66820096
    num_steps_trained: 66820096
  iterations_since_restore: 413
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65
    gpu_util_percent0: 0.35093749999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467223794415408
    mean_env_wait_ms: 1.183025193733956
    mean_inference_ms: 4.299150557705072
    mean_raw_obs_processing_ms: 0.3776796658724755
  time_since_restore: 10656.527882099152
  time_this_iter_s: 25.923251152038574
  time_total_s: 10656.527882099152
  timers:
    learn_throughput: 8643.97
    learn_time_ms: 18717.326
    sample_throughput: 23829.496
    sample_time_ms: 6789.569
    update_time_ms: 44.121
  timestamp: 1602809893
  timesteps_since_restore: 0
  timesteps_total: 66820096
  training_iteration: 413
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:58:15,600	WARNING util.py:136 -- The `process_trial` operation took 1.1199138164520264 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    413 |          10656.5 | 66820096 |  288.976 |              308.667 |              136.242 |            802.596 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3008.1912726312885
    time_step_min: 2878
  date: 2020-10-16_00-58-41
  done: false
  episode_len_mean: 802.625715073816
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 289.0121680205866
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 83383
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29844449548695823
        cur_lr: 5.0e-05
        entropy: 0.06414976250380278
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0076446238478335244
        total_loss: .inf
        vf_explained_var: 0.9995229840278625
        vf_loss: 0.26525242378314334
    num_steps_sampled: 66981888
    num_steps_trained: 66981888
  iterations_since_restore: 414
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.177419354838708
    gpu_util_percent0: 0.29548387096774187
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672138368517568
    mean_env_wait_ms: 1.1829684929191124
    mean_inference_ms: 4.299105446940944
    mean_raw_obs_processing_ms: 0.37767546532081014
  time_since_restore: 10682.81860923767
  time_this_iter_s: 26.290727138519287
  time_total_s: 10682.81860923767
  timers:
    learn_throughput: 8631.654
    learn_time_ms: 18744.032
    sample_throughput: 23802.089
    sample_time_ms: 6797.386
    update_time_ms: 38.266
  timestamp: 1602809921
  timesteps_since_restore: 0
  timesteps_total: 66981888
  training_iteration: 414
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 00:58:43,543	WARNING util.py:136 -- The `process_trial` operation took 1.173886775970459 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    414 |          10682.8 | 66981888 |  289.012 |              308.667 |              136.242 |            802.626 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3007.979122773415
    time_step_min: 2878
  date: 2020-10-16_00-59-09
  done: false
  episode_len_mean: 802.6536519408414
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 289.0450575007719
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 83572
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4476667432304373
        cur_lr: 5.0e-05
        entropy: 0.060543314243356385
        entropy_coeff: 0.0005000000000000001
        kl: 0.0011080578697146848
        model: {}
        policy_loss: -0.007823806605301797
        total_loss: 0.15696643541256586
        vf_explained_var: 0.9997429847717285
        vf_loss: 0.164324468622605
    num_steps_sampled: 67143680
    num_steps_trained: 67143680
  iterations_since_restore: 415
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.378124999999997
    gpu_util_percent0: 0.2953125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672053843345068
    mean_env_wait_ms: 1.1829182386857406
    mean_inference_ms: 4.299058902273969
    mean_raw_obs_processing_ms: 0.3776707813672619
  time_since_restore: 10708.972366809845
  time_this_iter_s: 26.153757572174072
  time_total_s: 10708.972366809845
  timers:
    learn_throughput: 8627.892
    learn_time_ms: 18752.206
    sample_throughput: 23815.884
    sample_time_ms: 6793.449
    update_time_ms: 39.89
  timestamp: 1602809949
  timesteps_since_restore: 0
  timesteps_total: 67143680
  training_iteration: 415
  trial_id: 0f5d2_00000
  
2020-10-16 00:59:11,213	WARNING util.py:136 -- The `process_trial` operation took 1.1090278625488281 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    415 |            10709 | 67143680 |  289.045 |              308.667 |              136.242 |            802.654 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3007.7552551117906
    time_step_min: 2878
  date: 2020-10-16_00-59-37
  done: false
  episode_len_mean: 802.6808891647963
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 289.0783029666316
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 192
  episodes_total: 83764
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.22383337161521866
        cur_lr: 5.0e-05
        entropy: 0.06322682214279969
        entropy_coeff: 0.0005000000000000001
        kl: 0.0016570922161918133
        model: {}
        policy_loss: -0.006392307986970991
        total_loss: 0.3172358547647794
        vf_explained_var: 0.9994165301322937
        vf_loss: 0.32328887035449344
    num_steps_sampled: 67305472
    num_steps_trained: 67305472
  iterations_since_restore: 416
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.009677419354844
    gpu_util_percent0: 0.2783870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671943165887696
    mean_env_wait_ms: 1.1828673453710548
    mean_inference_ms: 4.299007497500477
    mean_raw_obs_processing_ms: 0.3776655177338533
  time_since_restore: 10734.856398582458
  time_this_iter_s: 25.884031772613525
  time_total_s: 10734.856398582458
  timers:
    learn_throughput: 8632.293
    learn_time_ms: 18742.644
    sample_throughput: 23802.653
    sample_time_ms: 6797.225
    update_time_ms: 39.383
  timestamp: 1602809977
  timesteps_since_restore: 0
  timesteps_total: 67305472
  training_iteration: 416
  trial_id: 0f5d2_00000
  
2020-10-16 00:59:38,638	WARNING util.py:136 -- The `process_trial` operation took 1.112121820449829 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    416 |          10734.9 | 67305472 |  289.078 |              308.667 |              136.242 |            802.681 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3007.5266293339687
    time_step_min: 2878
  date: 2020-10-16_01-00-04
  done: false
  episode_len_mean: 802.7076673891813
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 289.11336803713436
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 202
  episodes_total: 83966
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.11191668580760933
        cur_lr: 5.0e-05
        entropy: 0.06647884845733643
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008389618296253806
        total_loss: .inf
        vf_explained_var: 0.9994776248931885
        vf_loss: 0.2841415616373221
    num_steps_sampled: 67467264
    num_steps_trained: 67467264
  iterations_since_restore: 417
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63225806451613
    gpu_util_percent0: 0.2893548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671873066954416
    mean_env_wait_ms: 1.1828141093290794
    mean_inference_ms: 4.29897312115505
    mean_raw_obs_processing_ms: 0.3776621630642253
  time_since_restore: 10760.773233175278
  time_this_iter_s: 25.916834592819214
  time_total_s: 10760.773233175278
  timers:
    learn_throughput: 8638.348
    learn_time_ms: 18729.507
    sample_throughput: 23831.093
    sample_time_ms: 6789.114
    update_time_ms: 38.739
  timestamp: 1602810004
  timesteps_since_restore: 0
  timesteps_total: 67467264
  training_iteration: 417
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 01:00:06,222	WARNING util.py:136 -- The `process_trial` operation took 1.1659672260284424 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    417 |          10760.8 | 67467264 |  289.113 |              308.667 |              136.242 |            802.708 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3007.2985904780016
    time_step_min: 2878
  date: 2020-10-16_01-00-31
  done: false
  episode_len_mean: 802.7360830620827
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 289.14937754556144
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 212
  episodes_total: 84178
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.16787502871141405
        cur_lr: 5.0e-05
        entropy: 0.06282417134692271
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007212905690721527
        total_loss: .inf
        vf_explained_var: 0.9997043013572693
        vf_loss: 0.16975170746445656
    num_steps_sampled: 67629056
    num_steps_trained: 67629056
  iterations_since_restore: 418
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53870967741935
    gpu_util_percent0: 0.34161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467177158100286
    mean_env_wait_ms: 1.1827581287425681
    mean_inference_ms: 4.29892687766461
    mean_raw_obs_processing_ms: 0.3776578647789932
  time_since_restore: 10786.428161621094
  time_this_iter_s: 25.65492844581604
  time_total_s: 10786.428161621094
  timers:
    learn_throughput: 8653.248
    learn_time_ms: 18697.256
    sample_throughput: 23831.201
    sample_time_ms: 6789.083
    update_time_ms: 35.848
  timestamp: 1602810031
  timesteps_since_restore: 0
  timesteps_total: 67629056
  training_iteration: 418
  trial_id: 0f5d2_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 01:00:33,408	WARNING util.py:136 -- The `process_trial` operation took 1.1406996250152588 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | RUNNING  | 172.17.0.4:59344 |    418 |          10786.4 | 67629056 |  289.149 |              308.667 |              136.242 |            802.736 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0f5d2_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3007.085829479426
    time_step_min: 2878
  date: 2020-10-16_01-00-59
  done: true
  episode_len_mean: 802.763577744589
  episode_reward_max: 308.6666666666666
  episode_reward_mean: 289.1824424459368
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 188
  episodes_total: 84366
  experiment_id: bc277001785c4b549bb2d98c6e459f35
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2518125430671211
        cur_lr: 5.0e-05
        entropy: 0.06342354913552602
        entropy_coeff: 0.0005000000000000001
        kl: 0.0016118221004338313
        model: {}
        policy_loss: -0.008136977524069758
        total_loss: 0.29088647415240604
        vf_explained_var: 0.9994263648986816
        vf_loss: 0.29864928623040515
    num_steps_sampled: 67790848
    num_steps_trained: 67790848
  iterations_since_restore: 419
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53548387096774
    gpu_util_percent0: 0.31774193548387103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 59344
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467167962263171
    mean_env_wait_ms: 1.1827082854428923
    mean_inference_ms: 4.298879700082267
    mean_raw_obs_processing_ms: 0.3776530425754147
  time_since_restore: 10812.37517619133
  time_this_iter_s: 25.947014570236206
  time_total_s: 10812.37517619133
  timers:
    learn_throughput: 8653.956
    learn_time_ms: 18695.726
    sample_throughput: 23843.199
    sample_time_ms: 6785.667
    update_time_ms: 36.262
  timestamp: 1602810059
  timesteps_since_restore: 0
  timesteps_total: 67790848
  training_iteration: 419
  trial_id: 0f5d2_00000
  
2020-10-16 01:01:01,075	WARNING util.py:136 -- The `process_trial` operation took 1.3169560432434082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 26.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | TERMINATED |       |    419 |          10812.4 | 67790848 |  289.182 |              308.667 |              136.242 |            802.764 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0f5d2_00000 | TERMINATED |       |    419 |          10812.4 | 67790848 |  289.182 |              308.667 |              136.242 |            802.764 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


