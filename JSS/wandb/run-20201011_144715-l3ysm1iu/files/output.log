2020-10-11 14:47:19,495	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_aa989_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=30378)[0m 2020-10-11 14:47:22,295	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=30277)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30277)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30263)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30263)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30347)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30347)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30346)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30346)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30334)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30334)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30372)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30372)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30331)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30331)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30335)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30335)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30368)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30368)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30395)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30395)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30318)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30318)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30350)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30350)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30351)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30351)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30354)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30354)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30269)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30269)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30364)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30364)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30266)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30266)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30265)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30265)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30333)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30333)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30267)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30267)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30314)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30314)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30284)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30284)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30271)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30271)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30323)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30323)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30338)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30338)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30317)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30317)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30363)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30363)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30348)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30348)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30359)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30359)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30274)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30274)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30275)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30275)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30380)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30380)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30356)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30356)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30391)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30391)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30384)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30384)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30289)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30289)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30278)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30278)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30388)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30388)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30402)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30402)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30398)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30398)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30342)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30342)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30262)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30262)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30332)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30332)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30326)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30326)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30379)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30379)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30341)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30341)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30321)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30321)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30259)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30259)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30330)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30330)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30344)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30344)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30286)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30286)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30287)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30287)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30339)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30339)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30385)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30385)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30288)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30288)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30357)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30357)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30328)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30328)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30280)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30280)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30261)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30261)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30329)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30329)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30270)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30270)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30358)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30358)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30290)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30290)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30292)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30292)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30260)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30260)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30281)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30281)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30279)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30279)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30268)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30268)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30340)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30340)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30273)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30273)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30371)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30371)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30282)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30282)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30258)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30258)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30276)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30276)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30324)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30324)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30369)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30369)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30352)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30352)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30311)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30311)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30382)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30382)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_aa989_00000:
  custom_metrics: {}
  date: 2020-10-11_14-47-43
  done: false
  episode_len_mean: .nan
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1914540827274323
        entropy_coeff: 0.0001
        kl: 0.007823126390576363
        model: {}
        policy_loss: -0.015033794334158301
        total_loss: 562.9246368408203
        vf_explained_var: -0.7487409114837646
        vf_loss: 562.938232421875
    num_steps_sampled: 60672
    num_steps_trained: 60672
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.126315789473686
    gpu_util_percent0: 0.2715789473684211
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.321052631578947
    vram_util_percent0: 0.08482936596986071
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
  time_since_restore: 16.101393938064575
  time_this_iter_s: 16.101393938064575
  time_total_s: 16.101393938064575
  timers:
    learn_throughput: 7147.299
    learn_time_ms: 8488.802
    sample_throughput: 8107.298
    sample_time_ms: 7483.628
    update_time_ms: 40.196
  timestamp: 1602427663
  timesteps_since_restore: 0
  timesteps_total: 60672
  training_iteration: 1
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      1 |          16.1014 | 60672 |      nan |                  nan |                  nan |                nan |
+-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4058
    time_step_mean: 3611.3670886075947
    time_step_min: 3306
  date: 2020-10-11_14-47-57
  done: false
  episode_len_mean: 886.0379746835443
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 218.84337041299042
  episode_reward_min: 151.1717171717169
  episodes_this_iter: 79
  episodes_total: 79
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1658275425434113
        entropy_coeff: 0.0001
        kl: 0.005513080162927508
        model: {}
        policy_loss: -0.010292008926626295
        total_loss: 595.6605682373047
        vf_explained_var: 0.18877363204956055
        vf_loss: 595.6699066162109
    num_steps_sampled: 121344
    num_steps_trained: 121344
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.35333333333333
    gpu_util_percent0: 0.18400000000000002
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1353082761870322
    mean_env_wait_ms: 0.649634201882847
    mean_inference_ms: 5.982030458370913
    mean_raw_obs_processing_ms: 0.3024412316962542
  time_since_restore: 30.174316883087158
  time_this_iter_s: 14.072922945022583
  time_total_s: 30.174316883087158
  timers:
    learn_throughput: 7229.464
    learn_time_ms: 8392.323
    sample_throughput: 9193.229
    sample_time_ms: 6599.639
    update_time_ms: 28.913
  timestamp: 1602427677
  timesteps_since_restore: 0
  timesteps_total: 121344
  training_iteration: 2
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      2 |          30.1743 | 121344 |  218.843 |              265.111 |              151.172 |            886.038 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3614.0443037974683
    time_step_min: 3306
  date: 2020-10-11_14-48-11
  done: false
  episode_len_mean: 880.4050632911392
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 218.43773174785815
  episode_reward_min: 135.8686868686869
  episodes_this_iter: 79
  episodes_total: 158
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1451095044612885
        entropy_coeff: 0.0001
        kl: 0.006658109603449702
        model: {}
        policy_loss: -0.009628374013118446
        total_loss: 343.8333053588867
        vf_explained_var: 0.582950234413147
        vf_loss: 343.8417205810547
    num_steps_sampled: 182016
    num_steps_trained: 182016
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.606250000000003
    gpu_util_percent0: 0.27625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13283139408501185
    mean_env_wait_ms: 0.6483184285076065
    mean_inference_ms: 5.788295846912503
    mean_raw_obs_processing_ms: 0.2956042049387476
  time_since_restore: 43.59142303466797
  time_this_iter_s: 13.41710615158081
  time_total_s: 43.59142303466797
  timers:
    learn_throughput: 7291.419
    learn_time_ms: 8321.014
    sample_throughput: 9949.588
    sample_time_ms: 6097.941
    update_time_ms: 54.955
  timestamp: 1602427691
  timesteps_since_restore: 0
  timesteps_total: 182016
  training_iteration: 3
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      3 |          43.5914 | 182016 |  218.438 |              265.111 |              135.869 |            880.405 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4213
    time_step_mean: 3624.0
    time_step_min: 3306
  date: 2020-10-11_14-48-24
  done: false
  episode_len_mean: 877.9535864978903
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 216.92929292929279
  episode_reward_min: 127.68686868686834
  episodes_this_iter: 79
  episodes_total: 237
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1370092332363129
        entropy_coeff: 0.0001
        kl: 0.0060931689804419875
        model: {}
        policy_loss: -0.015206624171696603
        total_loss: 237.94325637817383
        vf_explained_var: 0.7285435795783997
        vf_loss: 237.95736694335938
    num_steps_sampled: 242688
    num_steps_trained: 242688
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.760000000000005
    gpu_util_percent0: 0.32599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.131088020562995
    mean_env_wait_ms: 0.6481277160168207
    mean_inference_ms: 5.63176925772184
    mean_raw_obs_processing_ms: 0.29100826100299026
  time_since_restore: 56.75381684303284
  time_this_iter_s: 13.162393808364868
  time_total_s: 56.75381684303284
  timers:
    learn_throughput: 7303.971
    learn_time_ms: 8306.714
    sample_throughput: 10495.447
    sample_time_ms: 5780.792
    update_time_ms: 51.0
  timestamp: 1602427704
  timesteps_since_restore: 0
  timesteps_total: 242688
  training_iteration: 4
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      4 |          56.7538 | 242688 |  216.929 |              265.111 |              127.687 |            877.954 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4220
    time_step_mean: 3626.240506329114
    time_step_min: 3306
  date: 2020-10-11_14-48-37
  done: false
  episode_len_mean: 876.5949367088608
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 216.58982227336645
  episode_reward_min: 126.62626262626257
  episodes_this_iter: 79
  episodes_total: 316
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.12206369638443
        entropy_coeff: 0.0001
        kl: 0.006890057120472193
        model: {}
        policy_loss: -0.011395521811209619
        total_loss: 132.92739486694336
        vf_explained_var: 0.8328981995582581
        vf_loss: 132.9375228881836
    num_steps_sampled: 303360
    num_steps_trained: 303360
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.926666666666666
    gpu_util_percent0: 0.4253333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12977563879098883
    mean_env_wait_ms: 0.6484338119868207
    mean_inference_ms: 5.507404139768565
    mean_raw_obs_processing_ms: 0.2876684028763285
  time_since_restore: 69.77894926071167
  time_this_iter_s: 13.025132417678833
  time_total_s: 69.77894926071167
  timers:
    learn_throughput: 7305.841
    learn_time_ms: 8304.588
    sample_throughput: 10916.428
    sample_time_ms: 5557.862
    update_time_ms: 45.848
  timestamp: 1602427717
  timesteps_since_restore: 0
  timesteps_total: 303360
  training_iteration: 5
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      5 |          69.7789 | 303360 |   216.59 |              265.111 |              126.626 |            876.595 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3624.9088607594936
    time_step_min: 3306
  date: 2020-10-11_14-48-50
  done: false
  episode_len_mean: 873.2886075949367
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 216.7915867536119
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 395
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1141673624515533
        entropy_coeff: 0.0001
        kl: 0.006491948966868222
        model: {}
        policy_loss: -0.012061259825713933
        total_loss: 95.29883575439453
        vf_explained_var: 0.8731156587600708
        vf_loss: 95.30970764160156
    num_steps_sampled: 364032
    num_steps_trained: 364032
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.36428571428572
    gpu_util_percent0: 0.3792857142857144
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1287156511526069
    mean_env_wait_ms: 0.6488228309557317
    mean_inference_ms: 5.4062742835181306
    mean_raw_obs_processing_ms: 0.28500371357615617
  time_since_restore: 82.6038908958435
  time_this_iter_s: 12.824941635131836
  time_total_s: 82.6038908958435
  timers:
    learn_throughput: 7323.161
    learn_time_ms: 8284.947
    sample_throughput: 11250.121
    sample_time_ms: 5393.008
    update_time_ms: 44.266
  timestamp: 1602427730
  timesteps_since_restore: 0
  timesteps_total: 364032
  training_iteration: 6
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      6 |          82.6039 | 364032 |  216.792 |              265.111 |              118.293 |            873.289 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3619.0021097046415
    time_step_min: 3289
  date: 2020-10-11_14-49-03
  done: false
  episode_len_mean: 868.9451476793249
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 217.68654903465014
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 474
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0996873378753662
        entropy_coeff: 0.0001
        kl: 0.007531901705078781
        model: {}
        policy_loss: -0.010992132098181173
        total_loss: 71.36981773376465
        vf_explained_var: 0.8989866375923157
        vf_loss: 71.3794174194336
    num_steps_sampled: 424704
    num_steps_trained: 424704
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.793333333333337
    gpu_util_percent0: 0.29600000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12785548063034805
    mean_env_wait_ms: 0.6493148476274758
    mean_inference_ms: 5.322844403613065
    mean_raw_obs_processing_ms: 0.28287895450910405
  time_since_restore: 95.52371096611023
  time_this_iter_s: 12.919820070266724
  time_total_s: 95.52371096611023
  timers:
    learn_throughput: 7324.942
    learn_time_ms: 8282.933
    sample_throughput: 11494.883
    sample_time_ms: 5278.175
    update_time_ms: 42.136
  timestamp: 1602427743
  timesteps_since_restore: 0
  timesteps_total: 424704
  training_iteration: 7
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      7 |          95.5237 | 424704 |  217.687 |              267.687 |              118.293 |            868.945 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3617.5623869801084
    time_step_min: 3289
  date: 2020-10-11_14-49-16
  done: false
  episode_len_mean: 863.7233273056058
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 217.9046888413976
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 553
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.080336093902588
        entropy_coeff: 0.0001
        kl: 0.00649541465099901
        model: {}
        policy_loss: -0.014029796118848026
        total_loss: 65.09321212768555
        vf_explained_var: 0.9129441976547241
        vf_loss: 65.10604953765869
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.31333333333333
    gpu_util_percent0: 0.35133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4333333333333327
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12714315858196026
    mean_env_wait_ms: 0.6499228345088768
    mean_inference_ms: 5.252964204882852
    mean_raw_obs_processing_ms: 0.2811312550628777
  time_since_restore: 108.57829403877258
  time_this_iter_s: 13.054583072662354
  time_total_s: 108.57829403877258
  timers:
    learn_throughput: 7312.975
    learn_time_ms: 8296.486
    sample_throughput: 11684.656
    sample_time_ms: 5192.451
    update_time_ms: 41.438
  timestamp: 1602427756
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 8
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      8 |          108.578 | 485376 |  217.905 |              267.687 |              118.293 |            863.723 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3613.845276872964
    time_step_min: 3289
  date: 2020-10-11_14-49-29
  done: false
  episode_len_mean: 860.5602605863193
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 218.46788734248003
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 61
  episodes_total: 614
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0620779395103455
        entropy_coeff: 0.0001
        kl: 0.0070233645383268595
        model: {}
        policy_loss: -0.011862239392939955
        total_loss: 53.99057388305664
        vf_explained_var: 0.9240583181381226
        vf_loss: 54.00113868713379
    num_steps_sampled: 546048
    num_steps_trained: 546048
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.933333333333337
    gpu_util_percent0: 0.30133333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1266455878797975
    mean_env_wait_ms: 0.6503698409329477
    mean_inference_ms: 5.20612271537529
    mean_raw_obs_processing_ms: 0.2799062414992872
  time_since_restore: 121.55057454109192
  time_this_iter_s: 12.972280502319336
  time_total_s: 121.55057454109192
  timers:
    learn_throughput: 7316.869
    learn_time_ms: 8292.072
    sample_throughput: 11842.208
    sample_time_ms: 5123.369
    update_time_ms: 48.014
  timestamp: 1602427769
  timesteps_since_restore: 0
  timesteps_total: 546048
  training_iteration: 9
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |      9 |          121.551 | 546048 |  218.468 |              267.687 |              118.293 |             860.56 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3611.2753846153846
    time_step_min: 3289
  date: 2020-10-11_14-49-42
  done: false
  episode_len_mean: 859.0492307692308
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 218.85726495726487
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 36
  episodes_total: 650
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0485284924507141
        entropy_coeff: 0.0001
        kl: 0.006600828724913299
        model: {}
        policy_loss: -0.017158268485218287
        total_loss: 33.94167709350586
        vf_explained_var: 0.9404016137123108
        vf_loss: 33.95761775970459
    num_steps_sampled: 606720
    num_steps_trained: 606720
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.90666666666667
    gpu_util_percent0: 0.348
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1264161972479877
    mean_env_wait_ms: 0.6508235562773642
    mean_inference_ms: 5.1787589908961555
    mean_raw_obs_processing_ms: 0.2792088672252598
  time_since_restore: 134.47225499153137
  time_this_iter_s: 12.921680450439453
  time_total_s: 134.47225499153137
  timers:
    learn_throughput: 7327.027
    learn_time_ms: 8280.575
    sample_throughput: 11960.916
    sample_time_ms: 5072.521
    update_time_ms: 52.722
  timestamp: 1602427782
  timesteps_since_restore: 0
  timesteps_total: 606720
  training_iteration: 10
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     10 |          134.472 | 606720 |  218.857 |              267.687 |              118.293 |            859.049 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3606.5876577840113
    time_step_min: 3289
  date: 2020-10-11_14-49-55
  done: false
  episode_len_mean: 856.6143057503506
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 219.56752659838202
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 63
  episodes_total: 713
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0584369599819183
        entropy_coeff: 0.0001
        kl: 0.005829800385981798
        model: {}
        policy_loss: -0.01342546020168811
        total_loss: 33.587894439697266
        vf_explained_var: 0.931602954864502
        vf_loss: 33.60025978088379
    num_steps_sampled: 667392
    num_steps_trained: 667392
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.306666666666665
    gpu_util_percent0: 0.20533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12595370785638055
    mean_env_wait_ms: 0.6511042667730063
    mean_inference_ms: 5.136032815902064
    mean_raw_obs_processing_ms: 0.2780245787968382
  time_since_restore: 147.44169783592224
  time_this_iter_s: 12.96944284439087
  time_total_s: 147.44169783592224
  timers:
    learn_throughput: 7342.154
    learn_time_ms: 8263.515
    sample_throughput: 12688.941
    sample_time_ms: 4781.486
    update_time_ms: 53.012
  timestamp: 1602427795
  timesteps_since_restore: 0
  timesteps_total: 667392
  training_iteration: 11
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     11 |          147.442 | 667392 |  219.568 |              267.687 |              118.293 |            856.614 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3600.558227848101
    time_step_min: 3235
  date: 2020-10-11_14-50-08
  done: false
  episode_len_mean: 853.659493670886
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 220.48107658867144
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 77
  episodes_total: 790
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.044329285621643
        entropy_coeff: 0.0001
        kl: 0.0065484626684337854
        model: {}
        policy_loss: -0.014785136096179485
        total_loss: 35.80673599243164
        vf_explained_var: 0.9390555024147034
        vf_loss: 35.820316314697266
    num_steps_sampled: 728064
    num_steps_trained: 728064
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.278571428571425
    gpu_util_percent0: 0.4357142857142856
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4214285714285713
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12547292705254384
    mean_env_wait_ms: 0.6516200365130129
    mean_inference_ms: 5.08991119095177
    mean_raw_obs_processing_ms: 0.276733398682132
  time_since_restore: 160.31700205802917
  time_this_iter_s: 12.875304222106934
  time_total_s: 160.31700205802917
  timers:
    learn_throughput: 7339.2
    learn_time_ms: 8266.841
    sample_throughput: 13024.14
    sample_time_ms: 4658.426
    update_time_ms: 53.389
  timestamp: 1602427808
  timesteps_since_restore: 0
  timesteps_total: 728064
  training_iteration: 12
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     12 |          160.317 | 728064 |  220.481 |              275.869 |              118.293 |            853.659 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3593.910241657077
    time_step_min: 3235
  date: 2020-10-11_14-50-21
  done: false
  episode_len_mean: 850.2543153049482
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 221.48834722367513
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 869
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0323504209518433
        entropy_coeff: 0.0001
        kl: 0.005593539914116263
        model: {}
        policy_loss: -0.007711198413744569
        total_loss: 33.82645511627197
        vf_explained_var: 0.9453220367431641
        vf_loss: 33.83315181732178
    num_steps_sampled: 788736
    num_steps_trained: 788736
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.926666666666662
    gpu_util_percent0: 0.3453333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12503921943097748
    mean_env_wait_ms: 0.652143801885267
    mean_inference_ms: 5.048366292008042
    mean_raw_obs_processing_ms: 0.27555612622309206
  time_since_restore: 173.18758249282837
  time_this_iter_s: 12.870580434799194
  time_total_s: 173.18758249282837
  timers:
    learn_throughput: 7329.535
    learn_time_ms: 8277.742
    sample_throughput: 13186.176
    sample_time_ms: 4601.182
    update_time_ms: 45.127
  timestamp: 1602427821
  timesteps_since_restore: 0
  timesteps_total: 788736
  training_iteration: 13
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     13 |          173.188 | 788736 |  221.488 |              275.869 |              118.293 |            850.254 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3588.6867088607596
    time_step_min: 3235
  date: 2020-10-11_14-50-34
  done: false
  episode_len_mean: 847.3449367088608
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 222.2797915867535
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 948
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.013579249382019
        entropy_coeff: 0.0001
        kl: 0.0068066451931372285
        model: {}
        policy_loss: -0.014376593055203557
        total_loss: 28.295745849609375
        vf_explained_var: 0.9575772881507874
        vf_loss: 28.308862686157227
    num_steps_sampled: 849408
    num_steps_trained: 849408
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.21333333333333
    gpu_util_percent0: 0.2753333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4199999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12465345866675773
    mean_env_wait_ms: 0.6526681348875534
    mean_inference_ms: 5.011768356592469
    mean_raw_obs_processing_ms: 0.2745004617109849
  time_since_restore: 185.90577864646912
  time_this_iter_s: 12.718196153640747
  time_total_s: 185.90577864646912
  timers:
    learn_throughput: 7334.234
    learn_time_ms: 8272.438
    sample_throughput: 13294.576
    sample_time_ms: 4563.666
    update_time_ms: 43.107
  timestamp: 1602427834
  timesteps_since_restore: 0
  timesteps_total: 849408
  training_iteration: 14
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     14 |          185.906 | 849408 |   222.28 |              275.869 |              118.293 |            847.345 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3583.4829600778967
    time_step_min: 3235
  date: 2020-10-11_14-50-47
  done: false
  episode_len_mean: 844.6426484907497
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 223.06823837203578
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1027
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9933836460113525
        entropy_coeff: 0.0001
        kl: 0.00562152813654393
        model: {}
        policy_loss: -0.011437032371759415
        total_loss: 25.94105339050293
        vf_explained_var: 0.9608175158500671
        vf_loss: 25.951465129852295
    num_steps_sampled: 910080
    num_steps_trained: 910080
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.666666666666675
    gpu_util_percent0: 0.3526666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12431285493161397
    mean_env_wait_ms: 0.6532080179815016
    mean_inference_ms: 4.979332084217276
    mean_raw_obs_processing_ms: 0.27356378945775
  time_since_restore: 198.81229877471924
  time_this_iter_s: 12.906520128250122
  time_total_s: 198.81229877471924
  timers:
    learn_throughput: 7339.963
    learn_time_ms: 8265.981
    sample_throughput: 13310.697
    sample_time_ms: 4558.138
    update_time_ms: 43.736
  timestamp: 1602427847
  timesteps_since_restore: 0
  timesteps_total: 910080
  training_iteration: 15
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     15 |          198.812 | 910080 |  223.068 |              275.869 |              118.293 |            844.643 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3577.5723327305604
    time_step_min: 3235
  date: 2020-10-11_14-51-00
  done: false
  episode_len_mean: 841.9891500904159
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 223.963787970117
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1106
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9824153780937195
        entropy_coeff: 0.0001
        kl: 0.005990388686768711
        model: {}
        policy_loss: -0.007877310505136847
        total_loss: 21.027120113372803
        vf_explained_var: 0.9672377705574036
        vf_loss: 21.033896446228027
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.58
    gpu_util_percent0: 0.3606666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12400905884143851
    mean_env_wait_ms: 0.6537594747037538
    mean_inference_ms: 4.9502559467252825
    mean_raw_obs_processing_ms: 0.27271462310642164
  time_since_restore: 211.76999950408936
  time_this_iter_s: 12.957700729370117
  time_total_s: 211.76999950408936
  timers:
    learn_throughput: 7334.729
    learn_time_ms: 8271.88
    sample_throughput: 13308.725
    sample_time_ms: 4558.814
    update_time_ms: 42.298
  timestamp: 1602427860
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 16
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     16 |           211.77 | 970752 |  223.964 |              275.869 |              118.293 |            841.989 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3572.2556962025315
    time_step_min: 3235
  date: 2020-10-11_14-51-13
  done: false
  episode_len_mean: 839.464135021097
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 224.76933895921235
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1185
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9703270643949509
        entropy_coeff: 0.0001
        kl: 0.005524565000087023
        model: {}
        policy_loss: -0.01052850013365969
        total_loss: 24.28107976913452
        vf_explained_var: 0.9627320766448975
        vf_loss: 24.290600299835205
    num_steps_sampled: 1031424
    num_steps_trained: 1031424
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.385714285714286
    gpu_util_percent0: 0.34142857142857136
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1237347073052231
    mean_env_wait_ms: 0.6543202270320436
    mean_inference_ms: 4.9240364808609
    mean_raw_obs_processing_ms: 0.2719430279370468
  time_since_restore: 224.54811787605286
  time_this_iter_s: 12.778118371963501
  time_total_s: 224.54811787605286
  timers:
    learn_throughput: 7339.12
    learn_time_ms: 8266.931
    sample_throughput: 13333.696
    sample_time_ms: 4550.276
    update_time_ms: 41.151
  timestamp: 1602427873
  timesteps_since_restore: 0
  timesteps_total: 1031424
  training_iteration: 17
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     17 |          224.548 | 1031424 |  224.769 |              275.869 |              118.293 |            839.464 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3566.5245253164558
    time_step_min: 3235
  date: 2020-10-11_14-51-26
  done: false
  episode_len_mean: 837.506329113924
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 225.63769818437535
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1264
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9576845765113831
        entropy_coeff: 0.0001
        kl: 0.006472750450484455
        model: {}
        policy_loss: -0.01621266547590494
        total_loss: 19.243000507354736
        vf_explained_var: 0.9701140522956848
        vf_loss: 19.258015155792236
    num_steps_sampled: 1092096
    num_steps_trained: 1092096
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.953333333333333
    gpu_util_percent0: 0.316
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1234859061171375
    mean_env_wait_ms: 0.6548650697834195
    mean_inference_ms: 4.90020685495471
    mean_raw_obs_processing_ms: 0.27124150206018965
  time_since_restore: 237.47010207176208
  time_this_iter_s: 12.921984195709229
  time_total_s: 237.47010207176208
  timers:
    learn_throughput: 7351.781
    learn_time_ms: 8252.695
    sample_throughput: 13327.926
    sample_time_ms: 4552.246
    update_time_ms: 39.23
  timestamp: 1602427886
  timesteps_since_restore: 0
  timesteps_total: 1092096
  training_iteration: 18
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     18 |           237.47 | 1092096 |  225.638 |              275.869 |              118.293 |            837.506 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3560.9985107967236
    time_step_min: 3206
  date: 2020-10-11_14-51-39
  done: false
  episode_len_mean: 835.8816083395384
  episode_reward_max: 280.26262626262593
  episode_reward_mean: 226.47497311160745
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1343
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9319173842668533
        entropy_coeff: 0.0001
        kl: 0.006037887651473284
        model: {}
        policy_loss: -0.012360059190541506
        total_loss: 18.858981609344482
        vf_explained_var: 0.971014678478241
        vf_loss: 18.87022590637207
    num_steps_sampled: 1152768
    num_steps_trained: 1152768
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.946666666666662
    gpu_util_percent0: 0.37333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12325786261858288
    mean_env_wait_ms: 0.6553831537708115
    mean_inference_ms: 4.878422743818669
    mean_raw_obs_processing_ms: 0.27059673978998283
  time_since_restore: 250.22734022140503
  time_this_iter_s: 12.757238149642944
  time_total_s: 250.22734022140503
  timers:
    learn_throughput: 7358.864
    learn_time_ms: 8244.751
    sample_throughput: 13365.577
    sample_time_ms: 4539.422
    update_time_ms: 38.92
  timestamp: 1602427899
  timesteps_since_restore: 0
  timesteps_total: 1152768
  training_iteration: 19
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     19 |          250.227 | 1152768 |  226.475 |              280.263 |              118.293 |            835.882 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3557.6068917018283
    time_step_min: 3206
  date: 2020-10-11_14-51-52
  done: false
  episode_len_mean: 834.2637130801688
  episode_reward_max: 280.26262626262593
  episode_reward_mean: 226.9888547926522
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1422
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9170082658529282
        entropy_coeff: 0.0001
        kl: 0.005881667952053249
        model: {}
        policy_loss: -0.009232628857716918
        total_loss: 21.077390670776367
        vf_explained_var: 0.9689568877220154
        vf_loss: 21.0855393409729
    num_steps_sampled: 1213440
    num_steps_trained: 1213440
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.75714285714286
    gpu_util_percent0: 0.20785714285714282
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.414285714285714
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12304888683912533
    mean_env_wait_ms: 0.6558821895833474
    mean_inference_ms: 4.858389563488416
    mean_raw_obs_processing_ms: 0.2700032545267339
  time_since_restore: 262.96310806274414
  time_this_iter_s: 12.735767841339111
  time_total_s: 262.96310806274414
  timers:
    learn_throughput: 7357.534
    learn_time_ms: 8246.241
    sample_throughput: 13403.312
    sample_time_ms: 4526.642
    update_time_ms: 31.452
  timestamp: 1602427912
  timesteps_since_restore: 0
  timesteps_total: 1213440
  training_iteration: 20
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     20 |          262.963 | 1213440 |  226.989 |              280.263 |              118.293 |            834.264 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3553.070619586942
    time_step_min: 3206
  date: 2020-10-11_14-52-05
  done: false
  episode_len_mean: 832.6069287141905
  episode_reward_max: 280.26262626262593
  episode_reward_mean: 227.67616874945318
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1501
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9074065536260605
        entropy_coeff: 0.0001
        kl: 0.005816309945657849
        model: {}
        policy_loss: -0.015069264685735106
        total_loss: 16.58755850791931
        vf_explained_var: 0.9737280011177063
        vf_loss: 16.601556301116943
    num_steps_sampled: 1274112
    num_steps_trained: 1274112
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.513333333333335
    gpu_util_percent0: 0.14533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12285460119718666
    mean_env_wait_ms: 0.6563595525384185
    mean_inference_ms: 4.839865820264126
    mean_raw_obs_processing_ms: 0.2694527426779773
  time_since_restore: 275.8283269405365
  time_this_iter_s: 12.865218877792358
  time_total_s: 275.8283269405365
  timers:
    learn_throughput: 7351.45
    learn_time_ms: 8253.066
    sample_throughput: 13454.156
    sample_time_ms: 4509.536
    update_time_ms: 31.093
  timestamp: 1602427925
  timesteps_since_restore: 0
  timesteps_total: 1274112
  training_iteration: 21
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     21 |          275.828 | 1274112 |  227.676 |              280.263 |              118.293 |            832.607 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3546.6664556962023
    time_step_min: 3206
  date: 2020-10-11_14-52-17
  done: false
  episode_len_mean: 831.3810126582279
  episode_reward_max: 280.26262626262593
  episode_reward_mean: 228.64649661168644
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1580
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8895199149847031
        entropy_coeff: 0.0001
        kl: 0.006235960638150573
        model: {}
        policy_loss: -0.016274704947136343
        total_loss: 14.437464952468872
        vf_explained_var: 0.9757154583930969
        vf_loss: 14.45258092880249
    num_steps_sampled: 1334784
    num_steps_trained: 1334784
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.640000000000004
    gpu_util_percent0: 0.4166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12267414774631347
    mean_env_wait_ms: 0.6568038344783947
    mean_inference_ms: 4.822673854077089
    mean_raw_obs_processing_ms: 0.26893973005150973
  time_since_restore: 288.69322776794434
  time_this_iter_s: 12.864900827407837
  time_total_s: 288.69322776794434
  timers:
    learn_throughput: 7359.24
    learn_time_ms: 8244.329
    sample_throughput: 13451.748
    sample_time_ms: 4510.343
    update_time_ms: 37.968
  timestamp: 1602427937
  timesteps_since_restore: 0
  timesteps_total: 1334784
  training_iteration: 22
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     22 |          288.693 | 1334784 |  228.646 |              280.263 |              118.293 |            831.381 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3541.2525617842075
    time_step_min: 3199
  date: 2020-10-11_14-52-30
  done: false
  episode_len_mean: 829.9795057263411
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 229.46678356804938
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1659
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8658313006162643
        entropy_coeff: 0.0001
        kl: 0.005380009184591472
        model: {}
        policy_loss: -0.008646945061627775
        total_loss: 20.08211898803711
        vf_explained_var: 0.9681438207626343
        vf_loss: 20.08977699279785
    num_steps_sampled: 1395456
    num_steps_trained: 1395456
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.050000000000004
    gpu_util_percent0: 0.4414285714285714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.414285714285714
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12250588645121233
    mean_env_wait_ms: 0.6572267549415766
    mean_inference_ms: 4.806699588903894
    mean_raw_obs_processing_ms: 0.2684630946614512
  time_since_restore: 301.44871520996094
  time_this_iter_s: 12.755487442016602
  time_total_s: 301.44871520996094
  timers:
    learn_throughput: 7362.785
    learn_time_ms: 8240.36
    sample_throughput: 13479.091
    sample_time_ms: 4501.194
    update_time_ms: 38.936
  timestamp: 1602427950
  timesteps_since_restore: 0
  timesteps_total: 1395456
  training_iteration: 23
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     23 |          301.449 | 1395456 |  229.467 |              281.323 |              118.293 |             829.98 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3536.58918296893
    time_step_min: 3199
  date: 2020-10-11_14-52-43
  done: false
  episode_len_mean: 829.3107019562716
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 230.1733561158187
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1738
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8665132224559784
        entropy_coeff: 0.0001
        kl: 0.005198416416533291
        model: {}
        policy_loss: -0.011439272901043296
        total_loss: 17.11276149749756
        vf_explained_var: 0.9721271991729736
        vf_loss: 17.12324619293213
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.426666666666666
    gpu_util_percent0: 0.24066666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4466666666666663
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12234825149070479
    mean_env_wait_ms: 0.6576126991689294
    mean_inference_ms: 4.791808475892309
    mean_raw_obs_processing_ms: 0.26801903209083283
  time_since_restore: 314.25941705703735
  time_this_iter_s: 12.810701847076416
  time_total_s: 314.25941705703735
  timers:
    learn_throughput: 7353.007
    learn_time_ms: 8251.319
    sample_throughput: 13494.35
    sample_time_ms: 4496.104
    update_time_ms: 40.548
  timestamp: 1602427963
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 24
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     24 |          314.259 | 1456128 |  230.173 |              281.323 |              118.293 |            829.311 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3531.2894881673087
    time_step_min: 3199
  date: 2020-10-11_14-52-56
  done: false
  episode_len_mean: 828.4171711612548
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 230.9763401766704
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1817
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8617505133152008
        entropy_coeff: 0.0001
        kl: 0.0055166283855214715
        model: {}
        policy_loss: -0.012109191156923771
        total_loss: 12.609570026397705
        vf_explained_var: 0.9781627655029297
        vf_loss: 12.620662212371826
    num_steps_sampled: 1516800
    num_steps_trained: 1516800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.94285714285714
    gpu_util_percent0: 0.23500000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.442857142857143
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12220007626450559
    mean_env_wait_ms: 0.6579674778371506
    mean_inference_ms: 4.777886453814694
    mean_raw_obs_processing_ms: 0.26760158411270163
  time_since_restore: 327.11176109313965
  time_this_iter_s: 12.852344036102295
  time_total_s: 327.11176109313965
  timers:
    learn_throughput: 7343.737
    learn_time_ms: 8261.734
    sample_throughput: 13546.238
    sample_time_ms: 4478.882
    update_time_ms: 41.285
  timestamp: 1602427976
  timesteps_since_restore: 0
  timesteps_total: 1516800
  training_iteration: 25
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     25 |          327.112 | 1516800 |  230.976 |              281.323 |              118.293 |            828.417 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3525.5443505807816
    time_step_min: 3199
  date: 2020-10-11_14-53-09
  done: false
  episode_len_mean: 827.2967265047519
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 231.84681556856845
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 77
  episodes_total: 1894
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8505872040987015
        entropy_coeff: 0.0001
        kl: 0.005504266591742635
        model: {}
        policy_loss: -0.013713978929445148
        total_loss: 14.596449613571167
        vf_explained_var: 0.9737269878387451
        vf_loss: 14.609147310256958
    num_steps_sampled: 1577472
    num_steps_trained: 1577472
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.860000000000003
    gpu_util_percent0: 0.30466666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12206247307375404
    mean_env_wait_ms: 0.6582994622828627
    mean_inference_ms: 4.765167746679992
    mean_raw_obs_processing_ms: 0.2672237938965896
  time_since_restore: 339.92621898651123
  time_this_iter_s: 12.814457893371582
  time_total_s: 339.92621898651123
  timers:
    learn_throughput: 7341.174
    learn_time_ms: 8264.618
    sample_throughput: 13575.839
    sample_time_ms: 4469.116
    update_time_ms: 41.007
  timestamp: 1602427989
  timesteps_since_restore: 0
  timesteps_total: 1577472
  training_iteration: 26
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     26 |          339.926 | 1577472 |  231.847 |              281.323 |              118.293 |            827.297 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3521.043279022403
    time_step_min: 3199
  date: 2020-10-11_14-53-22
  done: false
  episode_len_mean: 826.1883910386965
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 232.52879610771666
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 70
  episodes_total: 1964
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8386607617139816
        entropy_coeff: 0.0001
        kl: 0.006047990289516747
        model: {}
        policy_loss: -0.015535812475718558
        total_loss: 14.120278358459473
        vf_explained_var: 0.9748682379722595
        vf_loss: 14.134687900543213
    num_steps_sampled: 1638144
    num_steps_trained: 1638144
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.946666666666665
    gpu_util_percent0: 0.3626666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12194517256059809
    mean_env_wait_ms: 0.6585797022373939
    mean_inference_ms: 4.75417602343179
    mean_raw_obs_processing_ms: 0.2668919473907244
  time_since_restore: 352.8753459453583
  time_this_iter_s: 12.949126958847046
  time_total_s: 352.8753459453583
  timers:
    learn_throughput: 7333.696
    learn_time_ms: 8273.045
    sample_throughput: 13559.318
    sample_time_ms: 4474.561
    update_time_ms: 43.736
  timestamp: 1602428002
  timesteps_since_restore: 0
  timesteps_total: 1638144
  training_iteration: 27
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     27 |          352.875 | 1638144 |  232.529 |              281.323 |              118.293 |            826.188 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3516.673720472441
    time_step_min: 3199
  date: 2020-10-11_14-53-35
  done: false
  episode_len_mean: 825.3661417322835
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 233.19085043346854
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 68
  episodes_total: 2032
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8244215697050095
        entropy_coeff: 0.0001
        kl: 0.005533277872018516
        model: {}
        policy_loss: -0.01628575964423362
        total_loss: 14.40134334564209
        vf_explained_var: 0.974392294883728
        vf_loss: 14.41660475730896
    num_steps_sampled: 1698816
    num_steps_trained: 1698816
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.17857142857144
    gpu_util_percent0: 0.30142857142857143
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.428571428571428
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12183073386559173
    mean_env_wait_ms: 0.6588753102214618
    mean_inference_ms: 4.744120508750853
    mean_raw_obs_processing_ms: 0.26658921663103674
  time_since_restore: 365.65469121932983
  time_this_iter_s: 12.779345273971558
  time_total_s: 365.65469121932983
  timers:
    learn_throughput: 7332.967
    learn_time_ms: 8273.868
    sample_throughput: 13606.1
    sample_time_ms: 4459.176
    update_time_ms: 43.874
  timestamp: 1602428015
  timesteps_since_restore: 0
  timesteps_total: 1698816
  training_iteration: 28
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     28 |          365.655 | 1698816 |  233.191 |              281.323 |              118.293 |            825.366 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3512.8546927108146
    time_step_min: 3111
  date: 2020-10-11_14-53-48
  done: false
  episode_len_mean: 824.6441162458314
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 233.7694910034119
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 67
  episodes_total: 2099
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8053620457649231
        entropy_coeff: 0.0001
        kl: 0.004583484609611332
        model: {}
        policy_loss: -0.011118872789666057
        total_loss: 10.728185892105103
        vf_explained_var: 0.979694128036499
        vf_loss: 10.738468647003174
    num_steps_sampled: 1759488
    num_steps_trained: 1759488
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.76
    gpu_util_percent0: 0.28933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4199999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12173404020161536
    mean_env_wait_ms: 0.6591291266650579
    mean_inference_ms: 4.734578217333354
    mean_raw_obs_processing_ms: 0.2663074777346303
  time_since_restore: 378.49544954299927
  time_this_iter_s: 12.840758323669434
  time_total_s: 378.49544954299927
  timers:
    learn_throughput: 7329.739
    learn_time_ms: 8277.511
    sample_throughput: 13573.324
    sample_time_ms: 4469.944
    update_time_ms: 37.662
  timestamp: 1602428028
  timesteps_since_restore: 0
  timesteps_total: 1759488
  training_iteration: 29
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     29 |          378.495 | 1759488 |  233.769 |              294.657 |              118.293 |            824.644 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3509.5301982480405
    time_step_min: 3111
  date: 2020-10-11_14-54-01
  done: false
  episode_len_mean: 823.8326417704011
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 234.2732022856504
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 70
  episodes_total: 2169
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.8010383099317551
        entropy_coeff: 0.0001
        kl: 0.005840748781338334
        model: {}
        policy_loss: -0.00798800599295646
        total_loss: 11.50804591178894
        vf_explained_var: 0.9788138270378113
        vf_loss: 11.515530347824097
    num_steps_sampled: 1820160
    num_steps_trained: 1820160
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.842857142857138
    gpu_util_percent0: 0.4235714285714285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12163288460882281
    mean_env_wait_ms: 0.6593830453234737
    mean_inference_ms: 4.725118458976588
    mean_raw_obs_processing_ms: 0.2660189862136188
  time_since_restore: 391.29025769233704
  time_this_iter_s: 12.794808149337769
  time_total_s: 391.29025769233704
  timers:
    learn_throughput: 7323.283
    learn_time_ms: 8284.809
    sample_throughput: 13582.945
    sample_time_ms: 4466.778
    update_time_ms: 39.261
  timestamp: 1602428041
  timesteps_since_restore: 0
  timesteps_total: 1820160
  training_iteration: 30
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     30 |           391.29 | 1820160 |  234.273 |              294.657 |              118.293 |            823.833 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3506.5406976744184
    time_step_min: 3111
  date: 2020-10-11_14-54-14
  done: false
  episode_len_mean: 823.0178890876565
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 234.7261569180174
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 67
  episodes_total: 2236
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7931597828865051
        entropy_coeff: 0.0001
        kl: 0.00602615880779922
        model: {}
        policy_loss: -0.007323025201912969
        total_loss: 12.5483980178833
        vf_explained_var: 0.9768276810646057
        vf_loss: 12.555197715759277
    num_steps_sampled: 1880832
    num_steps_trained: 1880832
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.980000000000004
    gpu_util_percent0: 0.43000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4399999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1215348891564357
    mean_env_wait_ms: 0.6596170927931669
    mean_inference_ms: 4.716433072692993
    mean_raw_obs_processing_ms: 0.2657620108508695
  time_since_restore: 404.21544122695923
  time_this_iter_s: 12.925183534622192
  time_total_s: 404.21544122695923
  timers:
    learn_throughput: 7322.775
    learn_time_ms: 8285.384
    sample_throughput: 13567.329
    sample_time_ms: 4471.919
    update_time_ms: 38.899
  timestamp: 1602428054
  timesteps_since_restore: 0
  timesteps_total: 1880832
  training_iteration: 31
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     31 |          404.215 | 1880832 |  234.726 |              294.657 |              118.293 |            823.018 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3503.4176062445795
    time_step_min: 3111
  date: 2020-10-11_14-54-27
  done: false
  episode_len_mean: 822.153512575889
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 235.19935258920518
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 70
  episodes_total: 2306
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7887319028377533
        entropy_coeff: 0.0001
        kl: 0.006466412800364196
        model: {}
        policy_loss: -0.011869820766150951
        total_loss: 10.237318277359009
        vf_explained_var: 0.9808382987976074
        vf_loss: 10.248620510101318
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.253333333333334
    gpu_util_percent0: 0.3353333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12143859271709237
    mean_env_wait_ms: 0.6598490910078433
    mean_inference_ms: 4.707725523268732
    mean_raw_obs_processing_ms: 0.2654865415242206
  time_since_restore: 417.0642886161804
  time_this_iter_s: 12.848847389221191
  time_total_s: 417.0642886161804
  timers:
    learn_throughput: 7325.106
    learn_time_ms: 8282.747
    sample_throughput: 13575.538
    sample_time_ms: 4469.215
    update_time_ms: 40.018
  timestamp: 1602428067
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 32
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     32 |          417.064 | 1941504 |  235.199 |              294.657 |              118.293 |            822.154 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3499.610012620951
    time_step_min: 3111
  date: 2020-10-11_14-54-40
  done: false
  episode_len_mean: 821.4859066049643
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 235.77626071399737
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 71
  episodes_total: 2377
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7735395282506943
        entropy_coeff: 0.0001
        kl: 0.006109715439379215
        model: {}
        policy_loss: -0.013684868696145713
        total_loss: 9.02634072303772
        vf_explained_var: 0.9823529720306396
        vf_loss: 9.039491653442383
    num_steps_sampled: 2002176
    num_steps_trained: 2002176
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.759999999999998
    gpu_util_percent0: 0.2733333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12135047465721231
    mean_env_wait_ms: 0.660079740919627
    mean_inference_ms: 4.699384408095418
    mean_raw_obs_processing_ms: 0.2652290656408341
  time_since_restore: 429.9861912727356
  time_this_iter_s: 12.921902656555176
  time_total_s: 429.9861912727356
  timers:
    learn_throughput: 7315.582
    learn_time_ms: 8293.53
    sample_throughput: 13578.974
    sample_time_ms: 4468.084
    update_time_ms: 39.873
  timestamp: 1602428080
  timesteps_since_restore: 0
  timesteps_total: 2002176
  training_iteration: 33
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     33 |          429.986 | 2002176 |  235.776 |              294.657 |              118.293 |            821.486 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3495.8756117455137
    time_step_min: 3111
  date: 2020-10-11_14-54-53
  done: false
  episode_len_mean: 820.7389885807504
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 236.34207902845748
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 75
  episodes_total: 2452
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7676493227481842
        entropy_coeff: 0.0001
        kl: 0.006269674748182297
        model: {}
        policy_loss: -0.01027127931592986
        total_loss: 10.94404411315918
        vf_explained_var: 0.9788558483123779
        vf_loss: 10.953765153884888
    num_steps_sampled: 2062848
    num_steps_trained: 2062848
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.920000000000005
    gpu_util_percent0: 0.18800000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.44
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12125829544826763
    mean_env_wait_ms: 0.660303209874392
    mean_inference_ms: 4.690864363293233
    mean_raw_obs_processing_ms: 0.26495963661088284
  time_since_restore: 442.88846588134766
  time_this_iter_s: 12.90227460861206
  time_total_s: 442.88846588134766
  timers:
    learn_throughput: 7322.409
    learn_time_ms: 8285.798
    sample_throughput: 13547.139
    sample_time_ms: 4478.584
    update_time_ms: 40.131
  timestamp: 1602428093
  timesteps_since_restore: 0
  timesteps_total: 2062848
  training_iteration: 34
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     34 |          442.888 | 2062848 |  236.342 |              294.657 |              118.293 |            820.739 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3492.986956521739
    time_step_min: 3111
  date: 2020-10-11_14-55-06
  done: false
  episode_len_mean: 819.8332015810277
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 236.77975406236274
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 78
  episodes_total: 2530
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7546486258506775
        entropy_coeff: 0.0001
        kl: 0.0058147438103333116
        model: {}
        policy_loss: -0.012304193340241909
        total_loss: 10.538083553314209
        vf_explained_var: 0.980566680431366
        vf_loss: 10.549881935119629
    num_steps_sampled: 2123520
    num_steps_trained: 2123520
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.042857142857144
    gpu_util_percent0: 0.285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1211656984847536
    mean_env_wait_ms: 0.6605341194347947
    mean_inference_ms: 4.682443660025644
    mean_raw_obs_processing_ms: 0.26469004825660664
  time_since_restore: 455.58453154563904
  time_this_iter_s: 12.696065664291382
  time_total_s: 455.58453154563904
  timers:
    learn_throughput: 7333.085
    learn_time_ms: 8273.735
    sample_throughput: 13553.674
    sample_time_ms: 4476.425
    update_time_ms: 38.036
  timestamp: 1602428106
  timesteps_since_restore: 0
  timesteps_total: 2123520
  training_iteration: 35
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     35 |          455.585 | 2123520 |   236.78 |              294.657 |              118.293 |            819.833 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3488.9290916059795
    time_step_min: 3111
  date: 2020-10-11_14-55-19
  done: false
  episode_len_mean: 818.9126101954772
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 237.39458207990214
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 2609
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7400196939706802
        entropy_coeff: 0.0001
        kl: 0.0058068325743079185
        model: {}
        policy_loss: -0.013815922429785132
        total_loss: 10.05512261390686
        vf_explained_var: 0.9806376099586487
        vf_loss: 10.068431377410889
    num_steps_sampled: 2184192
    num_steps_trained: 2184192
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.40666666666667
    gpu_util_percent0: 0.5099999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12107590653974439
    mean_env_wait_ms: 0.6607655107133611
    mean_inference_ms: 4.674335427180369
    mean_raw_obs_processing_ms: 0.2644309335817367
  time_since_restore: 468.476092338562
  time_this_iter_s: 12.891560792922974
  time_total_s: 468.476092338562
  timers:
    learn_throughput: 7324.24
    learn_time_ms: 8283.727
    sample_throughput: 13568.691
    sample_time_ms: 4471.47
    update_time_ms: 39.449
  timestamp: 1602428119
  timesteps_since_restore: 0
  timesteps_total: 2184192
  training_iteration: 36
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     36 |          468.476 | 2184192 |  237.395 |              294.657 |              118.293 |            818.913 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3485.9352678571427
    time_step_min: 3111
  date: 2020-10-11_14-55-32
  done: false
  episode_len_mean: 818.0517113095239
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 237.84819173881675
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 2688
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7301702946424484
        entropy_coeff: 0.0001
        kl: 0.005751452641561627
        model: {}
        policy_loss: -0.013575302669778466
        total_loss: 9.521034479141235
        vf_explained_var: 0.982122540473938
        vf_loss: 9.534107208251953
    num_steps_sampled: 2244864
    num_steps_trained: 2244864
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.453333333333333
    gpu_util_percent0: 0.37
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12099014109422618
    mean_env_wait_ms: 0.6609919616885195
    mean_inference_ms: 4.666600035578396
    mean_raw_obs_processing_ms: 0.2641818136224133
  time_since_restore: 481.48514008522034
  time_this_iter_s: 13.009047746658325
  time_total_s: 481.48514008522034
  timers:
    learn_throughput: 7315.257
    learn_time_ms: 8293.899
    sample_throughput: 13581.942
    sample_time_ms: 4467.108
    update_time_ms: 39.054
  timestamp: 1602428132
  timesteps_since_restore: 0
  timesteps_total: 2244864
  training_iteration: 37
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     37 |          481.485 | 2244864 |  237.848 |              294.657 |              118.293 |            818.052 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3482.712942877802
    time_step_min: 3111
  date: 2020-10-11_14-55-45
  done: false
  episode_len_mean: 817.2541576283442
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 238.33642279629268
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 78
  episodes_total: 2766
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7123904228210449
        entropy_coeff: 0.0001
        kl: 0.006374098709784448
        model: {}
        policy_loss: -0.013502237037755549
        total_loss: 11.306285858154297
        vf_explained_var: 0.9792220592498779
        vf_loss: 11.319222211837769
    num_steps_sampled: 2305536
    num_steps_trained: 2305536
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.77857142857142
    gpu_util_percent0: 0.3378571428571428
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.407142857142856
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12090944212907186
    mean_env_wait_ms: 0.661212187696142
    mean_inference_ms: 4.659301530913864
    mean_raw_obs_processing_ms: 0.26394518537259776
  time_since_restore: 494.3030438423157
  time_this_iter_s: 12.817903757095337
  time_total_s: 494.3030438423157
  timers:
    learn_throughput: 7310.474
    learn_time_ms: 8299.325
    sample_throughput: 13589.213
    sample_time_ms: 4464.718
    update_time_ms: 40.492
  timestamp: 1602428145
  timesteps_since_restore: 0
  timesteps_total: 2305536
  training_iteration: 38
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     38 |          494.303 | 2305536 |  238.336 |              294.657 |              118.293 |            817.254 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3479.0738137082603
    time_step_min: 3111
  date: 2020-10-11_14-55-58
  done: false
  episode_len_mean: 816.4987697715289
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 238.887806003799
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 2845
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7063906639814377
        entropy_coeff: 0.0001
        kl: 0.006619708146899939
        model: {}
        policy_loss: -0.008540161070413888
        total_loss: 10.110114336013794
        vf_explained_var: 0.9805809855461121
        vf_loss: 10.118062973022461
    num_steps_sampled: 2366208
    num_steps_trained: 2366208
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.06
    gpu_util_percent0: 0.2793333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12083054580488334
    mean_env_wait_ms: 0.6614261407606844
    mean_inference_ms: 4.652212114091645
    mean_raw_obs_processing_ms: 0.2637147061382379
  time_since_restore: 507.125301361084
  time_this_iter_s: 12.82225751876831
  time_total_s: 507.125301361084
  timers:
    learn_throughput: 7298.762
    learn_time_ms: 8312.643
    sample_throughput: 13637.273
    sample_time_ms: 4448.983
    update_time_ms: 40.687
  timestamp: 1602428158
  timesteps_since_restore: 0
  timesteps_total: 2366208
  training_iteration: 39
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     39 |          507.125 | 2366208 |  238.888 |              294.657 |              118.293 |            816.499 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3476.669859733151
    time_step_min: 3111
  date: 2020-10-11_14-56-11
  done: false
  episode_len_mean: 815.7071501881628
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 239.25204145457312
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 78
  episodes_total: 2923
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6949535459280014
        entropy_coeff: 0.0001
        kl: 0.005586441489867866
        model: {}
        policy_loss: -0.009916623908793554
        total_loss: 12.35196328163147
        vf_explained_var: 0.9776116609573364
        vf_loss: 12.361390352249146
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.84666666666667
    gpu_util_percent0: 0.328
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12075583387622384
    mean_env_wait_ms: 0.6616118662427545
    mean_inference_ms: 4.645521398644301
    mean_raw_obs_processing_ms: 0.2634973274986382
  time_since_restore: 519.9028582572937
  time_this_iter_s: 12.777556896209717
  time_total_s: 519.9028582572937
  timers:
    learn_throughput: 7295.11
    learn_time_ms: 8316.804
    sample_throughput: 13658.482
    sample_time_ms: 4442.075
    update_time_ms: 41.178
  timestamp: 1602428171
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 40
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     40 |          519.903 | 2426880 |  239.252 |              294.657 |              118.293 |            815.707 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3473.2671552298466
    time_step_min: 3111
  date: 2020-10-11_14-56-23
  done: false
  episode_len_mean: 814.9037308461026
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 239.76760274295256
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3002
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6896779984235764
        entropy_coeff: 0.0001
        kl: 0.00545170099940151
        model: {}
        policy_loss: -0.006760447518900037
        total_loss: 10.799469709396362
        vf_explained_var: 0.9795577526092529
        vf_loss: 10.805754661560059
    num_steps_sampled: 2487552
    num_steps_trained: 2487552
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.27142857142857
    gpu_util_percent0: 0.315
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.414285714285714
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12068299758383619
    mean_env_wait_ms: 0.6618155184073969
    mean_inference_ms: 4.639004157673648
    mean_raw_obs_processing_ms: 0.26328312205898957
  time_since_restore: 532.7221517562866
  time_this_iter_s: 12.81929349899292
  time_total_s: 532.7221517562866
  timers:
    learn_throughput: 7307.809
    learn_time_ms: 8302.351
    sample_throughput: 13647.004
    sample_time_ms: 4445.811
    update_time_ms: 41.117
  timestamp: 1602428183
  timesteps_since_restore: 0
  timesteps_total: 2487552
  training_iteration: 41
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     41 |          532.722 | 2487552 |  239.768 |              294.657 |              118.293 |            814.904 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3469.998052580331
    time_step_min: 3111
  date: 2020-10-11_14-56-36
  done: false
  episode_len_mean: 814.205452775073
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 240.26292132621253
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3081
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6865971684455872
        entropy_coeff: 0.0001
        kl: 0.005908183171413839
        model: {}
        policy_loss: -0.013723642565310001
        total_loss: 10.921100378036499
        vf_explained_var: 0.9789117574691772
        vf_loss: 10.934301376342773
    num_steps_sampled: 2548224
    num_steps_trained: 2548224
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.9
    gpu_util_percent0: 0.3646666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12061345458390671
    mean_env_wait_ms: 0.6620150578185433
    mean_inference_ms: 4.632758214341117
    mean_raw_obs_processing_ms: 0.26307767477043403
  time_since_restore: 545.5967059135437
  time_this_iter_s: 12.87455415725708
  time_total_s: 545.5967059135437
  timers:
    learn_throughput: 7307.2
    learn_time_ms: 8303.043
    sample_throughput: 13609.269
    sample_time_ms: 4458.138
    update_time_ms: 32.909
  timestamp: 1602428196
  timesteps_since_restore: 0
  timesteps_total: 2548224
  training_iteration: 42
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     42 |          545.597 | 2548224 |  240.263 |              294.657 |              118.293 |            814.205 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3466.8977848101267
    time_step_min: 3111
  date: 2020-10-11_14-56-49
  done: false
  episode_len_mean: 813.7351265822784
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 240.73265886715257
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3160
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.673320397734642
        entropy_coeff: 0.0001
        kl: 0.005441875662654638
        model: {}
        policy_loss: -0.01596468430943787
        total_loss: 9.379735231399536
        vf_explained_var: 0.9821313619613647
        vf_loss: 9.395222902297974
    num_steps_sampled: 2608896
    num_steps_trained: 2608896
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.01428571428571
    gpu_util_percent0: 0.3864285714285714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4357142857142855
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12054719299088809
    mean_env_wait_ms: 0.6622107411404029
    mean_inference_ms: 4.626774104338643
    mean_raw_obs_processing_ms: 0.262880171655244
  time_since_restore: 558.4876661300659
  time_this_iter_s: 12.890960216522217
  time_total_s: 558.4876661300659
  timers:
    learn_throughput: 7309.357
    learn_time_ms: 8300.593
    sample_throughput: 13590.475
    sample_time_ms: 4464.303
    update_time_ms: 32.743
  timestamp: 1602428209
  timesteps_since_restore: 0
  timesteps_total: 2608896
  training_iteration: 43
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     43 |          558.488 | 2608896 |  240.733 |              294.657 |              118.293 |            813.735 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3464.2837295461563
    time_step_min: 3111
  date: 2020-10-11_14-57-02
  done: false
  episode_len_mean: 813.19851806113
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 241.128727846542
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3239
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6631857454776764
        entropy_coeff: 0.0001
        kl: 0.005600042990408838
        model: {}
        policy_loss: -0.013800082611851394
        total_loss: 8.741819620132446
        vf_explained_var: 0.983812153339386
        vf_loss: 8.755126237869263
    num_steps_sampled: 2669568
    num_steps_trained: 2669568
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.633333333333326
    gpu_util_percent0: 0.24733333333333332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12048383742756033
    mean_env_wait_ms: 0.6624026104425397
    mean_inference_ms: 4.621022230831381
    mean_raw_obs_processing_ms: 0.26269015779477184
  time_since_restore: 571.2364234924316
  time_this_iter_s: 12.748757362365723
  time_total_s: 571.2364234924316
  timers:
    learn_throughput: 7312.235
    learn_time_ms: 8297.326
    sample_throughput: 13605.7
    sample_time_ms: 4459.308
    update_time_ms: 32.534
  timestamp: 1602428222
  timesteps_since_restore: 0
  timesteps_total: 2669568
  training_iteration: 44
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     44 |          571.236 | 2669568 |  241.129 |              294.657 |              118.293 |            813.199 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3461.2525617842075
    time_step_min: 3111
  date: 2020-10-11_14-57-15
  done: false
  episode_len_mean: 812.5304400241109
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 241.58799568926156
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3318
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6549868285655975
        entropy_coeff: 0.0001
        kl: 0.004678940866142511
        model: {}
        policy_loss: -0.012409038899932057
        total_loss: 8.848819255828857
        vf_explained_var: 0.9826209545135498
        vf_loss: 8.860825538635254
    num_steps_sampled: 2730240
    num_steps_trained: 2730240
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.566666666666666
    gpu_util_percent0: 0.364
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12042257631144071
    mean_env_wait_ms: 0.6625899771759679
    mean_inference_ms: 4.615485161038933
    mean_raw_obs_processing_ms: 0.2625069356077095
  time_since_restore: 584.0689632892609
  time_this_iter_s: 12.832539796829224
  time_total_s: 584.0689632892609
  timers:
    learn_throughput: 7303.409
    learn_time_ms: 8307.354
    sample_throughput: 13601.312
    sample_time_ms: 4460.746
    update_time_ms: 34.857
  timestamp: 1602428235
  timesteps_since_restore: 0
  timesteps_total: 2730240
  training_iteration: 45
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     45 |          584.069 | 2730240 |  241.588 |              294.657 |              118.293 |             812.53 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3458.4942596408596
    time_step_min: 3111
  date: 2020-10-11_14-57-28
  done: false
  episode_len_mean: 811.9428907859876
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 242.00592025643545
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3397
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05
        cur_lr: 5.0e-05
        entropy: 0.6498712301254272
        entropy_coeff: 0.0001
        kl: 0.006043577450327575
        model: {}
        policy_loss: -0.013496566331014037
        total_loss: 10.97796106338501
        vf_explained_var: 0.9792823791503906
        vf_loss: 10.991219997406006
    num_steps_sampled: 2790912
    num_steps_trained: 2790912
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.646666666666665
    gpu_util_percent0: 0.38466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1203635101345648
    mean_env_wait_ms: 0.6627730494240113
    mean_inference_ms: 4.610156389481937
    mean_raw_obs_processing_ms: 0.26233076077496337
  time_since_restore: 596.9440293312073
  time_this_iter_s: 12.875066041946411
  time_total_s: 596.9440293312073
  timers:
    learn_throughput: 7315.02
    learn_time_ms: 8294.167
    sample_throughput: 13584.312
    sample_time_ms: 4466.329
    update_time_ms: 41.192
  timestamp: 1602428248
  timesteps_since_restore: 0
  timesteps_total: 2790912
  training_iteration: 46
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | RUNNING  | 172.17.0.4:30378 |     46 |          596.944 | 2790912 |  242.006 |              294.657 |              118.293 |            811.943 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_aa989_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3455.911104718067
    time_step_min: 3111
  date: 2020-10-11_14-57-41
  done: true
  episode_len_mean: 811.5350978135788
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 242.39730736594953
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3476
  experiment_id: ba5f3505c1d7476591d2835c30dfda72
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05
        cur_lr: 5.0e-05
        entropy: 0.6342495381832123
        entropy_coeff: 0.0001
        kl: 0.006048538372851908
        model: {}
        policy_loss: -0.010996688564773649
        total_loss: 8.571913242340088
        vf_explained_var: 0.9842838048934937
        vf_loss: 8.582670211791992
    num_steps_sampled: 2851584
    num_steps_trained: 2851584
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.285714285714285
    gpu_util_percent0: 0.09499999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4642857142857144
    vram_util_percent0: 0.11634962282715648
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30378
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12030700443289846
    mean_env_wait_ms: 0.6629518989680405
    mean_inference_ms: 4.605029494850388
    mean_raw_obs_processing_ms: 0.26216238752738485
  time_since_restore: 609.7330024242401
  time_this_iter_s: 12.788973093032837
  time_total_s: 609.7330024242401
  timers:
    learn_throughput: 7324.169
    learn_time_ms: 8283.806
    sample_throughput: 13614.628
    sample_time_ms: 4456.383
    update_time_ms: 39.685
  timestamp: 1602428261
  timesteps_since_restore: 0
  timesteps_total: 2851584
  training_iteration: 47
  trial_id: aa989_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | TERMINATED |       |     47 |          609.733 | 2851584 |  242.397 |              294.657 |              118.293 |            811.535 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


2020-10-11 14:57:41,835	WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffffe9f6a7cf01000000.
2020-10-11 14:57:41,835	WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffffd8f83c3801000000.
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_aa989_00000 | TERMINATED |       |     47 |          609.733 | 2851584 |  242.397 |              294.657 |              118.293 |            811.535 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


[2m[36m(pid=30329)[0m 2020-10-11 14:57:41,813	ERROR worker.py:372 -- SystemExit was raised from the worker
[2m[36m(pid=30329)[0m Traceback (most recent call last):
[2m[36m(pid=30329)[0m   File "python/ray/_raylet.pyx", line 483, in ray._raylet.execute_task
[2m[36m(pid=30329)[0m   File "python/ray/_raylet.pyx", line 484, in ray._raylet.execute_task
[2m[36m(pid=30329)[0m   File "python/ray/_raylet.pyx", line 438, in ray._raylet.execute_task.function_executor
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/function_manager.py", line 553, in actor_method_executor
[2m[36m(pid=30329)[0m     return method(actor, *args, **kwargs)
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 929, in __ray_terminate__
[2m[36m(pid=30329)[0m     ray.actor.exit_actor()
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 996, in exit_actor
[2m[36m(pid=30329)[0m     raise exit
[2m[36m(pid=30329)[0m SystemExit: 0
[2m[36m(pid=30329)[0m 
[2m[36m(pid=30329)[0m During handling of the above exception, another exception occurred:
[2m[36m(pid=30329)[0m 
[2m[36m(pid=30329)[0m Traceback (most recent call last):
[2m[36m(pid=30329)[0m   File "python/ray/_raylet.pyx", line 553, in ray._raylet.task_execution_handler
[2m[36m(pid=30329)[0m   File "python/ray/_raylet.pyx", line 440, in ray._raylet.execute_task
[2m[36m(pid=30329)[0m   File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
[2m[36m(pid=30329)[0m   File "python/ray/includes/libcoreworker.pxi", line 33, in ray._raylet.ProfileEvent.__exit__
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 167, in format_exc
[2m[36m(pid=30329)[0m     return "".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 120, in format_exception
[2m[36m(pid=30329)[0m     return list(TracebackException(
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 509, in __init__
[2m[36m(pid=30329)[0m     self.stack = StackSummary.extract(
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 366, in extract
[2m[36m(pid=30329)[0m     f.line
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 288, in line
[2m[36m(pid=30329)[0m     self._line = linecache.getline(self.filename, self.lineno).strip()
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/linecache.py", line 16, in getline
[2m[36m(pid=30329)[0m     lines = getlines(filename, module_globals)
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/linecache.py", line 47, in getlines
[2m[36m(pid=30329)[0m     return updatecache(filename, module_globals)
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/linecache.py", line 136, in updatecache
[2m[36m(pid=30329)[0m     with tokenize.open(fullname) as fp:
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/tokenize.py", line 392, in open
[2m[36m(pid=30329)[0m     buffer = _builtin_open(filename, 'rb')
[2m[36m(pid=30329)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/worker.py", line 369, in sigterm_handler
[2m[36m(pid=30329)[0m     sys.exit(1)
[2m[36m(pid=30329)[0m SystemExit: 1
[2m[36m(pid=30321)[0m 2020-10-11 14:57:41,812	ERROR worker.py:372 -- SystemExit was raised from the worker
[2m[36m(pid=30321)[0m Traceback (most recent call last):
[2m[36m(pid=30321)[0m   File "python/ray/_raylet.pyx", line 553, in ray._raylet.task_execution_handler
[2m[36m(pid=30321)[0m   File "python/ray/_raylet.pyx", line 440, in ray._raylet.execute_task
[2m[36m(pid=30321)[0m   File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
[2m[36m(pid=30321)[0m   File "python/ray/_raylet.pyx", line 483, in ray._raylet.execute_task
[2m[36m(pid=30321)[0m   File "python/ray/_raylet.pyx", line 484, in ray._raylet.execute_task
[2m[36m(pid=30321)[0m   File "python/ray/_raylet.pyx", line 438, in ray._raylet.execute_task.function_executor
[2m[36m(pid=30321)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/function_manager.py", line 553, in actor_method_executor
[2m[36m(pid=30321)[0m     return method(actor, *args, **kwargs)
[2m[36m(pid=30321)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 929, in __ray_terminate__
[2m[36m(pid=30321)[0m     ray.actor.exit_actor()
[2m[36m(pid=30321)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 989, in exit_actor
[2m[36m(pid=30321)[0m     ray.disconnect()
[2m[36m(pid=30321)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/worker.py", line 1324, in disconnect
[2m[36m(pid=30321)[0m     ray_actor.ActorClassMethodMetadata.reset_cache()
[2m[36m(pid=30321)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 185, in reset_cache
[2m[36m(pid=30321)[0m     cls._cache.clear()
[2m[36m(pid=30321)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/worker.py", line 369, in sigterm_handler
[2m[36m(pid=30321)[0m     sys.exit(1)
[2m[36m(pid=30321)[0m SystemExit: 1
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 2895, in get_loc
    return self._engine.get_loc(casted_key)
  File "pandas/_libs/index.pyx", line 70, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 101, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'custom_metrics/time_step_min'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "train.py", line 72, in <module>
    train_func()
  File "train.py", line 57, in train_func
    result = analysis.dataframe(metric='custom_metrics/time_step_min', mode='min').to_dict('index')[0]
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py", line 92, in dataframe
    rows = self._retrieve_rows(metric=metric, mode=mode)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py", line 254, in _retrieve_rows
    idx = df[metric].idxmin()
  File "/root/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py", line 2902, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/root/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 2897, in get_loc
    raise KeyError(key) from err
KeyError: 'custom_metrics/time_step_min'
