2020-10-12 17:01:01,788	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_82a3d_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=40435)[0m 2020-10-12 17:01:04,582	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=40422)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40422)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40306)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40306)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40410)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40410)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40313)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40313)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40326)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40326)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40379)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40379)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40398)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40398)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40385)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40385)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40383)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40383)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40394)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40394)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40391)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40391)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40318)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40318)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40384)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40384)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40324)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40324)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40336)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40336)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40395)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40395)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40416)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40416)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40407)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40407)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40418)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40418)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40396)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40396)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40413)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40413)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40320)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40320)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40387)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40387)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40321)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40321)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40421)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40421)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40339)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40339)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40388)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40388)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40380)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40380)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40441)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40441)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40401)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40401)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40429)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40429)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40397)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40397)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40338)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40338)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40316)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40316)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40400)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40400)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40386)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40386)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40317)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40317)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40382)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40382)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40304)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40304)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40309)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40309)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40310)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40310)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40423)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40423)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40403)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40403)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40311)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40311)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40406)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40406)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40322)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40322)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40323)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40323)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40424)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40424)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40392)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40392)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40389)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40389)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40409)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40409)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40330)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40330)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40371)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40371)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40431)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40431)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40327)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40327)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40414)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40414)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40370)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40370)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40372)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40372)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40378)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40378)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40346)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40346)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40417)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40417)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40307)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40307)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40444)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40444)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40377)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40377)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40332)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40332)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40308)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40308)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40381)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40381)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40390)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40390)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40343)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40343)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40368)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40368)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40333)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40333)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40404)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40404)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40341)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40341)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40312)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40312)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40425)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40425)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40399)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40399)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40305)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40305)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40393)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40393)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=40319)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40319)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 3924
    time_step_mean: 3604.0258620689656
    time_step_min: 3359
  date: 2020-10-12_17-01-38
  done: false
  episode_len_mean: 901.8924050632911
  episode_reward_max: 263.14141414141426
  episode_reward_mean: 220.7612837233088
  episode_reward_min: 158.89898989898953
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1581422984600067
        entropy_coeff: 0.009999999999999998
        kl: 0.0076019543533523875
        model: {}
        policy_loss: -0.010653694461022193
        total_loss: 406.1452102661133
        vf_explained_var: 0.5482549071311951
        vf_loss: 406.1659342447917
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.723529411764705
    gpu_util_percent0: 0.2776470588235294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5676470588235296
    vram_util_percent0: 0.08659058900700328
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17176392270518365
    mean_env_wait_ms: 1.1617956606095572
    mean_inference_ms: 5.826894110019798
    mean_raw_obs_processing_ms: 0.4587531995169724
  time_since_restore: 28.983879327774048
  time_this_iter_s: 28.983879327774048
  time_total_s: 28.983879327774048
  timers:
    learn_throughput: 8204.547
    learn_time_ms: 19719.796
    sample_throughput: 17610.85
    sample_time_ms: 9187.064
    update_time_ms: 45.747
  timestamp: 1602522098
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 27.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      1 |          28.9839 | 161792 |  220.761 |              263.141 |              158.899 |            901.892 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 3924
    time_step_mean: 3600.78102189781
    time_step_min: 3307
  date: 2020-10-12_17-02-06
  done: false
  episode_len_mean: 901.1867088607595
  episode_reward_max: 265.56565656565635
  episode_reward_mean: 221.66701828410663
  episode_reward_min: 158.89898989898953
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1338126262029011
        entropy_coeff: 0.009999999999999998
        kl: 0.008458657966305813
        model: {}
        policy_loss: -0.011326148536075683
        total_loss: 90.80210177103679
        vf_explained_var: 0.8205927014350891
        vf_loss: 90.82307243347168
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.675
    gpu_util_percent0: 0.3053125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7625
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16739331529615745
    mean_env_wait_ms: 1.1599762495574477
    mean_inference_ms: 5.610784491778422
    mean_raw_obs_processing_ms: 0.4487238576838736
  time_since_restore: 56.22317028045654
  time_this_iter_s: 27.239290952682495
  time_total_s: 56.22317028045654
  timers:
    learn_throughput: 8288.148
    learn_time_ms: 19520.888
    sample_throughput: 19022.745
    sample_time_ms: 8505.187
    update_time_ms: 47.459
  timestamp: 1602522126
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      2 |          56.2232 | 323584 |  221.667 |              265.566 |              158.899 |            901.187 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3592.347222222222
    time_step_min: 3236
  date: 2020-10-12_17-02-33
  done: false
  episode_len_mean: 896.3502109704641
  episode_reward_max: 278.1414141414142
  episode_reward_mean: 223.37316199974404
  episode_reward_min: 155.8686868686863
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.121572216351827
        entropy_coeff: 0.009999999999999998
        kl: 0.010371573579808077
        model: {}
        policy_loss: -0.013688113501605889
        total_loss: 46.78648567199707
        vf_explained_var: 0.8903157114982605
        vf_loss: 46.809316317240395
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.074193548387093
    gpu_util_percent0: 0.32645161290322583
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16443092092721456
    mean_env_wait_ms: 1.1596537221988203
    mean_inference_ms: 5.435622566869007
    mean_raw_obs_processing_ms: 0.4406076950341285
  time_since_restore: 83.20120167732239
  time_this_iter_s: 26.978031396865845
  time_total_s: 83.20120167732239
  timers:
    learn_throughput: 8263.097
    learn_time_ms: 19580.068
    sample_throughput: 20056.016
    sample_time_ms: 8067.006
    update_time_ms: 45.101
  timestamp: 1602522153
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      3 |          83.2012 | 485376 |  223.373 |              278.141 |              155.869 |             896.35 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3577.327118644068
    time_step_min: 3236
  date: 2020-10-12_17-02-59
  done: false
  episode_len_mean: 892.5411392405064
  episode_reward_max: 278.1414141414142
  episode_reward_mean: 225.44147167881326
  episode_reward_min: 155.8686868686863
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1133481164773305
        entropy_coeff: 0.009999999999999998
        kl: 0.00893251815189918
        model: {}
        policy_loss: -0.011717147989353785
        total_loss: 31.85045576095581
        vf_explained_var: 0.919109582901001
        vf_loss: 31.871520678202312
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.332258064516132
    gpu_util_percent0: 0.3190322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16227386963935372
    mean_env_wait_ms: 1.1600463149921751
    mean_inference_ms: 5.302495847251851
    mean_raw_obs_processing_ms: 0.434185583035826
  time_since_restore: 109.83361983299255
  time_this_iter_s: 26.632418155670166
  time_total_s: 109.83361983299255
  timers:
    learn_throughput: 8280.238
    learn_time_ms: 19539.535
    sample_throughput: 20654.734
    sample_time_ms: 7833.168
    update_time_ms: 43.048
  timestamp: 1602522179
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      4 |          109.834 | 647168 |  225.441 |              278.141 |              155.869 |            892.541 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3570.4665775401068
    time_step_min: 3236
  date: 2020-10-12_17-03-26
  done: false
  episode_len_mean: 888.659493670886
  episode_reward_max: 278.1414141414142
  episode_reward_mean: 225.95710267229234
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.085977057615916
        entropy_coeff: 0.009999999999999998
        kl: 0.009866746452947458
        model: {}
        policy_loss: -0.01207711334185054
        total_loss: 31.57342306772868
        vf_explained_var: 0.9264079928398132
        vf_loss: 31.5943865776062
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.219354838709677
    gpu_util_percent0: 0.3541935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16061461709932243
    mean_env_wait_ms: 1.1610142082590982
    mean_inference_ms: 5.197858593682224
    mean_raw_obs_processing_ms: 0.428790685668846
  time_since_restore: 136.48200368881226
  time_this_iter_s: 26.648383855819702
  time_total_s: 136.48200368881226
  timers:
    learn_throughput: 8280.731
    learn_time_ms: 19538.372
    sample_throughput: 21079.197
    sample_time_ms: 7675.435
    update_time_ms: 38.717
  timestamp: 1602522206
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      5 |          136.482 | 808960 |  225.957 |              278.141 |              124.657 |            888.659 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3547.471163245357
    time_step_min: 3230
  date: 2020-10-12_17-03-53
  done: false
  episode_len_mean: 875.793427230047
  episode_reward_max: 278.8989898989898
  episode_reward_mean: 229.61673068715305
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 275
  episodes_total: 1065
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.0417975684007008
        entropy_coeff: 0.009999999999999998
        kl: 0.007881337155898413
        model: {}
        policy_loss: -0.012281686281009266
        total_loss: 30.140108744303387
        vf_explained_var: 0.9486353397369385
        vf_loss: 30.161231835683186
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.061290322580653
    gpu_util_percent0: 0.3109677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15857529085859254
    mean_env_wait_ms: 1.1650066300055815
    mean_inference_ms: 5.067082642732204
    mean_raw_obs_processing_ms: 0.42205708891230437
  time_since_restore: 163.22340726852417
  time_this_iter_s: 26.741403579711914
  time_total_s: 163.22340726852417
  timers:
    learn_throughput: 8275.039
    learn_time_ms: 19551.811
    sample_throughput: 21378.07
    sample_time_ms: 7568.13
    update_time_ms: 39.059
  timestamp: 1602522233
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      6 |          163.223 | 970752 |  229.617 |              278.899 |              124.657 |            875.793 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3532.3052373158757
    time_step_min: 3230
  date: 2020-10-12_17-04-19
  done: false
  episode_len_mean: 867.4556962025316
  episode_reward_max: 283.7474747474747
  episode_reward_mean: 231.66857658867139
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 199
  episodes_total: 1264
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.0534568627675374
        entropy_coeff: 0.009999999999999998
        kl: 0.010327032844846448
        model: {}
        policy_loss: -0.014832363250510147
        total_loss: 16.06687879562378
        vf_explained_var: 0.958442747592926
        vf_loss: 16.090180317560833
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.38666666666667
    gpu_util_percent0: 0.37099999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15746399370995268
    mean_env_wait_ms: 1.1672145729804322
    mean_inference_ms: 4.997831464757293
    mean_raw_obs_processing_ms: 0.41844478048281
  time_since_restore: 189.65867018699646
  time_this_iter_s: 26.43526291847229
  time_total_s: 189.65867018699646
  timers:
    learn_throughput: 8280.188
    learn_time_ms: 19539.652
    sample_throughput: 21659.686
    sample_time_ms: 7469.73
    update_time_ms: 38.972
  timestamp: 1602522259
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      7 |          189.659 | 1132544 |  231.669 |              283.747 |              124.657 |            867.456 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3522.3746376811596
    time_step_min: 3211
  date: 2020-10-12_17-04-46
  done: false
  episode_len_mean: 861.8551336146273
  episode_reward_max: 283.7474747474747
  episode_reward_mean: 232.9539913907001
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.035190373659134
        entropy_coeff: 0.009999999999999998
        kl: 0.009323710032428304
        model: {}
        policy_loss: -0.015143743366934359
        total_loss: 15.048800230026245
        vf_explained_var: 0.9601005911827087
        vf_loss: 15.072431246439615
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.993548387096777
    gpu_util_percent0: 0.36032258064516137
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15674108831699976
    mean_env_wait_ms: 1.1692004285575839
    mean_inference_ms: 4.951569502500211
    mean_raw_obs_processing_ms: 0.41597129097993163
  time_since_restore: 216.3138506412506
  time_this_iter_s: 26.65518045425415
  time_total_s: 216.3138506412506
  timers:
    learn_throughput: 8281.681
    learn_time_ms: 19536.13
    sample_throughput: 21811.018
    sample_time_ms: 7417.902
    update_time_ms: 38.787
  timestamp: 1602522286
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      8 |          216.314 | 1294336 |  232.954 |              283.747 |              124.657 |            861.855 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3513.379063719116
    time_step_min: 3199
  date: 2020-10-12_17-05-13
  done: false
  episode_len_mean: 857.1151898734178
  episode_reward_max: 283.7474747474747
  episode_reward_mean: 234.10833013681105
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.0016641368468602
        entropy_coeff: 0.009999999999999998
        kl: 0.010285263881087303
        model: {}
        policy_loss: -0.014098684332566336
        total_loss: 16.910085678100586
        vf_explained_var: 0.9561101794242859
        vf_loss: 16.932144165039062
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793548387096777
    gpu_util_percent0: 0.33709677419354844
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15610348731092083
    mean_env_wait_ms: 1.1711791368402964
    mean_inference_ms: 4.910852053563649
    mean_raw_obs_processing_ms: 0.41372926075022265
  time_since_restore: 242.759117603302
  time_this_iter_s: 26.44526696205139
  time_total_s: 242.759117603302
  timers:
    learn_throughput: 8285.375
    learn_time_ms: 19527.421
    sample_throughput: 21977.49
    sample_time_ms: 7361.714
    update_time_ms: 36.882
  timestamp: 1602522313
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |      9 |          242.759 | 1456128 |  234.108 |              283.747 |              124.657 |            857.115 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3502.4657534246576
    time_step_min: 3199
  date: 2020-10-12_17-05-39
  done: false
  episode_len_mean: 849.9812533476165
  episode_reward_max: 283.7474747474747
  episode_reward_mean: 235.65752327776954
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 287
  episodes_total: 1867
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.9623135576645533
        entropy_coeff: 0.009999999999999998
        kl: 0.009416531460980574
        model: {}
        policy_loss: -0.012676293088588864
        total_loss: 22.148826281229656
        vf_explained_var: 0.965167224407196
        vf_loss: 22.16924238204956
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.99000000000001
    gpu_util_percent0: 0.29766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.155159756736953
    mean_env_wait_ms: 1.174767275120298
    mean_inference_ms: 4.849775968895633
    mean_raw_obs_processing_ms: 0.4104237767023693
  time_since_restore: 269.0789976119995
  time_this_iter_s: 26.31988000869751
  time_total_s: 269.0789976119995
  timers:
    learn_throughput: 8292.733
    learn_time_ms: 19510.093
    sample_throughput: 22118.844
    sample_time_ms: 7314.668
    update_time_ms: 35.199
  timestamp: 1602522339
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     10 |          269.079 | 1617920 |  235.658 |              283.747 |              124.657 |            849.981 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3496.023856858847
    time_step_min: 3199
  date: 2020-10-12_17-06-06
  done: false
  episode_len_mean: 845.3286270691334
  episode_reward_max: 283.7474747474747
  episode_reward_mean: 236.7606640897779
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 187
  episodes_total: 2054
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.961670051018397
        entropy_coeff: 0.009999999999999998
        kl: 0.008361596070850888
        model: {}
        policy_loss: -0.014380326863223067
        total_loss: 12.288408199946085
        vf_explained_var: 0.9701172709465027
        vf_loss: 12.3107328414917
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98709677419355
    gpu_util_percent0: 0.3335483870967741
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15463628417249994
    mean_env_wait_ms: 1.1768290437372744
    mean_inference_ms: 4.817020192319125
    mean_raw_obs_processing_ms: 0.40864625545425626
  time_since_restore: 295.5709900856018
  time_this_iter_s: 26.491992473602295
  time_total_s: 295.5709900856018
  timers:
    learn_throughput: 8301.815
    learn_time_ms: 19488.749
    sample_throughput: 22838.809
    sample_time_ms: 7084.082
    update_time_ms: 35.363
  timestamp: 1602522366
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     11 |          295.571 | 1779712 |  236.761 |              283.747 |              124.657 |            845.329 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3487.8294930875577
    time_step_min: 3162
  date: 2020-10-12_17-06-32
  done: false
  episode_len_mean: 841.2762206148282
  episode_reward_max: 287.5353535353536
  episode_reward_mean: 238.0014064697608
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.9562084426482519
        entropy_coeff: 0.009999999999999998
        kl: 0.007446964814638098
        model: {}
        policy_loss: -0.012289907109031143
        total_loss: 11.696509520212809
        vf_explained_var: 0.9673168659210205
        vf_loss: 11.716871738433838
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.280645161290323
    gpu_util_percent0: 0.29709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15424591759218423
    mean_env_wait_ms: 1.1785573940852143
    mean_inference_ms: 4.79217557522272
    mean_raw_obs_processing_ms: 0.40727245764226133
  time_since_restore: 322.1236867904663
  time_this_iter_s: 26.552696704864502
  time_total_s: 322.1236867904663
  timers:
    learn_throughput: 8299.049
    learn_time_ms: 19495.246
    sample_throughput: 23077.938
    sample_time_ms: 7010.678
    update_time_ms: 32.478
  timestamp: 1602522392
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     12 |          322.124 | 1941504 |  238.001 |              287.535 |              124.657 |            841.276 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3478.9396878954026
    time_step_min: 3162
  date: 2020-10-12_17-06-59
  done: false
  episode_len_mean: 836.5333609614588
  episode_reward_max: 287.5353535353536
  episode_reward_mean: 239.25350898123372
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 201
  episodes_total: 2413
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8854124893744787
        entropy_coeff: 0.009999999999999998
        kl: 0.008535464915136496
        model: {}
        policy_loss: -0.01035699452040717
        total_loss: 14.242549816767374
        vf_explained_var: 0.9706783294677734
        vf_loss: 14.260053793589273
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.258064516129036
    gpu_util_percent0: 0.33806451612903227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1537996367490063
    mean_env_wait_ms: 1.180947307942228
    mean_inference_ms: 4.763552353564393
    mean_raw_obs_processing_ms: 0.4056996000338859
  time_since_restore: 348.8112530708313
  time_this_iter_s: 26.68756628036499
  time_total_s: 348.8112530708313
  timers:
    learn_throughput: 8301.847
    learn_time_ms: 19488.676
    sample_throughput: 23152.079
    sample_time_ms: 6988.228
    update_time_ms: 31.639
  timestamp: 1602522419
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     13 |          348.811 | 2103296 |  239.254 |              287.535 |              124.657 |            836.533 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3468.8339636913765
    time_step_min: 3132
  date: 2020-10-12_17-07-26
  done: false
  episode_len_mean: 830.9024571854058
  episode_reward_max: 292.0808080808083
  episode_reward_mean: 240.7883037372985
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 273
  episodes_total: 2686
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8907966166734695
        entropy_coeff: 0.009999999999999998
        kl: 0.007651247937853138
        model: {}
        policy_loss: -0.012370601044191668
        total_loss: 12.828261613845825
        vf_explained_var: 0.9744144082069397
        vf_loss: 12.848010301589966
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.806451612903235
    gpu_util_percent0: 0.2629032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15327741321139765
    mean_env_wait_ms: 1.1837358587113818
    mean_inference_ms: 4.730459449181103
    mean_raw_obs_processing_ms: 0.40387269403169773
  time_since_restore: 375.69336915016174
  time_this_iter_s: 26.882116079330444
  time_total_s: 375.69336915016174
  timers:
    learn_throughput: 8285.221
    learn_time_ms: 19527.784
    sample_throughput: 23204.8
    sample_time_ms: 6972.35
    update_time_ms: 31.758
  timestamp: 1602522446
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     14 |          375.693 | 2265088 |  240.788 |              292.081 |              124.657 |            830.902 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3462.450749464668
    time_step_min: 3132
  date: 2020-10-12_17-07-53
  done: false
  episode_len_mean: 828.0657524613221
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 241.79892809956095
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8834512283404669
        entropy_coeff: 0.009999999999999998
        kl: 0.008237777239022156
        model: {}
        policy_loss: -0.014040695173510661
        total_loss: 9.925102869669596
        vf_explained_var: 0.9722784161567688
        vf_loss: 9.946330626805624
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.490322580645163
    gpu_util_percent0: 0.3022580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1530111552520517
    mean_env_wait_ms: 1.1852965078954538
    mean_inference_ms: 4.71340247819235
    mean_raw_obs_processing_ms: 0.4029297772624155
  time_since_restore: 402.21317172050476
  time_this_iter_s: 26.519802570343018
  time_total_s: 402.21317172050476
  timers:
    learn_throughput: 8287.643
    learn_time_ms: 19522.076
    sample_throughput: 23236.519
    sample_time_ms: 6962.833
    update_time_ms: 33.235
  timestamp: 1602522473
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     15 |          402.213 | 2426880 |  241.799 |              297.535 |              124.657 |            828.066 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3456.4131897711977
    time_step_min: 3132
  date: 2020-10-12_17-08-20
  done: false
  episode_len_mean: 825.4077637690776
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 242.7798690287077
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 170
  episodes_total: 3014
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8499494989713033
        entropy_coeff: 0.009999999999999998
        kl: 0.007955161039717495
        model: {}
        policy_loss: -0.014217967604054138
        total_loss: 11.089516639709473
        vf_explained_var: 0.9735724329948425
        vf_loss: 11.11064330736796
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.725806451612907
    gpu_util_percent0: 0.3409677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1527459424781258
    mean_env_wait_ms: 1.1869566075763442
    mean_inference_ms: 4.696383775412399
    mean_raw_obs_processing_ms: 0.40198623611416284
  time_since_restore: 428.96040654182434
  time_this_iter_s: 26.74723482131958
  time_total_s: 428.96040654182434
  timers:
    learn_throughput: 8285.617
    learn_time_ms: 19526.849
    sample_throughput: 23259.783
    sample_time_ms: 6955.869
    update_time_ms: 35.113
  timestamp: 1602522500
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     16 |           428.96 | 2588672 |   242.78 |              297.535 |              124.657 |            825.408 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3446.911728772144
    time_step_min: 3107
  date: 2020-10-12_17-08-46
  done: false
  episode_len_mean: 821.3793727382389
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 244.2830780665521
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 302
  episodes_total: 3316
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8400506029526392
        entropy_coeff: 0.009999999999999998
        kl: 0.007774756872095168
        model: {}
        policy_loss: -0.012053211247499954
        total_loss: 12.643061558405558
        vf_explained_var: 0.9773240685462952
        vf_loss: 12.661960204442343
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.861290322580643
    gpu_util_percent0: 0.36064516129032265
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7612903225806447
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1523300534391524
    mean_env_wait_ms: 1.189626488888563
    mean_inference_ms: 4.669542791385696
    mean_raw_obs_processing_ms: 0.40049800028191496
  time_since_restore: 455.6511039733887
  time_this_iter_s: 26.69069743156433
  time_total_s: 455.6511039733887
  timers:
    learn_throughput: 8282.294
    learn_time_ms: 19534.685
    sample_throughput: 23230.298
    sample_time_ms: 6964.698
    update_time_ms: 35.217
  timestamp: 1602522526
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     17 |          455.651 | 2750464 |  244.283 |              297.535 |              124.657 |            821.379 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3442.0576587070473
    time_step_min: 3107
  date: 2020-10-12_17-09-13
  done: false
  episode_len_mean: 819.0963751438435
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 245.07157594355516
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 160
  episodes_total: 3476
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8448764433463415
        entropy_coeff: 0.009999999999999998
        kl: 0.007135747543846567
        model: {}
        policy_loss: -0.012845230948490402
        total_loss: 10.076656262079874
        vf_explained_var: 0.9728644490242004
        vf_loss: 10.096522808074951
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.609677419354842
    gpu_util_percent0: 0.27322580645161293
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15213202610830046
    mean_env_wait_ms: 1.1909113086431038
    mean_inference_ms: 4.656743586716528
    mean_raw_obs_processing_ms: 0.39978431482324767
  time_since_restore: 482.1091413497925
  time_this_iter_s: 26.45803737640381
  time_total_s: 482.1091413497925
  timers:
    learn_throughput: 8288.335
    learn_time_ms: 19520.447
    sample_throughput: 23277.226
    sample_time_ms: 6950.656
    update_time_ms: 42.196
  timestamp: 1602522553
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     18 |          482.109 | 2912256 |  245.072 |              297.535 |              124.657 |            819.096 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3437.445431824493
    time_step_min: 3107
  date: 2020-10-12_17-09-40
  done: false
  episode_len_mean: 817.1323085369202
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 245.76192337872268
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 167
  episodes_total: 3643
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8205798069636027
        entropy_coeff: 0.009999999999999998
        kl: 0.008363151301940283
        model: {}
        policy_loss: -0.014158377957452709
        total_loss: 12.00688886642456
        vf_explained_var: 0.9711735248565674
        vf_loss: 12.027580499649048
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.39
    gpu_util_percent0: 0.30999999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15194235141104867
    mean_env_wait_ms: 1.192249667921443
    mean_inference_ms: 4.644290993320759
    mean_raw_obs_processing_ms: 0.39909192746310146
  time_since_restore: 508.4915156364441
  time_this_iter_s: 26.38237428665161
  time_total_s: 508.4915156364441
  timers:
    learn_throughput: 8290.244
    learn_time_ms: 19515.952
    sample_throughput: 23293.497
    sample_time_ms: 6945.801
    update_time_ms: 44.113
  timestamp: 1602522580
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     19 |          508.492 | 3074048 |  245.762 |              297.535 |              124.657 |            817.132 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3429.9679487179487
    time_step_min: 3107
  date: 2020-10-12_17-10-07
  done: false
  episode_len_mean: 813.775240994419
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 247.02888089417766
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 299
  episodes_total: 3942
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.8010198672612509
        entropy_coeff: 0.009999999999999998
        kl: 0.007202951819635928
        model: {}
        policy_loss: -0.01187936831653739
        total_loss: 12.747746308644613
        vf_explained_var: 0.9776988625526428
        vf_loss: 12.766195138295492
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.525
    gpu_util_percent0: 0.28625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7593750000000004
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15163134423814137
    mean_env_wait_ms: 1.1944922563976494
    mean_inference_ms: 4.624022144776301
    mean_raw_obs_processing_ms: 0.3979824712394773
  time_since_restore: 535.3704466819763
  time_this_iter_s: 26.878931045532227
  time_total_s: 535.3704466819763
  timers:
    learn_throughput: 8283.182
    learn_time_ms: 19532.59
    sample_throughput: 23197.271
    sample_time_ms: 6974.614
    update_time_ms: 46.0
  timestamp: 1602522607
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     20 |           535.37 | 3235840 |  247.029 |              297.535 |              124.657 |            813.775 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3425.4286768322677
    time_step_min: 3107
  date: 2020-10-12_17-10-33
  done: false
  episode_len_mean: 811.9712755598831
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 247.72157554119573
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 166
  episodes_total: 4108
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.811262384057045
        entropy_coeff: 0.009999999999999998
        kl: 0.007837199761221806
        model: {}
        policy_loss: -0.015364427623959879
        total_loss: 8.901207447052002
        vf_explained_var: 0.9762666821479797
        vf_loss: 8.92311724026998
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.387096774193548
    gpu_util_percent0: 0.35032258064516125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.790322580645161
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15146703848946827
    mean_env_wait_ms: 1.1955922243526433
    mean_inference_ms: 4.613644225222723
    mean_raw_obs_processing_ms: 0.3974066397322399
  time_since_restore: 561.9364023208618
  time_this_iter_s: 26.565955638885498
  time_total_s: 561.9364023208618
  timers:
    learn_throughput: 8290.601
    learn_time_ms: 19515.11
    sample_throughput: 23137.241
    sample_time_ms: 6992.709
    update_time_ms: 51.725
  timestamp: 1602522633
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     21 |          561.936 | 3397632 |  247.722 |              297.535 |              124.657 |            811.971 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3420.7549042779483
    time_step_min: 3107
  date: 2020-10-12_17-11-00
  done: false
  episode_len_mean: 810.4079101333957
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 248.34675564443873
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 165
  episodes_total: 4273
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.7913782149553299
        entropy_coeff: 0.009999999999999998
        kl: 0.00783646855658541
        model: {}
        policy_loss: -0.01136058549551914
        total_loss: 10.503544092178345
        vf_explained_var: 0.9734947681427002
        vf_loss: 10.521251201629639
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.90666666666667
    gpu_util_percent0: 0.39
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15131727804586703
    mean_env_wait_ms: 1.1966902438466729
    mean_inference_ms: 4.603992681325404
    mean_raw_obs_processing_ms: 0.39687114197322326
  time_since_restore: 588.4019601345062
  time_this_iter_s: 26.46555781364441
  time_total_s: 588.4019601345062
  timers:
    learn_throughput: 8285.076
    learn_time_ms: 19528.126
    sample_throughput: 23217.1
    sample_time_ms: 6968.657
    update_time_ms: 53.201
  timestamp: 1602522660
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | RUNNING  | 172.17.0.4:40435 |     22 |          588.402 | 3559424 |  248.347 |              297.535 |              124.657 |            810.408 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_82a3d_00000:
  custom_metrics:
    time_step_max: 4031
    time_step_mean: 3414.13746690203
    time_step_min: 3107
  date: 2020-10-12_17-11-26
  done: true
  episode_len_mean: 807.9396589418452
  episode_reward_max: 297.5353535353537
  episode_reward_mean: 249.41829532756506
  episode_reward_min: 124.65656565656568
  episodes_this_iter: 301
  episodes_total: 4574
  experiment_id: 63d8a68b3a13437d8441cb30e97df75a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 0.7712003091971079
        entropy_coeff: 0.009999999999999998
        kl: 0.007626185659319162
        model: {}
        policy_loss: -0.013356432168317648
        total_loss: 13.23843820889791
        vf_explained_var: 0.9768280386924744
        vf_loss: 13.257981618245443
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.967741935483872
    gpu_util_percent0: 0.33290322580645165
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 40435
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1510730150575853
    mean_env_wait_ms: 1.1985855122642064
    mean_inference_ms: 4.587855935112632
    mean_raw_obs_processing_ms: 0.3959818572996283
  time_since_restore: 614.5604195594788
  time_this_iter_s: 26.158459424972534
  time_total_s: 614.5604195594788
  timers:
    learn_throughput: 8302.466
    learn_time_ms: 19487.222
    sample_throughput: 23256.676
    sample_time_ms: 6956.798
    update_time_ms: 52.248
  timestamp: 1602522686
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 82a3d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | TERMINATED |       |     23 |           614.56 | 3721216 |  249.418 |              297.535 |              124.657 |             807.94 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_82a3d_00000 | TERMINATED |       |     23 |           614.56 | 3721216 |  249.418 |              297.535 |              124.657 |             807.94 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


[2m[33m(pid=raylet)[0m E1012 17:11:26.952373 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.952694 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.952715 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.952975 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.952999 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.953019 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.953255 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.956120 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.956162 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.963340 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.963385 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.963425 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.963449 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.967504 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.967552 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.967581 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.967970 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.968019 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.968056 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.968261 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.968487 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.968734 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.968772 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.968834 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970559 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970629 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970652 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970674 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970700 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970717 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970736 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970765 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970788 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.970840 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1012 17:11:26.971017 40263 40263 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
