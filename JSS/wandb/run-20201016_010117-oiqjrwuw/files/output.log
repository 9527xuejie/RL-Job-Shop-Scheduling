2020-10-16 01:01:21,407	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_1bbc1_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=80043)[0m 2020-10-16 01:01:24,220	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=80035)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80035)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80003)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80003)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80038)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80038)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80044)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80044)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80005)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80005)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80028)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80028)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80050)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80050)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80022)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80022)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80046)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80046)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80004)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80004)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80021)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80021)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80039)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80039)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80052)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80052)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79985)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79985)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80045)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80045)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79993)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79993)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80002)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80002)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80015)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80015)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79953)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79953)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79919)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79919)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79947)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79947)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79938)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79938)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80051)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80051)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79999)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79999)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79990)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79990)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79939)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79939)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79936)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79936)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80016)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80016)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79931)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79931)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79952)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79952)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79926)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79926)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79923)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79923)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79942)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79942)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79978)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79978)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79940)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79940)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79994)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79994)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80001)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80001)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79995)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79995)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79921)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79921)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79980)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79980)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79975)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79975)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79951)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79951)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79988)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79988)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79986)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79986)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80042)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80042)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79934)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79934)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79933)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79933)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79996)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79996)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79998)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79998)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79984)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79984)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79987)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79987)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79932)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79932)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79920)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79920)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80025)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80025)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79925)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79925)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79930)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79930)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80056)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80056)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80009)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80009)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80007)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80007)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80026)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80026)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79928)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79928)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80048)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80048)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79983)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79983)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80057)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80057)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79941)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79941)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79997)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79997)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80012)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80012)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80000)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80000)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79918)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79918)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79935)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79935)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79922)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79922)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80029)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80029)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79981)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79981)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79927)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79927)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79937)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79937)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79943)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79943)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=80054)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=80054)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79944)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79944)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79950)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79950)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3691
    time_step_mean: 3418.869230769231
    time_step_min: 3176
  date: 2020-10-16_01-01-57
  done: false
  episode_len_mean: 886.753164556962
  episode_reward_max: 282.343434343434
  episode_reward_mean: 241.94642628819824
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1849032640457153
        entropy_coeff: 0.0005000000000000001
        kl: 0.004784370268074174
        model: {}
        policy_loss: -0.008921080657576871
        total_loss: 488.36802927652997
        vf_explained_var: 0.49538537859916687
        vf_loss: 488.3765920003255
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.584375
    gpu_util_percent0: 0.30125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5625
    vram_util_percent0: 0.08698036241390619
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16906201232175244
    mean_env_wait_ms: 1.1542814990258872
    mean_inference_ms: 5.557516073878808
    mean_raw_obs_processing_ms: 0.44346035612553264
  time_since_restore: 27.806232690811157
  time_this_iter_s: 27.806232690811157
  time_total_s: 27.806232690811157
  timers:
    learn_throughput: 8544.692
    learn_time_ms: 18934.796
    sample_throughput: 18374.545
    sample_time_ms: 8805.225
    update_time_ms: 26.969
  timestamp: 1602810117
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      1 |          27.8062 | 161792 |  241.946 |              282.343 |              165.677 |            886.753 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3759
    time_step_mean: 3440.1076388888887
    time_step_min: 3176
  date: 2020-10-16_01-02-23
  done: false
  episode_len_mean: 883.6139240506329
  episode_reward_max: 282.343434343434
  episode_reward_mean: 240.93616545198802
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1520658334096272
        entropy_coeff: 0.0005000000000000001
        kl: 0.00744400251035889
        model: {}
        policy_loss: -0.009384491364471614
        total_loss: 127.12748146057129
        vf_explained_var: 0.7977762818336487
        vf_loss: 127.13669649759929
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.958064516129035
    gpu_util_percent0: 0.3048387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.748387096774193
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16554400766929458
    mean_env_wait_ms: 1.1521032568863236
    mean_inference_ms: 5.394188769646329
    mean_raw_obs_processing_ms: 0.43645003936433957
  time_since_restore: 54.269917488098145
  time_this_iter_s: 26.463684797286987
  time_total_s: 54.269917488098145
  timers:
    learn_throughput: 8628.309
    learn_time_ms: 18751.299
    sample_throughput: 19454.102
    sample_time_ms: 8316.601
    update_time_ms: 23.108
  timestamp: 1602810143
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      2 |          54.2699 | 323584 |  240.936 |              282.343 |              165.677 |            883.614 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3441.580717488789
    time_step_min: 3176
  date: 2020-10-16_01-02-49
  done: false
  episode_len_mean: 879.1814345991561
  episode_reward_max: 282.343434343434
  episode_reward_mean: 241.48644674594024
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1369050244490306
        entropy_coeff: 0.0005000000000000001
        kl: 0.011053523048758507
        model: {}
        policy_loss: -0.012346668188304951
        total_loss: 52.218987782796226
        vf_explained_var: 0.8901453614234924
        vf_loss: 52.23079872131348
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.466666666666665
    gpu_util_percent0: 0.34166666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16310863693008612
    mean_env_wait_ms: 1.151988655917845
    mean_inference_ms: 5.258828278896161
    mean_raw_obs_processing_ms: 0.4305553493562292
  time_since_restore: 80.2223608493805
  time_this_iter_s: 25.95244336128235
  time_total_s: 80.2223608493805
  timers:
    learn_throughput: 8652.586
    learn_time_ms: 18698.687
    sample_throughput: 20289.941
    sample_time_ms: 7974.001
    update_time_ms: 21.771
  timestamp: 1602810169
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      3 |          80.2224 | 485376 |  241.486 |              282.343 |              165.677 |            879.181 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3438.958609271523
    time_step_min: 3176
  date: 2020-10-16_01-03-15
  done: false
  episode_len_mean: 875.8180379746835
  episode_reward_max: 286.13131313131294
  episode_reward_mean: 242.47984592763052
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1127794881661732
        entropy_coeff: 0.0005000000000000001
        kl: 0.009236348637690147
        model: {}
        policy_loss: -0.01357063123335441
        total_loss: 39.06841214497884
        vf_explained_var: 0.9182509779930115
        vf_loss: 39.08161576588949
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.03666666666667
    gpu_util_percent0: 0.37333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16121986165444532
    mean_env_wait_ms: 1.1522263607323069
    mean_inference_ms: 5.154201177608311
    mean_raw_obs_processing_ms: 0.4253873754433028
  time_since_restore: 105.95109915733337
  time_this_iter_s: 25.72873830795288
  time_total_s: 105.95109915733337
  timers:
    learn_throughput: 8676.116
    learn_time_ms: 18647.975
    sample_throughput: 20816.852
    sample_time_ms: 7772.165
    update_time_ms: 21.096
  timestamp: 1602810195
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      4 |          105.951 | 647168 |   242.48 |              286.131 |              165.677 |            875.818 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3433.030183727034
    time_step_min: 3154
  date: 2020-10-16_01-03-41
  done: false
  episode_len_mean: 871.8582278481013
  episode_reward_max: 286.13131313131294
  episode_reward_mean: 243.81274773046908
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.075794368982315
        entropy_coeff: 0.0005000000000000001
        kl: 0.007777562830597162
        model: {}
        policy_loss: -0.011302593076834455
        total_loss: 29.236744085947674
        vf_explained_var: 0.947382390499115
        vf_loss: 29.247807184855144
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.789655172413788
    gpu_util_percent0: 0.39586206896551723
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15975602638099218
    mean_env_wait_ms: 1.153325527633902
    mean_inference_ms: 5.071650967857683
    mean_raw_obs_processing_ms: 0.421080744983264
  time_since_restore: 131.91459155082703
  time_this_iter_s: 25.963492393493652
  time_total_s: 131.91459155082703
  timers:
    learn_throughput: 8664.516
    learn_time_ms: 18672.942
    sample_throughput: 21184.571
    sample_time_ms: 7637.256
    update_time_ms: 25.123
  timestamp: 1602810221
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      5 |          131.915 | 808960 |  243.813 |              286.131 |              165.677 |            871.858 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3423.0584415584417
    time_step_min: 3154
  date: 2020-10-16_01-04-07
  done: false
  episode_len_mean: 861.6636528028934
  episode_reward_max: 287.7979797979799
  episode_reward_mean: 245.7200577200576
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 316
  episodes_total: 1106
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0736831625302632
        entropy_coeff: 0.0005000000000000001
        kl: 0.00783942472965767
        model: {}
        policy_loss: -0.010518070853625735
        total_loss: 27.994863510131836
        vf_explained_var: 0.9606881737709045
        vf_loss: 28.00513442357381
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793333333333337
    gpu_util_percent0: 0.337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15772643982212423
    mean_env_wait_ms: 1.1565805741601467
    mean_inference_ms: 4.956414427212716
    mean_raw_obs_processing_ms: 0.41528451401101557
  time_since_restore: 157.7323703765869
  time_this_iter_s: 25.817778825759888
  time_total_s: 157.7323703765869
  timers:
    learn_throughput: 8656.901
    learn_time_ms: 18689.366
    sample_throughput: 21495.69
    sample_time_ms: 7526.718
    update_time_ms: 24.606
  timestamp: 1602810247
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      6 |          157.732 | 970752 |   245.72 |              287.798 |              165.677 |            861.664 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3417.413430420712
    time_step_min: 3154
  date: 2020-10-16_01-04-33
  done: false
  episode_len_mean: 856.7879746835443
  episode_reward_max: 290.6767676767679
  episode_reward_mean: 246.58477975962143
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.053666094938914
        entropy_coeff: 0.0005000000000000001
        kl: 0.00802672584541142
        model: {}
        policy_loss: -0.011159917781090675
        total_loss: 19.830265998840332
        vf_explained_var: 0.9614474773406982
        vf_loss: 19.841150124867756
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.943333333333335
    gpu_util_percent0: 0.3433333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.156995113182648
    mean_env_wait_ms: 1.157977578965271
    mean_inference_ms: 4.914344095364427
    mean_raw_obs_processing_ms: 0.41310399808578907
  time_since_restore: 183.51299047470093
  time_this_iter_s: 25.780620098114014
  time_total_s: 183.51299047470093
  timers:
    learn_throughput: 8658.38
    learn_time_ms: 18686.175
    sample_throughput: 21730.519
    sample_time_ms: 7445.381
    update_time_ms: 24.396
  timestamp: 1602810273
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      7 |          183.513 | 1132544 |  246.585 |              290.677 |              165.677 |            856.788 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3414.711621233859
    time_step_min: 3154
  date: 2020-10-16_01-04-59
  done: false
  episode_len_mean: 853.1174402250351
  episode_reward_max: 290.6767676767679
  episode_reward_mean: 246.98339939479166
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0378634532292683
        entropy_coeff: 0.0005000000000000001
        kl: 0.006901592792322238
        model: {}
        policy_loss: -0.010785317203650871
        total_loss: 20.057416280110676
        vf_explained_var: 0.9610694050788879
        vf_loss: 20.06803019841512
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.437931034482766
    gpu_util_percent0: 0.34482758620689646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15635163987343298
    mean_env_wait_ms: 1.1593560658474091
    mean_inference_ms: 4.877330799271783
    mean_raw_obs_processing_ms: 0.4111066767068855
  time_since_restore: 209.18725109100342
  time_this_iter_s: 25.67426061630249
  time_total_s: 209.18725109100342
  timers:
    learn_throughput: 8656.517
    learn_time_ms: 18690.196
    sample_throughput: 21938.179
    sample_time_ms: 7374.906
    update_time_ms: 23.617
  timestamp: 1602810299
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      8 |          209.187 | 1294336 |  246.983 |              290.677 |              165.677 |            853.117 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3409.408622908623
    time_step_min: 3154
  date: 2020-10-16_01-05-25
  done: false
  episode_len_mean: 849.3470290771176
  episode_reward_max: 292.79797979797934
  episode_reward_mean: 247.87210920839226
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 160
  episodes_total: 1582
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9882466793060303
        entropy_coeff: 0.0005000000000000001
        kl: 0.0072092009941115975
        model: {}
        policy_loss: -0.0129079805725875
        total_loss: 17.17254622777303
        vf_explained_var: 0.9699942469596863
        vf_loss: 17.185227394104004
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.203333333333337
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15577659861079507
    mean_env_wait_ms: 1.1608674832537655
    mean_inference_ms: 4.844262669310352
    mean_raw_obs_processing_ms: 0.4092856624809603
  time_since_restore: 235.0282106399536
  time_this_iter_s: 25.840959548950195
  time_total_s: 235.0282106399536
  timers:
    learn_throughput: 8661.592
    learn_time_ms: 18679.246
    sample_throughput: 22003.493
    sample_time_ms: 7353.015
    update_time_ms: 23.605
  timestamp: 1602810325
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |      9 |          235.028 | 1456128 |  247.872 |              292.798 |              165.677 |            849.347 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3399.7323340471094
    time_step_min: 3102
  date: 2020-10-16_01-05-50
  done: false
  episode_len_mean: 842.037447257384
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 249.20564825469876
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 314
  episodes_total: 1896
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9735767543315887
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065773469395935535
        model: {}
        policy_loss: -0.011116733350415112
        total_loss: 21.422983805338543
        vf_explained_var: 0.9707332253456116
        vf_loss: 21.43393023808797
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.448275862068968
    gpu_util_percent0: 0.3472413793103448
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15487473229779436
    mean_env_wait_ms: 1.163856751569919
    mean_inference_ms: 4.791618094088291
    mean_raw_obs_processing_ms: 0.40646087734682107
  time_since_restore: 260.55055975914
  time_this_iter_s: 25.5223491191864
  time_total_s: 260.55055975914
  timers:
    learn_throughput: 8668.959
    learn_time_ms: 18663.37
    sample_throughput: 22131.162
    sample_time_ms: 7310.597
    update_time_ms: 23.509
  timestamp: 1602810350
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     10 |          260.551 | 1617920 |  249.206 |              295.071 |              165.677 |            842.037 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3394.6860809476802
    time_step_min: 3102
  date: 2020-10-16_01-06-17
  done: false
  episode_len_mean: 838.7770204479066
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 250.0402417554315
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9505527665217718
        entropy_coeff: 0.0005000000000000001
        kl: 0.006363538365500669
        model: {}
        policy_loss: -0.010260378786673149
        total_loss: 13.269665638605753
        vf_explained_var: 0.974217414855957
        vf_loss: 13.279764652252197
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.23666666666667
    gpu_util_percent0: 0.2906666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1544992024362377
    mean_env_wait_ms: 1.165136187208822
    mean_inference_ms: 4.769750402065578
    mean_raw_obs_processing_ms: 0.4052632290336177
  time_since_restore: 286.5472455024719
  time_this_iter_s: 25.99668574333191
  time_total_s: 286.5472455024719
  timers:
    learn_throughput: 8676.155
    learn_time_ms: 18647.891
    sample_throughput: 22652.066
    sample_time_ms: 7142.483
    update_time_ms: 24.455
  timestamp: 1602810377
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     11 |          286.547 | 1779712 |   250.04 |              295.071 |              165.677 |            838.777 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3389.5119047619046
    time_step_min: 3102
  date: 2020-10-16_01-06-43
  done: false
  episode_len_mean: 835.617088607595
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 250.86917547993488
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9334837744633356
        entropy_coeff: 0.0005000000000000001
        kl: 0.006569515137622754
        model: {}
        policy_loss: -0.012398959409135083
        total_loss: 11.898517767588297
        vf_explained_var: 0.9744560122489929
        vf_loss: 11.910726388295492
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.549999999999997
    gpu_util_percent0: 0.27899999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15415440910343406
    mean_env_wait_ms: 1.1664412436529614
    mean_inference_ms: 4.749744824137099
    mean_raw_obs_processing_ms: 0.40412521478412317
  time_since_restore: 312.4649307727814
  time_this_iter_s: 25.91768527030945
  time_total_s: 312.4649307727814
  timers:
    learn_throughput: 8668.186
    learn_time_ms: 18665.036
    sample_throughput: 22890.76
    sample_time_ms: 7068.005
    update_time_ms: 26.53
  timestamp: 1602810403
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     12 |          312.465 | 1941504 |  250.869 |              295.071 |              165.677 |            835.617 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3380.3735772357722
    time_step_min: 3102
  date: 2020-10-16_01-07-08
  done: false
  episode_len_mean: 830.5144694533763
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 252.2050123420701
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 276
  episodes_total: 2488
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8941035866737366
        entropy_coeff: 0.0005000000000000001
        kl: 0.006269749913675089
        model: {}
        policy_loss: -0.00925206916872412
        total_loss: 16.93541383743286
        vf_explained_var: 0.9757897853851318
        vf_loss: 16.944485187530518
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.646666666666672
    gpu_util_percent0: 0.3226666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1536221463269698
    mean_env_wait_ms: 1.1689115728081805
    mean_inference_ms: 4.719285929327549
    mean_raw_obs_processing_ms: 0.40241148803442994
  time_since_restore: 338.29497146606445
  time_this_iter_s: 25.83004069328308
  time_total_s: 338.29497146606445
  timers:
    learn_throughput: 8665.119
    learn_time_ms: 18671.642
    sample_throughput: 22954.635
    sample_time_ms: 7048.337
    update_time_ms: 26.886
  timestamp: 1602810428
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     13 |          338.295 | 2103296 |  252.205 |              295.071 |              165.677 |            830.514 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3372.4428141459744
    time_step_min: 3102
  date: 2020-10-16_01-07-34
  done: false
  episode_len_mean: 827.0316455696203
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 253.20390050918706
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 2686
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8781916697820028
        entropy_coeff: 0.0005000000000000001
        kl: 0.006277749276099105
        model: {}
        policy_loss: -0.011392164584928347
        total_loss: 12.567232608795166
        vf_explained_var: 0.9758232235908508
        vf_loss: 12.578436215718588
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.375862068965517
    gpu_util_percent0: 0.36758620689655175
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1533014756111773
    mean_env_wait_ms: 1.170397160502589
    mean_inference_ms: 4.700137470094737
    mean_raw_obs_processing_ms: 0.4013693556717503
  time_since_restore: 363.85833382606506
  time_this_iter_s: 25.56336236000061
  time_total_s: 363.85833382606506
  timers:
    learn_throughput: 8661.031
    learn_time_ms: 18680.456
    sample_throughput: 23042.139
    sample_time_ms: 7021.57
    update_time_ms: 27.321
  timestamp: 1602810454
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     14 |          363.858 | 2265088 |  253.204 |              295.071 |              165.677 |            827.032 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3367.6711647727275
    time_step_min: 3102
  date: 2020-10-16_01-08-00
  done: false
  episode_len_mean: 824.3741209563995
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 253.88053531091498
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8726825912793478
        entropy_coeff: 0.0005000000000000001
        kl: 0.005345542721139888
        model: {}
        policy_loss: -0.01224327425006777
        total_loss: 12.99477251370748
        vf_explained_var: 0.971616268157959
        vf_loss: 13.006917715072632
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.49333333333334
    gpu_util_percent0: 0.31900000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15306183336896395
    mean_env_wait_ms: 1.1715717646389732
    mean_inference_ms: 4.686136169388646
    mean_raw_obs_processing_ms: 0.4005693092397199
  time_since_restore: 389.5230174064636
  time_this_iter_s: 25.66468358039856
  time_total_s: 389.5230174064636
  timers:
    learn_throughput: 8666.312
    learn_time_ms: 18669.072
    sample_throughput: 23099.948
    sample_time_ms: 7003.998
    update_time_ms: 25.819
  timestamp: 1602810480
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     15 |          389.523 | 2426880 |  253.881 |              295.071 |              165.677 |            824.374 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3361.8309258043337
    time_step_min: 3102
  date: 2020-10-16_01-08-26
  done: false
  episode_len_mean: 820.5810019518542
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 254.76314215676598
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 230
  episodes_total: 3074
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.832097460826238
        entropy_coeff: 0.0005000000000000001
        kl: 0.005490843905135989
        model: {}
        policy_loss: -0.010333318127474437
        total_loss: 13.920937776565552
        vf_explained_var: 0.9784757494926453
        vf_loss: 13.931138038635254
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.273333333333337
    gpu_util_percent0: 0.3146666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1527404340615667
    mean_env_wait_ms: 1.1733852889157699
    mean_inference_ms: 4.667413938865162
    mean_raw_obs_processing_ms: 0.3994756843683066
  time_since_restore: 415.6610863208771
  time_this_iter_s: 26.138068914413452
  time_total_s: 415.6610863208771
  timers:
    learn_throughput: 8653.276
    learn_time_ms: 18697.197
    sample_throughput: 23089.322
    sample_time_ms: 7007.222
    update_time_ms: 25.422
  timestamp: 1602810506
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     16 |          415.661 | 2588672 |  254.763 |              295.071 |              165.677 |            820.581 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3354.8726443768996
    time_step_min: 3076
  date: 2020-10-16_01-08-52
  done: false
  episode_len_mean: 817.1323086196504
  episode_reward_max: 297.49494949494914
  episode_reward_mean: 255.8385116992711
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 244
  episodes_total: 3318
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8182818094889323
        entropy_coeff: 0.0005000000000000001
        kl: 0.005390155672406157
        model: {}
        policy_loss: -0.008973776893981267
        total_loss: 10.671323935190836
        vf_explained_var: 0.9803421497344971
        vf_loss: 10.68016799290975
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.36666666666667
    gpu_util_percent0: 0.3236666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1524407888851298
    mean_env_wait_ms: 1.1750950023101903
    mean_inference_ms: 4.649752213469436
    mean_raw_obs_processing_ms: 0.3984767616234441
  time_since_restore: 441.5433166027069
  time_this_iter_s: 25.882230281829834
  time_total_s: 441.5433166027069
  timers:
    learn_throughput: 8651.695
    learn_time_ms: 18700.613
    sample_throughput: 23048.464
    sample_time_ms: 7019.644
    update_time_ms: 27.118
  timestamp: 1602810532
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     17 |          441.543 | 2750464 |  255.839 |              297.495 |              165.677 |            817.132 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3350.8645591647332
    time_step_min: 3076
  date: 2020-10-16_01-09-18
  done: false
  episode_len_mean: 815.0719217491369
  episode_reward_max: 297.49494949494914
  episode_reward_mean: 256.3523962292661
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8158556421597799
        entropy_coeff: 0.0005000000000000001
        kl: 0.005813289627743264
        model: {}
        policy_loss: -0.01080344397147807
        total_loss: 9.102331479390463
        vf_explained_var: 0.979426383972168
        vf_loss: 9.112961610158285
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.33666666666667
    gpu_util_percent0: 0.30700000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15226072182226252
    mean_env_wait_ms: 1.176149736749547
    mean_inference_ms: 4.639220467862724
    mean_raw_obs_processing_ms: 0.39785899415017056
  time_since_restore: 467.55099177360535
  time_this_iter_s: 26.007675170898438
  time_total_s: 467.55099177360535
  timers:
    learn_throughput: 8648.251
    learn_time_ms: 18708.061
    sample_throughput: 22972.06
    sample_time_ms: 7042.991
    update_time_ms: 29.185
  timestamp: 1602810558
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     18 |          467.551 | 2912256 |  256.352 |              297.495 |              165.677 |            815.072 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3346.2407103825135
    time_step_min: 3076
  date: 2020-10-16_01-09-44
  done: false
  episode_len_mean: 812.5924620390456
  episode_reward_max: 298.2525252525252
  episode_reward_mean: 256.9936101798899
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 212
  episodes_total: 3688
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.783480112751325
        entropy_coeff: 0.0005000000000000001
        kl: 0.005505596792014937
        model: {}
        policy_loss: -0.010511156249170503
        total_loss: 13.502816677093506
        vf_explained_var: 0.978053629398346
        vf_loss: 13.513169129689535
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.613333333333337
    gpu_util_percent0: 0.2906666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520311639360966
    mean_env_wait_ms: 1.1775846437999018
    mean_inference_ms: 4.6258951370625585
    mean_raw_obs_processing_ms: 0.39705600374611527
  time_since_restore: 493.57455229759216
  time_this_iter_s: 26.023560523986816
  time_total_s: 493.57455229759216
  timers:
    learn_throughput: 8636.601
    learn_time_ms: 18733.296
    sample_throughput: 23004.107
    sample_time_ms: 7033.179
    update_time_ms: 30.466
  timestamp: 1602810584
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     19 |          493.575 | 3074048 |  256.994 |              298.253 |              165.677 |            812.592 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3341.0226925038246
    time_step_min: 3076
  date: 2020-10-16_01-10-10
  done: false
  episode_len_mean: 809.9488607594936
  episode_reward_max: 301.4343434343433
  episode_reward_mean: 257.75697481140514
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 262
  episodes_total: 3950
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7689148386319479
        entropy_coeff: 0.0005000000000000001
        kl: 0.005313604371622205
        model: {}
        policy_loss: -0.010639291993963221
        total_loss: 11.459484736124674
        vf_explained_var: 0.9803845286369324
        vf_loss: 11.469976902008057
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.886666666666667
    gpu_util_percent0: 0.317
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15178250520756528
    mean_env_wait_ms: 1.179243336955849
    mean_inference_ms: 4.611241097646665
    mean_raw_obs_processing_ms: 0.39622183426737434
  time_since_restore: 519.4781296253204
  time_this_iter_s: 25.90357732772827
  time_total_s: 519.4781296253204
  timers:
    learn_throughput: 8625.865
    learn_time_ms: 18756.613
    sample_throughput: 22993.402
    sample_time_ms: 7036.453
    update_time_ms: 40.688
  timestamp: 1602810610
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     20 |          519.478 | 3235840 |  257.757 |              301.434 |              165.677 |            809.949 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3338.275
    time_step_min: 3059
  date: 2020-10-16_01-10-36
  done: false
  episode_len_mean: 808.4980525803311
  episode_reward_max: 301.4343434343433
  episode_reward_mean: 258.1885702202157
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 4108
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7616968055566152
        entropy_coeff: 0.0005000000000000001
        kl: 0.0063063996300722165
        model: {}
        policy_loss: -0.012424984869236747
        total_loss: 9.233952124913534
        vf_explained_var: 0.9793080687522888
        vf_loss: 9.246127367019653
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.396551724137932
    gpu_util_percent0: 0.3737931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15164024573302187
    mean_env_wait_ms: 1.1801673105328045
    mean_inference_ms: 4.602988638822371
    mean_raw_obs_processing_ms: 0.395733407359186
  time_since_restore: 545.1086714267731
  time_this_iter_s: 25.630541801452637
  time_total_s: 545.1086714267731
  timers:
    learn_throughput: 8635.604
    learn_time_ms: 18735.458
    sample_throughput: 23042.702
    sample_time_ms: 7021.399
    update_time_ms: 39.358
  timestamp: 1602810636
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     21 |          545.109 | 3397632 |  258.189 |              301.434 |              165.677 |            808.498 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3334.966557530402
    time_step_min: 3059
  date: 2020-10-16_01-11-02
  done: false
  episode_len_mean: 806.9435408921933
  episode_reward_max: 301.4343434343433
  episode_reward_mean: 258.63860022154626
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 4304
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7464395960172018
        entropy_coeff: 0.0005000000000000001
        kl: 0.005762962042354047
        model: {}
        policy_loss: -0.011503236756349603
        total_loss: 11.199698448181152
        vf_explained_var: 0.9805458188056946
        vf_loss: 11.210998773574829
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.096666666666675
    gpu_util_percent0: 0.3193333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15146978653293974
    mean_env_wait_ms: 1.1813033497772574
    mean_inference_ms: 4.593248305768274
    mean_raw_obs_processing_ms: 0.39513539329759756
  time_since_restore: 570.8640441894531
  time_this_iter_s: 25.755372762680054
  time_total_s: 570.8640441894531
  timers:
    learn_throughput: 8634.64
    learn_time_ms: 18737.551
    sample_throughput: 23104.183
    sample_time_ms: 7002.715
    update_time_ms: 38.676
  timestamp: 1602810662
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     22 |          570.864 | 3559424 |  258.639 |              301.434 |              165.677 |            806.944 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3331.2821695213
    time_step_min: 3059
  date: 2020-10-16_01-11-28
  done: false
  episode_len_mean: 804.9172850283719
  episode_reward_max: 301.4343434343433
  episode_reward_mean: 259.23495981200034
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 278
  episodes_total: 4582
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7183542102575302
        entropy_coeff: 0.0005000000000000001
        kl: 0.005707078341705103
        model: {}
        policy_loss: -0.0085253130334119
        total_loss: 10.668026685714722
        vf_explained_var: 0.9826880097389221
        vf_loss: 10.676340103149414
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.082758620689663
    gpu_util_percent0: 0.2975862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15125560371785
    mean_env_wait_ms: 1.182876851668871
    mean_inference_ms: 4.58058580252967
    mean_raw_obs_processing_ms: 0.3944144645072452
  time_since_restore: 596.556777715683
  time_this_iter_s: 25.69273352622986
  time_total_s: 596.556777715683
  timers:
    learn_throughput: 8627.443
    learn_time_ms: 18753.182
    sample_throughput: 23207.062
    sample_time_ms: 6971.671
    update_time_ms: 39.589
  timestamp: 1602810688
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     23 |          596.557 | 3721216 |  259.235 |              301.434 |              165.677 |            804.917 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3329.1632003395584
    time_step_min: 3059
  date: 2020-10-16_01-11-54
  done: false
  episode_len_mean: 803.915611814346
  episode_reward_max: 302.64646464646495
  episode_reward_mean: 259.54113924050625
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 4740
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7287645439306895
        entropy_coeff: 0.0005000000000000001
        kl: 0.005742862975845735
        model: {}
        policy_loss: -0.010577253678396422
        total_loss: 9.842362721761068
        vf_explained_var: 0.9788429141044617
        vf_loss: 9.852729797363281
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.053333333333335
    gpu_util_percent0: 0.3446666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333323
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1511405746262281
    mean_env_wait_ms: 1.183697174137448
    mean_inference_ms: 4.5738536627081094
    mean_raw_obs_processing_ms: 0.39401699025322373
  time_since_restore: 622.2197806835175
  time_this_iter_s: 25.663002967834473
  time_total_s: 622.2197806835175
  timers:
    learn_throughput: 8621.01
    learn_time_ms: 18767.175
    sample_throughput: 23229.033
    sample_time_ms: 6965.077
    update_time_ms: 41.329
  timestamp: 1602810714
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     24 |           622.22 | 3883008 |  259.541 |              302.646 |              165.677 |            803.916 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3326.6037620118586
    time_step_min: 3016
  date: 2020-10-16_01-12-19
  done: false
  episode_len_mean: 802.8786338686725
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 259.88879032241493
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 4919
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7098510215679804
        entropy_coeff: 0.0005000000000000001
        kl: 0.005541195278055966
        model: {}
        policy_loss: -0.010613679187372327
        total_loss: 9.693046967188517
        vf_explained_var: 0.9825088977813721
        vf_loss: 9.703461488087973
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.60689655172414
    gpu_util_percent0: 0.3751724137931034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15101351826638923
    mean_env_wait_ms: 1.1846168000357185
    mean_inference_ms: 4.566468054945183
    mean_raw_obs_processing_ms: 0.39356885340356024
  time_since_restore: 647.7172634601593
  time_this_iter_s: 25.497482776641846
  time_total_s: 647.7172634601593
  timers:
    learn_throughput: 8626.39
    learn_time_ms: 18755.469
    sample_throughput: 23251.051
    sample_time_ms: 6958.481
    update_time_ms: 42.128
  timestamp: 1602810739
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     25 |          647.717 | 4044800 |  259.889 |              306.586 |              165.677 |            802.879 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3322.5940212150435
    time_step_min: 3016
  date: 2020-10-16_01-12-45
  done: false
  episode_len_mean: 801.3819297909073
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 260.41701496065576
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 294
  episodes_total: 5213
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6829637338717779
        entropy_coeff: 0.0005000000000000001
        kl: 0.005084280312682192
        model: {}
        policy_loss: -0.010475764116563369
        total_loss: 10.875974178314209
        vf_explained_var: 0.9832141399383545
        vf_loss: 10.886282841364542
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.86896551724138
    gpu_util_percent0: 0.3306896551724138
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1508267810251618
    mean_env_wait_ms: 1.1860727797692179
    mean_inference_ms: 4.555398762910561
    mean_raw_obs_processing_ms: 0.39292610798615474
  time_since_restore: 673.3003373146057
  time_this_iter_s: 25.58307385444641
  time_total_s: 673.3003373146057
  timers:
    learn_throughput: 8649.539
    learn_time_ms: 18705.275
    sample_throughput: 23278.451
    sample_time_ms: 6950.291
    update_time_ms: 44.066
  timestamp: 1602810765
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     26 |            673.3 | 4206592 |  260.417 |              306.586 |              165.677 |            801.382 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3320.630613772455
    time_step_min: 3016
  date: 2020-10-16_01-13-11
  done: false
  episode_len_mean: 800.5368577810871
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 260.703392826252
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 5372
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.684943675994873
        entropy_coeff: 0.0005000000000000001
        kl: 0.005801629857160151
        model: {}
        policy_loss: -0.012382653173214445
        total_loss: 8.087796211242676
        vf_explained_var: 0.9820259213447571
        vf_loss: 8.099941174189249
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.19
    gpu_util_percent0: 0.327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1507303478244537
    mean_env_wait_ms: 1.1867999579441988
    mean_inference_ms: 4.549726579553944
    mean_raw_obs_processing_ms: 0.39259136966543307
  time_since_restore: 699.2721095085144
  time_this_iter_s: 25.97177219390869
  time_total_s: 699.2721095085144
  timers:
    learn_throughput: 8646.742
    learn_time_ms: 18711.326
    sample_throughput: 23272.483
    sample_time_ms: 6952.073
    update_time_ms: 44.028
  timestamp: 1602810791
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     27 |          699.272 | 4368384 |  260.703 |              306.586 |              165.677 |            800.537 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3318.2403480152257
    time_step_min: 3016
  date: 2020-10-16_01-13-37
  done: false
  episode_len_mean: 799.6009017132552
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 261.03152353107265
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 173
  episodes_total: 5545
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.666491910815239
        entropy_coeff: 0.0005000000000000001
        kl: 0.005610223355082174
        model: {}
        policy_loss: -0.009728390451831123
        total_loss: 9.874438285827637
        vf_explained_var: 0.9808456897735596
        vf_loss: 9.883938709894815
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.190000000000005
    gpu_util_percent0: 0.338
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1506271167954476
    mean_env_wait_ms: 1.1875830905273883
    mean_inference_ms: 4.543748516357313
    mean_raw_obs_processing_ms: 0.39223154996004855
  time_since_restore: 725.2373187541962
  time_this_iter_s: 25.965209245681763
  time_total_s: 725.2373187541962
  timers:
    learn_throughput: 8647.426
    learn_time_ms: 18709.845
    sample_throughput: 23279.047
    sample_time_ms: 6950.113
    update_time_ms: 42.488
  timestamp: 1602810817
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     28 |          725.237 | 4530176 |  261.032 |              306.586 |              165.677 |            799.601 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3315.009802235598
    time_step_min: 3016
  date: 2020-10-16_01-14-04
  done: false
  episode_len_mean: 798.0891665240458
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 261.47272139502155
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 298
  episodes_total: 5843
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6293653895457586
        entropy_coeff: 0.0005000000000000001
        kl: 0.005152029761423667
        model: {}
        policy_loss: -0.00865899131288946
        total_loss: 11.526518026987711
        vf_explained_var: 0.9827463626861572
        vf_loss: 11.534976243972778
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73666666666667
    gpu_util_percent0: 0.2990000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15046615284176382
    mean_env_wait_ms: 1.1889024038878047
    mean_inference_ms: 4.534267299751873
    mean_raw_obs_processing_ms: 0.39167310962345964
  time_since_restore: 751.2308766841888
  time_this_iter_s: 25.993557929992676
  time_total_s: 751.2308766841888
  timers:
    learn_throughput: 8651.678
    learn_time_ms: 18700.65
    sample_throughput: 23261.292
    sample_time_ms: 6955.418
    update_time_ms: 42.475
  timestamp: 1602810844
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     29 |          751.231 | 4691968 |  261.473 |              306.586 |              165.677 |            798.089 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3313.2441432396254
    time_step_min: 3016
  date: 2020-10-16_01-14-29
  done: false
  episode_len_mean: 797.2253497668221
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 261.7548873141811
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 161
  episodes_total: 6004
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6357591251532236
        entropy_coeff: 0.0005000000000000001
        kl: 0.005754904045412938
        model: {}
        policy_loss: -0.010691317261565322
        total_loss: 7.284097353617351
        vf_explained_var: 0.9835760593414307
        vf_loss: 7.294531146685283
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.444827586206898
    gpu_util_percent0: 0.31999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275853
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15038448160341142
    mean_env_wait_ms: 1.1895651256839281
    mean_inference_ms: 4.529401590848884
    mean_raw_obs_processing_ms: 0.39138707237379433
  time_since_restore: 776.9900534152985
  time_this_iter_s: 25.75917673110962
  time_total_s: 776.9900534152985
  timers:
    learn_throughput: 8656.42
    learn_time_ms: 18690.407
    sample_throughput: 23248.233
    sample_time_ms: 6959.325
    update_time_ms: 33.772
  timestamp: 1602810869
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     30 |           776.99 | 4853760 |  261.755 |              306.586 |              165.677 |            797.225 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3311.225345247766
    time_step_min: 3016
  date: 2020-10-16_01-14-55
  done: false
  episode_len_mean: 796.2982371017306
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 262.065031685119
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 6183
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6245539039373398
        entropy_coeff: 0.0005000000000000001
        kl: 0.005535768112167716
        model: {}
        policy_loss: -0.008870539992737273
        total_loss: 8.690814336140951
        vf_explained_var: 0.9829214215278625
        vf_loss: 8.699443578720093
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.890000000000004
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15029476748130646
    mean_env_wait_ms: 1.1903099012135643
    mean_inference_ms: 4.524194477830649
    mean_raw_obs_processing_ms: 0.39107607033885733
  time_since_restore: 802.8577609062195
  time_this_iter_s: 25.86770749092102
  time_total_s: 802.8577609062195
  timers:
    learn_throughput: 8654.143
    learn_time_ms: 18695.323
    sample_throughput: 23192.396
    sample_time_ms: 6976.08
    update_time_ms: 34.956
  timestamp: 1602810895
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     31 |          802.858 | 5015552 |  262.065 |              306.586 |              165.677 |            796.298 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3308.3779087806392
    time_step_min: 3016
  date: 2020-10-16_01-15-21
  done: false
  episode_len_mean: 795.0565338276182
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 262.4935452766777
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 291
  episodes_total: 6474
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6012036552031835
        entropy_coeff: 0.0005000000000000001
        kl: 0.005087119837601979
        model: {}
        policy_loss: -0.008742266528618833
        total_loss: 9.66909925142924
        vf_explained_var: 0.9845057129859924
        vf_loss: 9.677633126576742
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.389655172413793
    gpu_util_percent0: 0.30965517241379303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034485
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15016003172187228
    mean_env_wait_ms: 1.1914755730331834
    mean_inference_ms: 4.516259797586642
    mean_raw_obs_processing_ms: 0.39060771277509737
  time_since_restore: 828.5673532485962
  time_this_iter_s: 25.70959234237671
  time_total_s: 828.5673532485962
  timers:
    learn_throughput: 8651.777
    learn_time_ms: 18700.435
    sample_throughput: 23231.197
    sample_time_ms: 6964.428
    update_time_ms: 34.995
  timestamp: 1602810921
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     32 |          828.567 | 5177344 |  262.494 |              306.586 |              165.677 |            795.057 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3306.840647699758
    time_step_min: 3016
  date: 2020-10-16_01-15-47
  done: false
  episode_len_mean: 794.4975889089814
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 262.688544577785
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 6636
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6159155319134394
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054813608682403965
        model: {}
        policy_loss: -0.011568933009736307
        total_loss: 6.278260111808777
        vf_explained_var: 0.9860866665840149
        vf_loss: 6.289588888486226
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84666666666667
    gpu_util_percent0: 0.3156666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15008909407184182
    mean_env_wait_ms: 1.1920823107244458
    mean_inference_ms: 4.5120386975106666
    mean_raw_obs_processing_ms: 0.39036152719683065
  time_since_restore: 854.4055619239807
  time_this_iter_s: 25.83820867538452
  time_total_s: 854.4055619239807
  timers:
    learn_throughput: 8652.904
    learn_time_ms: 18698.001
    sample_throughput: 23181.311
    sample_time_ms: 6979.416
    update_time_ms: 35.758
  timestamp: 1602810947
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     33 |          854.406 | 5339136 |  262.689 |              306.586 |              165.677 |            794.498 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3305.0464464759657
    time_step_min: 3016
  date: 2020-10-16_01-16-13
  done: false
  episode_len_mean: 793.947577092511
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 262.9731010545988
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 174
  episodes_total: 6810
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6004733194907507
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055042091601838665
        model: {}
        policy_loss: -0.010720125612958023
        total_loss: 7.237987200419108
        vf_explained_var: 0.9852418899536133
        vf_loss: 7.248456994692485
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.741379310344833
    gpu_util_percent0: 0.30896551724137933
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275867
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1500116280377397
    mean_env_wait_ms: 1.192718386015391
    mean_inference_ms: 4.507609853487902
    mean_raw_obs_processing_ms: 0.39009344632132265
  time_since_restore: 880.1469309329987
  time_this_iter_s: 25.741369009017944
  time_total_s: 880.1469309329987
  timers:
    learn_throughput: 8651.871
    learn_time_ms: 18700.232
    sample_throughput: 23170.544
    sample_time_ms: 6982.659
    update_time_ms: 35.273
  timestamp: 1602810973
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     34 |          880.147 | 5500928 |  262.973 |              306.586 |              165.677 |            793.948 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3301.853327681221
    time_step_min: 3016
  date: 2020-10-16_01-16-39
  done: false
  episode_len_mean: 793.0447572132301
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 263.4259839777081
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 295
  episodes_total: 7105
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5662781298160553
        entropy_coeff: 0.0005000000000000001
        kl: 0.004756226437166333
        model: {}
        policy_loss: -0.010773680560911695
        total_loss: 9.240336974461874
        vf_explained_var: 0.9854938387870789
        vf_loss: 9.25091822942098
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.32
    gpu_util_percent0: 0.3233333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14989489391384717
    mean_env_wait_ms: 1.1937725810987012
    mean_inference_ms: 4.500703358112265
    mean_raw_obs_processing_ms: 0.3896842194765767
  time_since_restore: 905.9917213916779
  time_this_iter_s: 25.8447904586792
  time_total_s: 905.9917213916779
  timers:
    learn_throughput: 8643.341
    learn_time_ms: 18718.688
    sample_throughput: 23121.641
    sample_time_ms: 6997.427
    update_time_ms: 35.735
  timestamp: 1602810999
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     35 |          905.992 | 5662720 |  263.426 |              306.586 |              165.677 |            793.045 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3300.475276243094
    time_step_min: 3016
  date: 2020-10-16_01-17-05
  done: false
  episode_len_mean: 792.5401761144744
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 263.65293134982176
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 163
  episodes_total: 7268
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5714666495720545
        entropy_coeff: 0.0005000000000000001
        kl: 0.005248239535527925
        model: {}
        policy_loss: -0.009869688520363221
        total_loss: 6.231145858764648
        vf_explained_var: 0.985950767993927
        vf_loss: 6.241038918495178
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.52413793103448
    gpu_util_percent0: 0.3641379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14983273206871076
    mean_env_wait_ms: 1.194323645426138
    mean_inference_ms: 4.497014022818796
    mean_raw_obs_processing_ms: 0.3894693475886714
  time_since_restore: 931.722062587738
  time_this_iter_s: 25.73034119606018
  time_total_s: 931.722062587738
  timers:
    learn_throughput: 8631.229
    learn_time_ms: 18744.955
    sample_throughput: 23156.858
    sample_time_ms: 6986.786
    update_time_ms: 34.54
  timestamp: 1602811025
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     36 |          931.722 | 5824512 |  263.653 |              306.586 |              165.677 |             792.54 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3298.755060728745
    time_step_min: 3016
  date: 2020-10-16_01-17-31
  done: false
  episode_len_mean: 792.0992202204894
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 263.90828967274246
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 170
  episodes_total: 7438
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5747150182723999
        entropy_coeff: 0.0005000000000000001
        kl: 0.005940825406772395
        model: {}
        policy_loss: -0.009506770729785785
        total_loss: 6.700782895088196
        vf_explained_var: 0.985985517501831
        vf_loss: 6.710279941558838
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000002
    gpu_util_percent0: 0.3266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14976792086930715
    mean_env_wait_ms: 1.19488233173313
    mean_inference_ms: 4.493267210576674
    mean_raw_obs_processing_ms: 0.3892447927182084
  time_since_restore: 957.5777795314789
  time_this_iter_s: 25.855716943740845
  time_total_s: 957.5777795314789
  timers:
    learn_throughput: 8633.213
    learn_time_ms: 18740.648
    sample_throughput: 23176.597
    sample_time_ms: 6980.835
    update_time_ms: 32.512
  timestamp: 1602811051
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     37 |          957.578 | 5986304 |  263.908 |              306.586 |              165.677 |            792.099 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3295.8188894654904
    time_step_min: 3016
  date: 2020-10-16_01-17-57
  done: false
  episode_len_mean: 791.482032057911
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 264.3536541735869
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 298
  episodes_total: 7736
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5408161381880442
        entropy_coeff: 0.0005000000000000001
        kl: 0.005141363246366382
        model: {}
        policy_loss: -0.009341358963865787
        total_loss: 7.610303680102031
        vf_explained_var: 0.9880974888801575
        vf_loss: 7.61965827147166
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43666666666667
    gpu_util_percent0: 0.33866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14966267105075193
    mean_env_wait_ms: 1.195824630600854
    mean_inference_ms: 4.4871252882338615
    mean_raw_obs_processing_ms: 0.38887369622891715
  time_since_restore: 983.247095823288
  time_this_iter_s: 25.669316291809082
  time_total_s: 983.247095823288
  timers:
    learn_throughput: 8640.404
    learn_time_ms: 18725.051
    sample_throughput: 23255.445
    sample_time_ms: 6957.166
    update_time_ms: 33.625
  timestamp: 1602811077
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     38 |          983.247 | 6148096 |  264.354 |              306.586 |              165.677 |            791.482 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3294.1681910569105
    time_step_min: 3016
  date: 2020-10-16_01-18-23
  done: false
  episode_len_mean: 791.1965822784811
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 264.5999744278225
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 164
  episodes_total: 7900
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5411226153373718
        entropy_coeff: 0.0005000000000000001
        kl: 0.005913850696136554
        model: {}
        policy_loss: -0.011923030091566034
        total_loss: 5.805917024612427
        vf_explained_var: 0.9870100021362305
        vf_loss: 5.817814946174622
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.758620689655178
    gpu_util_percent0: 0.3389655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7862068965517235
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14960787489103047
    mean_env_wait_ms: 1.1963144277045423
    mean_inference_ms: 4.483880674432336
    mean_raw_obs_processing_ms: 0.3886860674238128
  time_since_restore: 1009.0835437774658
  time_this_iter_s: 25.836447954177856
  time_total_s: 1009.0835437774658
  timers:
    learn_throughput: 8640.797
    learn_time_ms: 18724.2
    sample_throughput: 23303.353
    sample_time_ms: 6942.864
    update_time_ms: 32.139
  timestamp: 1602811103
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     39 |          1009.08 | 6309888 |    264.6 |              306.586 |              165.677 |            791.197 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3292.529499626587
    time_step_min: 3016
  date: 2020-10-16_01-18-49
  done: false
  episode_len_mean: 790.8907219052344
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 264.81483026744746
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 8062
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5440810869137446
        entropy_coeff: 0.0005000000000000001
        kl: 0.005609358583266537
        model: {}
        policy_loss: -0.010298911026135707
        total_loss: 7.1397877136866255
        vf_explained_var: 0.9844804406166077
        vf_loss: 7.150078336397807
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.58666666666667
    gpu_util_percent0: 0.3523333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14955407403918275
    mean_env_wait_ms: 1.1967778794532535
    mean_inference_ms: 4.480779462600558
    mean_raw_obs_processing_ms: 0.38849973769153656
  time_since_restore: 1034.774028301239
  time_this_iter_s: 25.690484523773193
  time_total_s: 1034.774028301239
  timers:
    learn_throughput: 8643.121
    learn_time_ms: 18719.163
    sample_throughput: 23314.25
    sample_time_ms: 6939.618
    update_time_ms: 32.462
  timestamp: 1602811129
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     40 |          1034.77 | 6471680 |  264.815 |              306.586 |              165.677 |            790.891 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3290.0100840336136
    time_step_min: 3016
  date: 2020-10-16_01-19-15
  done: false
  episode_len_mean: 790.2048336922709
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 265.21100089190537
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 296
  episodes_total: 8358
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.509361982345581
        entropy_coeff: 0.0005000000000000001
        kl: 0.005469926167279482
        model: {}
        policy_loss: -0.010394827579148114
        total_loss: 8.237964987754822
        vf_explained_var: 0.9871106147766113
        vf_loss: 8.248341043790182
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.206666666666663
    gpu_util_percent0: 0.32000000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.149462318314159
    mean_env_wait_ms: 1.1976220474877841
    mean_inference_ms: 4.4754749516047925
    mean_raw_obs_processing_ms: 0.3881800568383478
  time_since_restore: 1060.662667274475
  time_this_iter_s: 25.888638973236084
  time_total_s: 1060.662667274475
  timers:
    learn_throughput: 8638.636
    learn_time_ms: 18728.883
    sample_throughput: 23343.382
    sample_time_ms: 6930.958
    update_time_ms: 32.627
  timestamp: 1602811155
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     41 |          1060.66 | 6633472 |  265.211 |              306.586 |              165.677 |            790.205 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3288.4494355597367
    time_step_min: 3016
  date: 2020-10-16_01-19-41
  done: false
  episode_len_mean: 789.8383731833098
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 265.45972382048325
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 174
  episodes_total: 8532
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.507927214105924
        entropy_coeff: 0.0005000000000000001
        kl: 0.005216488498263061
        model: {}
        policy_loss: -0.010664286892279051
        total_loss: 5.711553494135539
        vf_explained_var: 0.9873116612434387
        vf_loss: 5.722210804621379
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.86666666666667
    gpu_util_percent0: 0.35666666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14941088998915528
    mean_env_wait_ms: 1.198079659556626
    mean_inference_ms: 4.4724225568681675
    mean_raw_obs_processing_ms: 0.38800363311041824
  time_since_restore: 1086.4963381290436
  time_this_iter_s: 25.83367085456848
  time_total_s: 1086.4963381290436
  timers:
    learn_throughput: 8645.231
    learn_time_ms: 18714.595
    sample_throughput: 23277.618
    sample_time_ms: 6950.539
    update_time_ms: 38.374
  timestamp: 1602811181
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     42 |           1086.5 | 6795264 |   265.46 |              306.586 |              165.677 |            789.838 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3286.827025155781
    time_step_min: 3016
  date: 2020-10-16_01-20-07
  done: false
  episode_len_mean: 789.556590752243
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 265.7101588695791
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 8694
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5209666738907496
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053579983456681175
        model: {}
        policy_loss: -0.011443275609053671
        total_loss: 5.225985884666443
        vf_explained_var: 0.9881942868232727
        vf_loss: 5.237421830495198
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.189999999999998
    gpu_util_percent0: 0.29933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14936320956231913
    mean_env_wait_ms: 1.198490174290157
    mean_inference_ms: 4.469707859479857
    mean_raw_obs_processing_ms: 0.3878393241028502
  time_since_restore: 1112.5142114162445
  time_this_iter_s: 26.017873287200928
  time_total_s: 1112.5142114162445
  timers:
    learn_throughput: 8651.357
    learn_time_ms: 18701.343
    sample_throughput: 23203.436
    sample_time_ms: 6972.76
    update_time_ms: 38.116
  timestamp: 1602811207
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     43 |          1112.51 | 6957056 |   265.71 |              306.586 |              165.677 |            789.557 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3283.8893731143144
    time_step_min: 3016
  date: 2020-10-16_01-20-33
  done: false
  episode_len_mean: 789.039099922023
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 266.1412037271455
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 283
  episodes_total: 8977
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4873087281982104
        entropy_coeff: 0.0005000000000000001
        kl: 0.005598535644821823
        model: {}
        policy_loss: -0.010866215345837796
        total_loss: 7.635515332221985
        vf_explained_var: 0.9877046942710876
        vf_loss: 7.646345337231954
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.931034482758626
    gpu_util_percent0: 0.32724137931034486
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492847395942105
    mean_env_wait_ms: 1.1992121501566515
    mean_inference_ms: 4.4652009632959295
    mean_raw_obs_processing_ms: 0.3875693080946247
  time_since_restore: 1138.248262643814
  time_this_iter_s: 25.73405122756958
  time_total_s: 1138.248262643814
  timers:
    learn_throughput: 8655.14
    learn_time_ms: 18693.17
    sample_throughput: 23177.498
    sample_time_ms: 6980.564
    update_time_ms: 38.411
  timestamp: 1602811233
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     44 |          1138.25 | 7118848 |  266.141 |              306.586 |              165.677 |            789.039 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3282.002955341506
    time_step_min: 3016
  date: 2020-10-16_01-20-59
  done: false
  episode_len_mean: 788.8099083369708
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 266.4225383472437
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 9164
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47847431153059006
        entropy_coeff: 0.0005000000000000001
        kl: 0.005613565483751397
        model: {}
        policy_loss: -0.008747315519334128
        total_loss: 5.270971179008484
        vf_explained_var: 0.9885722994804382
        vf_loss: 5.279677311579387
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.93
    gpu_util_percent0: 0.32099999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14923661294062085
    mean_env_wait_ms: 1.199649102820452
    mean_inference_ms: 4.4623202371902
    mean_raw_obs_processing_ms: 0.38740344503239826
  time_since_restore: 1163.988874912262
  time_this_iter_s: 25.740612268447876
  time_total_s: 1163.988874912262
  timers:
    learn_throughput: 8658.572
    learn_time_ms: 18685.759
    sample_throughput: 23188.994
    sample_time_ms: 6977.103
    update_time_ms: 38.17
  timestamp: 1602811259
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     45 |          1163.99 | 7280640 |  266.423 |              306.586 |              165.677 |             788.81 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3280.2321678321678
    time_step_min: 3016
  date: 2020-10-16_01-21-25
  done: false
  episode_len_mean: 788.601201330044
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 266.6601128738852
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 9323
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49601898342370987
        entropy_coeff: 0.0005000000000000001
        kl: 0.00511051594124486
        model: {}
        policy_loss: -0.012326105507478738
        total_loss: 5.88115390141805
        vf_explained_var: 0.9861772060394287
        vf_loss: 5.893472512563069
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43666666666667
    gpu_util_percent0: 0.337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14919614832510006
    mean_env_wait_ms: 1.2000109965789967
    mean_inference_ms: 4.459954841549204
    mean_raw_obs_processing_ms: 0.3872643643545326
  time_since_restore: 1189.6763997077942
  time_this_iter_s: 25.687524795532227
  time_total_s: 1189.6763997077942
  timers:
    learn_throughput: 8660.589
    learn_time_ms: 18681.409
    sample_throughput: 23200.405
    sample_time_ms: 6973.671
    update_time_ms: 40.417
  timestamp: 1602811285
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     46 |          1189.68 | 7442432 |   266.66 |              306.586 |              165.677 |            788.601 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3277.4158125915083
    time_step_min: 3016
  date: 2020-10-16_01-21-51
  done: false
  episode_len_mean: 788.1692387904067
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 267.0626968327698
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 267
  episodes_total: 9590
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4737584764758746
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053332661821817355
        model: {}
        policy_loss: -0.00873103212021912
        total_loss: 7.418438156445821
        vf_explained_var: 0.9876915812492371
        vf_loss: 7.427139401435852
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.573333333333334
    gpu_util_percent0: 0.351
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1491313792486176
    mean_env_wait_ms: 1.2006209339000375
    mean_inference_ms: 4.456135704184572
    mean_raw_obs_processing_ms: 0.38704004330534936
  time_since_restore: 1215.5139253139496
  time_this_iter_s: 25.837525606155396
  time_total_s: 1215.5139253139496
  timers:
    learn_throughput: 8657.122
    learn_time_ms: 18688.891
    sample_throughput: 23264.361
    sample_time_ms: 6954.5
    update_time_ms: 40.536
  timestamp: 1602811311
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     47 |          1215.51 | 7604224 |  267.063 |              306.586 |              165.677 |            788.169 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3275.51033988534
    time_step_min: 3016
  date: 2020-10-16_01-22-17
  done: false
  episode_len_mean: 787.9017966516946
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 267.34459437164617
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 206
  episodes_total: 9796
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.45716172208388645
        entropy_coeff: 0.0005000000000000001
        kl: 0.005090787773951888
        model: {}
        policy_loss: -0.010548666915080199
        total_loss: 5.173902789751689
        vf_explained_var: 0.9894575476646423
        vf_loss: 5.184425473213196
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.703448275862073
    gpu_util_percent0: 0.3279310344827587
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14908104468745803
    mean_env_wait_ms: 1.20103094598201
    mean_inference_ms: 4.453261938814512
    mean_raw_obs_processing_ms: 0.3868687572400621
  time_since_restore: 1241.1134259700775
  time_this_iter_s: 25.59950065612793
  time_total_s: 1241.1134259700775
  timers:
    learn_throughput: 8654.717
    learn_time_ms: 18694.083
    sample_throughput: 23285.027
    sample_time_ms: 6948.328
    update_time_ms: 40.926
  timestamp: 1602811337
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     48 |          1241.11 | 7766016 |  267.345 |              306.586 |              165.677 |            787.902 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3273.9775360128942
    time_step_min: 3016
  date: 2020-10-16_01-22-43
  done: false
  episode_len_mean: 787.6919136112506
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 267.5595584169165
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 9955
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47419848789771396
        entropy_coeff: 0.0005000000000000001
        kl: 0.005596162169240415
        model: {}
        policy_loss: -0.010498188300213466
        total_loss: 5.168001453081767
        vf_explained_var: 0.9879141449928284
        vf_loss: 5.178456664085388
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.113333333333333
    gpu_util_percent0: 0.3236666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1490446435412956
    mean_env_wait_ms: 1.2013518490743602
    mean_inference_ms: 4.451129597598263
    mean_raw_obs_processing_ms: 0.3867438500338126
  time_since_restore: 1266.8555419445038
  time_this_iter_s: 25.74211597442627
  time_total_s: 1266.8555419445038
  timers:
    learn_throughput: 8658.993
    learn_time_ms: 18684.852
    sample_throughput: 23294.43
    sample_time_ms: 6945.523
    update_time_ms: 42.372
  timestamp: 1602811363
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     49 |          1266.86 | 7927808 |   267.56 |              306.586 |              165.677 |            787.692 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3271.724764336214
    time_step_min: 3016
  date: 2020-10-16_01-23-09
  done: false
  episode_len_mean: 787.2959263611438
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 267.90575555792947
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 257
  episodes_total: 10212
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4528145541747411
        entropy_coeff: 0.0005000000000000001
        kl: 0.004967670189216733
        model: {}
        policy_loss: -0.008880688847663501
        total_loss: 6.6970163981119795
        vf_explained_var: 0.9885924458503723
        vf_loss: 6.705875118573506
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.920689655172417
    gpu_util_percent0: 0.3303448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14898794553259054
    mean_env_wait_ms: 1.2018842039462314
    mean_inference_ms: 4.447829622863461
    mean_raw_obs_processing_ms: 0.38655069357525407
  time_since_restore: 1292.634330034256
  time_this_iter_s: 25.778788089752197
  time_total_s: 1292.634330034256
  timers:
    learn_throughput: 8651.607
    learn_time_ms: 18700.803
    sample_throughput: 23315.06
    sample_time_ms: 6939.377
    update_time_ms: 40.263
  timestamp: 1602811389
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     50 |          1292.63 | 8089600 |  267.906 |              306.586 |              165.677 |            787.296 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3269.6870192307692
    time_step_min: 3011
  date: 2020-10-16_01-23-35
  done: false
  episode_len_mean: 786.9913693901036
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 268.2156490102405
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 10428
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.43528473128875095
        entropy_coeff: 0.0005000000000000001
        kl: 0.005461744071605305
        model: {}
        policy_loss: -0.010802024335134774
        total_loss: 4.955641071001689
        vf_explained_var: 0.9899734854698181
        vf_loss: 4.966524203618367
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.073333333333338
    gpu_util_percent0: 0.30566666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1489409863021991
    mean_env_wait_ms: 1.2022700378705418
    mean_inference_ms: 4.445074236291402
    mean_raw_obs_processing_ms: 0.3863906021958231
  time_since_restore: 1318.5277693271637
  time_this_iter_s: 25.893439292907715
  time_total_s: 1318.5277693271637
  timers:
    learn_throughput: 8650.586
    learn_time_ms: 18703.01
    sample_throughput: 23324.171
    sample_time_ms: 6936.667
    update_time_ms: 40.509
  timestamp: 1602811415
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     51 |          1318.53 | 8251392 |  268.216 |              307.343 |              165.677 |            786.991 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3268.3336490197935
    time_step_min: 3011
  date: 2020-10-16_01-24-01
  done: false
  episode_len_mean: 786.7751015396241
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 268.4333931551273
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 10587
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.454522043466568
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056253249834602075
        model: {}
        policy_loss: -0.010930568018617729
        total_loss: 5.377806742986043
        vf_explained_var: 0.9872221946716309
        vf_loss: 5.388823827107747
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.15666666666667
    gpu_util_percent0: 0.38333333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1489080000404967
    mean_env_wait_ms: 1.2025586157849693
    mean_inference_ms: 4.443140282722608
    mean_raw_obs_processing_ms: 0.38627913645357204
  time_since_restore: 1344.2405889034271
  time_this_iter_s: 25.712819576263428
  time_total_s: 1344.2405889034271
  timers:
    learn_throughput: 8647.12
    learn_time_ms: 18710.506
    sample_throughput: 23395.941
    sample_time_ms: 6915.387
    update_time_ms: 34.619
  timestamp: 1602811441
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     52 |          1344.24 | 8413184 |  268.433 |              307.343 |              165.677 |            786.775 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3266.329290872061
    time_step_min: 3011
  date: 2020-10-16_01-24-27
  done: false
  episode_len_mean: 786.4809787626962
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 268.7601919471726
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 243
  episodes_total: 10830
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4375423813859622
        entropy_coeff: 0.0005000000000000001
        kl: 0.005306682161365946
        model: {}
        policy_loss: -0.009265490055743916
        total_loss: 6.695862730344136
        vf_explained_var: 0.9884181618690491
        vf_loss: 6.7052143812179565
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.789655172413795
    gpu_util_percent0: 0.3675862068965518
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034485
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14885861750464582
    mean_env_wait_ms: 1.2030014884623803
    mean_inference_ms: 4.440232687812624
    mean_raw_obs_processing_ms: 0.3861112143714062
  time_since_restore: 1369.9999487400055
  time_this_iter_s: 25.75935983657837
  time_total_s: 1369.9999487400055
  timers:
    learn_throughput: 8639.281
    learn_time_ms: 18727.485
    sample_throughput: 23510.832
    sample_time_ms: 6881.594
    update_time_ms: 32.944
  timestamp: 1602811467
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     53 |             1370 | 8574976 |   268.76 |              307.343 |              165.677 |            786.481 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3264.1519216823785
    time_step_min: 3011
  date: 2020-10-16_01-24-53
  done: false
  episode_len_mean: 786.3327305605786
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 269.0695837214824
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 230
  episodes_total: 11060
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4162285253405571
        entropy_coeff: 0.0005000000000000001
        kl: 0.005450062919408083
        model: {}
        policy_loss: -0.010840619617738412
        total_loss: 5.2562704881032305
        vf_explained_var: 0.9900563359260559
        vf_loss: 5.267183065414429
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.796666666666674
    gpu_util_percent0: 0.3126666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14881389352769797
    mean_env_wait_ms: 1.2033774829768986
    mean_inference_ms: 4.4376292119700915
    mean_raw_obs_processing_ms: 0.385962596854839
  time_since_restore: 1395.6251745224
  time_this_iter_s: 25.62522578239441
  time_total_s: 1395.6251745224
  timers:
    learn_throughput: 8638.71
    learn_time_ms: 18728.722
    sample_throughput: 23555.322
    sample_time_ms: 6868.597
    update_time_ms: 33.403
  timestamp: 1602811493
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     54 |          1395.63 | 8736768 |   269.07 |              307.343 |              165.677 |            786.333 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3262.643731570012
    time_step_min: 3011
  date: 2020-10-16_01-25-19
  done: false
  episode_len_mean: 786.1614225866833
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 269.29395388955055
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 11219
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.432820700109005
        entropy_coeff: 0.0005000000000000001
        kl: 0.00562728460257252
        model: {}
        policy_loss: -0.010601595315771798
        total_loss: 5.018101851145427
        vf_explained_var: 0.9877817034721375
        vf_loss: 5.028779149055481
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.263333333333332
    gpu_util_percent0: 0.3926666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14878398388755315
    mean_env_wait_ms: 1.20363328954227
    mean_inference_ms: 4.435867454629573
    mean_raw_obs_processing_ms: 0.38586238365583164
  time_since_restore: 1421.3690013885498
  time_this_iter_s: 25.743826866149902
  time_total_s: 1421.3690013885498
  timers:
    learn_throughput: 8635.656
    learn_time_ms: 18735.346
    sample_throughput: 23609.116
    sample_time_ms: 6852.946
    update_time_ms: 40.61
  timestamp: 1602811519
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     55 |          1421.37 | 8898560 |  269.294 |              307.343 |              165.677 |            786.161 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3260.655247941125
    time_step_min: 3011
  date: 2020-10-16_01-25-45
  done: false
  episode_len_mean: 786.0182660374061
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 269.5930278135312
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 11442
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4235244368513425
        entropy_coeff: 0.0005000000000000001
        kl: 0.005360846097270648
        model: {}
        policy_loss: -0.00987688044673026
        total_loss: 5.838384707768758
        vf_explained_var: 0.9893782138824463
        vf_loss: 5.848339041074117
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.527586206896554
    gpu_util_percent0: 0.35724137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14874125855544035
    mean_env_wait_ms: 1.2039799318379243
    mean_inference_ms: 4.433384821916408
    mean_raw_obs_processing_ms: 0.38571903707707084
  time_since_restore: 1446.9487771987915
  time_this_iter_s: 25.5797758102417
  time_total_s: 1446.9487771987915
  timers:
    learn_throughput: 8644.825
    learn_time_ms: 18715.475
    sample_throughput: 23577.997
    sample_time_ms: 6861.991
    update_time_ms: 39.623
  timestamp: 1602811545
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     56 |          1446.95 | 9060352 |  269.593 |              307.343 |              165.677 |            786.018 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3258.3082925992626
    time_step_min: 2985
  date: 2020-10-16_01-26-11
  done: false
  episode_len_mean: 785.8540508170074
  episode_reward_max: 311.2828282828276
  episode_reward_mean: 269.9349539539461
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 247
  episodes_total: 11689
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.39607641845941544
        entropy_coeff: 0.0005000000000000001
        kl: 0.005123959970660508
        model: {}
        policy_loss: -0.010596955481976996
        total_loss: 5.108719905217488
        vf_explained_var: 0.9907188415527344
        vf_loss: 5.119386752446492
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.933333333333337
    gpu_util_percent0: 0.311
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14869814334682704
    mean_env_wait_ms: 1.2043473231246873
    mean_inference_ms: 4.430870834315471
    mean_raw_obs_processing_ms: 0.3855769325033114
  time_since_restore: 1472.6428391933441
  time_this_iter_s: 25.694061994552612
  time_total_s: 1472.6428391933441
  timers:
    learn_throughput: 8647.333
    learn_time_ms: 18710.047
    sample_throughput: 23592.074
    sample_time_ms: 6857.897
    update_time_ms: 42.431
  timestamp: 1602811571
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     57 |          1472.64 | 9222144 |  269.935 |              311.283 |              165.677 |            785.854 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3256.762327666413
    time_step_min: 2979
  date: 2020-10-16_01-26-37
  done: false
  episode_len_mean: 785.7850814277276
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 270.16314269179003
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 11851
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4082030827800433
        entropy_coeff: 0.0005000000000000001
        kl: 0.005373570253141224
        model: {}
        policy_loss: -0.012292492187649865
        total_loss: 4.091778020064036
        vf_explained_var: 0.9902692437171936
        vf_loss: 4.104140281677246
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.46896551724138
    gpu_util_percent0: 0.3079310344827586
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789655172413793
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14867095345352865
    mean_env_wait_ms: 1.2045771123803641
    mean_inference_ms: 4.429220615567547
    mean_raw_obs_processing_ms: 0.3854848865291545
  time_since_restore: 1498.3391180038452
  time_this_iter_s: 25.6962788105011
  time_total_s: 1498.3391180038452
  timers:
    learn_throughput: 8642.238
    learn_time_ms: 18721.076
    sample_throughput: 23601.13
    sample_time_ms: 6855.265
    update_time_ms: 42.583
  timestamp: 1602811597
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     58 |          1498.34 | 9383936 |  270.163 |              312.192 |              165.677 |            785.785 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3255.045117789062
    time_step_min: 2979
  date: 2020-10-16_01-27-03
  done: false
  episode_len_mean: 785.7113196578357
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 270.4221955456902
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 190
  episodes_total: 12041
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.40389055013656616
        entropy_coeff: 0.0005000000000000001
        kl: 0.005006047586599986
        model: {}
        policy_loss: -0.011286400248839831
        total_loss: 5.464943885803223
        vf_explained_var: 0.989168643951416
        vf_loss: 5.476307074228923
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.503333333333337
    gpu_util_percent0: 0.30166666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486382464405446
    mean_env_wait_ms: 1.204835975079082
    mean_inference_ms: 4.42729413534788
    mean_raw_obs_processing_ms: 0.38537519716545304
  time_since_restore: 1524.1755712032318
  time_this_iter_s: 25.836453199386597
  time_total_s: 1524.1755712032318
  timers:
    learn_throughput: 8638.969
    learn_time_ms: 18728.161
    sample_throughput: 23594.17
    sample_time_ms: 6857.287
    update_time_ms: 42.511
  timestamp: 1602811623
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     59 |          1524.18 | 9545728 |  270.422 |              312.192 |              165.677 |            785.711 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3252.57003257329
    time_step_min: 2979
  date: 2020-10-16_01-27-29
  done: false
  episode_len_mean: 785.6135034124147
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 270.78911145908216
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 267
  episodes_total: 12308
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.37611258774995804
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053737818185860915
        model: {}
        policy_loss: -0.009690653552146008
        total_loss: 5.3002186218897505
        vf_explained_var: 0.9910141825675964
        vf_loss: 5.30996310710907
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.366666666666667
    gpu_util_percent0: 0.352
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14859478897989373
    mean_env_wait_ms: 1.2051893298337242
    mean_inference_ms: 4.424771770610665
    mean_raw_obs_processing_ms: 0.3852320948784134
  time_since_restore: 1549.8822972774506
  time_this_iter_s: 25.70672607421875
  time_total_s: 1549.8822972774506
  timers:
    learn_throughput: 8639.982
    learn_time_ms: 18725.965
    sample_throughput: 23619.862
    sample_time_ms: 6849.828
    update_time_ms: 44.167
  timestamp: 1602811649
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     60 |          1549.88 | 9707520 |  270.789 |              312.192 |              165.677 |            785.614 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3250.9123976232536
    time_step_min: 2979
  date: 2020-10-16_01-27-54
  done: false
  episode_len_mean: 785.6085563211024
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 271.0354053271053
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 174
  episodes_total: 12482
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.38262255241473514
        entropy_coeff: 0.0005000000000000001
        kl: 0.005104592729670306
        model: {}
        policy_loss: -0.010922989531536587
        total_loss: 4.422545830408732
        vf_explained_var: 0.9898379445075989
        vf_loss: 4.43353255589803
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.06896551724138
    gpu_util_percent0: 0.3844827586206898
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.793103448275862
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14856801327536756
    mean_env_wait_ms: 1.205394516048685
    mean_inference_ms: 4.4231358977142365
    mean_raw_obs_processing_ms: 0.3851411891328614
  time_since_restore: 1575.4916667938232
  time_this_iter_s: 25.60936951637268
  time_total_s: 1575.4916667938232
  timers:
    learn_throughput: 8642.355
    learn_time_ms: 18720.823
    sample_throughput: 23698.922
    sample_time_ms: 6826.977
    update_time_ms: 42.566
  timestamp: 1602811674
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     61 |          1575.49 | 9869312 |  271.035 |              312.192 |              165.677 |            785.609 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3249.362592093797
    time_step_min: 2979
  date: 2020-10-16_01-28-21
  done: false
  episode_len_mean: 785.619397676073
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 271.2794149701904
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 169
  episodes_total: 12651
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.38693956782420474
        entropy_coeff: 0.0005000000000000001
        kl: 0.005684521088066201
        model: {}
        policy_loss: -0.010501185975347957
        total_loss: 4.495320757230123
        vf_explained_var: 0.9902675747871399
        vf_loss: 4.505873401959737
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.043333333333337
    gpu_util_percent0: 0.352
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.796666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14854190025552116
    mean_env_wait_ms: 1.2055887619234869
    mean_inference_ms: 4.421574020422676
    mean_raw_obs_processing_ms: 0.38505310416930993
  time_since_restore: 1601.548725605011
  time_this_iter_s: 26.057058811187744
  time_total_s: 1601.548725605011
  timers:
    learn_throughput: 8635.119
    learn_time_ms: 18736.51
    sample_throughput: 23613.464
    sample_time_ms: 6851.684
    update_time_ms: 43.77
  timestamp: 1602811701
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     62 |          1601.55 | 10031104 |  271.279 |              312.192 |              165.677 |            785.619 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3247.032268073224
    time_step_min: 2942
  date: 2020-10-16_01-28-47
  done: false
  episode_len_mean: 785.6776315789474
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 271.6277832504612
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 269
  episodes_total: 12920
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.36051372687021893
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046567306853830814
        model: {}
        policy_loss: -0.01011651087901555
        total_loss: 5.73840069770813
        vf_explained_var: 0.9904703497886658
        vf_loss: 5.748581091562907
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.31
    gpu_util_percent0: 0.30833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485003824633505
    mean_env_wait_ms: 1.205893350688142
    mean_inference_ms: 4.419207976884905
    mean_raw_obs_processing_ms: 0.3849178371949001
  time_since_restore: 1627.2219305038452
  time_this_iter_s: 25.67320489883423
  time_total_s: 1627.2219305038452
  timers:
    learn_throughput: 8643.643
    learn_time_ms: 18718.033
    sample_throughput: 23589.956
    sample_time_ms: 6858.512
    update_time_ms: 45.867
  timestamp: 1602811727
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     63 |          1627.22 | 10192896 |  271.628 |              317.798 |              165.677 |            785.678 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3245.41433593153
    time_step_min: 2942
  date: 2020-10-16_01-29-13
  done: false
  episode_len_mean: 785.8016623455849
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 271.8679767015896
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 13114
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.35336187730232876
        entropy_coeff: 0.0005000000000000001
        kl: 0.005059316637925804
        model: {}
        policy_loss: -0.008540955333349606
        total_loss: 4.587351322174072
        vf_explained_var: 0.9907009601593018
        vf_loss: 4.596005876859029
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.516666666666666
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14847384089359447
    mean_env_wait_ms: 1.2060871479797994
    mean_inference_ms: 4.417538421847849
    mean_raw_obs_processing_ms: 0.3848269371498183
  time_since_restore: 1652.9083304405212
  time_this_iter_s: 25.686399936676025
  time_total_s: 1652.9083304405212
  timers:
    learn_throughput: 8642.688
    learn_time_ms: 18720.101
    sample_throughput: 23578.076
    sample_time_ms: 6861.968
    update_time_ms: 44.863
  timestamp: 1602811753
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     64 |          1652.91 | 10354688 |  271.868 |              317.798 |              165.677 |            785.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3243.9032087580217
    time_step_min: 2942
  date: 2020-10-16_01-29-39
  done: false
  episode_len_mean: 785.8371129360355
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 272.09423855065376
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 13273
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3649902318914731
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056065128883346915
        model: {}
        policy_loss: -0.012942626276829591
        total_loss: 3.630454699198405
        vf_explained_var: 0.9914636611938477
        vf_loss: 3.6435096859931946
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.582758620689653
    gpu_util_percent0: 0.30517241379310345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7896551724137932
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14845138024555452
    mean_env_wait_ms: 1.2062402770017377
    mean_inference_ms: 4.416207529683875
    mean_raw_obs_processing_ms: 0.38475215718735367
  time_since_restore: 1678.5427992343903
  time_this_iter_s: 25.63446879386902
  time_total_s: 1678.5427992343903
  timers:
    learn_throughput: 8643.266
    learn_time_ms: 18718.851
    sample_throughput: 23589.702
    sample_time_ms: 6858.586
    update_time_ms: 37.611
  timestamp: 1602811779
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     65 |          1678.54 | 10516480 |  272.094 |              317.798 |              165.677 |            785.837 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3241.8498035436282
    time_step_min: 2942
  date: 2020-10-16_01-30-05
  done: false
  episode_len_mean: 785.8663904712585
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 272.41918631457725
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 244
  episodes_total: 13517
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.35202441612879437
        entropy_coeff: 0.0005000000000000001
        kl: 0.005318918381817639
        model: {}
        policy_loss: -0.008715767316364994
        total_loss: 4.88840115070343
        vf_explained_var: 0.9913294911384583
        vf_loss: 4.897226373354594
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.203333333333333
    gpu_util_percent0: 0.30799999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484163076536767
    mean_env_wait_ms: 1.2064561525274444
    mean_inference_ms: 4.414197036733155
    mean_raw_obs_processing_ms: 0.38463568225919254
  time_since_restore: 1704.251299381256
  time_this_iter_s: 25.708500146865845
  time_total_s: 1704.251299381256
  timers:
    learn_throughput: 8633.038
    learn_time_ms: 18741.028
    sample_throughput: 23622.258
    sample_time_ms: 6849.134
    update_time_ms: 36.104
  timestamp: 1602811805
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     66 |          1704.25 | 10678272 |  272.419 |              317.798 |              165.677 |            785.866 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3240.02428529755
    time_step_min: 2942
  date: 2020-10-16_01-30-31
  done: false
  episode_len_mean: 785.9453420669578
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 272.7044351815094
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 13740
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.33126793801784515
        entropy_coeff: 0.0005000000000000001
        kl: 0.00451814573413382
        model: {}
        policy_loss: -0.010082538991506832
        total_loss: 4.8893570105234785
        vf_explained_var: 0.991076648235321
        vf_loss: 4.899548768997192
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.650000000000002
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483873051304729
    mean_env_wait_ms: 1.2066577173193813
    mean_inference_ms: 4.412459472426654
    mean_raw_obs_processing_ms: 0.3845389704498615
  time_since_restore: 1729.8201620578766
  time_this_iter_s: 25.568862676620483
  time_total_s: 1729.8201620578766
  timers:
    learn_throughput: 8636.707
    learn_time_ms: 18733.066
    sample_throughput: 23632.518
    sample_time_ms: 6846.16
    update_time_ms: 33.631
  timestamp: 1602811831
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     67 |          1729.82 | 10840064 |  272.704 |              317.798 |              165.677 |            785.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3238.678869991352
    time_step_min: 2942
  date: 2020-10-16_01-30-57
  done: false
  episode_len_mean: 785.9606588032221
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 272.9143186758261
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 164
  episodes_total: 13904
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.34320970873037976
        entropy_coeff: 0.0005000000000000001
        kl: 0.005112749873660505
        model: {}
        policy_loss: -0.01100804218246291
        total_loss: 3.653915266195933
        vf_explained_var: 0.9914707541465759
        vf_loss: 3.665062963962555
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.072413793103443
    gpu_util_percent0: 0.333448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14836560574376292
    mean_env_wait_ms: 1.2067823538005595
    mean_inference_ms: 4.411168538779703
    mean_raw_obs_processing_ms: 0.38446662159193407
  time_since_restore: 1755.4774193763733
  time_this_iter_s: 25.657257318496704
  time_total_s: 1755.4774193763733
  timers:
    learn_throughput: 8638.344
    learn_time_ms: 18729.516
    sample_throughput: 23630.085
    sample_time_ms: 6846.865
    update_time_ms: 31.79
  timestamp: 1602811857
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     68 |          1755.48 | 11001856 |  272.914 |              317.798 |              165.677 |            785.961 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3236.9997868561277
    time_step_min: 2942
  date: 2020-10-16_01-31-22
  done: false
  episode_len_mean: 785.9723463092959
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 273.1579612332643
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 14103
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.34640548129876453
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053304360480979085
        model: {}
        policy_loss: -0.009727492325813122
        total_loss: 5.206940253575643
        vf_explained_var: 0.9899138808250427
        vf_loss: 5.21680764357249
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.213333333333335
    gpu_util_percent0: 0.36033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14833951150076297
    mean_env_wait_ms: 1.2069347616770854
    mean_inference_ms: 4.4096297063749414
    mean_raw_obs_processing_ms: 0.3843791154372896
  time_since_restore: 1781.0876820087433
  time_this_iter_s: 25.610262632369995
  time_total_s: 1781.0876820087433
  timers:
    learn_throughput: 8641.7
    learn_time_ms: 18722.243
    sample_throughput: 23683.174
    sample_time_ms: 6831.517
    update_time_ms: 30.186
  timestamp: 1602811882
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     69 |          1781.09 | 11163648 |  273.158 |              317.798 |              165.677 |            785.972 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3234.9337891578875
    time_step_min: 2942
  date: 2020-10-16_01-31-49
  done: false
  episode_len_mean: 786.0126035791379
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 273.47167940107147
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 258
  episodes_total: 14361
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.31610632687807083
        entropy_coeff: 0.0005000000000000001
        kl: 0.005180867893310885
        model: {}
        policy_loss: -0.009844114615892371
        total_loss: 4.691938559214274
        vf_explained_var: 0.9920149445533752
        vf_loss: 4.701908349990845
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87
    gpu_util_percent0: 0.3630000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.850000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14830764555079137
    mean_env_wait_ms: 1.2071210780645936
    mean_inference_ms: 4.40779556915771
    mean_raw_obs_processing_ms: 0.38427262130029616
  time_since_restore: 1806.8621654510498
  time_this_iter_s: 25.77448344230652
  time_total_s: 1806.8621654510498
  timers:
    learn_throughput: 8638.981
    learn_time_ms: 18728.134
    sample_throughput: 23685.918
    sample_time_ms: 6830.725
    update_time_ms: 30.759
  timestamp: 1602811909
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     70 |          1806.86 | 11325440 |  273.472 |              317.798 |              165.677 |            786.013 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3233.61435070306
    time_step_min: 2942
  date: 2020-10-16_01-32-15
  done: false
  episode_len_mean: 786.0021326362136
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 273.68195021208226
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 175
  episodes_total: 14536
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3203122367461522
        entropy_coeff: 0.0005000000000000001
        kl: 0.004666645700732867
        model: {}
        policy_loss: -0.01171706517440422
        total_loss: 3.8123207092285156
        vf_explained_var: 0.9915096759796143
        vf_loss: 3.824168781439463
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.362068965517242
    gpu_util_percent0: 0.41793103448275865
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14828629077562647
    mean_env_wait_ms: 1.2072303570536584
    mean_inference_ms: 4.406507181682112
    mean_raw_obs_processing_ms: 0.3841994435244379
  time_since_restore: 1832.5463378429413
  time_this_iter_s: 25.68417239189148
  time_total_s: 1832.5463378429413
  timers:
    learn_throughput: 8634.849
    learn_time_ms: 18737.097
    sample_throughput: 23702.796
    sample_time_ms: 6825.861
    update_time_ms: 32.829
  timestamp: 1602811935
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     71 |          1832.55 | 11487232 |  273.682 |              317.798 |              165.677 |            786.002 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3232.17431880109
    time_step_min: 2942
  date: 2020-10-16_01-32-40
  done: false
  episode_len_mean: 785.9649850421539
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 273.89390986283826
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 172
  episodes_total: 14708
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3309631322820981
        entropy_coeff: 0.0005000000000000001
        kl: 0.004985143624556561
        model: {}
        policy_loss: -0.010613858525175601
        total_loss: 3.6648205518722534
        vf_explained_var: 0.9920353889465332
        vf_loss: 3.675584355990092
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.583333333333332
    gpu_util_percent0: 0.38
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826459412932516
    mean_env_wait_ms: 1.2073331668248959
    mean_inference_ms: 4.405260346584555
    mean_raw_obs_processing_ms: 0.38412547885428644
  time_since_restore: 1858.0645215511322
  time_this_iter_s: 25.518183708190918
  time_total_s: 1858.0645215511322
  timers:
    learn_throughput: 8646.109
    learn_time_ms: 18712.694
    sample_throughput: 23807.406
    sample_time_ms: 6795.868
    update_time_ms: 32.531
  timestamp: 1602811960
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     72 |          1858.06 | 11649024 |  273.894 |              317.798 |              165.677 |            785.965 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3230.119863472092
    time_step_min: 2942
  date: 2020-10-16_01-33-06
  done: false
  episode_len_mean: 785.9823647294589
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 274.20767798222704
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 262
  episodes_total: 14970
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3055466338992119
        entropy_coeff: 0.0005000000000000001
        kl: 0.00490261847153306
        model: {}
        policy_loss: -0.008200637511132905
        total_loss: 4.579335172971089
        vf_explained_var: 0.992362916469574
        vf_loss: 4.587680856386821
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.156666666666663
    gpu_util_percent0: 0.321
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482347256525747
    mean_env_wait_ms: 1.2074933950139655
    mean_inference_ms: 4.403512210665872
    mean_raw_obs_processing_ms: 0.38402241991491276
  time_since_restore: 1883.7940366268158
  time_this_iter_s: 25.729515075683594
  time_total_s: 1883.7940366268158
  timers:
    learn_throughput: 8636.905
    learn_time_ms: 18732.636
    sample_throughput: 23860.102
    sample_time_ms: 6780.86
    update_time_ms: 32.494
  timestamp: 1602811986
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     73 |          1883.79 | 11810816 |  274.208 |              317.798 |              165.677 |            785.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3228.4809114927343
    time_step_min: 2942
  date: 2020-10-16_01-33-33
  done: false
  episode_len_mean: 785.9644646624473
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 274.45477720240376
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 15168
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.2969528113802274
        entropy_coeff: 0.0005000000000000001
        kl: 0.005436556258549293
        model: {}
        policy_loss: -0.009357313266567266
        total_loss: 3.4393052458763123
        vf_explained_var: 0.9927964806556702
        vf_loss: 3.4488067428270974
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.636666666666667
    gpu_util_percent0: 0.3466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14821236987956912
    mean_env_wait_ms: 1.2075999258470607
    mean_inference_ms: 4.40218286651624
    mean_raw_obs_processing_ms: 0.38394410559318337
  time_since_restore: 1909.734213590622
  time_this_iter_s: 25.940176963806152
  time_total_s: 1909.734213590622
  timers:
    learn_throughput: 8637.222
    learn_time_ms: 18731.949
    sample_throughput: 23771.496
    sample_time_ms: 6806.134
    update_time_ms: 32.73
  timestamp: 1602812013
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     74 |          1909.73 | 11972608 |  274.455 |              317.798 |              165.677 |            785.964 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3227.1255309416456
    time_step_min: 2942
  date: 2020-10-16_01-33-59
  done: false
  episode_len_mean: 785.9414910964712
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 274.6555747284336
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 163
  episodes_total: 15331
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.30914776027202606
        entropy_coeff: 0.0005000000000000001
        kl: 0.00479302101302892
        model: {}
        policy_loss: -0.010842657589819282
        total_loss: 3.573561429977417
        vf_explained_var: 0.991995096206665
        vf_loss: 3.5845548113187156
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.620000000000005
    gpu_util_percent0: 0.332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14819428281136074
    mean_env_wait_ms: 1.2076813259663153
    mean_inference_ms: 4.401112596943647
    mean_raw_obs_processing_ms: 0.3838803756444738
  time_since_restore: 1935.7922096252441
  time_this_iter_s: 26.057996034622192
  time_total_s: 1935.7922096252441
  timers:
    learn_throughput: 8638.891
    learn_time_ms: 18728.329
    sample_throughput: 23614.989
    sample_time_ms: 6851.242
    update_time_ms: 32.978
  timestamp: 1602812039
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     75 |          1935.79 | 12134400 |  274.656 |              317.798 |              165.677 |            785.941 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3225.0863776691535
    time_step_min: 2942
  date: 2020-10-16_01-34-25
  done: false
  episode_len_mean: 785.8928479712378
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 274.96832539571363
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 245
  episodes_total: 15576
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.2980467254916827
        entropy_coeff: 0.0005000000000000001
        kl: 0.005235202998543779
        model: {}
        policy_loss: -0.008933737524785101
        total_loss: 5.186015446980794
        vf_explained_var: 0.9907950758934021
        vf_loss: 5.195096095403035
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.88666666666667
    gpu_util_percent0: 0.4013333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14816724431971454
    mean_env_wait_ms: 1.2078029915543145
    mean_inference_ms: 4.39955832945313
    mean_raw_obs_processing_ms: 0.38378818526519143
  time_since_restore: 1961.5738446712494
  time_this_iter_s: 25.78163504600525
  time_total_s: 1961.5738446712494
  timers:
    learn_throughput: 8636.325
    learn_time_ms: 18733.894
    sample_throughput: 23616.176
    sample_time_ms: 6850.897
    update_time_ms: 34.65
  timestamp: 1602812065
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     76 |          1961.57 | 12296192 |  274.968 |              317.798 |              165.677 |            785.893 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3223.327393785669
    time_step_min: 2942
  date: 2020-10-16_01-34-51
  done: false
  episode_len_mean: 785.8466894543614
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 275.25340184986976
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 15798
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.27664656440416974
        entropy_coeff: 0.0005000000000000001
        kl: 0.004761220188811421
        model: {}
        policy_loss: -0.007907829436589964
        total_loss: 2.923918386300405
        vf_explained_var: 0.9942214488983154
        vf_loss: 2.931962788105011
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.55172413793104
    gpu_util_percent0: 0.34689655172413797
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14814352373024242
    mean_env_wait_ms: 1.2079054605847415
    mean_inference_ms: 4.398175191980529
    mean_raw_obs_processing_ms: 0.3837048113872048
  time_since_restore: 1987.1896886825562
  time_this_iter_s: 25.615844011306763
  time_total_s: 1987.1896886825562
  timers:
    learn_throughput: 8632.341
    learn_time_ms: 18742.54
    sample_throughput: 23641.832
    sample_time_ms: 6843.463
    update_time_ms: 36.357
  timestamp: 1602812091
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     77 |          1987.19 | 12457984 |  275.253 |              317.798 |              165.677 |            785.847 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3221.937609841828
    time_step_min: 2942
  date: 2020-10-16_01-35-17
  done: false
  episode_len_mean: 785.8052631578947
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 275.4586750968329
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 15960
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.290430227915446
        entropy_coeff: 0.0005000000000000001
        kl: 0.004963470622897148
        model: {}
        policy_loss: -0.011461609130492434
        total_loss: 3.2642138799031577
        vf_explained_var: 0.9922669529914856
        vf_loss: 3.2758195996284485
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.710000000000004
    gpu_util_percent0: 0.3453333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14812599578419583
    mean_env_wait_ms: 1.2079711512924698
    mean_inference_ms: 4.397176322373093
    mean_raw_obs_processing_ms: 0.38364437474607455
  time_since_restore: 2012.813583612442
  time_this_iter_s: 25.623894929885864
  time_total_s: 2012.813583612442
  timers:
    learn_throughput: 8638.243
    learn_time_ms: 18729.734
    sample_throughput: 23622.958
    sample_time_ms: 6848.93
    update_time_ms: 38.342
  timestamp: 1602812117
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     78 |          2012.81 | 12619776 |  275.459 |              317.798 |              165.677 |            785.805 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3220.2338080495356
    time_step_min: 2942
  date: 2020-10-16_01-35-44
  done: false
  episode_len_mean: 785.7675856100877
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 275.70733668743304
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 218
  episodes_total: 16178
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2907758379975955
        entropy_coeff: 0.0005000000000000001
        kl: 0.004659340716898441
        model: {}
        policy_loss: -0.01010728104544493
        total_loss: 4.916222969690959
        vf_explained_var: 0.9909214377403259
        vf_loss: 4.926475167274475
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.606666666666673
    gpu_util_percent0: 0.3416666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14810374072596735
    mean_env_wait_ms: 1.2080627521848224
    mean_inference_ms: 4.395861375873534
    mean_raw_obs_processing_ms: 0.3835667827026735
  time_since_restore: 2038.9785161018372
  time_this_iter_s: 26.16493248939514
  time_total_s: 2038.9785161018372
  timers:
    learn_throughput: 8625.442
    learn_time_ms: 18757.533
    sample_throughput: 23533.112
    sample_time_ms: 6875.079
    update_time_ms: 39.494
  timestamp: 1602812144
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     79 |          2038.98 | 12781568 |  275.707 |              317.798 |              165.677 |            785.768 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3218.2997620935766
    time_step_min: 2942
  date: 2020-10-16_01-36-10
  done: false
  episode_len_mean: 785.7324767066561
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 275.9981687651744
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 243
  episodes_total: 16421
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.26534537474314374
        entropy_coeff: 0.0005000000000000001
        kl: 0.005787815627021094
        model: {}
        policy_loss: -0.008603085453311602
        total_loss: 3.8923116525014243
        vf_explained_var: 0.9929180145263672
        vf_loss: 3.9010471304257712
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.916666666666664
    gpu_util_percent0: 0.33299999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14807895209883787
    mean_env_wait_ms: 1.2081529403914888
    mean_inference_ms: 4.394482342753303
    mean_raw_obs_processing_ms: 0.3834814046847391
  time_since_restore: 2064.84139251709
  time_this_iter_s: 25.862876415252686
  time_total_s: 2064.84139251709
  timers:
    learn_throughput: 8631.994
    learn_time_ms: 18743.293
    sample_throughput: 23462.539
    sample_time_ms: 6895.758
    update_time_ms: 39.56
  timestamp: 1602812170
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     80 |          2064.84 | 12943360 |  275.998 |              317.798 |              165.677 |            785.732 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3216.9125762241138
    time_step_min: 2942
  date: 2020-10-16_01-36-36
  done: false
  episode_len_mean: 785.7410041588813
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 276.2069930819252
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 170
  episodes_total: 16591
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2716033384203911
        entropy_coeff: 0.0005000000000000001
        kl: 0.004915759743501742
        model: {}
        policy_loss: -0.009365745064618144
        total_loss: 3.0018479426701865
        vf_explained_var: 0.9930481910705566
        vf_loss: 3.0113492012023926
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.860000000000003
    gpu_util_percent0: 0.358
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806282307063603
    mean_env_wait_ms: 1.2082054600431527
    mean_inference_ms: 4.393496661907253
    mean_raw_obs_processing_ms: 0.3834220148323112
  time_since_restore: 2090.8382744789124
  time_this_iter_s: 25.99688196182251
  time_total_s: 2090.8382744789124
  timers:
    learn_throughput: 8629.402
    learn_time_ms: 18748.923
    sample_throughput: 23372.923
    sample_time_ms: 6922.198
    update_time_ms: 37.921
  timestamp: 1602812196
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     81 |          2090.84 | 13105152 |  276.207 |              317.798 |              165.677 |            785.741 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3215.4304046794796
    time_step_min: 2942
  date: 2020-10-16_01-37-02
  done: false
  episode_len_mean: 785.725181742343
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 276.43181547328845
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 191
  episodes_total: 16782
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.2780342971285184
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047616838322331505
        model: {}
        policy_loss: -0.009352372610010207
        total_loss: 4.1234805782636
        vf_explained_var: 0.9917870163917542
        vf_loss: 4.132971882820129
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98666666666667
    gpu_util_percent0: 0.28733333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480430367063363
    mean_env_wait_ms: 1.2082610513571275
    mean_inference_ms: 4.392392400341953
    mean_raw_obs_processing_ms: 0.3833556958555886
  time_since_restore: 2116.574226617813
  time_this_iter_s: 25.735952138900757
  time_total_s: 2116.574226617813
  timers:
    learn_throughput: 8633.258
    learn_time_ms: 18740.55
    sample_throughput: 23303.545
    sample_time_ms: 6942.806
    update_time_ms: 35.721
  timestamp: 1602812222
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     82 |          2116.57 | 13266944 |  276.432 |              317.798 |              165.677 |            785.725 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3213.4538294245576
    time_step_min: 2942
  date: 2020-10-16_01-37-28
  done: false
  episode_len_mean: 785.7449680183088
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 276.734377398775
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 259
  episodes_total: 17041
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2593287130196889
        entropy_coeff: 0.0005000000000000001
        kl: 0.004659708434094985
        model: {}
        policy_loss: -0.008873817008861806
        total_loss: 4.259411493937175
        vf_explained_var: 0.9925722479820251
        vf_loss: 4.268415053685506
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03666666666667
    gpu_util_percent0: 0.3476666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14801916115459676
    mean_env_wait_ms: 1.208340118451876
    mean_inference_ms: 4.391033266916393
    mean_raw_obs_processing_ms: 0.38327044407345207
  time_since_restore: 2142.355238199234
  time_this_iter_s: 25.7810115814209
  time_total_s: 2142.355238199234
  timers:
    learn_throughput: 8634.513
    learn_time_ms: 18737.825
    sample_throughput: 23278.722
    sample_time_ms: 6950.21
    update_time_ms: 35.544
  timestamp: 1602812248
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     83 |          2142.36 | 13428736 |  276.734 |              317.798 |              165.677 |            785.745 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3212.0624636501107
    time_step_min: 2942
  date: 2020-10-16_01-37-54
  done: false
  episode_len_mean: 785.7514806642666
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 276.94040861524303
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 17222
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.2544606688121955
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049046853091567755
        model: {}
        policy_loss: -0.00876463590914985
        total_loss: 3.397805949052175
        vf_explained_var: 0.9926281571388245
        vf_loss: 3.4066977898279824
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.420689655172417
    gpu_util_percent0: 0.3724137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480034203087232
    mean_env_wait_ms: 1.2083866709787632
    mean_inference_ms: 4.3900532391290445
    mean_raw_obs_processing_ms: 0.38321191639557345
  time_since_restore: 2167.882865667343
  time_this_iter_s: 25.52762746810913
  time_total_s: 2167.882865667343
  timers:
    learn_throughput: 8636.522
    learn_time_ms: 18733.468
    sample_throughput: 23409.143
    sample_time_ms: 6911.487
    update_time_ms: 36.477
  timestamp: 1602812274
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     84 |          2167.88 | 13590528 |   276.94 |              317.798 |              165.677 |            785.751 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3210.8620630075447
    time_step_min: 2942
  date: 2020-10-16_01-38-21
  done: false
  episode_len_mean: 785.7808636651141
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 277.12649872887914
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 169
  episodes_total: 17391
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2648752157886823
        entropy_coeff: 0.0005000000000000001
        kl: 0.005617462059793373
        model: {}
        policy_loss: -0.009434527669024343
        total_loss: 3.6594002644220986
        vf_explained_var: 0.9920105934143066
        vf_loss: 3.6689672072728476
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.279999999999998
    gpu_util_percent0: 0.325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479879607942415
    mean_env_wait_ms: 1.2084266922122873
    mean_inference_ms: 4.389150649544716
    mean_raw_obs_processing_ms: 0.3831563693250547
  time_since_restore: 2193.6983513832092
  time_this_iter_s: 25.81548571586609
  time_total_s: 2193.6983513832092
  timers:
    learn_throughput: 8629.279
    learn_time_ms: 18749.19
    sample_throughput: 23547.638
    sample_time_ms: 6870.838
    update_time_ms: 36.7
  timestamp: 1602812301
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     85 |           2193.7 | 13752320 |  277.126 |              317.798 |              165.677 |            785.781 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3209.1355710796843
    time_step_min: 2942
  date: 2020-10-16_01-38-46
  done: false
  episode_len_mean: 785.8791607598525
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 277.39138478633794
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 244
  episodes_total: 17635
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2523770493765672
        entropy_coeff: 0.0005000000000000001
        kl: 0.004732664519300063
        model: {}
        policy_loss: -0.009494934978041178
        total_loss: 3.6212570667266846
        vf_explained_var: 0.993767499923706
        vf_loss: 3.630878190199534
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.01
    gpu_util_percent0: 0.34766666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14796619966086147
    mean_env_wait_ms: 1.2084797943108216
    mean_inference_ms: 4.38794372226657
    mean_raw_obs_processing_ms: 0.38308131671116663
  time_since_restore: 2219.1906428337097
  time_this_iter_s: 25.49229145050049
  time_total_s: 2219.1906428337097
  timers:
    learn_throughput: 8642.79
    learn_time_ms: 18719.88
    sample_throughput: 23549.967
    sample_time_ms: 6870.158
    update_time_ms: 37.084
  timestamp: 1602812326
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     86 |          2219.19 | 13914112 |  277.391 |              317.798 |              165.677 |            785.879 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3207.5718453683444
    time_step_min: 2942
  date: 2020-10-16_01-39-12
  done: false
  episode_len_mean: 785.9679009579295
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 277.62622980689184
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 17851
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.23134482031067213
        entropy_coeff: 0.0005000000000000001
        kl: 0.00436233247940739
        model: {}
        policy_loss: -0.009052471694303676
        total_loss: 2.8776000340779624
        vf_explained_var: 0.9944209456443787
        vf_loss: 2.886768182118734
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.2
    gpu_util_percent0: 0.33896551724137935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479482626849898
    mean_env_wait_ms: 1.2085162615339922
    mean_inference_ms: 4.386834791449804
    mean_raw_obs_processing_ms: 0.38301455824527064
  time_since_restore: 2244.689036846161
  time_this_iter_s: 25.498394012451172
  time_total_s: 2244.689036846161
  timers:
    learn_throughput: 8647.944
    learn_time_ms: 18708.725
    sample_throughput: 23545.167
    sample_time_ms: 6871.559
    update_time_ms: 35.138
  timestamp: 1602812352
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     87 |          2244.69 | 14075904 |  277.626 |              317.798 |              165.677 |            785.968 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3206.3540693795862
    time_step_min: 2942
  date: 2020-10-16_01-39-38
  done: false
  episode_len_mean: 786.028696714032
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 277.8067010020273
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 165
  episodes_total: 18016
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.24300405383110046
        entropy_coeff: 0.0005000000000000001
        kl: 0.004606607835739851
        model: {}
        policy_loss: -0.0074194288851382835
        total_loss: 2.8293345173199973
        vf_explained_var: 0.9936473369598389
        vf_loss: 2.8368754188219705
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280000000000005
    gpu_util_percent0: 0.3223333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14793432738276244
    mean_env_wait_ms: 1.2085398370928468
    mean_inference_ms: 4.385997737964077
    mean_raw_obs_processing_ms: 0.38296302969242996
  time_since_restore: 2270.4887013435364
  time_this_iter_s: 25.79966449737549
  time_total_s: 2270.4887013435364
  timers:
    learn_throughput: 8643.18
    learn_time_ms: 18719.037
    sample_throughput: 23511.935
    sample_time_ms: 6881.271
    update_time_ms: 33.004
  timestamp: 1602812378
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     88 |          2270.49 | 14237696 |  277.807 |              317.798 |              165.677 |            786.029 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3204.880536322673
    time_step_min: 2942
  date: 2020-10-16_01-40-04
  done: false
  episode_len_mean: 786.1340941512126
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.02559336368176
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 210
  episodes_total: 18226
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.24540683875481287
        entropy_coeff: 0.0005000000000000001
        kl: 0.004586299997754395
        model: {}
        policy_loss: -0.01081360654400972
        total_loss: 3.405385891596476
        vf_explained_var: 0.9936063289642334
        vf_loss: 3.4163222511609397
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.656666666666673
    gpu_util_percent0: 0.29133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14791654172621904
    mean_env_wait_ms: 1.2085687566700372
    mean_inference_ms: 4.384983602780637
    mean_raw_obs_processing_ms: 0.38290067258473204
  time_since_restore: 2296.0560159683228
  time_this_iter_s: 25.567314624786377
  time_total_s: 2296.0560159683228
  timers:
    learn_throughput: 8655.36
    learn_time_ms: 18692.694
    sample_throughput: 23633.863
    sample_time_ms: 6845.77
    update_time_ms: 33.443
  timestamp: 1602812404
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     89 |          2296.06 | 14399488 |  278.026 |              317.798 |              165.677 |            786.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3203.2486712224754
    time_step_min: 2942
  date: 2020-10-16_01-40-30
  done: false
  episode_len_mean: 786.212444492581
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.2610011082338
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 240
  episodes_total: 18466
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.22144933665792146
        entropy_coeff: 0.0005000000000000001
        kl: 0.004486944526433945
        model: {}
        policy_loss: -0.007682374775564919
        total_loss: 3.3583309253056846
        vf_explained_var: 0.9941323399543762
        vf_loss: 3.366123914718628
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.23793103448276
    gpu_util_percent0: 0.3182758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14789687210072122
    mean_env_wait_ms: 1.2085940045156576
    mean_inference_ms: 4.383862318956645
    mean_raw_obs_processing_ms: 0.3828293162912536
  time_since_restore: 2321.61448264122
  time_this_iter_s: 25.55846667289734
  time_total_s: 2321.61448264122
  timers:
    learn_throughput: 8655.611
    learn_time_ms: 18692.153
    sample_throughput: 23735.039
    sample_time_ms: 6816.589
    update_time_ms: 33.65
  timestamp: 1602812430
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     90 |          2321.61 | 14561280 |  278.261 |              317.798 |              165.677 |            786.212 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3202.0811667382895
    time_step_min: 2942
  date: 2020-10-16_01-40-56
  done: false
  episode_len_mean: 786.2794464707144
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.43792895702353
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 178
  episodes_total: 18644
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.22619690001010895
        entropy_coeff: 0.0005000000000000001
        kl: 0.004538902974066635
        model: {}
        policy_loss: -0.009121924629046893
        total_loss: 3.2765854597091675
        vf_explained_var: 0.9929046630859375
        vf_loss: 3.285820504029592
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.470000000000002
    gpu_util_percent0: 0.29900000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788383289630475
    mean_env_wait_ms: 1.2086087655698892
    mean_inference_ms: 4.383007017250708
    mean_raw_obs_processing_ms: 0.3827786998456735
  time_since_restore: 2347.2506461143494
  time_this_iter_s: 25.636163473129272
  time_total_s: 2347.2506461143494
  timers:
    learn_throughput: 8660.612
    learn_time_ms: 18681.359
    sample_throughput: 23830.386
    sample_time_ms: 6789.315
    update_time_ms: 34.748
  timestamp: 1602812456
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     91 |          2347.25 | 14723072 |  278.438 |              317.798 |              165.677 |            786.279 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3200.830274009045
    time_step_min: 2942
  date: 2020-10-16_01-41-22
  done: false
  episode_len_mean: 786.3159963873984
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.6219153764709
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 18823
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.2380774455765883
        entropy_coeff: 0.0005000000000000001
        kl: 0.004809639339024822
        model: {}
        policy_loss: -0.00822032520469899
        total_loss: 3.004819949467977
        vf_explained_var: 0.9936773777008057
        vf_loss: 3.013159394264221
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.82413793103449
    gpu_util_percent0: 0.32034482758620686
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787010969079334
    mean_env_wait_ms: 1.2086220945825887
    mean_inference_ms: 4.382169676403128
    mean_raw_obs_processing_ms: 0.3827265974506532
  time_since_restore: 2372.2724010944366
  time_this_iter_s: 25.02175498008728
  time_total_s: 2372.2724010944366
  timers:
    learn_throughput: 8684.919
    learn_time_ms: 18629.074
    sample_throughput: 23863.785
    sample_time_ms: 6779.813
    update_time_ms: 35.359
  timestamp: 1602812482
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     92 |          2372.27 | 14884864 |  278.622 |              317.798 |              165.677 |            786.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3199.0758649656113
    time_step_min: 2942
  date: 2020-10-16_01-41-48
  done: false
  episode_len_mean: 786.3570642201835
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.8779909183578
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 252
  episodes_total: 19075
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.22229414929946265
        entropy_coeff: 0.0005000000000000001
        kl: 0.004338357403563957
        model: {}
        policy_loss: -0.007636490821217497
        total_loss: 3.7447853287061057
        vf_explained_var: 0.9936156868934631
        vf_loss: 3.752533038457235
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89333333333333
    gpu_util_percent0: 0.3276666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14785024383852785
    mean_env_wait_ms: 1.2086331863126663
    mean_inference_ms: 4.381059922756928
    mean_raw_obs_processing_ms: 0.38265707833725904
  time_since_restore: 2397.9973318576813
  time_this_iter_s: 25.72493076324463
  time_total_s: 2397.9973318576813
  timers:
    learn_throughput: 8689.198
    learn_time_ms: 18619.899
    sample_throughput: 23859.067
    sample_time_ms: 6781.154
    update_time_ms: 35.759
  timestamp: 1602812508
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     93 |             2398 | 15046656 |  278.878 |              317.798 |              165.677 |            786.357 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3197.790709757872
    time_step_min: 2942
  date: 2020-10-16_01-42-14
  done: false
  episode_len_mean: 786.3781259728131
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 279.0753875792269
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 19274
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.21226180344820023
        entropy_coeff: 0.0005000000000000001
        kl: 0.004013098815145592
        model: {}
        policy_loss: -0.00900954079982815
        total_loss: 3.312894284725189
        vf_explained_var: 0.9932640194892883
        vf_loss: 3.3220099409421286
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49666666666667
    gpu_util_percent0: 0.3006666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14783626123345492
    mean_env_wait_ms: 1.2086408784367648
    mean_inference_ms: 4.380170135834469
    mean_raw_obs_processing_ms: 0.38260377428468473
  time_since_restore: 2423.8017518520355
  time_this_iter_s: 25.804419994354248
  time_total_s: 2423.8017518520355
  timers:
    learn_throughput: 8681.992
    learn_time_ms: 18635.354
    sample_throughput: 23848.438
    sample_time_ms: 6784.176
    update_time_ms: 35.113
  timestamp: 1602812534
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     94 |           2423.8 | 15208448 |  279.075 |              317.798 |              165.677 |            786.378 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3196.661001236094
    time_step_min: 2942
  date: 2020-10-16_01-42-40
  done: false
  episode_len_mean: 786.3810944250155
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 279.24538431008284
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 170
  episodes_total: 19444
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.2249310463666916
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045103810261934996
        model: {}
        policy_loss: -0.007979410632591074
        total_loss: 3.3010860482851663
        vf_explained_var: 0.9927241802215576
        vf_loss: 3.309178034464518
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.110344827586204
    gpu_util_percent0: 0.41000000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782352482627187
    mean_env_wait_ms: 1.2086410892171846
    mean_inference_ms: 4.379401248796293
    mean_raw_obs_processing_ms: 0.38255694573748655
  time_since_restore: 2449.566656112671
  time_this_iter_s: 25.764904260635376
  time_total_s: 2449.566656112671
  timers:
    learn_throughput: 8680.705
    learn_time_ms: 18638.117
    sample_throughput: 23871.865
    sample_time_ms: 6777.518
    update_time_ms: 33.308
  timestamp: 1602812560
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     95 |          2449.57 | 15370240 |  279.245 |              317.798 |              165.677 |            786.381 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3195.1431988193985
    time_step_min: 2942
  date: 2020-10-16_01-43-06
  done: false
  episode_len_mean: 786.3685146602978
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 279.4854192619831
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 19679
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.22384763633211455
        entropy_coeff: 0.0005000000000000001
        kl: 0.004928315756842494
        model: {}
        policy_loss: -0.008493121097368809
        total_loss: 3.06818288564682
        vf_explained_var: 0.9944605231285095
        vf_loss: 3.0767878691355386
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.246666666666666
    gpu_util_percent0: 0.3446666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478061036896256
    mean_env_wait_ms: 1.2086373751324584
    mean_inference_ms: 4.378407904759818
    mean_raw_obs_processing_ms: 0.38249518463390814
  time_since_restore: 2475.299699306488
  time_this_iter_s: 25.73304319381714
  time_total_s: 2475.299699306488
  timers:
    learn_throughput: 8674.213
    learn_time_ms: 18652.066
    sample_throughput: 23837.1
    sample_time_ms: 6787.403
    update_time_ms: 32.804
  timestamp: 1602812586
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     96 |           2475.3 | 15532032 |  279.485 |              317.798 |              165.677 |            786.369 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3193.613414511422
    time_step_min: 2936
  date: 2020-10-16_01-43-33
  done: false
  episode_len_mean: 786.345995377349
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 279.7114390818038
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 19902
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.2050087774793307
        entropy_coeff: 0.0005000000000000001
        kl: 0.004004717067194481
        model: {}
        policy_loss: -0.008170259432517923
        total_loss: 2.8481951554616294
        vf_explained_var: 0.9946429133415222
        vf_loss: 2.856467843055725
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.963333333333342
    gpu_util_percent0: 0.31266666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14779123373852976
    mean_env_wait_ms: 1.2086411710795104
    mean_inference_ms: 4.377483219628959
    mean_raw_obs_processing_ms: 0.38243786580413597
  time_since_restore: 2501.386937856674
  time_this_iter_s: 26.087238550186157
  time_total_s: 2501.386937856674
  timers:
    learn_throughput: 8666.859
    learn_time_ms: 18667.892
    sample_throughput: 23699.139
    sample_time_ms: 6826.915
    update_time_ms: 34.803
  timestamp: 1602812613
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     97 |          2501.39 | 15693824 |  279.711 |              318.707 |              165.677 |            786.346 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3192.5208821915076
    time_step_min: 2936
  date: 2020-10-16_01-43-59
  done: false
  episode_len_mean: 786.3337485674423
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 279.88309675055393
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 167
  episodes_total: 20069
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.21761814504861832
        entropy_coeff: 0.0005000000000000001
        kl: 0.004347234149463475
        model: {}
        policy_loss: -0.009369057399453595
        total_loss: 2.5891530513763428
        vf_explained_var: 0.994077205657959
        vf_loss: 2.598630905151367
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.726666666666667
    gpu_util_percent0: 0.3473333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14777984247610482
    mean_env_wait_ms: 1.2086361845492182
    mean_inference_ms: 4.376779310634501
    mean_raw_obs_processing_ms: 0.3823948354190541
  time_since_restore: 2527.1113855838776
  time_this_iter_s: 25.72444772720337
  time_total_s: 2527.1113855838776
  timers:
    learn_throughput: 8668.783
    learn_time_ms: 18663.75
    sample_throughput: 23721.25
    sample_time_ms: 6820.551
    update_time_ms: 36.753
  timestamp: 1602812639
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     98 |          2527.11 | 15855616 |  279.883 |              318.707 |              165.677 |            786.334 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3191.188209736348
    time_step_min: 2936
  date: 2020-10-16_01-44-25
  done: false
  episode_len_mean: 786.2985898826546
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.09027211270575
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 213
  episodes_total: 20282
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.22063125918308893
        entropy_coeff: 0.0005000000000000001
        kl: 0.004395618801936507
        model: {}
        policy_loss: -0.010148239402042236
        total_loss: 3.3593016862869263
        vf_explained_var: 0.9935986399650574
        vf_loss: 3.369560261567434
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.540000000000003
    gpu_util_percent0: 0.3223333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14776491733520952
    mean_env_wait_ms: 1.2086261301523709
    mean_inference_ms: 4.375907513424143
    mean_raw_obs_processing_ms: 0.3823392908859904
  time_since_restore: 2552.9672105312347
  time_this_iter_s: 25.855824947357178
  time_total_s: 2552.9672105312347
  timers:
    learn_throughput: 8659.18
    learn_time_ms: 18684.448
    sample_throughput: 23692.044
    sample_time_ms: 6828.959
    update_time_ms: 35.743
  timestamp: 1602812665
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |     99 |          2552.97 | 16017408 |   280.09 |              318.707 |              165.677 |            786.299 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3189.571414629386
    time_step_min: 2936
  date: 2020-10-16_01-44-51
  done: false
  episode_len_mean: 786.253204034891
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.3382083591137
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 239
  episodes_total: 20521
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.19899060080448785
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054977779897550745
        model: {}
        policy_loss: -0.00914790567185264
        total_loss: 2.60590926806132
        vf_explained_var: 0.995249330997467
        vf_loss: 2.6151567300160727
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.44666666666667
    gpu_util_percent0: 0.3923333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477493055809708
    mean_env_wait_ms: 1.2086135129658855
    mean_inference_ms: 4.374976431842593
    mean_raw_obs_processing_ms: 0.3822814614450019
  time_since_restore: 2578.790751695633
  time_this_iter_s: 25.823541164398193
  time_total_s: 2578.790751695633
  timers:
    learn_throughput: 8653.67
    learn_time_ms: 18696.346
    sample_throughput: 23637.052
    sample_time_ms: 6844.847
    update_time_ms: 33.323
  timestamp: 1602812691
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    100 |          2578.79 | 16179200 |  280.338 |              318.707 |              165.677 |            786.253 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3188.356313497823
    time_step_min: 2936
  date: 2020-10-16_01-45-18
  done: false
  episode_len_mean: 786.2260604889361
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.5185764300654
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 177
  episodes_total: 20698
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.20405827338496843
        entropy_coeff: 0.0005000000000000001
        kl: 0.004492445072780053
        model: {}
        policy_loss: -0.008968284473667154
        total_loss: 2.520637015501658
        vf_explained_var: 0.9943375587463379
        vf_loss: 2.529707392056783
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.483870967741932
    gpu_util_percent0: 0.3383870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14773819517276388
    mean_env_wait_ms: 1.2086041728367372
    mean_inference_ms: 4.374270553063901
    mean_raw_obs_processing_ms: 0.3822388223293401
  time_since_restore: 2604.834440469742
  time_this_iter_s: 26.043688774108887
  time_total_s: 2604.834440469742
  timers:
    learn_throughput: 8646.389
    learn_time_ms: 18712.088
    sample_throughput: 23558.585
    sample_time_ms: 6867.645
    update_time_ms: 33.413
  timestamp: 1602812718
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    101 |          2604.83 | 16340992 |  280.519 |              318.707 |              165.677 |            786.226 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3187.0574139748874
    time_step_min: 2936
  date: 2020-10-16_01-45-44
  done: false
  episode_len_mean: 786.187900832775
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.71924035995056
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 20894
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.21233457823594412
        entropy_coeff: 0.0005000000000000001
        kl: 0.004587849175247054
        model: {}
        policy_loss: -0.00771534312904502
        total_loss: 2.953911562760671
        vf_explained_var: 0.9940600395202637
        vf_loss: 2.961733063062032
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.29310344827586
    gpu_util_percent0: 0.35068965517241374
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477257103739726
    mean_env_wait_ms: 1.2085918375478863
    mean_inference_ms: 4.373511549850809
    mean_raw_obs_processing_ms: 0.3821920037733119
  time_since_restore: 2630.586882352829
  time_this_iter_s: 25.752441883087158
  time_total_s: 2630.586882352829
  timers:
    learn_throughput: 8615.831
    learn_time_ms: 18778.456
    sample_throughput: 23541.564
    sample_time_ms: 6872.61
    update_time_ms: 33.166
  timestamp: 1602812744
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    102 |          2630.59 | 16502784 |  280.719 |              318.707 |              165.677 |            786.188 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3185.383810155362
    time_step_min: 2936
  date: 2020-10-16_01-46-10
  done: false
  episode_len_mean: 786.1320245979186
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.9684020909186
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 246
  episodes_total: 21140
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.19720646987358728
        entropy_coeff: 0.0005000000000000001
        kl: 0.004155141243245453
        model: {}
        policy_loss: -0.00845454789911552
        total_loss: 2.8708957632382712
        vf_explained_var: 0.994846522808075
        vf_loss: 2.8794489900271096
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.82666666666667
    gpu_util_percent0: 0.36300000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14770950945757974
    mean_env_wait_ms: 1.2085675798274593
    mean_inference_ms: 4.3726009688610405
    mean_raw_obs_processing_ms: 0.38213374084003476
  time_since_restore: 2656.2531225681305
  time_this_iter_s: 25.666240215301514
  time_total_s: 2656.2531225681305
  timers:
    learn_throughput: 8619.101
    learn_time_ms: 18771.331
    sample_throughput: 23530.261
    sample_time_ms: 6875.912
    update_time_ms: 30.698
  timestamp: 1602812770
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    103 |          2656.25 | 16664576 |  280.968 |              318.707 |              165.677 |            786.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3184.148584573494
    time_step_min: 2936
  date: 2020-10-16_01-46-36
  done: false
  episode_len_mean: 786.0835482207324
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 281.152766352635
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 189
  episodes_total: 21329
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.19045389195283255
        entropy_coeff: 0.0005000000000000001
        kl: 0.004448773960272471
        model: {}
        policy_loss: -0.010119000047173662
        total_loss: 2.200990895430247
        vf_explained_var: 0.9952632784843445
        vf_loss: 2.2112051248550415
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.44666666666667
    gpu_util_percent0: 0.3093333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476985079770804
    mean_env_wait_ms: 1.2085503109835782
    mean_inference_ms: 4.371887669782289
    mean_raw_obs_processing_ms: 0.38208975558256264
  time_since_restore: 2681.963324546814
  time_this_iter_s: 25.71020197868347
  time_total_s: 2681.963324546814
  timers:
    learn_throughput: 8623.375
    learn_time_ms: 18762.028
    sample_throughput: 23504.739
    sample_time_ms: 6883.378
    update_time_ms: 31.189
  timestamp: 1602812796
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    104 |          2681.96 | 16826368 |  281.153 |              318.707 |              165.677 |            786.084 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3182.9835668730507
    time_step_min: 2936
  date: 2020-10-16_01-47-02
  done: false
  episode_len_mean: 786.0381700683434
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 281.3386541034502
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 21509
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.2013479694724083
        entropy_coeff: 0.0005000000000000001
        kl: 0.004613580415025353
        model: {}
        policy_loss: -0.00932157018299525
        total_loss: 2.9009637435277305
        vf_explained_var: 0.9937450885772705
        vf_loss: 2.9103859464327493
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.003333333333337
    gpu_util_percent0: 0.3403333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14768703978026507
    mean_env_wait_ms: 1.2085316886812376
    mean_inference_ms: 4.371215353366877
    mean_raw_obs_processing_ms: 0.3820473527209176
  time_since_restore: 2707.542938709259
  time_this_iter_s: 25.57961416244507
  time_total_s: 2707.542938709259
  timers:
    learn_throughput: 8639.242
    learn_time_ms: 18727.57
    sample_throughput: 23480.286
    sample_time_ms: 6890.546
    update_time_ms: 37.955
  timestamp: 1602812822
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    105 |          2707.54 | 16988160 |  281.339 |              318.707 |              165.677 |            786.038 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3181.3797165470273
    time_step_min: 2934
  date: 2020-10-16_01-47-28
  done: false
  episode_len_mean: 785.9590533088235
  episode_reward_max: 319.0101010101008
  episode_reward_mean: 281.5944230912061
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 251
  episodes_total: 21760
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.19328687836726507
        entropy_coeff: 0.0005000000000000001
        kl: 0.004194587546711166
        model: {}
        policy_loss: -0.007614445697981864
        total_loss: 2.6770079930623374
        vf_explained_var: 0.9951998591423035
        vf_loss: 2.6847190459569297
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.320689655172416
    gpu_util_percent0: 0.32999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14767226031757877
    mean_env_wait_ms: 1.2085038669588342
    mean_inference_ms: 4.370343587656189
    mean_raw_obs_processing_ms: 0.3819929018250755
  time_since_restore: 2733.168889760971
  time_this_iter_s: 25.625951051712036
  time_total_s: 2733.168889760971
  timers:
    learn_throughput: 8641.773
    learn_time_ms: 18722.084
    sample_throughput: 23499.367
    sample_time_ms: 6884.951
    update_time_ms: 37.596
  timestamp: 1602812848
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    106 |          2733.17 | 17149952 |  281.594 |               319.01 |              165.677 |            785.959 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3180.01595914459
    time_step_min: 2934
  date: 2020-10-16_01-47-54
  done: false
  episode_len_mean: 785.8881096589098
  episode_reward_max: 319.0101010101008
  episode_reward_mean: 281.7961057820795
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 21959
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.17738542829950651
        entropy_coeff: 0.0005000000000000001
        kl: 0.004116727815320094
        model: {}
        policy_loss: -0.007019599470368121
        total_loss: 1.9695641497770946
        vf_explained_var: 0.9958812594413757
        vf_loss: 1.9766724308331807
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90666666666667
    gpu_util_percent0: 0.3433333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476608579801831
    mean_env_wait_ms: 1.2084803432503346
    mean_inference_ms: 4.369633027414997
    mean_raw_obs_processing_ms: 0.38194866790773063
  time_since_restore: 2758.6777806282043
  time_this_iter_s: 25.508890867233276
  time_total_s: 2758.6777806282043
  timers:
    learn_throughput: 8653.491
    learn_time_ms: 18696.731
    sample_throughput: 23605.785
    sample_time_ms: 6853.913
    update_time_ms: 36.243
  timestamp: 1602812874
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    107 |          2758.68 | 17311744 |  281.796 |               319.01 |              165.677 |            785.888 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3178.8184285520424
    time_step_min: 2934
  date: 2020-10-16_01-48-21
  done: false
  episode_len_mean: 785.8128303591596
  episode_reward_max: 319.0101010101008
  episode_reward_mean: 281.97153600609664
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 176
  episodes_total: 22135
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.1931574965516726
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041800564310203
        model: {}
        policy_loss: -0.008269173985657593
        total_loss: 2.717939575513204
        vf_explained_var: 0.9939755797386169
        vf_loss: 2.7263052463531494
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.443333333333335
    gpu_util_percent0: 0.3456666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476504599679993
    mean_env_wait_ms: 1.2084574463965594
    mean_inference_ms: 4.368996358291588
    mean_raw_obs_processing_ms: 0.38190858396580996
  time_since_restore: 2784.386672973633
  time_this_iter_s: 25.708892345428467
  time_total_s: 2784.386672973633
  timers:
    learn_throughput: 8652.517
    learn_time_ms: 18698.836
    sample_throughput: 23621.304
    sample_time_ms: 6849.41
    update_time_ms: 36.373
  timestamp: 1602812901
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    108 |          2784.39 | 17473536 |  281.972 |               319.01 |              165.677 |            785.813 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3177.2293027834958
    time_step_min: 2934
  date: 2020-10-16_01-48-47
  done: false
  episode_len_mean: 785.7118530437115
  episode_reward_max: 319.31313131313163
  episode_reward_mean: 282.21284445419593
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 239
  episodes_total: 22374
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.19076383486390114
        entropy_coeff: 0.0005000000000000001
        kl: 0.004903997915486495
        model: {}
        policy_loss: -0.007217070371552836
        total_loss: 2.5927430192629495
        vf_explained_var: 0.9952414035797119
        vf_loss: 2.600055476029714
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.15666666666667
    gpu_util_percent0: 0.317
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14763658876573266
    mean_env_wait_ms: 1.2084248039021879
    mean_inference_ms: 4.368207441844556
    mean_raw_obs_processing_ms: 0.3818591920913977
  time_since_restore: 2810.257442712784
  time_this_iter_s: 25.870769739151
  time_total_s: 2810.257442712784
  timers:
    learn_throughput: 8652.763
    learn_time_ms: 18698.305
    sample_throughput: 23620.213
    sample_time_ms: 6849.727
    update_time_ms: 37.781
  timestamp: 1602812927
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    109 |          2810.26 | 17635328 |  282.213 |              319.313 |              165.677 |            785.712 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3175.8369454416525
    time_step_min: 2934
  date: 2020-10-16_01-49-13
  done: false
  episode_len_mean: 785.6018768536143
  episode_reward_max: 319.31313131313163
  episode_reward_mean: 282.42800811443186
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 217
  episodes_total: 22591
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.1729131688674291
        entropy_coeff: 0.0005000000000000001
        kl: 0.004271870207351943
        model: {}
        policy_loss: -0.009325368009740487
        total_loss: 2.267487347126007
        vf_explained_var: 0.9952816963195801
        vf_loss: 2.2768991589546204
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.82
    gpu_util_percent0: 0.3406666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762504870233004
    mean_env_wait_ms: 1.2083958494859082
    mean_inference_ms: 4.36747492082505
    mean_raw_obs_processing_ms: 0.38181316669818166
  time_since_restore: 2835.935301542282
  time_this_iter_s: 25.67785882949829
  time_total_s: 2835.935301542282
  timers:
    learn_throughput: 8656.342
    learn_time_ms: 18690.575
    sample_throughput: 23652.964
    sample_time_ms: 6840.242
    update_time_ms: 39.228
  timestamp: 1602812953
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    110 |          2835.94 | 17797120 |  282.428 |              319.313 |              165.677 |            785.602 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3174.7451402937813
    time_step_min: 2934
  date: 2020-10-16_01-49-39
  done: false
  episode_len_mean: 785.5117719406132
  episode_reward_max: 319.9191919191919
  episode_reward_mean: 282.5992783851872
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 175
  episodes_total: 22766
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.1819970396657785
        entropy_coeff: 0.0005000000000000001
        kl: 0.004261158329124252
        model: {}
        policy_loss: -0.008556412833665187
        total_loss: 2.0311067402362823
        vf_explained_var: 0.995327889919281
        vf_loss: 2.039754162232081
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.353333333333335
    gpu_util_percent0: 0.36133333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476152846858062
    mean_env_wait_ms: 1.2083693011287775
    mean_inference_ms: 4.366873989660076
    mean_raw_obs_processing_ms: 0.3817752491091022
  time_since_restore: 2861.6318879127502
  time_this_iter_s: 25.69658637046814
  time_total_s: 2861.6318879127502
  timers:
    learn_throughput: 8666.359
    learn_time_ms: 18668.97
    sample_throughput: 23698.099
    sample_time_ms: 6827.214
    update_time_ms: 39.341
  timestamp: 1602812979
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    111 |          2861.63 | 17958912 |  282.599 |              319.919 |              165.677 |            785.512 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3173.2476385321897
    time_step_min: 2934
  date: 2020-10-16_01-50-05
  done: false
  episode_len_mean: 785.411156036694
  episode_reward_max: 319.9191919191919
  episode_reward_mean: 282.82829468547476
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 23001
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.18373206754525503
        entropy_coeff: 0.0005000000000000001
        kl: 0.004336539811144273
        model: {}
        policy_loss: -0.009234675051023563
        total_loss: 2.506518284479777
        vf_explained_var: 0.9953103065490723
        vf_loss: 2.5158448815345764
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.766666666666666
    gpu_util_percent0: 0.3616666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760224596168361
    mean_env_wait_ms: 1.2083330505868097
    mean_inference_ms: 4.366126951945574
    mean_raw_obs_processing_ms: 0.3817277972351893
  time_since_restore: 2887.215184211731
  time_this_iter_s: 25.583296298980713
  time_total_s: 2887.215184211731
  timers:
    learn_throughput: 8669.373
    learn_time_ms: 18662.48
    sample_throughput: 23739.593
    sample_time_ms: 6815.281
    update_time_ms: 40.535
  timestamp: 1602813005
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    112 |          2887.22 | 18120704 |  282.828 |              319.919 |              165.677 |            785.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3171.819636081407
    time_step_min: 2934
  date: 2020-10-16_01-50-32
  done: false
  episode_len_mean: 785.3100344530577
  episode_reward_max: 319.9191919191919
  episode_reward_mean: 283.04338605695193
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 219
  episodes_total: 23220
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.16369764630993208
        entropy_coeff: 0.0005000000000000001
        kl: 0.004032092770406355
        model: {}
        policy_loss: -0.006494078901596367
        total_loss: 2.238641540209452
        vf_explained_var: 0.9954802989959717
        vf_loss: 2.245217442512512
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.240000000000006
    gpu_util_percent0: 0.32099999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14759067726279407
    mean_env_wait_ms: 1.2082982784801946
    mean_inference_ms: 4.365422215650918
    mean_raw_obs_processing_ms: 0.38168394655909654
  time_since_restore: 2913.0571253299713
  time_this_iter_s: 25.841941118240356
  time_total_s: 2913.0571253299713
  timers:
    learn_throughput: 8660.683
    learn_time_ms: 18681.206
    sample_throughput: 23781.845
    sample_time_ms: 6803.173
    update_time_ms: 49.17
  timestamp: 1602813032
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    113 |          2913.06 | 18282496 |  283.043 |              319.919 |              165.677 |             785.31 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3170.680347513481
    time_step_min: 2932
  date: 2020-10-16_01-50-58
  done: false
  episode_len_mean: 785.2377532700692
  episode_reward_max: 319.9191919191919
  episode_reward_mean: 283.21170281942267
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 174
  episodes_total: 23394
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.1712109073996544
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048668616606543464
        model: {}
        policy_loss: -0.005204594825045206
        total_loss: 1.9320645332336426
        vf_explained_var: 0.9956938624382019
        vf_loss: 1.9373546938101451
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92333333333334
    gpu_util_percent0: 0.2916666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475817130184395
    mean_env_wait_ms: 1.208271287696249
    mean_inference_ms: 4.364851846244404
    mean_raw_obs_processing_ms: 0.3816485981238024
  time_since_restore: 2938.7828562259674
  time_this_iter_s: 25.725730895996094
  time_total_s: 2938.7828562259674
  timers:
    learn_throughput: 8666.434
    learn_time_ms: 18668.809
    sample_throughput: 23763.167
    sample_time_ms: 6808.52
    update_time_ms: 46.876
  timestamp: 1602813058
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    114 |          2938.78 | 18444288 |  283.212 |              319.919 |              165.677 |            785.238 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3169.2730317547803
    time_step_min: 2928
  date: 2020-10-16_01-51-24
  done: false
  episode_len_mean: 785.1526995553673
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 283.4290951009138
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 23615
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.17937743663787842
        entropy_coeff: 0.0005000000000000001
        kl: 0.00433262715038533
        model: {}
        policy_loss: -0.005900731290845822
        total_loss: 2.396419088045756
        vf_explained_var: 0.9953761100769043
        vf_loss: 2.402409533659617
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.160000000000004
    gpu_util_percent0: 0.3853333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756955702590033
    mean_env_wait_ms: 1.208229175589488
    mean_inference_ms: 4.36417262585297
    mean_raw_obs_processing_ms: 0.38160566872368473
  time_since_restore: 2964.583787918091
  time_this_iter_s: 25.800931692123413
  time_total_s: 2964.583787918091
  timers:
    learn_throughput: 8654.341
    learn_time_ms: 18694.894
    sample_throughput: 23786.603
    sample_time_ms: 6801.812
    update_time_ms: 40.969
  timestamp: 1602813084
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    115 |          2964.58 | 18606080 |  283.429 |              321.283 |              165.677 |            785.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3167.808538325917
    time_step_min: 2926
  date: 2020-10-16_01-51-50
  done: false
  episode_len_mean: 785.0846540880503
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 283.65480592084356
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 23850
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.15947192907333374
        entropy_coeff: 0.0005000000000000001
        kl: 0.004415036373150845
        model: {}
        policy_loss: -0.008368280687136576
        total_loss: 2.0854901870091758
        vf_explained_var: 0.9960618615150452
        vf_loss: 2.093938171863556
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.499999999999996
    gpu_util_percent0: 0.30499999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475580784833882
    mean_env_wait_ms: 1.2081923899313813
    mean_inference_ms: 4.363463774102444
    mean_raw_obs_processing_ms: 0.3815617185049162
  time_since_restore: 2990.4876942634583
  time_this_iter_s: 25.90390634536743
  time_total_s: 2990.4876942634583
  timers:
    learn_throughput: 8647.938
    learn_time_ms: 18708.737
    sample_throughput: 23743.661
    sample_time_ms: 6814.113
    update_time_ms: 41.49
  timestamp: 1602813110
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    116 |          2990.49 | 18767872 |  283.655 |              321.283 |              165.677 |            785.085 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3166.7363617420297
    time_step_min: 2926
  date: 2020-10-16_01-52-16
  done: false
  episode_len_mean: 785.0346334762519
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 283.8157922731456
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 173
  episodes_total: 24023
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.16986916462580362
        entropy_coeff: 0.0005000000000000001
        kl: 0.00415391381829977
        model: {}
        policy_loss: -0.007707703747049284
        total_loss: 2.029173066218694
        vf_explained_var: 0.995383083820343
        vf_loss: 2.036965638399124
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05666666666667
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754946859507495
    mean_env_wait_ms: 1.208162427637506
    mean_inference_ms: 4.362923846125198
    mean_raw_obs_processing_ms: 0.38152853906160156
  time_since_restore: 3016.1789429187775
  time_this_iter_s: 25.691248655319214
  time_total_s: 3016.1789429187775
  timers:
    learn_throughput: 8640.272
    learn_time_ms: 18725.336
    sample_throughput: 23744.769
    sample_time_ms: 6813.796
    update_time_ms: 42.287
  timestamp: 1602813136
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    117 |          3016.18 | 18929664 |  283.816 |              321.283 |              165.677 |            785.035 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3165.460171872418
    time_step_min: 2926
  date: 2020-10-16_01-52-43
  done: false
  episode_len_mean: 784.9489105315286
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 284.00260403640226
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 209
  episodes_total: 24232
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17321191728115082
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039366522105410695
        model: {}
        policy_loss: -0.009872476609113315
        total_loss: 2.352258563041687
        vf_explained_var: 0.9954938888549805
        vf_loss: 2.362217585245768
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37
    gpu_util_percent0: 0.35133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753901993640242
    mean_env_wait_ms: 1.2081210836254221
    mean_inference_ms: 4.362302002770167
    mean_raw_obs_processing_ms: 0.38148985109485795
  time_since_restore: 3041.8957047462463
  time_this_iter_s: 25.716761827468872
  time_total_s: 3041.8957047462463
  timers:
    learn_throughput: 8639.234
    learn_time_ms: 18727.586
    sample_throughput: 23747.507
    sample_time_ms: 6813.01
    update_time_ms: 40.574
  timestamp: 1602813163
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    118 |           3041.9 | 19091456 |  284.003 |              321.283 |              165.677 |            784.949 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3163.957946410309
    time_step_min: 2926
  date: 2020-10-16_01-53-09
  done: false
  episode_len_mean: 784.8994401994034
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 284.23111142479416
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 241
  episodes_total: 24473
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.15587976823250452
        entropy_coeff: 0.0005000000000000001
        kl: 0.004592253516117732
        model: {}
        policy_loss: -0.007124626184425627
        total_loss: 2.442015528678894
        vf_explained_var: 0.995435893535614
        vf_loss: 2.4492180744806924
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.186666666666667
    gpu_util_percent0: 0.3256666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475273402603604
    mean_env_wait_ms: 1.2080762249099402
    mean_inference_ms: 4.361616780175905
    mean_raw_obs_processing_ms: 0.3814455840699785
  time_since_restore: 3067.732525587082
  time_this_iter_s: 25.83682084083557
  time_total_s: 3067.732525587082
  timers:
    learn_throughput: 8642.17
    learn_time_ms: 18721.223
    sample_throughput: 23743.528
    sample_time_ms: 6814.152
    update_time_ms: 40.851
  timestamp: 1602813189
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    119 |          3067.73 | 19253248 |  284.231 |              321.283 |              165.677 |            784.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3162.8459570320433
    time_step_min: 2926
  date: 2020-10-16_01-53-35
  done: false
  episode_len_mean: 784.8869011399132
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 284.39533954612443
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 178
  episodes_total: 24651
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.15485632419586182
        entropy_coeff: 0.0005000000000000001
        kl: 0.003862540858487288
        model: {}
        policy_loss: -0.008335680021749189
        total_loss: 2.322477638721466
        vf_explained_var: 0.994878351688385
        vf_loss: 2.3308907548586526
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09
    gpu_util_percent0: 0.3509999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14751930339664085
    mean_env_wait_ms: 1.208043303007963
    mean_inference_ms: 4.361086854296435
    mean_raw_obs_processing_ms: 0.3814143308210553
  time_since_restore: 3093.2999682426453
  time_this_iter_s: 25.567442655563354
  time_total_s: 3093.2999682426453
  timers:
    learn_throughput: 8647.872
    learn_time_ms: 18708.88
    sample_throughput: 23744.109
    sample_time_ms: 6813.985
    update_time_ms: 41.226
  timestamp: 1602813215
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    120 |           3093.3 | 19415040 |  284.395 |              321.283 |              165.677 |            784.887 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3161.62143864598
    time_step_min: 2925
  date: 2020-10-16_01-54-01
  done: false
  episode_len_mean: 784.8894658455098
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 284.5751220696275
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 192
  episodes_total: 24843
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.16783262168367705
        entropy_coeff: 0.0005000000000000001
        kl: 0.004981446312740445
        model: {}
        policy_loss: -0.00818172721968343
        total_loss: 2.158005952835083
        vf_explained_var: 0.9955466389656067
        vf_loss: 2.16627166668574
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.01379310344828
    gpu_util_percent0: 0.3034482758620689
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14751057434648068
    mean_env_wait_ms: 1.2080077325708576
    mean_inference_ms: 4.360550106469598
    mean_raw_obs_processing_ms: 0.3813823943770635
  time_since_restore: 3118.803628921509
  time_this_iter_s: 25.503660678863525
  time_total_s: 3118.803628921509
  timers:
    learn_throughput: 8652.075
    learn_time_ms: 18699.792
    sample_throughput: 23780.121
    sample_time_ms: 6803.666
    update_time_ms: 40.692
  timestamp: 1602813241
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    121 |           3118.8 | 19576832 |  284.575 |              321.283 |              165.677 |            784.889 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3160.123423782921
    time_step_min: 2925
  date: 2020-10-16_01-54-27
  done: false
  episode_len_mean: 784.8919802295918
  episode_reward_max: 322.949494949495
  episode_reward_mean: 284.7932775619717
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 245
  episodes_total: 25088
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.1570940762758255
        entropy_coeff: 0.0005000000000000001
        kl: 0.003961495550659795
        model: {}
        policy_loss: -0.00754543371052326
        total_loss: 2.493976056575775
        vf_explained_var: 0.9955318570137024
        vf_loss: 2.501599987347921
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.003333333333337
    gpu_util_percent0: 0.30233333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749820475772676
    mean_env_wait_ms: 1.2079508629293505
    mean_inference_ms: 4.35986803573374
    mean_raw_obs_processing_ms: 0.38133767522044676
  time_since_restore: 3144.584945201874
  time_this_iter_s: 25.78131628036499
  time_total_s: 3144.584945201874
  timers:
    learn_throughput: 8645.316
    learn_time_ms: 18714.411
    sample_throughput: 23770.325
    sample_time_ms: 6806.47
    update_time_ms: 41.033
  timestamp: 1602813267
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    122 |          3144.58 | 19738624 |  284.793 |              322.949 |              165.677 |            784.892 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3159.0028513722227
    time_step_min: 2920
  date: 2020-10-16_01-54-53
  done: false
  episode_len_mean: 784.9051782111634
  episode_reward_max: 322.949494949495
  episode_reward_mean: 284.96320377715995
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 191
  episodes_total: 25279
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.1477253089348475
        entropy_coeff: 0.0005000000000000001
        kl: 0.003945230622775853
        model: {}
        policy_loss: -0.00920618103312639
        total_loss: 1.7173584500948589
        vf_explained_var: 0.9963318705558777
        vf_loss: 1.7266384760538738
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.01666666666667
    gpu_util_percent0: 0.3346666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749008762639335
    mean_env_wait_ms: 1.2079117839294584
    mean_inference_ms: 4.359334814596142
    mean_raw_obs_processing_ms: 0.3813059152034239
  time_since_restore: 3170.1845331192017
  time_this_iter_s: 25.59958791732788
  time_total_s: 3170.1845331192017
  timers:
    learn_throughput: 8657.31
    learn_time_ms: 18688.485
    sample_throughput: 23741.883
    sample_time_ms: 6814.624
    update_time_ms: 34.806
  timestamp: 1602813293
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    123 |          3170.18 | 19900416 |  284.963 |              322.949 |              165.677 |            784.905 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3158.009083402147
    time_step_min: 2920
  date: 2020-10-16_01-55-20
  done: false
  episode_len_mean: 784.9131544836796
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.11666331407866
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 25459
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.1585122545560201
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038400359141329923
        model: {}
        policy_loss: -0.008884707873221487
        total_loss: 1.9652710159619649
        vf_explained_var: 0.99581378698349
        vf_loss: 1.9742349584897358
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.426666666666666
    gpu_util_percent0: 0.29566666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748194300306544
    mean_env_wait_ms: 1.2078710731377567
    mean_inference_ms: 4.358837655529566
    mean_raw_obs_processing_ms: 0.3812755091603063
  time_since_restore: 3195.9716169834137
  time_this_iter_s: 25.787083864212036
  time_total_s: 3195.9716169834137
  timers:
    learn_throughput: 8652.944
    learn_time_ms: 18697.913
    sample_throughput: 23729.66
    sample_time_ms: 6818.134
    update_time_ms: 36.769
  timestamp: 1602813320
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    124 |          3195.97 | 20062208 |  285.117 |              322.949 |              165.677 |            784.913 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3156.7205641704977
    time_step_min: 2920
  date: 2020-10-16_01-55-46
  done: false
  episode_len_mean: 784.9009496380478
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.3147034287766
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 25694
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.15613733480374017
        entropy_coeff: 0.0005000000000000001
        kl: 0.003700155473779887
        model: {}
        policy_loss: -0.0072184071856706096
        total_loss: 2.7436450521151223
        vf_explained_var: 0.9951500296592712
        vf_loss: 2.7509415547053018
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57666666666667
    gpu_util_percent0: 0.36133333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747092917761756
    mean_env_wait_ms: 1.2078129274786313
    mean_inference_ms: 4.358220699595637
    mean_raw_obs_processing_ms: 0.3812366418579158
  time_since_restore: 3221.7527482509613
  time_this_iter_s: 25.781131267547607
  time_total_s: 3221.7527482509613
  timers:
    learn_throughput: 8654.083
    learn_time_ms: 18695.454
    sample_throughput: 23706.577
    sample_time_ms: 6824.773
    update_time_ms: 36.382
  timestamp: 1602813346
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    125 |          3221.75 | 20224000 |  285.315 |              322.949 |              165.677 |            784.901 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3155.521772729029
    time_step_min: 2911
  date: 2020-10-16_01-56-12
  done: false
  episode_len_mean: 784.8975259562314
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.50459163404463
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 25909
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.14190075919032097
        entropy_coeff: 0.0005000000000000001
        kl: 0.005255653406493366
        model: {}
        policy_loss: -0.00850060413358733
        total_loss: 2.407831311225891
        vf_explained_var: 0.9952466487884521
        vf_loss: 2.416402816772461
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.35
    gpu_util_percent0: 0.36200000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474623234593681
    mean_env_wait_ms: 1.2077641677855027
    mean_inference_ms: 4.357656474694085
    mean_raw_obs_processing_ms: 0.3812017234743712
  time_since_restore: 3247.4038054943085
  time_this_iter_s: 25.651057243347168
  time_total_s: 3247.4038054943085
  timers:
    learn_throughput: 8658.496
    learn_time_ms: 18685.925
    sample_throughput: 23762.609
    sample_time_ms: 6808.68
    update_time_ms: 35.405
  timestamp: 1602813372
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    126 |           3247.4 | 20385792 |  285.505 |              322.949 |              165.677 |            784.898 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3154.512933681302
    time_step_min: 2911
  date: 2020-10-16_01-56-39
  done: false
  episode_len_mean: 784.8730639472474
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.6587141155458
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 175
  episodes_total: 26084
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.146172267695268
        entropy_coeff: 0.0005000000000000001
        kl: 0.004243072898437579
        model: {}
        policy_loss: -0.008694331017371345
        total_loss: 1.5353152354558308
        vf_explained_var: 0.996583878993988
        vf_loss: 1.5440826614697774
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64666666666667
    gpu_util_percent0: 0.349
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745471495833717
    mean_env_wait_ms: 1.2077212864041904
    mean_inference_ms: 4.357192547344555
    mean_raw_obs_processing_ms: 0.3811735288172342
  time_since_restore: 3273.4215846061707
  time_this_iter_s: 26.017779111862183
  time_total_s: 3273.4215846061707
  timers:
    learn_throughput: 8651.201
    learn_time_ms: 18701.68
    sample_throughput: 23708.725
    sample_time_ms: 6824.155
    update_time_ms: 35.743
  timestamp: 1602813399
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    127 |          3273.42 | 20547584 |  285.659 |              322.949 |              165.677 |            784.873 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3153.20134687821
    time_step_min: 2911
  date: 2020-10-16_01-57-05
  done: false
  episode_len_mean: 784.8453878605907
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.85933908658234
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 227
  episodes_total: 26311
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.1509584461649259
        entropy_coeff: 0.0005000000000000001
        kl: 0.004119076804878811
        model: {}
        policy_loss: -0.008656273285547892
        total_loss: 1.9305044114589691
        vf_explained_var: 0.9963381290435791
        vf_loss: 1.9392361442248027
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.890000000000008
    gpu_util_percent0: 0.35633333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14744445155261457
    mean_env_wait_ms: 1.2076613296170642
    mean_inference_ms: 4.356610647504659
    mean_raw_obs_processing_ms: 0.3811368860638262
  time_since_restore: 3299.1470398902893
  time_this_iter_s: 25.725455284118652
  time_total_s: 3299.1470398902893
  timers:
    learn_throughput: 8653.816
    learn_time_ms: 18696.03
    sample_throughput: 23699.718
    sample_time_ms: 6826.748
    update_time_ms: 37.502
  timestamp: 1602813425
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    128 |          3299.15 | 20709376 |  285.859 |              322.949 |              165.677 |            784.845 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3151.8901675218835
    time_step_min: 2911
  date: 2020-10-16_01-57-31
  done: false
  episode_len_mean: 784.8092492085029
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.0533169780116
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 26532
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.13286865005890527
        entropy_coeff: 0.0005000000000000001
        kl: 0.003924006809635709
        model: {}
        policy_loss: -0.008037711608267273
        total_loss: 1.8678389092286427
        vf_explained_var: 0.9963827729225159
        vf_loss: 1.8759430646896362
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.666666666666668
    gpu_util_percent0: 0.3566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743551728208726
    mean_env_wait_ms: 1.207602957793733
    mean_inference_ms: 4.356060171396466
    mean_raw_obs_processing_ms: 0.3811016423530986
  time_since_restore: 3324.959667444229
  time_this_iter_s: 25.81262755393982
  time_total_s: 3324.959667444229
  timers:
    learn_throughput: 8651.808
    learn_time_ms: 18700.368
    sample_throughput: 23726.074
    sample_time_ms: 6819.164
    update_time_ms: 37.601
  timestamp: 1602813451
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    129 |          3324.96 | 20871168 |  286.053 |              322.949 |              165.677 |            784.809 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3150.8030655074203
    time_step_min: 2911
  date: 2020-10-16_01-57-57
  done: false
  episode_len_mean: 784.7905810122792
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.2144105021463
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 26712
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.13310043513774872
        entropy_coeff: 0.0005000000000000001
        kl: 0.003846823936328292
        model: {}
        policy_loss: -0.005511985975317657
        total_loss: 1.8974691927433014
        vf_explained_var: 0.9959127902984619
        vf_loss: 1.9030477007230122
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.676666666666662
    gpu_util_percent0: 0.33266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474283914407785
    mean_env_wait_ms: 1.207558034633788
    mean_inference_ms: 4.355602889667015
    mean_raw_obs_processing_ms: 0.38107428494909507
  time_since_restore: 3350.488450050354
  time_this_iter_s: 25.528782606124878
  time_total_s: 3350.488450050354
  timers:
    learn_throughput: 8650.322
    learn_time_ms: 18703.582
    sample_throughput: 23748.278
    sample_time_ms: 6812.789
    update_time_ms: 35.93
  timestamp: 1602813477
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    130 |          3350.49 | 21032960 |  286.214 |              322.949 |              165.677 |            784.791 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3149.6121750771636
    time_step_min: 2911
  date: 2020-10-16_01-58-23
  done: false
  episode_len_mean: 784.7560087670419
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.39055325347533
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 207
  episodes_total: 26919
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.14104019726316133
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035227101955873272
        model: {}
        policy_loss: -0.006806320839435405
        total_loss: 1.7736331522464752
        vf_explained_var: 0.9965450763702393
        vf_loss: 1.7805099785327911
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.143333333333334
    gpu_util_percent0: 0.38866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14741981580511296
    mean_env_wait_ms: 1.2075005106941397
    mean_inference_ms: 4.355093723139002
    mean_raw_obs_processing_ms: 0.38104253540537875
  time_since_restore: 3376.323652267456
  time_this_iter_s: 25.83520221710205
  time_total_s: 3376.323652267456
  timers:
    learn_throughput: 8641.158
    learn_time_ms: 18723.417
    sample_throughput: 23706.737
    sample_time_ms: 6824.727
    update_time_ms: 36.598
  timestamp: 1602813503
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    131 |          3376.32 | 21194752 |  286.391 |              322.949 |              165.677 |            784.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3148.2026464192254
    time_step_min: 2911
  date: 2020-10-16_01-58-50
  done: false
  episode_len_mean: 784.6928090135866
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.6085364860355
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 240
  episodes_total: 27159
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.12843968719244003
        entropy_coeff: 0.0005000000000000001
        kl: 0.004247498620922367
        model: {}
        policy_loss: -0.005535748534991096
        total_loss: 1.692270855108897
        vf_explained_var: 0.9967558979988098
        vf_loss: 1.6978708108266194
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.273333333333333
    gpu_util_percent0: 0.3516666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14741018958273205
    mean_env_wait_ms: 1.2074308550557367
    mean_inference_ms: 4.354531396862896
    mean_raw_obs_processing_ms: 0.3810060441193969
  time_since_restore: 3401.9416320323944
  time_this_iter_s: 25.617979764938354
  time_total_s: 3401.9416320323944
  timers:
    learn_throughput: 8646.867
    learn_time_ms: 18711.055
    sample_throughput: 23715.092
    sample_time_ms: 6822.322
    update_time_ms: 35.284
  timestamp: 1602813530
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    132 |          3401.94 | 21356544 |  286.609 |              322.949 |              165.677 |            784.693 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3147.181092560047
    time_step_min: 2911
  date: 2020-10-16_01-59-16
  done: false
  episode_len_mean: 784.6425749817117
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.76279436648844
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 27340
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.12154343537986279
        entropy_coeff: 0.0005000000000000001
        kl: 0.003221072295370201
        model: {}
        policy_loss: -0.007368193871419256
        total_loss: 1.6749194363753002
        vf_explained_var: 0.9962978363037109
        vf_loss: 1.6823483606179555
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.816666666666674
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740316045493987
    mean_env_wait_ms: 1.2073842518834306
    mean_inference_ms: 4.3540865524884484
    mean_raw_obs_processing_ms: 0.3809794414731611
  time_since_restore: 3427.54647731781
  time_this_iter_s: 25.60484528541565
  time_total_s: 3427.54647731781
  timers:
    learn_throughput: 8646.891
    learn_time_ms: 18711.003
    sample_throughput: 23716.372
    sample_time_ms: 6821.954
    update_time_ms: 35.205
  timestamp: 1602813556
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    133 |          3427.55 | 21518336 |  286.763 |              322.949 |              165.677 |            784.643 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3146.0563451961175
    time_step_min: 2911
  date: 2020-10-16_01-59-42
  done: false
  episode_len_mean: 784.6101608744598
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.93258216768396
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 197
  episodes_total: 27537
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.13145457083980241
        entropy_coeff: 0.0005000000000000001
        kl: 0.003317481663543731
        model: {}
        policy_loss: -0.007001562073128298
        total_loss: 2.089047600825628
        vf_explained_var: 0.9958324432373047
        vf_loss: 2.0961148937543235
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.830000000000002
    gpu_util_percent0: 0.35033333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739525845136722
    mean_env_wait_ms: 1.207325402771301
    mean_inference_ms: 4.35362599372007
    mean_raw_obs_processing_ms: 0.3809506163083337
  time_since_restore: 3453.3998420238495
  time_this_iter_s: 25.85336470603943
  time_total_s: 3453.3998420238495
  timers:
    learn_throughput: 8642.836
    learn_time_ms: 18719.782
    sample_throughput: 23728.055
    sample_time_ms: 6818.595
    update_time_ms: 34.229
  timestamp: 1602813582
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    134 |           3453.4 | 21680128 |  286.933 |              322.949 |              165.677 |             784.61 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3144.7354954954953
    time_step_min: 2911
  date: 2020-10-16_02-00-08
  done: false
  episode_len_mean: 784.5569515443876
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.138361438563
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 241
  episodes_total: 27778
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.12677601476510367
        entropy_coeff: 0.0005000000000000001
        kl: 0.004002009110990912
        model: {}
        policy_loss: -0.007657081184637112
        total_loss: 1.9598196148872375
        vf_explained_var: 0.9963613152503967
        vf_loss: 1.9675400753815968
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.596666666666668
    gpu_util_percent0: 0.3330000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473854870388948
    mean_env_wait_ms: 1.2072516035371872
    mean_inference_ms: 4.353074946484196
    mean_raw_obs_processing_ms: 0.3809145070825055
  time_since_restore: 3479.08571100235
  time_this_iter_s: 25.685868978500366
  time_total_s: 3479.08571100235
  timers:
    learn_throughput: 8647.237
    learn_time_ms: 18710.253
    sample_throughput: 23734.165
    sample_time_ms: 6816.84
    update_time_ms: 35.025
  timestamp: 1602813608
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    135 |          3479.09 | 21841920 |  287.138 |              322.949 |              165.677 |            784.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3143.672083035075
    time_step_min: 2911
  date: 2020-10-16_02-00-35
  done: false
  episode_len_mean: 784.5266733409611
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.30164415898105
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 190
  episodes_total: 27968
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.11515582290788491
        entropy_coeff: 0.0005000000000000001
        kl: 0.003492694755550474
        model: {}
        policy_loss: -0.006612631312843102
        total_loss: 1.3455493052800496
        vf_explained_var: 0.9970569610595703
        vf_loss: 1.3522194921970367
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.040000000000003
    gpu_util_percent0: 0.323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473789646764408
    mean_env_wait_ms: 1.2072004203991602
    mean_inference_ms: 4.352637912819235
    mean_raw_obs_processing_ms: 0.3808880634295869
  time_since_restore: 3504.936125278473
  time_this_iter_s: 25.850414276123047
  time_total_s: 3504.936125278473
  timers:
    learn_throughput: 8646.461
    learn_time_ms: 18711.934
    sample_throughput: 23676.391
    sample_time_ms: 6833.474
    update_time_ms: 36.006
  timestamp: 1602813635
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    136 |          3504.94 | 22003712 |  287.302 |              322.949 |              165.677 |            784.527 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3142.6615231458436
    time_step_min: 2911
  date: 2020-10-16_02-01-01
  done: false
  episode_len_mean: 784.5001065567948
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.45032480089657
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 186
  episodes_total: 28154
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.12704486399888992
        entropy_coeff: 0.0005000000000000001
        kl: 0.003533348091877997
        model: {}
        policy_loss: -0.007090890683078517
        total_loss: 2.213720997174581
        vf_explained_var: 0.9953908920288086
        vf_loss: 2.220875402291616
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.39666666666667
    gpu_util_percent0: 0.3413333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147371732413541
    mean_env_wait_ms: 1.2071452314700837
    mean_inference_ms: 4.352223462809144
    mean_raw_obs_processing_ms: 0.38086153344782303
  time_since_restore: 3530.725163459778
  time_this_iter_s: 25.78903818130493
  time_total_s: 3530.725163459778
  timers:
    learn_throughput: 8647.735
    learn_time_ms: 18709.176
    sample_throughput: 23755.09
    sample_time_ms: 6810.835
    update_time_ms: 36.456
  timestamp: 1602813661
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    137 |          3530.73 | 22165504 |   287.45 |              322.949 |              165.677 |              784.5 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3141.3280936101223
    time_step_min: 2911
  date: 2020-10-16_02-01-27
  done: false
  episode_len_mean: 784.4622020351396
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.65329859277244
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 247
  episodes_total: 28401
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.12876132627328238
        entropy_coeff: 0.0005000000000000001
        kl: 0.005003980516145627
        model: {}
        policy_loss: -0.006421642643545056
        total_loss: 1.8043050467967987
        vf_explained_var: 0.9967013001441956
        vf_loss: 1.8107910950978596
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03666666666667
    gpu_util_percent0: 0.326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736238128429271
    mean_env_wait_ms: 1.20706730092969
    mean_inference_ms: 4.351680827824965
    mean_raw_obs_processing_ms: 0.38082632620064977
  time_since_restore: 3556.503339290619
  time_this_iter_s: 25.778175830841064
  time_total_s: 3556.503339290619
  timers:
    learn_throughput: 8642.546
    learn_time_ms: 18720.409
    sample_throughput: 23780.042
    sample_time_ms: 6803.689
    update_time_ms: 36.403
  timestamp: 1602813687
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    138 |           3556.5 | 22327296 |  287.653 |              322.949 |              165.677 |            784.462 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3140.2757377393496
    time_step_min: 2911
  date: 2020-10-16_02-01-54
  done: false
  episode_len_mean: 784.4417555516699
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.81059590484307
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 28595
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.11352283755938213
        entropy_coeff: 0.0005000000000000001
        kl: 0.005003925994969904
        model: {}
        policy_loss: -0.007645862928863305
        total_loss: 1.5644435087839763
        vf_explained_var: 0.9967303276062012
        vf_loss: 1.5721460978190105
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.580000000000002
    gpu_util_percent0: 0.3330000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735533926210645
    mean_env_wait_ms: 1.2070100675525426
    mean_inference_ms: 4.351253220864738
    mean_raw_obs_processing_ms: 0.3807994329668664
  time_since_restore: 3582.284632205963
  time_this_iter_s: 25.78129291534424
  time_total_s: 3582.284632205963
  timers:
    learn_throughput: 8646.781
    learn_time_ms: 18711.241
    sample_throughput: 23763.814
    sample_time_ms: 6808.335
    update_time_ms: 36.147
  timestamp: 1602813714
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    139 |          3582.28 | 22489088 |  287.811 |              322.949 |              165.677 |            784.442 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3139.264201481894
    time_step_min: 2911
  date: 2020-10-16_02-02-20
  done: false
  episode_len_mean: 784.445212858384
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.96042615556075
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 28775
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.12521460962792239
        entropy_coeff: 0.0005000000000000001
        kl: 0.005026774752574663
        model: {}
        policy_loss: -0.009392819076310843
        total_loss: 1.280622919400533
        vf_explained_var: 0.997205913066864
        vf_loss: 1.2900783717632294
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.243333333333336
    gpu_util_percent0: 0.3063333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734909107538507
    mean_env_wait_ms: 1.2069566575140906
    mean_inference_ms: 4.350865225100449
    mean_raw_obs_processing_ms: 0.3807755656729519
  time_since_restore: 3608.093282222748
  time_this_iter_s: 25.808650016784668
  time_total_s: 3608.093282222748
  timers:
    learn_throughput: 8639.234
    learn_time_ms: 18727.585
    sample_throughput: 23727.899
    sample_time_ms: 6818.64
    update_time_ms: 36.778
  timestamp: 1602813740
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    140 |          3608.09 | 22650880 |   287.96 |              322.949 |              165.677 |            784.445 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3138.099306442152
    time_step_min: 2911
  date: 2020-10-16_02-02-46
  done: false
  episode_len_mean: 784.4666827536282
  episode_reward_max: 322.949494949495
  episode_reward_mean: 288.1354456001289
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 234
  episodes_total: 29009
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.13129046062628427
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038973434905832014
        model: {}
        policy_loss: -0.007966956705786288
        total_loss: 2.2421520551045737
        vf_explained_var: 0.9959328174591064
        vf_loss: 2.2501845955848694
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.632258064516137
    gpu_util_percent0: 0.33451612903225814
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473396441206602
    mean_env_wait_ms: 1.206877429633912
    mean_inference_ms: 4.350363926113341
    mean_raw_obs_processing_ms: 0.38074184646981984
  time_since_restore: 3633.9851837158203
  time_this_iter_s: 25.89190149307251
  time_total_s: 3633.9851837158203
  timers:
    learn_throughput: 8639.871
    learn_time_ms: 18726.205
    sample_throughput: 23706.698
    sample_time_ms: 6824.738
    update_time_ms: 36.493
  timestamp: 1602813766
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    141 |          3633.99 | 22812672 |  288.135 |              322.949 |              165.677 |            784.467 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3137.044118654518
    time_step_min: 2911
  date: 2020-10-16_02-03-13
  done: false
  episode_len_mean: 784.5046540277873
  episode_reward_max: 322.949494949495
  episode_reward_mean: 288.29196730842745
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 213
  episodes_total: 29222
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.1140111219137907
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037463668074148395
        model: {}
        policy_loss: -0.0073797651063311305
        total_loss: 1.9700657427310944
        vf_explained_var: 0.9961625933647156
        vf_loss: 1.9775024751822154
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.873333333333335
    gpu_util_percent0: 0.31033333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733268095391208
    mean_env_wait_ms: 1.206810902631899
    mean_inference_ms: 4.349922854528261
    mean_raw_obs_processing_ms: 0.38071394711545137
  time_since_restore: 3659.7883853912354
  time_this_iter_s: 25.80320167541504
  time_total_s: 3659.7883853912354
  timers:
    learn_throughput: 8643.338
    learn_time_ms: 18718.694
    sample_throughput: 23651.905
    sample_time_ms: 6840.548
    update_time_ms: 44.764
  timestamp: 1602813793
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    142 |          3659.79 | 22974464 |  288.292 |              322.949 |              165.677 |            784.505 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3136.15835347792
    time_step_min: 2911
  date: 2020-10-16_02-03-39
  done: false
  episode_len_mean: 784.5476036599885
  episode_reward_max: 322.949494949495
  episode_reward_mean: 288.42321236103334
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 177
  episodes_total: 29399
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.11072564000884692
        entropy_coeff: 0.0005000000000000001
        kl: 0.003419145359657705
        model: {}
        policy_loss: -0.007451222176314332
        total_loss: 1.4723249475161235
        vf_explained_var: 0.9968392252922058
        vf_loss: 1.4798315068085988
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.680000000000003
    gpu_util_percent0: 0.30400000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732666037399647
    mean_env_wait_ms: 1.2067537414061884
    mean_inference_ms: 4.349542299459539
    mean_raw_obs_processing_ms: 0.38069021111833246
  time_since_restore: 3685.5319883823395
  time_this_iter_s: 25.743602991104126
  time_total_s: 3685.5319883823395
  timers:
    learn_throughput: 8640.88
    learn_time_ms: 18724.019
    sample_throughput: 23655.568
    sample_time_ms: 6839.489
    update_time_ms: 44.427
  timestamp: 1602813819
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    143 |          3685.53 | 23136256 |  288.423 |              322.949 |              165.677 |            784.548 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3135.1362975486054
    time_step_min: 2911
  date: 2020-10-16_02-04-06
  done: false
  episode_len_mean: 784.5920345910887
  episode_reward_max: 322.949494949495
  episode_reward_mean: 288.57595172752406
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 204
  episodes_total: 29603
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.388131789017202e-22
        cur_lr: 5.0e-05
        entropy: 0.11856410466134548
        entropy_coeff: 0.0005000000000000001
        kl: 0.003671085675402234
        model: {}
        policy_loss: -0.0070921553851803765
        total_loss: 1.8817374606927235
        vf_explained_var: 0.9964469075202942
        vf_loss: 1.8888888855775197
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.936666666666667
    gpu_util_percent0: 0.33199999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473195353676836
    mean_env_wait_ms: 1.2066857538595894
    mean_inference_ms: 4.349126004915582
    mean_raw_obs_processing_ms: 0.38066383622300287
  time_since_restore: 3711.2457382678986
  time_this_iter_s: 25.713749885559082
  time_total_s: 3711.2457382678986
  timers:
    learn_throughput: 8644.445
    learn_time_ms: 18716.296
    sample_throughput: 23698.109
    sample_time_ms: 6827.211
    update_time_ms: 43.311
  timestamp: 1602813846
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    144 |          3711.25 | 23298048 |  288.576 |              322.949 |              165.677 |            784.592 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3133.999832259796
    time_step_min: 2911
  date: 2020-10-16_02-04-32
  done: false
  episode_len_mean: 784.6462662555302
  episode_reward_max: 322.949494949495
  episode_reward_mean: 288.75755883002154
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 233
  episodes_total: 29836
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.694065894508601e-22
        cur_lr: 5.0e-05
        entropy: 0.109185924132665
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031885854744662843
        model: {}
        policy_loss: -0.006712160403064142
        total_loss: 1.2723818918069203
        vf_explained_var: 0.9976632595062256
        vf_loss: 1.2791486581166585
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.466666666666665
    gpu_util_percent0: 0.30666666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731101992987697
    mean_env_wait_ms: 1.2066007706921213
    mean_inference_ms: 4.348663495474078
    mean_raw_obs_processing_ms: 0.3806315343766918
  time_since_restore: 3736.9326531887054
  time_this_iter_s: 25.686914920806885
  time_total_s: 3736.9326531887054
  timers:
    learn_throughput: 8640.321
    learn_time_ms: 18725.23
    sample_throughput: 23725.084
    sample_time_ms: 6819.449
    update_time_ms: 43.328
  timestamp: 1602813872
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    145 |          3736.93 | 23459840 |  288.758 |              322.949 |              165.677 |            784.646 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3133.042173695616
    time_step_min: 2911
  date: 2020-10-16_02-04-58
  done: false
  episode_len_mean: 784.6897045598374
  episode_reward_max: 322.949494949495
  episode_reward_mean: 288.90268841026585
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 30023
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.470329472543005e-23
        cur_lr: 5.0e-05
        entropy: 0.10165795373419921
        entropy_coeff: 0.0005000000000000001
        kl: 0.002919704265271624
        model: {}
        policy_loss: -0.006598538432929975
        total_loss: 1.1393199861049652
        vf_explained_var: 0.9976431727409363
        vf_loss: 1.1459693312644958
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.37333333333333
    gpu_util_percent0: 0.33399999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730521846325448
    mean_env_wait_ms: 1.2065411964445438
    mean_inference_ms: 4.348290460808449
    mean_raw_obs_processing_ms: 0.3806093513729205
  time_since_restore: 3762.697762489319
  time_this_iter_s: 25.765109300613403
  time_total_s: 3762.697762489319
  timers:
    learn_throughput: 8634.023
    learn_time_ms: 18738.89
    sample_throughput: 23806.862
    sample_time_ms: 6796.024
    update_time_ms: 43.47
  timestamp: 1602813898
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    146 |           3762.7 | 23621632 |  288.903 |              322.949 |              165.677 |             784.69 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3132.074121935056
    time_step_min: 2911
  date: 2020-10-16_02-05-25
  done: false
  episode_len_mean: 784.736030190678
  episode_reward_max: 322.949494949495
  episode_reward_mean: 289.04640485897096
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 30208
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2351647362715025e-23
        cur_lr: 5.0e-05
        entropy: 0.11022570046285789
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065453955515598254
        model: {}
        policy_loss: -0.006718543183524162
        total_loss: 1.158207247654597
        vf_explained_var: 0.9976673126220703
        vf_loss: 1.164980838696162
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78666666666667
    gpu_util_percent0: 0.277
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729960053911326
    mean_env_wait_ms: 1.2064761240672848
    mean_inference_ms: 4.347934342969285
    mean_raw_obs_processing_ms: 0.38058708781120887
  time_since_restore: 3788.4532754421234
  time_this_iter_s: 25.755512952804565
  time_total_s: 3788.4532754421234
  timers:
    learn_throughput: 8640.667
    learn_time_ms: 18724.48
    sample_throughput: 23801.183
    sample_time_ms: 6797.645
    update_time_ms: 42.959
  timestamp: 1602813925
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    147 |          3788.45 | 23783424 |  289.046 |              322.949 |              165.677 |            784.736 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3130.825285164853
    time_step_min: 2911
  date: 2020-10-16_02-05-51
  done: false
  episode_len_mean: 784.7900752077244
  episode_reward_max: 322.949494949495
  episode_reward_mean: 289.23705775943927
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 241
  episodes_total: 30449
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2351647362715025e-23
        cur_lr: 5.0e-05
        entropy: 0.11330397923787434
        entropy_coeff: 0.0005000000000000001
        kl: 0.003984236779312293
        model: {}
        policy_loss: -0.006279718072619289
        total_loss: 1.4012575447559357
        vf_explained_var: 0.9974705576896667
        vf_loss: 1.4075938860575359
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.156666666666673
    gpu_util_percent0: 0.30733333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729126143619045
    mean_env_wait_ms: 1.2063880282598225
    mean_inference_ms: 4.3474702710738296
    mean_raw_obs_processing_ms: 0.38055583297887363
  time_since_restore: 3814.324530363083
  time_this_iter_s: 25.871254920959473
  time_total_s: 3814.324530363083
  timers:
    learn_throughput: 8634.647
    learn_time_ms: 18737.534
    sample_throughput: 23811.702
    sample_time_ms: 6794.642
    update_time_ms: 42.707
  timestamp: 1602813951
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    148 |          3814.32 | 23945216 |  289.237 |              322.949 |              165.677 |             784.79 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3129.816225219635
    time_step_min: 2911
  date: 2020-10-16_02-06-18
  done: false
  episode_len_mean: 784.8180572323555
  episode_reward_max: 322.949494949495
  episode_reward_mean: 289.3888765291838
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 30647
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1175823681357513e-23
        cur_lr: 5.0e-05
        entropy: 0.10384146434565385
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038161062984727323
        model: {}
        policy_loss: -0.006680615396665719
        total_loss: 1.3703153630097706
        vf_explained_var: 0.9972288608551025
        vf_loss: 1.3770479063193004
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.383333333333333
    gpu_util_percent0: 0.38266666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728459680758108
    mean_env_wait_ms: 1.2063171146502223
    mean_inference_ms: 4.347082397264857
    mean_raw_obs_processing_ms: 0.3805310201236453
  time_since_restore: 3839.7690410614014
  time_this_iter_s: 25.44451069831848
  time_total_s: 3839.7690410614014
  timers:
    learn_throughput: 8653.794
    learn_time_ms: 18696.077
    sample_throughput: 23811.958
    sample_time_ms: 6794.569
    update_time_ms: 41.134
  timestamp: 1602813978
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    149 |          3839.77 | 24107008 |  289.389 |              322.949 |              165.677 |            784.818 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3128.882792207792
    time_step_min: 2911
  date: 2020-10-16_02-06-44
  done: false
  episode_len_mean: 784.8415725963409
  episode_reward_max: 322.949494949495
  episode_reward_mean: 289.53243214551105
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 30828
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0587911840678756e-23
        cur_lr: 5.0e-05
        entropy: 0.10714372930427392
        entropy_coeff: 0.0005000000000000001
        kl: 0.007682011462748051
        model: {}
        policy_loss: -0.00800914247520268
        total_loss: 0.840355321764946
        vf_explained_var: 0.9981480240821838
        vf_loss: 0.8484180321296056
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32333333333333
    gpu_util_percent0: 0.3203333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727937766836333
    mean_env_wait_ms: 1.206253421343657
    mean_inference_ms: 4.3467439833854495
    mean_raw_obs_processing_ms: 0.3805099987844226
  time_since_restore: 3865.4374525547028
  time_this_iter_s: 25.66841149330139
  time_total_s: 3865.4374525547028
  timers:
    learn_throughput: 8656.167
    learn_time_ms: 18690.952
    sample_throughput: 23849.537
    sample_time_ms: 6783.863
    update_time_ms: 42.153
  timestamp: 1602814004
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    150 |          3865.44 | 24268800 |  289.532 |              322.949 |              165.677 |            784.842 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3127.7248371913083
    time_step_min: 2911
  date: 2020-10-16_02-07-10
  done: false
  episode_len_mean: 784.8589512336533
  episode_reward_max: 322.949494949495
  episode_reward_mean: 289.7090491984197
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 218
  episodes_total: 31046
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0587911840678756e-23
        cur_lr: 5.0e-05
        entropy: 0.1122653999676307
        entropy_coeff: 0.0005000000000000001
        kl: 0.027689526478449505
        model: {}
        policy_loss: -0.006132113805506378
        total_loss: 1.054743379354477
        vf_explained_var: 0.9980019927024841
        vf_loss: 1.0609316229820251
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85666666666667
    gpu_util_percent0: 0.36866666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727116190505818
    mean_env_wait_ms: 1.206167120723635
    mean_inference_ms: 4.346325732501549
    mean_raw_obs_processing_ms: 0.3804814198869733
  time_since_restore: 3891.31360578537
  time_this_iter_s: 25.876153230667114
  time_total_s: 3891.31360578537
  timers:
    learn_throughput: 8649.791
    learn_time_ms: 18704.73
    sample_throughput: 23905.456
    sample_time_ms: 6767.995
    update_time_ms: 41.55
  timestamp: 1602814030
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    151 |          3891.31 | 24430592 |  289.709 |              322.949 |              165.677 |            784.859 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3126.5568465527176
    time_step_min: 2911
  date: 2020-10-16_02-07-37
  done: false
  episode_len_mean: 784.8665174288456
  episode_reward_max: 322.949494949495
  episode_reward_mean: 289.8865873315824
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 224
  episodes_total: 31270
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.588186776101813e-23
        cur_lr: 5.0e-05
        entropy: 0.1023337251196305
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042389328591525555
        model: {}
        policy_loss: -0.006394250594894402
        total_loss: 0.960899422566096
        vf_explained_var: 0.9980986714363098
        vf_loss: 0.96734486023585
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.006666666666668
    gpu_util_percent0: 0.332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726508214931996
    mean_env_wait_ms: 1.206085899104954
    mean_inference_ms: 4.345921102261516
    mean_raw_obs_processing_ms: 0.3804556311555548
  time_since_restore: 3916.9960153102875
  time_this_iter_s: 25.682409524917603
  time_total_s: 3916.9960153102875
  timers:
    learn_throughput: 8646.054
    learn_time_ms: 18712.814
    sample_throughput: 23954.611
    sample_time_ms: 6754.107
    update_time_ms: 35.037
  timestamp: 1602814057
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    152 |             3917 | 24592384 |  289.887 |              322.949 |              165.677 |            784.867 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3125.6266628476865
    time_step_min: 2911
  date: 2020-10-16_02-08-03
  done: false
  episode_len_mean: 784.8770429252783
  episode_reward_max: 322.949494949495
  episode_reward_mean: 290.0268230797642
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 31450
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.940933880509065e-24
        cur_lr: 5.0e-05
        entropy: 0.09980548856159051
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033851595944724977
        model: {}
        policy_loss: -0.00639325799420476
        total_loss: 0.8838595946629842
        vf_explained_var: 0.9980394244194031
        vf_loss: 0.8903027971585592
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.380000000000003
    gpu_util_percent0: 0.34066666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147259493728854
    mean_env_wait_ms: 1.2060185967503205
    mean_inference_ms: 4.345584444658696
    mean_raw_obs_processing_ms: 0.3804345381707342
  time_since_restore: 3942.791830062866
  time_this_iter_s: 25.795814752578735
  time_total_s: 3942.791830062866
  timers:
    learn_throughput: 8640.872
    learn_time_ms: 18724.036
    sample_throughput: 23948.564
    sample_time_ms: 6755.812
    update_time_ms: 34.855
  timestamp: 1602814083
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    153 |          3942.79 | 24754176 |  290.027 |              322.949 |              165.677 |            784.877 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3124.5686776702714
    time_step_min: 2911
  date: 2020-10-16_02-08-29
  done: false
  episode_len_mean: 784.8738232134959
  episode_reward_max: 322.949494949495
  episode_reward_mean: 290.1883187086636
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 204
  episodes_total: 31654
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.970466940254533e-24
        cur_lr: 5.0e-05
        entropy: 0.10858679997424285
        entropy_coeff: 0.0005000000000000001
        kl: 0.005052414412299792
        model: {}
        policy_loss: -0.006402497742480288
        total_loss: 1.0036109238862991
        vf_explained_var: 0.9979655742645264
        vf_loss: 1.010067731142044
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79333333333334
    gpu_util_percent0: 0.32966666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472529741884691
    mean_env_wait_ms: 1.2059407795817014
    mean_inference_ms: 4.345215513509772
    mean_raw_obs_processing_ms: 0.38041097599201174
  time_since_restore: 3968.523793697357
  time_this_iter_s: 25.731963634490967
  time_total_s: 3968.523793697357
  timers:
    learn_throughput: 8638.983
    learn_time_ms: 18728.129
    sample_throughput: 23939.423
    sample_time_ms: 6758.392
    update_time_ms: 36.531
  timestamp: 1602814109
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    154 |          3968.52 | 24915968 |  290.188 |              322.949 |              165.677 |            784.874 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3123.4244954330015
    time_step_min: 2911
  date: 2020-10-16_02-08-56
  done: false
  episode_len_mean: 784.8950669551855
  episode_reward_max: 322.949494949495
  episode_reward_mean: 290.3605563585806
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 233
  episodes_total: 31887
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.970466940254533e-24
        cur_lr: 5.0e-05
        entropy: 0.11315377925833066
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046128739680474
        model: {}
        policy_loss: -0.007869291555834934
        total_loss: 1.36989626288414
        vf_explained_var: 0.9973950982093811
        vf_loss: 1.3778221309185028
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.19333333333334
    gpu_util_percent0: 0.3373333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724595764329015
    mean_env_wait_ms: 1.2058479051176254
    mean_inference_ms: 4.344805415050589
    mean_raw_obs_processing_ms: 0.380383141281105
  time_since_restore: 3994.399656057358
  time_this_iter_s: 25.87586236000061
  time_total_s: 3994.399656057358
  timers:
    learn_throughput: 8637.949
    learn_time_ms: 18730.373
    sample_throughput: 23879.012
    sample_time_ms: 6775.49
    update_time_ms: 34.921
  timestamp: 1602814136
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    155 |           3994.4 | 25077760 |  290.361 |              322.949 |              165.677 |            784.895 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3122.598477284074
    time_step_min: 2911
  date: 2020-10-16_02-09-22
  done: false
  episode_len_mean: 784.9023257263998
  episode_reward_max: 322.949494949495
  episode_reward_mean: 290.489201782131
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 189
  episodes_total: 32076
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9852334701272663e-24
        cur_lr: 5.0e-05
        entropy: 0.1076653574903806
        entropy_coeff: 0.0005000000000000001
        kl: 0.004928160342387855
        model: {}
        policy_loss: -0.006900109622317056
        total_loss: 1.541863073905309
        vf_explained_var: 0.9966740012168884
        vf_loss: 1.548817018667857
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.633333333333336
    gpu_util_percent0: 0.3539999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472406149263502
    mean_env_wait_ms: 1.2057796754508643
    mean_inference_ms: 4.344473805442597
    mean_raw_obs_processing_ms: 0.3803628902660732
  time_since_restore: 4020.130697488785
  time_this_iter_s: 25.731041431427002
  time_total_s: 4020.130697488785
  timers:
    learn_throughput: 8642.984
    learn_time_ms: 18719.46
    sample_throughput: 23853.713
    sample_time_ms: 6782.676
    update_time_ms: 34.237
  timestamp: 1602814162
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    156 |          4020.13 | 25239552 |  290.489 |              322.949 |              165.677 |            784.902 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3121.689105348058
    time_step_min: 2911
  date: 2020-10-16_02-09-49
  done: false
  episode_len_mean: 784.9205926109596
  episode_reward_max: 322.949494949495
  episode_reward_mean: 290.6225840101985
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 188
  episodes_total: 32264
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.926167350636332e-25
        cur_lr: 5.0e-05
        entropy: 0.10899090021848679
        entropy_coeff: 0.0005000000000000001
        kl: 0.00622218195348978
        model: {}
        policy_loss: -0.009700273299434533
        total_loss: 1.4313796659310658
        vf_explained_var: 0.9970443844795227
        vf_loss: 1.4411344329516094
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.71
    gpu_util_percent0: 0.3916666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14723578830610695
    mean_env_wait_ms: 1.2057065603898773
    mean_inference_ms: 4.344151652738533
    mean_raw_obs_processing_ms: 0.38034295178497607
  time_since_restore: 4045.8811519145966
  time_this_iter_s: 25.750454425811768
  time_total_s: 4045.8811519145966
  timers:
    learn_throughput: 8641.984
    learn_time_ms: 18721.627
    sample_throughput: 23829.719
    sample_time_ms: 6789.505
    update_time_ms: 33.683
  timestamp: 1602814189
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    157 |          4045.88 | 25401344 |  290.623 |              322.949 |              165.677 |            784.921 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3120.5276948181904
    time_step_min: 2911
  date: 2020-10-16_02-10-15
  done: false
  episode_len_mean: 784.932014643
  episode_reward_max: 322.949494949495
  episode_reward_mean: 290.79228125845765
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 243
  episodes_total: 32507
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.926167350636332e-25
        cur_lr: 5.0e-05
        entropy: 0.10831023442248504
        entropy_coeff: 0.0005000000000000001
        kl: 0.004024013391851137
        model: {}
        policy_loss: -0.0047031886836824315
        total_loss: 1.6413616041342418
        vf_explained_var: 0.9970272183418274
        vf_loss: 1.646118958791097
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.19677419354839
    gpu_util_percent0: 0.32032258064516134
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472281214874948
    mean_env_wait_ms: 1.2056096287871858
    mean_inference_ms: 4.343736613851961
    mean_raw_obs_processing_ms: 0.38031479993859507
  time_since_restore: 4071.559717655182
  time_this_iter_s: 25.678565740585327
  time_total_s: 4071.559717655182
  timers:
    learn_throughput: 8651.66
    learn_time_ms: 18700.689
    sample_throughput: 23831.378
    sample_time_ms: 6789.032
    update_time_ms: 33.731
  timestamp: 1602814215
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    158 |          4071.56 | 25563136 |  290.792 |              322.949 |              165.677 |            784.932 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3119.5897475133893
    time_step_min: 2911
  date: 2020-10-16_02-10-41
  done: false
  episode_len_mean: 784.9533070360517
  episode_reward_max: 322.949494949495
  episode_reward_mean: 290.9389528715277
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 32703
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.963083675318166e-25
        cur_lr: 5.0e-05
        entropy: 0.09658111135164897
        entropy_coeff: 0.0005000000000000001
        kl: 0.003561567592745026
        model: {}
        policy_loss: -0.007376952193832646
        total_loss: 0.9705705145994822
        vf_explained_var: 0.9979267716407776
        vf_loss: 0.977995773156484
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.360000000000007
    gpu_util_percent0: 0.3583333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472223579963529
    mean_env_wait_ms: 1.205534125902415
    mean_inference_ms: 4.343398639431036
    mean_raw_obs_processing_ms: 0.38029339522496836
  time_since_restore: 4097.316283464432
  time_this_iter_s: 25.756565809249878
  time_total_s: 4097.316283464432
  timers:
    learn_throughput: 8636.172
    learn_time_ms: 18734.227
    sample_throughput: 23817.798
    sample_time_ms: 6792.903
    update_time_ms: 35.913
  timestamp: 1602814241
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    159 |          4097.32 | 25724928 |  290.939 |              322.949 |              165.677 |            784.953 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3118.6612692132094
    time_step_min: 2911
  date: 2020-10-16_02-11-08
  done: false
  episode_len_mean: 784.972903932123
  episode_reward_max: 322.949494949495
  episode_reward_mean: 291.07895117584
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 32883
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.481541837659083e-25
        cur_lr: 5.0e-05
        entropy: 0.0931390995780627
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032191025093197823
        model: {}
        policy_loss: -0.0061185018421383575
        total_loss: 0.9399406015872955
        vf_explained_var: 0.9979282021522522
        vf_loss: 0.9461056639750799
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.880000000000003
    gpu_util_percent0: 0.327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472176456249089
    mean_env_wait_ms: 1.2054643139989092
    mean_inference_ms: 4.343093466403586
    mean_raw_obs_processing_ms: 0.38027454238303754
  time_since_restore: 4123.0501618385315
  time_this_iter_s: 25.73387837409973
  time_total_s: 4123.0501618385315
  timers:
    learn_throughput: 8637.653
    learn_time_ms: 18731.014
    sample_throughput: 23792.937
    sample_time_ms: 6800.001
    update_time_ms: 36.108
  timestamp: 1602814268
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    160 |          4123.05 | 25886720 |  291.079 |              322.949 |              165.677 |            784.973 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3117.5562241103016
    time_step_min: 2911
  date: 2020-10-16_02-11-34
  done: false
  episode_len_mean: 785.0092746442706
  episode_reward_max: 322.949494949495
  episode_reward_mean: 291.2429045599341
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 218
  episodes_total: 33101
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2407709188295415e-25
        cur_lr: 5.0e-05
        entropy: 0.09919483276704948
        entropy_coeff: 0.0005000000000000001
        kl: 0.003580261894967407
        model: {}
        policy_loss: -0.006728150610191126
        total_loss: 0.9554808040459951
        vf_explained_var: 0.9982123970985413
        vf_loss: 0.9622585624456406
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.645161290322584
    gpu_util_percent0: 0.3267741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721086894230995
    mean_env_wait_ms: 1.2053734742452051
    mean_inference_ms: 4.342726021038663
    mean_raw_obs_processing_ms: 0.38024957355092925
  time_since_restore: 4149.197988033295
  time_this_iter_s: 26.147826194763184
  time_total_s: 4149.197988033295
  timers:
    learn_throughput: 8628.92
    learn_time_ms: 18749.971
    sample_throughput: 23774.989
    sample_time_ms: 6805.135
    update_time_ms: 36.831
  timestamp: 1602814294
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    161 |           4149.2 | 26048512 |  291.243 |              322.949 |              165.677 |            785.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3116.447514641838
    time_step_min: 2911
  date: 2020-10-16_02-12-01
  done: false
  episode_len_mean: 785.0520661405036
  episode_reward_max: 322.949494949495
  episode_reward_mean: 291.41087706886094
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 33323
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.203854594147707e-26
        cur_lr: 5.0e-05
        entropy: 0.09146088485916455
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043162448176493244
        model: {}
        policy_loss: -0.006042320397682488
        total_loss: 0.8077837278445562
        vf_explained_var: 0.9983841776847839
        vf_loss: 0.8138718058665594
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.776666666666664
    gpu_util_percent0: 0.3126666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472049900000051
    mean_env_wait_ms: 1.2052834507936674
    mean_inference_ms: 4.342372077145553
    mean_raw_obs_processing_ms: 0.3802259312055493
  time_since_restore: 4175.041079998016
  time_this_iter_s: 25.84309196472168
  time_total_s: 4175.041079998016
  timers:
    learn_throughput: 8621.72
    learn_time_ms: 18765.63
    sample_throughput: 23781.676
    sample_time_ms: 6803.221
    update_time_ms: 37.243
  timestamp: 1602814321
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    162 |          4175.04 | 26210304 |  291.411 |              322.949 |              165.677 |            785.052 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3115.531620612397
    time_step_min: 2911
  date: 2020-10-16_02-12-28
  done: false
  episode_len_mean: 785.0822612900338
  episode_reward_max: 322.949494949495
  episode_reward_mean: 291.54924494926865
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 33503
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1019272970738537e-26
        cur_lr: 5.0e-05
        entropy: 0.08848617412149906
        entropy_coeff: 0.0005000000000000001
        kl: 0.004298558943749716
        model: {}
        policy_loss: -0.005143022446039443
        total_loss: 0.6448891758918762
        vf_explained_var: 0.9985284209251404
        vf_loss: 0.6500764638185501
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.763333333333332
    gpu_util_percent0: 0.33666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472001439734141
    mean_env_wait_ms: 1.2052132700583045
    mean_inference_ms: 4.342065961379452
    mean_raw_obs_processing_ms: 0.38020747716934616
  time_since_restore: 4200.913504123688
  time_this_iter_s: 25.872424125671387
  time_total_s: 4200.913504123688
  timers:
    learn_throughput: 8617.476
    learn_time_ms: 18774.87
    sample_throughput: 23783.595
    sample_time_ms: 6802.672
    update_time_ms: 35.648
  timestamp: 1602814348
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    163 |          4200.91 | 26372096 |  291.549 |              322.949 |              165.677 |            785.082 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3114.535839420393
    time_step_min: 2911
  date: 2020-10-16_02-12-54
  done: false
  episode_len_mean: 785.1048774698867
  episode_reward_max: 322.949494949495
  episode_reward_mean: 291.7007648429946
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 203
  episodes_total: 33706
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5509636485369268e-26
        cur_lr: 5.0e-05
        entropy: 0.09310799154142539
        entropy_coeff: 0.0005000000000000001
        kl: 0.003395163919776678
        model: {}
        policy_loss: -0.008558671055652667
        total_loss: 0.7704602827628454
        vf_explained_var: 0.9984163641929626
        vf_loss: 0.7790655195713043
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84
    gpu_util_percent0: 0.376
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719463870798746
    mean_env_wait_ms: 1.2051296301949572
    mean_inference_ms: 4.3417445418699705
    mean_raw_obs_processing_ms: 0.3801869039625539
  time_since_restore: 4226.771390914917
  time_this_iter_s: 25.857886791229248
  time_total_s: 4226.771390914917
  timers:
    learn_throughput: 8614.46
    learn_time_ms: 18781.443
    sample_throughput: 23769.27
    sample_time_ms: 6806.772
    update_time_ms: 36.105
  timestamp: 1602814374
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    164 |          4226.77 | 26533888 |  291.701 |              322.949 |              165.677 |            785.105 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3113.3557442793112
    time_step_min: 2911
  date: 2020-10-16_02-13-20
  done: false
  episode_len_mean: 785.1280789628756
  episode_reward_max: 322.949494949495
  episode_reward_mean: 291.88104527895325
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 234
  episodes_total: 33940
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.754818242684634e-27
        cur_lr: 5.0e-05
        entropy: 0.09353343024849892
        entropy_coeff: 0.0005000000000000001
        kl: 0.004312323871999979
        model: {}
        policy_loss: -0.008740249943609038
        total_loss: 0.728402853012085
        vf_explained_var: 0.9985737800598145
        vf_loss: 0.7371898839871088
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.996666666666666
    gpu_util_percent0: 0.35433333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471884596459064
    mean_env_wait_ms: 1.2050315752213951
    mean_inference_ms: 4.341381944130051
    mean_raw_obs_processing_ms: 0.3801625555114187
  time_since_restore: 4252.503201723099
  time_this_iter_s: 25.731810808181763
  time_total_s: 4252.503201723099
  timers:
    learn_throughput: 8613.815
    learn_time_ms: 18782.85
    sample_throughput: 23835.361
    sample_time_ms: 6787.898
    update_time_ms: 38.186
  timestamp: 1602814400
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    165 |           4252.5 | 26695680 |  291.881 |              322.949 |              165.677 |            785.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3112.415642687469
    time_step_min: 2911
  date: 2020-10-16_02-13-47
  done: false
  episode_len_mean: 785.1393910979576
  episode_reward_max: 322.949494949495
  episode_reward_mean: 292.0200966502721
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 34127
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.877409121342317e-27
        cur_lr: 5.0e-05
        entropy: 0.08637927162150542
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035971637698821723
        model: {}
        policy_loss: -0.006252204504562542
        total_loss: 0.9100011040767034
        vf_explained_var: 0.9979696869850159
        vf_loss: 0.9162964820861816
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.696666666666665
    gpu_util_percent0: 0.3043333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718327209848953
    mean_env_wait_ms: 1.2049569193109362
    mean_inference_ms: 4.341082524672403
    mean_raw_obs_processing_ms: 0.38014325750003664
  time_since_restore: 4278.276359796524
  time_this_iter_s: 25.773158073425293
  time_total_s: 4278.276359796524
  timers:
    learn_throughput: 8609.848
    learn_time_ms: 18791.504
    sample_throughput: 23857.204
    sample_time_ms: 6781.683
    update_time_ms: 38.872
  timestamp: 1602814427
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    166 |          4278.28 | 26857472 |   292.02 |              322.949 |              165.677 |            785.139 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3111.526893011317
    time_step_min: 2911
  date: 2020-10-16_02-14-13
  done: false
  episode_len_mean: 785.1514339006761
  episode_reward_max: 322.949494949495
  episode_reward_mean: 292.15683560953426
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 34312
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9387045606711585e-27
        cur_lr: 5.0e-05
        entropy: 0.08582764801879723
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034452443554376564
        model: {}
        policy_loss: -0.005716490947330992
        total_loss: 0.7858838985363642
        vf_explained_var: 0.9983651638031006
        vf_loss: 0.7916433016459147
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56666666666667
    gpu_util_percent0: 0.3473333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471788564483331
    mean_env_wait_ms: 1.2048795627686169
    mean_inference_ms: 4.34079916457652
    mean_raw_obs_processing_ms: 0.3801255519733692
  time_since_restore: 4304.16649389267
  time_this_iter_s: 25.89013409614563
  time_total_s: 4304.16649389267
  timers:
    learn_throughput: 8603.685
    learn_time_ms: 18804.966
    sample_throughput: 23860.645
    sample_time_ms: 6780.705
    update_time_ms: 39.502
  timestamp: 1602814453
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    167 |          4304.17 | 27019264 |  292.157 |              322.949 |              165.677 |            785.151 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3110.3057494569152
    time_step_min: 2911
  date: 2020-10-16_02-14-40
  done: false
  episode_len_mean: 785.1713888808497
  episode_reward_max: 322.949494949495
  episode_reward_mean: 292.34016751311907
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 241
  episodes_total: 34553
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.693522803355793e-28
        cur_lr: 5.0e-05
        entropy: 0.09155099838972092
        entropy_coeff: 0.0005000000000000001
        kl: 0.004305910008649032
        model: {}
        policy_loss: -0.007175538376031909
        total_loss: 0.751959890127182
        vf_explained_var: 0.9985924363136292
        vf_loss: 0.7591811865568161
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.836666666666666
    gpu_util_percent0: 0.3413333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717260038742086
    mean_env_wait_ms: 1.204776918296679
    mean_inference_ms: 4.3404337985146055
    mean_raw_obs_processing_ms: 0.3801007348441085
  time_since_restore: 4329.788554906845
  time_this_iter_s: 25.622061014175415
  time_total_s: 4329.788554906845
  timers:
    learn_throughput: 8604.152
    learn_time_ms: 18803.946
    sample_throughput: 23875.571
    sample_time_ms: 6776.466
    update_time_ms: 39.361
  timestamp: 1602814480
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    168 |          4329.79 | 27181056 |   292.34 |              322.949 |              165.677 |            785.171 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3109.3084144445083
    time_step_min: 2911
  date: 2020-10-16_02-15-06
  done: false
  episode_len_mean: 785.2018184957127
  episode_reward_max: 322.949494949495
  episode_reward_mean: 292.4891075687529
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 201
  episodes_total: 34754
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.846761401677896e-28
        cur_lr: 5.0e-05
        entropy: 0.08357557592292626
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034137693194982908
        model: {}
        policy_loss: -0.0077872208882278455
        total_loss: 0.6555629769961039
        vf_explained_var: 0.9986140131950378
        vf_loss: 0.6633919676144918
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793333333333337
    gpu_util_percent0: 0.3223333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471671241921211
    mean_env_wait_ms: 1.204693086963136
    mean_inference_ms: 4.340121567841661
    mean_raw_obs_processing_ms: 0.3800805243249866
  time_since_restore: 4355.348658323288
  time_this_iter_s: 25.56010341644287
  time_total_s: 4355.348658323288
  timers:
    learn_throughput: 8608.353
    learn_time_ms: 18794.769
    sample_throughput: 23917.451
    sample_time_ms: 6764.6
    update_time_ms: 38.755
  timestamp: 1602814506
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    169 |          4355.35 | 27342848 |  292.489 |              322.949 |              165.677 |            785.202 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3108.421773384902
    time_step_min: 2911
  date: 2020-10-16_02-15-32
  done: false
  episode_len_mean: 785.2368534050897
  episode_reward_max: 322.949494949495
  episode_reward_mean: 292.62171597172875
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 34933
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.423380700838948e-28
        cur_lr: 5.0e-05
        entropy: 0.08095347136259079
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034613196621648967
        model: {}
        policy_loss: -0.007188076355305384
        total_loss: 0.5808649261792501
        vf_explained_var: 0.9986791014671326
        vf_loss: 0.588093489408493
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60333333333334
    gpu_util_percent0: 0.342
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471629449057623
    mean_env_wait_ms: 1.2046194861027892
    mean_inference_ms: 4.339853604570912
    mean_raw_obs_processing_ms: 0.38006361996433596
  time_since_restore: 4380.947330236435
  time_this_iter_s: 25.598671913146973
  time_total_s: 4380.947330236435
  timers:
    learn_throughput: 8613.069
    learn_time_ms: 18784.478
    sample_throughput: 23934.178
    sample_time_ms: 6759.873
    update_time_ms: 38.635
  timestamp: 1602814532
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    170 |          4380.95 | 27504640 |  292.622 |              322.949 |              165.677 |            785.237 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3107.335317234309
    time_step_min: 2911
  date: 2020-10-16_02-15-59
  done: false
  episode_len_mean: 785.2782836330526
  episode_reward_max: 322.949494949495
  episode_reward_mean: 292.783636214179
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 211
  episodes_total: 35144
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.211690350419474e-28
        cur_lr: 5.0e-05
        entropy: 0.08778457343578339
        entropy_coeff: 0.0005000000000000001
        kl: 0.004638482234440744
        model: {}
        policy_loss: -0.006932026532012969
        total_loss: 0.5159081667661667
        vf_explained_var: 0.9990249276161194
        vf_loss: 0.5228840832908949
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.538709677419355
    gpu_util_percent0: 0.3561290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715737103237864
    mean_env_wait_ms: 1.2045274450232504
    mean_inference_ms: 4.339540027375847
    mean_raw_obs_processing_ms: 0.38004298729472596
  time_since_restore: 4406.944608926773
  time_this_iter_s: 25.997278690338135
  time_total_s: 4406.944608926773
  timers:
    learn_throughput: 8626.792
    learn_time_ms: 18754.595
    sample_throughput: 23908.53
    sample_time_ms: 6767.125
    update_time_ms: 38.326
  timestamp: 1602814559
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    171 |          4406.94 | 27666432 |  292.784 |              322.949 |              165.677 |            785.278 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3106.272295156179
    time_step_min: 2911
  date: 2020-10-16_02-16-25
  done: false
  episode_len_mean: 785.3236458215538
  episode_reward_max: 322.949494949495
  episode_reward_mean: 292.9409022944587
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 228
  episodes_total: 35372
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.05845175209737e-29
        cur_lr: 5.0e-05
        entropy: 0.09214654378592968
        entropy_coeff: 0.0005000000000000001
        kl: 0.004726446932181716
        model: {}
        policy_loss: -0.008114141470286995
        total_loss: 0.7745508005221685
        vf_explained_var: 0.9984853863716125
        vf_loss: 0.7827110141515732
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.210344827586212
    gpu_util_percent0: 0.27689655172413785
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715156687467085
    mean_env_wait_ms: 1.2044282388441878
    mean_inference_ms: 4.339210702801202
    mean_raw_obs_processing_ms: 0.38002062694271654
  time_since_restore: 4432.618064165115
  time_this_iter_s: 25.673455238342285
  time_total_s: 4432.618064165115
  timers:
    learn_throughput: 8637.166
    learn_time_ms: 18732.069
    sample_throughput: 23881.052
    sample_time_ms: 6774.911
    update_time_ms: 36.023
  timestamp: 1602814585
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: 1bbc1_00000
  
2020-10-16 02:16:26,389	WARNING util.py:136 -- The `process_trial` operation took 0.5236575603485107 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    172 |          4432.62 | 27828224 |  292.941 |              322.949 |              165.677 |            785.324 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3105.4314989444056
    time_step_min: 2911
  date: 2020-10-16_02-16-52
  done: false
  episode_len_mean: 785.3590133040813
  episode_reward_max: 322.949494949495
  episode_reward_mean: 293.07136592488024
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 35553
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.029225876048685e-29
        cur_lr: 5.0e-05
        entropy: 0.08283252703646819
        entropy_coeff: 0.0005000000000000001
        kl: 0.00479399257649978
        model: {}
        policy_loss: -0.006134660390671343
        total_loss: 0.4933510993917783
        vf_explained_var: 0.9988719820976257
        vf_loss: 0.499527171254158
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.580645161290327
    gpu_util_percent0: 0.29806451612903223
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714752303711975
    mean_env_wait_ms: 1.2043536849021688
    mean_inference_ms: 4.338945517017149
    mean_raw_obs_processing_ms: 0.3800040478822569
  time_since_restore: 4458.623368263245
  time_this_iter_s: 26.005304098129272
  time_total_s: 4458.623368263245
  timers:
    learn_throughput: 8634.747
    learn_time_ms: 18737.318
    sample_throughput: 23854.449
    sample_time_ms: 6782.466
    update_time_ms: 36.032
  timestamp: 1602814612
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    173 |          4458.62 | 27990016 |  293.071 |              322.949 |              165.677 |            785.359 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3104.5177906553567
    time_step_min: 2911
  date: 2020-10-16_02-17-18
  done: false
  episode_len_mean: 785.3829757475734
  episode_reward_max: 322.949494949495
  episode_reward_mean: 293.21319378574117
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 35749
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5146129380243426e-29
        cur_lr: 5.0e-05
        entropy: 0.0851676482707262
        entropy_coeff: 0.0005000000000000001
        kl: 0.004261176101863384
        model: {}
        policy_loss: -0.008063906139189688
        total_loss: 0.5817714631557465
        vf_explained_var: 0.9987987875938416
        vf_loss: 0.5898779481649399
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.75666666666667
    gpu_util_percent0: 0.312
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714285170058666
    mean_env_wait_ms: 1.2042676803090295
    mean_inference_ms: 4.338665729923968
    mean_raw_obs_processing_ms: 0.3799864100765283
  time_since_restore: 4484.229665994644
  time_this_iter_s: 25.606297731399536
  time_total_s: 4484.229665994644
  timers:
    learn_throughput: 8646.631
    learn_time_ms: 18711.564
    sample_throughput: 23857.337
    sample_time_ms: 6781.645
    update_time_ms: 36.184
  timestamp: 1602814638
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    174 |          4484.23 | 28151808 |  293.213 |              322.949 |              165.677 |            785.383 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3103.4222543593737
    time_step_min: 2911
  date: 2020-10-16_02-17-45
  done: false
  episode_len_mean: 785.410393219397
  episode_reward_max: 322.949494949495
  episode_reward_mean: 293.3828615458459
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 236
  episodes_total: 35985
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.573064690121713e-30
        cur_lr: 5.0e-05
        entropy: 0.0884343832731247
        entropy_coeff: 0.0005000000000000001
        kl: 0.003649043404341986
        model: {}
        policy_loss: -0.007304549051089755
        total_loss: 0.555912010371685
        vf_explained_var: 0.9989088177680969
        vf_loss: 0.5632607887188593
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.706451612903226
    gpu_util_percent0: 0.344516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147136788448426
    mean_env_wait_ms: 1.2041631202712753
    mean_inference_ms: 4.338334061839233
    mean_raw_obs_processing_ms: 0.3799634459543534
  time_since_restore: 4510.113600254059
  time_this_iter_s: 25.883934259414673
  time_total_s: 4510.113600254059
  timers:
    learn_throughput: 8642.296
    learn_time_ms: 18720.952
    sample_throughput: 23837.793
    sample_time_ms: 6787.206
    update_time_ms: 35.784
  timestamp: 1602814665
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: 1bbc1_00000
  
2020-10-16 02:17:46,083	WARNING util.py:136 -- The `process_trial` operation took 0.517841100692749 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    175 |          4510.11 | 28313600 |  293.383 |              322.949 |              165.677 |             785.41 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3102.4748969599736
    time_step_min: 2911
  date: 2020-10-16_02-18-11
  done: false
  episode_len_mean: 785.4392327040548
  episode_reward_max: 322.949494949495
  episode_reward_mean: 293.5219390343357
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 36179
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7865323450608565e-30
        cur_lr: 5.0e-05
        entropy: 0.0787006306151549
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036709000123664737
        model: {}
        policy_loss: -0.004595174075802788
        total_loss: 0.7225270171960195
        vf_explained_var: 0.9984931349754333
        vf_loss: 0.7271615614493688
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583333333333332
    gpu_util_percent0: 0.2856666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713227989447766
    mean_env_wait_ms: 1.2040816848956775
    mean_inference_ms: 4.33806116128588
    mean_raw_obs_processing_ms: 0.3799459016572399
  time_since_restore: 4535.618237733841
  time_this_iter_s: 25.504637479782104
  time_total_s: 4535.618237733841
  timers:
    learn_throughput: 8652.833
    learn_time_ms: 18698.154
    sample_throughput: 23851.321
    sample_time_ms: 6783.356
    update_time_ms: 33.907
  timestamp: 1602814691
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: 1bbc1_00000
  
2020-10-16 02:18:12,475	WARNING util.py:136 -- The `process_trial` operation took 0.5098490715026855 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    176 |          4535.62 | 28475392 |  293.522 |              322.949 |              165.677 |            785.439 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3101.597412962708
    time_step_min: 2911
  date: 2020-10-16_02-18-38
  done: false
  episode_len_mean: 785.466105656849
  episode_reward_max: 322.949494949495
  episode_reward_mean: 293.6533155996895
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 184
  episodes_total: 36363
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8932661725304283e-30
        cur_lr: 5.0e-05
        entropy: 0.08089341161151727
        entropy_coeff: 0.0005000000000000001
        kl: 0.00411803296689565
        model: {}
        policy_loss: -0.00911318116413895
        total_loss: 0.35520243148008984
        vf_explained_var: 0.9991965889930725
        vf_loss: 0.3643560732404391
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.033333333333335
    gpu_util_percent0: 0.3383333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147128103514862
    mean_env_wait_ms: 1.2040013637478892
    mean_inference_ms: 4.337803767773457
    mean_raw_obs_processing_ms: 0.37992953899354637
  time_since_restore: 4561.433084487915
  time_this_iter_s: 25.814846754074097
  time_total_s: 4561.433084487915
  timers:
    learn_throughput: 8656.153
    learn_time_ms: 18690.983
    sample_throughput: 23856.912
    sample_time_ms: 6781.766
    update_time_ms: 34.288
  timestamp: 1602814718
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    177 |          4561.43 | 28637184 |  293.653 |              322.949 |              165.677 |            785.466 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3100.5856881035097
    time_step_min: 2911
  date: 2020-10-16_02-19-04
  done: false
  episode_len_mean: 785.5065190651907
  episode_reward_max: 322.949494949495
  episode_reward_mean: 293.805624924936
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 36585
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.466330862652141e-31
        cur_lr: 5.0e-05
        entropy: 0.08789925587673982
        entropy_coeff: 0.0005000000000000001
        kl: 0.003104199713561684
        model: {}
        policy_loss: -0.006723941087936207
        total_loss: 0.6277522544066111
        vf_explained_var: 0.9987578988075256
        vf_loss: 0.6345201581716537
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.69
    gpu_util_percent0: 0.3426666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712249388154783
    mean_env_wait_ms: 1.2039013351548487
    mean_inference_ms: 4.337500709866229
    mean_raw_obs_processing_ms: 0.37990925834499484
  time_since_restore: 4587.14660525322
  time_this_iter_s: 25.713520765304565
  time_total_s: 4587.14660525322
  timers:
    learn_throughput: 8657.689
    learn_time_ms: 18687.665
    sample_throughput: 23819.242
    sample_time_ms: 6792.492
    update_time_ms: 34.786
  timestamp: 1602814744
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: 1bbc1_00000
  
2020-10-16 02:19:05,421	WARNING util.py:136 -- The `process_trial` operation took 0.5201728343963623 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    178 |          4587.15 | 28798976 |  293.806 |              322.949 |              165.677 |            785.507 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3099.5804872743092
    time_step_min: 2911
  date: 2020-10-16_02-19-31
  done: false
  episode_len_mean: 785.5543147483969
  episode_reward_max: 322.949494949495
  episode_reward_mean: 293.9584350185914
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 219
  episodes_total: 36804
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.733165431326071e-31
        cur_lr: 5.0e-05
        entropy: 0.0805077850818634
        entropy_coeff: 0.0005000000000000001
        kl: 0.003880276325313995
        model: {}
        policy_loss: -0.007647591837060948
        total_loss: 0.30943258106708527
        vf_explained_var: 0.9993446469306946
        vf_loss: 0.31712042291959125
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78709677419355
    gpu_util_percent0: 0.33129032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711780053536064
    mean_env_wait_ms: 1.2038052774359007
    mean_inference_ms: 4.337204458436287
    mean_raw_obs_processing_ms: 0.37988967853374495
  time_since_restore: 4613.003264427185
  time_this_iter_s: 25.856659173965454
  time_total_s: 4613.003264427185
  timers:
    learn_throughput: 8651.383
    learn_time_ms: 18701.287
    sample_throughput: 23758.886
    sample_time_ms: 6809.747
    update_time_ms: 34.228
  timestamp: 1602814771
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    179 |             4613 | 28960768 |  293.958 |              322.949 |              165.677 |            785.554 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3098.7491746495643
    time_step_min: 2911
  date: 2020-10-16_02-19-57
  done: false
  episode_len_mean: 785.587556108377
  episode_reward_max: 322.949494949495
  episode_reward_mean: 294.08453935275077
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 178
  episodes_total: 36982
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3665827156630353e-31
        cur_lr: 5.0e-05
        entropy: 0.07300609660645326
        entropy_coeff: 0.0005000000000000001
        kl: 0.003307671887644877
        model: {}
        policy_loss: -0.006765755659822996
        total_loss: 0.29058662553628284
        vf_explained_var: 0.9993298053741455
        vf_loss: 0.29738887399435043
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.136666666666667
    gpu_util_percent0: 0.328
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711403095161477
    mean_env_wait_ms: 1.203727089130846
    mean_inference_ms: 4.3369597899426555
    mean_raw_obs_processing_ms: 0.3798735697190415
  time_since_restore: 4638.862505435944
  time_this_iter_s: 25.859241008758545
  time_total_s: 4638.862505435944
  timers:
    learn_throughput: 8639.575
    learn_time_ms: 18726.846
    sample_throughput: 23753.109
    sample_time_ms: 6811.403
    update_time_ms: 34.186
  timestamp: 1602814797
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: 1bbc1_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    180 |          4638.86 | 29122560 |  294.085 |              322.949 |              165.677 |            785.588 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3097.848772741953
    time_step_min: 2911
  date: 2020-10-16_02-20-24
  done: false
  episode_len_mean: 785.6351118760757
  episode_reward_max: 322.949494949495
  episode_reward_mean: 294.2211318216588
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 202
  episodes_total: 37184
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1832913578315177e-31
        cur_lr: 5.0e-05
        entropy: 0.0785882230848074
        entropy_coeff: 0.0005000000000000001
        kl: 0.003213603030114124
        model: {}
        policy_loss: -0.0062722993607167155
        total_loss: 0.525318351884683
        vf_explained_var: 0.9989460110664368
        vf_loss: 0.5316299324234327
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.986666666666665
    gpu_util_percent0: 0.32300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147109364317297
    mean_env_wait_ms: 1.203636772384577
    mean_inference_ms: 4.336699635583368
    mean_raw_obs_processing_ms: 0.3798565948722807
  time_since_restore: 4664.73545384407
  time_this_iter_s: 25.87294840812683
  time_total_s: 4664.73545384407
  timers:
    learn_throughput: 8634.835
    learn_time_ms: 18737.128
    sample_throughput: 23800.213
    sample_time_ms: 6797.922
    update_time_ms: 32.697
  timestamp: 1602814824
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: 1bbc1_00000
  
2020-10-16 02:20:25,245	WARNING util.py:136 -- The `process_trial` operation took 0.5019245147705078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    181 |          4664.74 | 29284352 |  294.221 |              322.949 |              165.677 |            785.635 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3096.8056595699154
    time_step_min: 2911
  date: 2020-10-16_02-20-50
  done: false
  episode_len_mean: 785.6838251015608
  episode_reward_max: 322.949494949495
  episode_reward_mean: 294.37878139962794
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 232
  episodes_total: 37416
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.916456789157588e-32
        cur_lr: 5.0e-05
        entropy: 0.08470822994907697
        entropy_coeff: 0.0005000000000000001
        kl: 0.005409430867681901
        model: {}
        policy_loss: -0.0077686492101444555
        total_loss: 0.4901705930630366
        vf_explained_var: 0.99903804063797
        vf_loss: 0.49798159549633664
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.238709677419358
    gpu_util_percent0: 0.33322580645161287
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710396074477933
    mean_env_wait_ms: 1.203529772817149
    mean_inference_ms: 4.336395137281662
    mean_raw_obs_processing_ms: 0.37983549058605065
  time_since_restore: 4690.488206386566
  time_this_iter_s: 25.752752542495728
  time_total_s: 4690.488206386566
  timers:
    learn_throughput: 8628.778
    learn_time_ms: 18750.279
    sample_throughput: 23823.925
    sample_time_ms: 6791.156
    update_time_ms: 33.91
  timestamp: 1602814850
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: 1bbc1_00000
  
2020-10-16 02:20:51,985	WARNING util.py:136 -- The `process_trial` operation took 0.5175173282623291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    182 |          4690.49 | 29446144 |  294.379 |              322.949 |              165.677 |            785.684 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3096.0026879574184
    time_step_min: 2911
  date: 2020-10-16_02-21-17
  done: false
  episode_len_mean: 785.722522139191
  episode_reward_max: 322.949494949495
  episode_reward_mean: 294.500680823607
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 37603
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.916456789157588e-32
        cur_lr: 5.0e-05
        entropy: 0.08573338265220325
        entropy_coeff: 0.0005000000000000001
        kl: 0.007577754906378686
        model: {}
        policy_loss: -0.00856442174942155
        total_loss: 0.531982071697712
        vf_explained_var: 0.9988100528717041
        vf_loss: 0.5405893698334694
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.980000000000004
    gpu_util_percent0: 0.3350000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470999627710658
    mean_env_wait_ms: 1.2034490988076538
    mean_inference_ms: 4.336154804883044
    mean_raw_obs_processing_ms: 0.3798201296069053
  time_since_restore: 4716.443042039871
  time_this_iter_s: 25.954835653305054
  time_total_s: 4716.443042039871
  timers:
    learn_throughput: 8630.648
    learn_time_ms: 18746.217
    sample_throughput: 23843.48
    sample_time_ms: 6785.587
    update_time_ms: 36.408
  timestamp: 1602814877
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: 1bbc1_00000
  
2020-10-16 02:21:18,661	WARNING util.py:136 -- The `process_trial` operation took 0.5387675762176514 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    183 |          4716.44 | 29607936 |  294.501 |              322.949 |              165.677 |            785.723 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3095.24792097039
    time_step_min: 2911
  date: 2020-10-16_02-21-44
  done: false
  episode_len_mean: 785.7557296353147
  episode_reward_max: 322.949494949495
  episode_reward_mean: 294.6128454929861
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 183
  episodes_total: 37786
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.916456789157588e-32
        cur_lr: 5.0e-05
        entropy: 0.0843937061727047
        entropy_coeff: 0.0005000000000000001
        kl: 0.003957904370812078
        model: {}
        policy_loss: -0.008532061687825868
        total_loss: 0.6073164343833923
        vf_explained_var: 0.9986926913261414
        vf_loss: 0.6158907115459442
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35333333333334
    gpu_util_percent0: 0.30600000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709622546298623
    mean_env_wait_ms: 1.2033661904820883
    mean_inference_ms: 4.335920976814477
    mean_raw_obs_processing_ms: 0.3798053129851546
  time_since_restore: 4742.234532356262
  time_this_iter_s: 25.79149031639099
  time_total_s: 4742.234532356262
  timers:
    learn_throughput: 8622.035
    learn_time_ms: 18764.943
    sample_throughput: 23844.611
    sample_time_ms: 6785.265
    update_time_ms: 35.772
  timestamp: 1602814904
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: 1bbc1_00000
  
2020-10-16 02:21:45,191	WARNING util.py:136 -- The `process_trial` operation took 0.5560519695281982 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    184 |          4742.23 | 29769728 |  294.613 |              322.949 |              165.677 |            785.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3094.2574022897747
    time_step_min: 2911
  date: 2020-10-16_02-22-10
  done: false
  episode_len_mean: 785.7963337979644
  episode_reward_max: 322.949494949495
  episode_reward_mean: 294.7646403280097
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 237
  episodes_total: 38023
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.958228394578794e-32
        cur_lr: 5.0e-05
        entropy: 0.08325838918487231
        entropy_coeff: 0.0005000000000000001
        kl: 0.004747617679337661
        model: {}
        policy_loss: -0.008388149144593626
        total_loss: 0.5851376901070277
        vf_explained_var: 0.9988760352134705
        vf_loss: 0.593567485610644
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06129032258065
    gpu_util_percent0: 0.34225806451612906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709050890421588
    mean_env_wait_ms: 1.2032569261554207
    mean_inference_ms: 4.335623853019004
    mean_raw_obs_processing_ms: 0.37978478476133687
  time_since_restore: 4768.021245002747
  time_this_iter_s: 25.786712646484375
  time_total_s: 4768.021245002747
  timers:
    learn_throughput: 8625.867
    learn_time_ms: 18756.607
    sample_throughput: 23855.706
    sample_time_ms: 6782.109
    update_time_ms: 36.225
  timestamp: 1602814930
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: 1bbc1_00000
  
2020-10-16 02:22:11,829	WARNING util.py:136 -- The `process_trial` operation took 0.5001020431518555 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    185 |          4768.02 | 29931520 |  294.765 |              322.949 |              165.677 |            785.796 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3093.4021622470614
    time_step_min: 2911
  date: 2020-10-16_02-22-37
  done: false
  episode_len_mean: 785.835517539041
  episode_reward_max: 322.949494949495
  episode_reward_mean: 294.8986379529421
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 206
  episodes_total: 38229
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.479114197289397e-32
        cur_lr: 5.0e-05
        entropy: 0.07829085178673267
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037970840154836574
        model: {}
        policy_loss: -0.006745670407932873
        total_loss: 0.41868604967991513
        vf_explained_var: 0.9991149306297302
        vf_loss: 0.42547085881233215
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.88
    gpu_util_percent0: 0.33866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708643145460482
    mean_env_wait_ms: 1.2031636812493023
    mean_inference_ms: 4.335361688485476
    mean_raw_obs_processing_ms: 0.37976772720305096
  time_since_restore: 4793.831898927689
  time_this_iter_s: 25.810653924942017
  time_total_s: 4793.831898927689
  timers:
    learn_throughput: 8619.638
    learn_time_ms: 18770.163
    sample_throughput: 23797.996
    sample_time_ms: 6798.556
    update_time_ms: 36.532
  timestamp: 1602814957
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: 1bbc1_00000
  
2020-10-16 02:22:38,394	WARNING util.py:136 -- The `process_trial` operation took 0.5110936164855957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    186 |          4793.83 | 30093312 |  294.899 |              322.949 |              165.677 |            785.836 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3092.6298593017195
    time_step_min: 2911
  date: 2020-10-16_02-23-04
  done: false
  episode_len_mean: 785.8742970214539
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.0164846233633
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 38408
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.395570986446985e-33
        cur_lr: 5.0e-05
        entropy: 0.07268281032641728
        entropy_coeff: 0.0005000000000000001
        kl: 0.004206029271396498
        model: {}
        policy_loss: -0.007032360059383791
        total_loss: 0.26508701716860134
        vf_explained_var: 0.999407947063446
        vf_loss: 0.27215572198232013
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87333333333334
    gpu_util_percent0: 0.30199999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708300960422338
    mean_env_wait_ms: 1.2030831809937725
    mean_inference_ms: 4.335140554384618
    mean_raw_obs_processing_ms: 0.379753549613337
  time_since_restore: 4819.747030973434
  time_this_iter_s: 25.91513204574585
  time_total_s: 4819.747030973434
  timers:
    learn_throughput: 8617.989
    learn_time_ms: 18773.753
    sample_throughput: 23775.7
    sample_time_ms: 6804.931
    update_time_ms: 35.68
  timestamp: 1602814984
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: 1bbc1_00000
  
2020-10-16 02:23:05,040	WARNING util.py:136 -- The `process_trial` operation took 0.5463607311248779 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    187 |          4819.75 | 30255104 |  295.016 |              322.949 |              165.677 |            785.874 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3091.748114552287
    time_step_min: 2911
  date: 2020-10-16_02-23-30
  done: false
  episode_len_mean: 785.9196384637298
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.14749075715577
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 205
  episodes_total: 38613
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.697785493223493e-33
        cur_lr: 5.0e-05
        entropy: 0.07721356861293316
        entropy_coeff: 0.0005000000000000001
        kl: 0.004006128680581848
        model: {}
        policy_loss: -0.005455593258375302
        total_loss: 0.3599165081977844
        vf_explained_var: 0.9992647767066956
        vf_loss: 0.365410715341568
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.560000000000006
    gpu_util_percent0: 0.32866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470785023773459
    mean_env_wait_ms: 1.2029890903340794
    mean_inference_ms: 4.33489145238446
    mean_raw_obs_processing_ms: 0.3797371365928092
  time_since_restore: 4845.500331878662
  time_this_iter_s: 25.75330090522766
  time_total_s: 4845.500331878662
  timers:
    learn_throughput: 8612.382
    learn_time_ms: 18785.975
    sample_throughput: 23802.412
    sample_time_ms: 6797.294
    update_time_ms: 35.013
  timestamp: 1602815010
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: 1bbc1_00000
  
2020-10-16 02:23:31,571	WARNING util.py:136 -- The `process_trial` operation took 0.5757691860198975 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    188 |           4845.5 | 30416896 |  295.147 |              322.949 |              165.677 |             785.92 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3090.79042147568
    time_step_min: 2911
  date: 2020-10-16_02-23-57
  done: false
  episode_len_mean: 785.9709350221399
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.29540929842113
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 231
  episodes_total: 38844
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8488927466117464e-33
        cur_lr: 5.0e-05
        entropy: 0.07835169819494088
        entropy_coeff: 0.0005000000000000001
        kl: 0.005314636665085952
        model: {}
        policy_loss: -0.008501465509956082
        total_loss: 0.41707634429136914
        vf_explained_var: 0.9991911053657532
        vf_loss: 0.4256169870495796
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.920000000000005
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707370595125363
    mean_env_wait_ms: 1.202879038856924
    mean_inference_ms: 4.3346162247100075
    mean_raw_obs_processing_ms: 0.3797181641820498
  time_since_restore: 4871.448748588562
  time_this_iter_s: 25.948416709899902
  time_total_s: 4871.448748588562
  timers:
    learn_throughput: 8607.124
    learn_time_ms: 18797.453
    sample_throughput: 23816.175
    sample_time_ms: 6793.366
    update_time_ms: 35.154
  timestamp: 1602815037
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: 1bbc1_00000
  
2020-10-16 02:23:58,359	WARNING util.py:136 -- The `process_trial` operation took 0.5542030334472656 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    189 |          4871.45 | 30578688 |  295.295 |              322.949 |              165.677 |            785.971 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3090.077284032924
    time_step_min: 2911
  date: 2020-10-16_02-24-24
  done: false
  episode_len_mean: 786.0047915545648
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.40162016816623
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 183
  episodes_total: 39027
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8488927466117464e-33
        cur_lr: 5.0e-05
        entropy: 0.08315085309247176
        entropy_coeff: 0.0005000000000000001
        kl: 0.00450809618147711
        model: {}
        policy_loss: -0.007139176023580755
        total_loss: 0.7199028879404068
        vf_explained_var: 0.9983866214752197
        vf_loss: 0.727083628376325
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.135483870967747
    gpu_util_percent0: 0.28193548387096773
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470699147122434
    mean_env_wait_ms: 1.2027984273260093
    mean_inference_ms: 4.334392110324522
    mean_raw_obs_processing_ms: 0.37970362948436404
  time_since_restore: 4897.128247261047
  time_this_iter_s: 25.67949867248535
  time_total_s: 4897.128247261047
  timers:
    learn_throughput: 8610.423
    learn_time_ms: 18790.251
    sample_throughput: 23850.287
    sample_time_ms: 6783.65
    update_time_ms: 33.424
  timestamp: 1602815064
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: 1bbc1_00000
  
2020-10-16 02:24:24,945	WARNING util.py:136 -- The `process_trial` operation took 0.5447940826416016 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    190 |          4897.13 | 30740480 |  295.402 |              322.949 |              165.677 |            786.005 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3089.395906701373
    time_step_min: 2911
  date: 2020-10-16_02-24-50
  done: false
  episode_len_mean: 786.0451879430816
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.50480528238455
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 39214
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.244463733058732e-34
        cur_lr: 5.0e-05
        entropy: 0.08228544083734353
        entropy_coeff: 0.0005000000000000001
        kl: 0.004302115761674941
        model: {}
        policy_loss: -0.008454511048815524
        total_loss: 0.6221217115720113
        vf_explained_var: 0.9986638426780701
        vf_loss: 0.6306173503398895
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.093333333333337
    gpu_util_percent0: 0.33699999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706646617505134
    mean_env_wait_ms: 1.2027108244109277
    mean_inference_ms: 4.3341779645368295
    mean_raw_obs_processing_ms: 0.3796900127426692
  time_since_restore: 4923.085923194885
  time_this_iter_s: 25.95767593383789
  time_total_s: 4923.085923194885
  timers:
    learn_throughput: 8611.155
    learn_time_ms: 18788.653
    sample_throughput: 23820.337
    sample_time_ms: 6792.179
    update_time_ms: 33.32
  timestamp: 1602815090
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: 1bbc1_00000
  
2020-10-16 02:24:51,678	WARNING util.py:136 -- The `process_trial` operation took 0.5608088970184326 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    191 |          4923.09 | 30902272 |  295.505 |              322.949 |              165.677 |            786.045 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3088.456736257325
    time_step_min: 2911
  date: 2020-10-16_02-25-17
  done: false
  episode_len_mean: 786.0944257142133
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.64073726696336
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 39449
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.622231866529366e-34
        cur_lr: 5.0e-05
        entropy: 0.07904429112871487
        entropy_coeff: 0.0005000000000000001
        kl: 0.003310457900321732
        model: {}
        policy_loss: -0.008906692261613594
        total_loss: 0.565423771739006
        vf_explained_var: 0.9989538192749023
        vf_loss: 0.5743699769179026
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.240000000000002
    gpu_util_percent0: 0.30166666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706116094512248
    mean_env_wait_ms: 1.2026003480088083
    mean_inference_ms: 4.3338995648267895
    mean_raw_obs_processing_ms: 0.3796711823627026
  time_since_restore: 4948.744556188583
  time_this_iter_s: 25.65863299369812
  time_total_s: 4948.744556188583
  timers:
    learn_throughput: 8618.311
    learn_time_ms: 18773.053
    sample_throughput: 23804.575
    sample_time_ms: 6796.677
    update_time_ms: 33.72
  timestamp: 1602815117
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: 1bbc1_00000
  
2020-10-16 02:25:18,045	WARNING util.py:136 -- The `process_trial` operation took 0.5205585956573486 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    192 |          4948.74 | 31064064 |  295.641 |              322.949 |              165.677 |            786.094 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3087.6482523659306
    time_step_min: 2911
  date: 2020-10-16_02-25-43
  done: false
  episode_len_mean: 786.1406955337553
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.764907287894
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 204
  episodes_total: 39653
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.311115933264683e-34
        cur_lr: 5.0e-05
        entropy: 0.06846049117545287
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037281381082721055
        model: {}
        policy_loss: -0.004948067895990486
        total_loss: 0.5651830931504568
        vf_explained_var: 0.998805582523346
        vf_loss: 0.5701653907696406
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.529999999999998
    gpu_util_percent0: 0.29633333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705737122590176
    mean_env_wait_ms: 1.2025068790713938
    mean_inference_ms: 4.333664368926518
    mean_raw_obs_processing_ms: 0.3796553675725515
  time_since_restore: 4974.2877469062805
  time_this_iter_s: 25.543190717697144
  time_total_s: 4974.2877469062805
  timers:
    learn_throughput: 8631.252
    learn_time_ms: 18744.905
    sample_throughput: 23846.227
    sample_time_ms: 6784.805
    update_time_ms: 32.766
  timestamp: 1602815143
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: 1bbc1_00000
  
2020-10-16 02:25:44,335	WARNING util.py:136 -- The `process_trial` operation took 0.5094964504241943 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    193 |          4974.29 | 31225856 |  295.765 |              322.949 |              165.677 |            786.141 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3086.9482715304994
    time_step_min: 2911
  date: 2020-10-16_02-26-10
  done: false
  episode_len_mean: 786.1848262703354
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.8730275743982
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 39832
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1555579666323415e-34
        cur_lr: 5.0e-05
        entropy: 0.06820526284476121
        entropy_coeff: 0.0005000000000000001
        kl: 0.004271335686401774
        model: {}
        policy_loss: -0.0054694356125158565
        total_loss: 0.3135365570584933
        vf_explained_var: 0.999306857585907
        vf_loss: 0.31904008736213046
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20645161290323
    gpu_util_percent0: 0.36322580645161295
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470541751084772
    mean_env_wait_ms: 1.2024245298779155
    mean_inference_ms: 4.33346208503282
    mean_raw_obs_processing_ms: 0.3796423315681737
  time_since_restore: 5000.107917308807
  time_this_iter_s: 25.820170402526855
  time_total_s: 5000.107917308807
  timers:
    learn_throughput: 8631.35
    learn_time_ms: 18744.692
    sample_throughput: 23863.225
    sample_time_ms: 6779.972
    update_time_ms: 31.524
  timestamp: 1602815170
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: 1bbc1_00000
  
2020-10-16 02:26:10,929	WARNING util.py:136 -- The `process_trial` operation took 0.5165913105010986 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    194 |          5000.11 | 31387648 |  295.873 |              322.949 |              165.677 |            786.185 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3086.098497913074
    time_step_min: 2911
  date: 2020-10-16_02-26-36
  done: false
  episode_len_mean: 786.2328229975774
  episode_reward_max: 322.949494949495
  episode_reward_mean: 295.999521173926
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 207
  episodes_total: 40039
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.777789833161707e-35
        cur_lr: 5.0e-05
        entropy: 0.07091664895415306
        entropy_coeff: 0.0005000000000000001
        kl: 0.004064586052360634
        model: {}
        policy_loss: -0.006466068734880537
        total_loss: 0.25912437463800114
        vf_explained_var: 0.999499499797821
        vf_loss: 0.2656259040037791
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.940000000000005
    gpu_util_percent0: 0.3123333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705003738361755
    mean_env_wait_ms: 1.2023280153749816
    mean_inference_ms: 4.333232141259753
    mean_raw_obs_processing_ms: 0.37962735851569573
  time_since_restore: 5026.066732645035
  time_this_iter_s: 25.958815336227417
  time_total_s: 5026.066732645035
  timers:
    learn_throughput: 8626.689
    learn_time_ms: 18754.82
    sample_throughput: 23840.693
    sample_time_ms: 6786.38
    update_time_ms: 31.185
  timestamp: 1602815196
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: 1bbc1_00000
  
2020-10-16 02:26:37,666	WARNING util.py:136 -- The `process_trial` operation took 0.5419023036956787 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    195 |          5026.07 | 31549440 |      296 |              322.949 |              165.677 |            786.233 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3085.2163908354455
    time_step_min: 2911
  date: 2020-10-16_02-27-03
  done: false
  episode_len_mean: 786.2840079463621
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.1351471005058
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 231
  episodes_total: 40270
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8888949165808537e-35
        cur_lr: 5.0e-05
        entropy: 0.0727594904601574
        entropy_coeff: 0.0005000000000000001
        kl: 0.005303874146193266
        model: {}
        policy_loss: -0.00934130552074445
        total_loss: 0.5401961728930473
        vf_explained_var: 0.9989192485809326
        vf_loss: 0.5495738536119461
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806453
    gpu_util_percent0: 0.35387096774193555
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470454547750567
    mean_env_wait_ms: 1.202216779529888
    mean_inference_ms: 4.332971856486421
    mean_raw_obs_processing_ms: 0.3796090320641342
  time_since_restore: 5051.864205598831
  time_this_iter_s: 25.797472953796387
  time_total_s: 5051.864205598831
  timers:
    learn_throughput: 8626.299
    learn_time_ms: 18755.669
    sample_throughput: 23854.569
    sample_time_ms: 6782.432
    update_time_ms: 32.191
  timestamp: 1602815223
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: 1bbc1_00000
  
2020-10-16 02:27:04,371	WARNING util.py:136 -- The `process_trial` operation took 0.5489566326141357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    196 |          5051.86 | 31711232 |  296.135 |              322.949 |              165.677 |            786.284 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3084.536935332245
    time_step_min: 2911
  date: 2020-10-16_02-27-30
  done: false
  episode_len_mean: 786.3229419035847
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.23820529148077
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 40450
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8888949165808537e-35
        cur_lr: 5.0e-05
        entropy: 0.0728314792116483
        entropy_coeff: 0.0005000000000000001
        kl: 0.00389979628380388
        model: {}
        policy_loss: -0.01023846716452681
        total_loss: 0.5385007510582606
        vf_explained_var: 0.9988248944282532
        vf_loss: 0.548775648077329
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970000000000002
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470420995705461
    mean_env_wait_ms: 1.2021347813766463
    mean_inference_ms: 4.332770220326664
    mean_raw_obs_processing_ms: 0.37959594896402193
  time_since_restore: 5077.80576467514
  time_this_iter_s: 25.941559076309204
  time_total_s: 5077.80576467514
  timers:
    learn_throughput: 8631.611
    learn_time_ms: 18744.126
    sample_throughput: 23805.639
    sample_time_ms: 6796.373
    update_time_ms: 31.067
  timestamp: 1602815250
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: 1bbc1_00000
  
2020-10-16 02:27:31,042	WARNING util.py:136 -- The `process_trial` operation took 0.5344939231872559 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    197 |          5077.81 | 31873024 |  296.238 |              322.949 |              165.677 |            786.323 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3083.837901061289
    time_step_min: 2911
  date: 2020-10-16_02-27-56
  done: false
  episode_len_mean: 786.3651664657103
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.34486005257907
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 189
  episodes_total: 40639
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4444474582904268e-35
        cur_lr: 5.0e-05
        entropy: 0.07301973551511765
        entropy_coeff: 0.0005000000000000001
        kl: 0.004301249418252458
        model: {}
        policy_loss: -0.006631361611653119
        total_loss: 0.448641004661719
        vf_explained_var: 0.9990231990814209
        vf_loss: 0.45530886699755985
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.174193548387098
    gpu_util_percent0: 0.32677419354838716
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470389096822902
    mean_env_wait_ms: 1.2020454180067812
    mean_inference_ms: 4.3325681091700545
    mean_raw_obs_processing_ms: 0.3795832834546769
  time_since_restore: 5103.613347291946
  time_this_iter_s: 25.80758261680603
  time_total_s: 5103.613347291946
  timers:
    learn_throughput: 8632.113
    learn_time_ms: 18743.035
    sample_throughput: 23788.953
    sample_time_ms: 6801.14
    update_time_ms: 31.715
  timestamp: 1602815276
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: 1bbc1_00000
  
2020-10-16 02:27:57,790	WARNING util.py:136 -- The `process_trial` operation took 0.5684571266174316 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    198 |          5103.61 | 32034816 |  296.345 |              322.949 |              165.677 |            786.365 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3082.9904022721153
    time_step_min: 2911
  date: 2020-10-16_02-28-23
  done: false
  episode_len_mean: 786.4197841990654
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.47858462781005
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 232
  episodes_total: 40871
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.222237291452134e-36
        cur_lr: 5.0e-05
        entropy: 0.07481701796253522
        entropy_coeff: 0.0005000000000000001
        kl: 0.005326704665397604
        model: {}
        policy_loss: -0.00641464573952059
        total_loss: 0.3305043776830037
        vf_explained_var: 0.9993717670440674
        vf_loss: 0.3369564314683278
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.633333333333336
    gpu_util_percent0: 0.3353333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703388255364896
    mean_env_wait_ms: 1.2019332477556115
    mean_inference_ms: 4.332314743910537
    mean_raw_obs_processing_ms: 0.3795657472265523
  time_since_restore: 5129.534283876419
  time_this_iter_s: 25.920936584472656
  time_total_s: 5129.534283876419
  timers:
    learn_throughput: 8630.806
    learn_time_ms: 18745.873
    sample_throughput: 23811.068
    sample_time_ms: 6794.823
    update_time_ms: 32.066
  timestamp: 1602815303
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: 1bbc1_00000
  
2020-10-16 02:28:24,593	WARNING util.py:136 -- The `process_trial` operation took 0.5463705062866211 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    199 |          5129.53 | 32196608 |  296.479 |              322.949 |              165.677 |             786.42 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3082.2483434028454
    time_step_min: 2911
  date: 2020-10-16_02-28-50
  done: false
  episode_len_mean: 786.4650891031259
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.5933059782751
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 205
  episodes_total: 41076
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.222237291452134e-36
        cur_lr: 5.0e-05
        entropy: 0.07406288757920265
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034350132724891105
        model: {}
        policy_loss: -0.008543795061996207
        total_loss: 0.46918126940727234
        vf_explained_var: 0.9989805221557617
        vf_loss: 0.47776208569606143
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.612903225806456
    gpu_util_percent0: 0.2845161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703023619172242
    mean_env_wait_ms: 1.201837743251801
    mean_inference_ms: 4.332096744354981
    mean_raw_obs_processing_ms: 0.3795508226696814
  time_since_restore: 5155.346269130707
  time_this_iter_s: 25.81198525428772
  time_total_s: 5155.346269130707
  timers:
    learn_throughput: 8632.233
    learn_time_ms: 18742.774
    sample_throughput: 23794.659
    sample_time_ms: 6799.509
    update_time_ms: 33.878
  timestamp: 1602815330
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: 1bbc1_00000
  
2020-10-16 02:28:51,156	WARNING util.py:136 -- The `process_trial` operation took 0.552715539932251 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    200 |          5155.35 | 32358400 |  296.593 |              322.949 |              165.677 |            786.465 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3081.688391384496
    time_step_min: 2911
  date: 2020-10-16_02-29-16
  done: false
  episode_len_mean: 786.4973579600543
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.68170506695793
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 41256
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.611118645726067e-36
        cur_lr: 5.0e-05
        entropy: 0.07693898429473241
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036665056056032577
        model: {}
        policy_loss: -0.007032993560036023
        total_loss: 0.792490616440773
        vf_explained_var: 0.9982430934906006
        vf_loss: 0.7995620717604955
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.32666666666667
    gpu_util_percent0: 0.32033333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702724797294728
    mean_env_wait_ms: 1.2017528318860162
    mean_inference_ms: 4.331907446549754
    mean_raw_obs_processing_ms: 0.379538620397898
  time_since_restore: 5181.103049278259
  time_this_iter_s: 25.75678014755249
  time_total_s: 5181.103049278259
  timers:
    learn_throughput: 8638.021
    learn_time_ms: 18730.215
    sample_throughput: 23821.997
    sample_time_ms: 6791.706
    update_time_ms: 33.866
  timestamp: 1602815356
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: 1bbc1_00000
  
2020-10-16 02:29:17,646	WARNING util.py:136 -- The `process_trial` operation took 0.5354490280151367 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    201 |           5181.1 | 32520192 |  296.682 |              322.949 |              165.677 |            786.497 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3080.9146697555443
    time_step_min: 2911
  date: 2020-10-16_02-29-43
  done: false
  episode_len_mean: 786.5471338654834
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.80233959923817
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 211
  episodes_total: 41467
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8055593228630335e-36
        cur_lr: 5.0e-05
        entropy: 0.0780160240828991
        entropy_coeff: 0.0005000000000000001
        kl: 0.005965898473126193
        model: {}
        policy_loss: -0.005708701627251382
        total_loss: 0.3407115663091342
        vf_explained_var: 0.9993352293968201
        vf_loss: 0.3464592819412549
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.070967741935487
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470229879563697
    mean_env_wait_ms: 1.2016506203472344
    mean_inference_ms: 4.331682581658485
    mean_raw_obs_processing_ms: 0.3795235135481373
  time_since_restore: 5206.892666578293
  time_this_iter_s: 25.78961730003357
  time_total_s: 5206.892666578293
  timers:
    learn_throughput: 8630.445
    learn_time_ms: 18746.658
    sample_throughput: 23867.11
    sample_time_ms: 6778.869
    update_time_ms: 34.047
  timestamp: 1602815383
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: 1bbc1_00000
  
2020-10-16 02:29:44,197	WARNING util.py:136 -- The `process_trial` operation took 0.5597915649414062 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    202 |          5206.89 | 32681984 |  296.802 |              322.949 |              165.677 |            786.547 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3080.136198910409
    time_step_min: 2911
  date: 2020-10-16_02-30-10
  done: false
  episode_len_mean: 786.5897349802135
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.91824710711853
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 228
  episodes_total: 41695
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8055593228630335e-36
        cur_lr: 5.0e-05
        entropy: 0.09047020847598712
        entropy_coeff: 0.0005000000000000001
        kl: 0.006410996002765994
        model: {}
        policy_loss: -0.008671728700088957
        total_loss: 0.7652355283498764
        vf_explained_var: 0.9984539151191711
        vf_loss: 0.773952474196752
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.08666666666667
    gpu_util_percent0: 0.30533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701908117353885
    mean_env_wait_ms: 1.2015404786279686
    mean_inference_ms: 4.331448442040992
    mean_raw_obs_processing_ms: 0.37950703951037984
  time_since_restore: 5232.7173204422
  time_this_iter_s: 25.82465386390686
  time_total_s: 5232.7173204422
  timers:
    learn_throughput: 8625.877
    learn_time_ms: 18756.586
    sample_throughput: 23815.749
    sample_time_ms: 6793.488
    update_time_ms: 34.728
  timestamp: 1602815410
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: 1bbc1_00000
  
2020-10-16 02:30:10,772	WARNING util.py:136 -- The `process_trial` operation took 0.5520038604736328 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    203 |          5232.72 | 32843776 |  296.918 |              322.949 |              165.677 |             786.59 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3079.600066910412
    time_step_min: 2911
  date: 2020-10-16_02-30-36
  done: false
  episode_len_mean: 786.6167164179104
  episode_reward_max: 322.949494949495
  episode_reward_mean: 296.9983452434794
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 41875
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8055593228630335e-36
        cur_lr: 5.0e-05
        entropy: 0.08882779690126578
        entropy_coeff: 0.0005000000000000001
        kl: 0.004422145197167993
        model: {}
        policy_loss: -0.008578147933197519
        total_loss: 0.9631506005922953
        vf_explained_var: 0.9978713393211365
        vf_loss: 0.971773142615954
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.103333333333335
    gpu_util_percent0: 0.37100000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701621525608666
    mean_env_wait_ms: 1.2014595479216483
    mean_inference_ms: 4.3312685957427846
    mean_raw_obs_processing_ms: 0.3794955957200075
  time_since_restore: 5258.3691120147705
  time_this_iter_s: 25.6517915725708
  time_total_s: 5258.3691120147705
  timers:
    learn_throughput: 8631.475
    learn_time_ms: 18744.422
    sample_throughput: 23808.885
    sample_time_ms: 6795.446
    update_time_ms: 35.894
  timestamp: 1602815436
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: 1bbc1_00000
  
2020-10-16 02:30:37,176	WARNING util.py:136 -- The `process_trial` operation took 0.5529298782348633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    204 |          5258.37 | 33005568 |  296.998 |              322.949 |              165.677 |            786.617 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3079.0308790293802
    time_step_min: 2911
  date: 2020-10-16_02-31-02
  done: false
  episode_len_mean: 786.6495494852959
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.08977082716456
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 188
  episodes_total: 42063
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.027796614315168e-37
        cur_lr: 5.0e-05
        entropy: 0.08273587996761005
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034619206562638283
        model: {}
        policy_loss: -0.008734089919016697
        total_loss: 0.5765533695618311
        vf_explained_var: 0.9987546801567078
        vf_loss: 0.5853288173675537
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.843333333333337
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470129834363587
    mean_env_wait_ms: 1.201369443660422
    mean_inference_ms: 4.331075449464172
    mean_raw_obs_processing_ms: 0.3794832873821645
  time_since_restore: 5284.120928764343
  time_this_iter_s: 25.751816749572754
  time_total_s: 5284.120928764343
  timers:
    learn_throughput: 8639.962
    learn_time_ms: 18726.009
    sample_throughput: 23817.274
    sample_time_ms: 6793.053
    update_time_ms: 35.849
  timestamp: 1602815462
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: 1bbc1_00000
  
2020-10-16 02:31:03,727	WARNING util.py:136 -- The `process_trial` operation took 0.5983235836029053 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    205 |          5284.12 | 33167360 |   297.09 |              322.949 |              165.677 |             786.65 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3078.188218594748
    time_step_min: 2911
  date: 2020-10-16_02-31-29
  done: false
  episode_len_mean: 786.706794647501
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.21582246408457
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 42298
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.513898307157584e-37
        cur_lr: 5.0e-05
        entropy: 0.07418506157894929
        entropy_coeff: 0.0005000000000000001
        kl: 0.003232161436850826
        model: {}
        policy_loss: -0.007618254467767353
        total_loss: 0.42052685966094333
        vf_explained_var: 0.9992091655731201
        vf_loss: 0.4281822095314662
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.633333333333336
    gpu_util_percent0: 0.30700000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470084715913108
    mean_env_wait_ms: 1.201255760361726
    mean_inference_ms: 4.33084861035727
    mean_raw_obs_processing_ms: 0.3794667508886611
  time_since_restore: 5309.781031370163
  time_this_iter_s: 25.660102605819702
  time_total_s: 5309.781031370163
  timers:
    learn_throughput: 8644.665
    learn_time_ms: 18715.82
    sample_throughput: 23828.075
    sample_time_ms: 6789.974
    update_time_ms: 34.531
  timestamp: 1602815489
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: 1bbc1_00000
  
2020-10-16 02:31:30,230	WARNING util.py:136 -- The `process_trial` operation took 0.5417740345001221 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    206 |          5309.78 | 33329152 |  297.216 |              322.949 |              165.677 |            786.707 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3077.4934068004145
    time_step_min: 2911
  date: 2020-10-16_02-31-56
  done: false
  episode_len_mean: 786.748046875
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.32065501589676
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 42496
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.256949153578792e-37
        cur_lr: 5.0e-05
        entropy: 0.06752750712136428
        entropy_coeff: 0.0005000000000000001
        kl: 0.003918035984194527
        model: {}
        policy_loss: -0.004497315812235077
        total_loss: 0.33990944425264996
        vf_explained_var: 0.9993225932121277
        vf_loss: 0.3444405272603035
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.048387096774196
    gpu_util_percent0: 0.31322580645161285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700502158279022
    mean_env_wait_ms: 1.2011629399804442
    mean_inference_ms: 4.330644885544208
    mean_raw_obs_processing_ms: 0.37945351714733083
  time_since_restore: 5335.6911680698395
  time_this_iter_s: 25.910136699676514
  time_total_s: 5335.6911680698395
  timers:
    learn_throughput: 8642.616
    learn_time_ms: 18720.257
    sample_throughput: 23890.513
    sample_time_ms: 6772.228
    update_time_ms: 41.823
  timestamp: 1602815516
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: 1bbc1_00000
  
2020-10-16 02:31:56,940	WARNING util.py:136 -- The `process_trial` operation took 0.6000921726226807 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    207 |          5335.69 | 33490944 |  297.321 |              322.949 |              165.677 |            786.748 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3076.8433330988723
    time_step_min: 2911
  date: 2020-10-16_02-32-22
  done: false
  episode_len_mean: 786.7910682068465
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.41808250976675
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 183
  episodes_total: 42679
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.128474576789396e-37
        cur_lr: 5.0e-05
        entropy: 0.07073415132860343
        entropy_coeff: 0.0005000000000000001
        kl: 0.006081308315818508
        model: {}
        policy_loss: -0.007663019826092447
        total_loss: 0.26951682070891064
        vf_explained_var: 0.9994280338287354
        vf_loss: 0.2772151951988538
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.06
    gpu_util_percent0: 0.31966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700210620666268
    mean_env_wait_ms: 1.2010761616549244
    mean_inference_ms: 4.330466065824959
    mean_raw_obs_processing_ms: 0.37944207283283016
  time_since_restore: 5361.589312076569
  time_this_iter_s: 25.898144006729126
  time_total_s: 5361.589312076569
  timers:
    learn_throughput: 8640.755
    learn_time_ms: 18724.289
    sample_throughput: 23877.478
    sample_time_ms: 6775.925
    update_time_ms: 41.974
  timestamp: 1602815542
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: 1bbc1_00000
  
2020-10-16 02:32:23,691	WARNING util.py:136 -- The `process_trial` operation took 0.5599799156188965 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    208 |          5361.59 | 33652736 |  297.418 |              322.949 |              165.677 |            786.791 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3076.11105148963
    time_step_min: 2911
  date: 2020-10-16_02-32-49
  done: false
  episode_len_mean: 786.8456552656735
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.5283110181338
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 212
  episodes_total: 42891
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.128474576789396e-37
        cur_lr: 5.0e-05
        entropy: 0.07713932543992996
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035041873537314436
        model: {}
        policy_loss: -0.007854466205268787
        total_loss: 0.7157115340232849
        vf_explained_var: 0.9986006617546082
        vf_loss: 0.7236045648654302
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.82903225806452
    gpu_util_percent0: 0.2925806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699826851750167
    mean_env_wait_ms: 1.2009737154836375
    mean_inference_ms: 4.330266200397701
    mean_raw_obs_processing_ms: 0.3794279807155467
  time_since_restore: 5387.463940382004
  time_this_iter_s: 25.87462830543518
  time_total_s: 5387.463940382004
  timers:
    learn_throughput: 8646.072
    learn_time_ms: 18712.776
    sample_throughput: 23860.901
    sample_time_ms: 6780.632
    update_time_ms: 41.96
  timestamp: 1602815569
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: 1bbc1_00000
  
2020-10-16 02:32:50,335	WARNING util.py:136 -- The `process_trial` operation took 0.560030460357666 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    209 |          5387.46 | 33814528 |  297.528 |              322.949 |              165.677 |            786.846 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3075.3730563935947
    time_step_min: 2911
  date: 2020-10-16_02-33-16
  done: false
  episode_len_mean: 786.8950786214574
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.64119018469853
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 227
  episodes_total: 43118
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.64237288394698e-38
        cur_lr: 5.0e-05
        entropy: 0.07537222653627396
        entropy_coeff: 0.0005000000000000001
        kl: 0.003095649629055212
        model: {}
        policy_loss: -0.007845964302153638
        total_loss: 0.4483365739385287
        vf_explained_var: 0.9991018176078796
        vf_loss: 0.45622022201617557
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.673333333333332
    gpu_util_percent0: 0.3073333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699449962978464
    mean_env_wait_ms: 1.200863474367204
    mean_inference_ms: 4.330041542573161
    mean_raw_obs_processing_ms: 0.3794125886000974
  time_since_restore: 5413.23171544075
  time_this_iter_s: 25.767775058746338
  time_total_s: 5413.23171544075
  timers:
    learn_throughput: 8646.399
    learn_time_ms: 18712.067
    sample_throughput: 23839.314
    sample_time_ms: 6786.772
    update_time_ms: 39.924
  timestamp: 1602815596
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: 1bbc1_00000
  
2020-10-16 02:33:16,954	WARNING util.py:136 -- The `process_trial` operation took 0.6476752758026123 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    210 |          5413.23 | 33976320 |  297.641 |              322.949 |              165.677 |            786.895 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3074.7954608486643
    time_step_min: 2911
  date: 2020-10-16_02-33-42
  done: false
  episode_len_mean: 786.9327420546932
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.7317794071534
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 178
  episodes_total: 43296
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.82118644197349e-38
        cur_lr: 5.0e-05
        entropy: 0.07094481587409973
        entropy_coeff: 0.0005000000000000001
        kl: 0.006205982489821811
        model: {}
        policy_loss: -0.007111676241038367
        total_loss: 0.33804280559221905
        vf_explained_var: 0.9992364048957825
        vf_loss: 0.3451899488766988
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73225806451613
    gpu_util_percent0: 0.29935483870967744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699170093321295
    mean_env_wait_ms: 1.2007818434664048
    mean_inference_ms: 4.329876022080264
    mean_raw_obs_processing_ms: 0.3794016020949141
  time_since_restore: 5439.115594148636
  time_this_iter_s: 25.883878707885742
  time_total_s: 5439.115594148636
  timers:
    learn_throughput: 8649.336
    learn_time_ms: 18705.713
    sample_throughput: 23788.072
    sample_time_ms: 6801.392
    update_time_ms: 41.773
  timestamp: 1602815622
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: 1bbc1_00000
  
2020-10-16 02:33:43,813	WARNING util.py:136 -- The `process_trial` operation took 0.5875482559204102 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    211 |          5439.12 | 34138112 |  297.732 |              322.949 |              165.677 |            786.933 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3074.16650176028
    time_step_min: 2911
  date: 2020-10-16_02-34-09
  done: false
  episode_len_mean: 786.9788672476832
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.83045205893393
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 191
  episodes_total: 43487
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.82118644197349e-38
        cur_lr: 5.0e-05
        entropy: 0.07422503891090552
        entropy_coeff: 0.0005000000000000001
        kl: 0.0060302223622178035
        model: {}
        policy_loss: -0.00566577786836812
        total_loss: 0.30096476276715595
        vf_explained_var: 0.9993476271629333
        vf_loss: 0.3066676581899325
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.143333333333334
    gpu_util_percent0: 0.322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698860510256162
    mean_env_wait_ms: 1.2006891201911514
    mean_inference_ms: 4.329692539987734
    mean_raw_obs_processing_ms: 0.37938976012611025
  time_since_restore: 5464.995755434036
  time_this_iter_s: 25.88016128540039
  time_total_s: 5464.995755434036
  timers:
    learn_throughput: 8646.473
    learn_time_ms: 18711.906
    sample_throughput: 23747.888
    sample_time_ms: 6812.901
    update_time_ms: 40.534
  timestamp: 1602815649
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: 1bbc1_00000
  
2020-10-16 02:34:10,476	WARNING util.py:136 -- The `process_trial` operation took 0.5689916610717773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    212 |             5465 | 34299904 |   297.83 |              322.949 |              165.677 |            786.979 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3073.454622445009
    time_step_min: 2911
  date: 2020-10-16_02-34-36
  done: false
  episode_len_mean: 787.029599469314
  episode_reward_max: 322.949494949495
  episode_reward_mean: 297.93466471564216
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 230
  episodes_total: 43717
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.82118644197349e-38
        cur_lr: 5.0e-05
        entropy: 0.09122018950680892
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009568875189870596
        total_loss: .inf
        vf_explained_var: 0.9982531666755676
        vf_loss: 0.9226216425498327
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72333333333334
    gpu_util_percent0: 0.3360000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469844376374466
    mean_env_wait_ms: 1.2005777047326673
    mean_inference_ms: 4.329480802651555
    mean_raw_obs_processing_ms: 0.3793745562211194
  time_since_restore: 5490.664108514786
  time_this_iter_s: 25.66835308074951
  time_total_s: 5490.664108514786
  timers:
    learn_throughput: 8645.955
    learn_time_ms: 18713.028
    sample_throughput: 23796.682
    sample_time_ms: 6798.931
    update_time_ms: 38.501
  timestamp: 1602815676
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:34:36,989	WARNING util.py:136 -- The `process_trial` operation took 0.6349513530731201 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    213 |          5490.66 | 34461696 |  297.935 |              322.949 |              165.677 |             787.03 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3072.91328320802
    time_step_min: 2911
  date: 2020-10-16_02-35-02
  done: false
  episode_len_mean: 787.0729086023954
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.0172106786705
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 201
  episodes_total: 43918
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.231779662960236e-38
        cur_lr: 5.0e-05
        entropy: 0.08913853950798512
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009563064983619066
        total_loss: .inf
        vf_explained_var: 0.9982032775878906
        vf_loss: 0.873305986324946
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.263333333333335
    gpu_util_percent0: 0.3063333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698129554600997
    mean_env_wait_ms: 1.2004832893635191
    mean_inference_ms: 4.329297030004243
    mean_raw_obs_processing_ms: 0.3793620506244936
  time_since_restore: 5516.3894782066345
  time_this_iter_s: 25.725369691848755
  time_total_s: 5516.3894782066345
  timers:
    learn_throughput: 8641.34
    learn_time_ms: 18723.022
    sample_throughput: 23811.933
    sample_time_ms: 6794.577
    update_time_ms: 38.917
  timestamp: 1602815702
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:35:03,603	WARNING util.py:136 -- The `process_trial` operation took 0.613795280456543 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    214 |          5516.39 | 34623488 |  298.017 |              322.949 |              165.677 |            787.073 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3072.4156457028766
    time_step_min: 2911
  date: 2020-10-16_02-35-29
  done: false
  episode_len_mean: 787.0996961726828
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.0912707246598
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 186
  episodes_total: 44104
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.347669494440352e-38
        cur_lr: 5.0e-05
        entropy: 0.08258051052689552
        entropy_coeff: 0.0005000000000000001
        kl: 0.004522460085960726
        model: {}
        policy_loss: -0.009001141278228411
        total_loss: 0.7563677529493967
        vf_explained_var: 0.9983444213867188
        vf_loss: 0.7654101600249609
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806453
    gpu_util_percent0: 0.3448387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697840643237783
    mean_env_wait_ms: 1.2003943002223942
    mean_inference_ms: 4.329126506659241
    mean_raw_obs_processing_ms: 0.379350841422925
  time_since_restore: 5542.2530291080475
  time_this_iter_s: 25.863550901412964
  time_total_s: 5542.2530291080475
  timers:
    learn_throughput: 8637.769
    learn_time_ms: 18730.762
    sample_throughput: 23836.579
    sample_time_ms: 6787.551
    update_time_ms: 37.856
  timestamp: 1602815729
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: 1bbc1_00000
  
2020-10-16 02:35:30,248	WARNING util.py:136 -- The `process_trial` operation took 0.5653116703033447 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    215 |          5542.25 | 34785280 |  298.091 |              322.949 |              165.677 |              787.1 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3071.735791541536
    time_step_min: 2911
  date: 2020-10-16_02-35-56
  done: false
  episode_len_mean: 787.1437436533905
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.191888420479
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 211
  episodes_total: 44315
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.173834747220176e-38
        cur_lr: 5.0e-05
        entropy: 0.07543146734436353
        entropy_coeff: 0.0005000000000000001
        kl: 0.004597813162642221
        model: {}
        policy_loss: -0.007596344661578769
        total_loss: 0.3917796437939008
        vf_explained_var: 0.9993005394935608
        vf_loss: 0.39941370735565823
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97666666666667
    gpu_util_percent0: 0.329
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697467043821982
    mean_env_wait_ms: 1.200292403895082
    mean_inference_ms: 4.328931849836757
    mean_raw_obs_processing_ms: 0.37933730860552484
  time_since_restore: 5568.241638422012
  time_this_iter_s: 25.988609313964844
  time_total_s: 5568.241638422012
  timers:
    learn_throughput: 8625.974
    learn_time_ms: 18756.375
    sample_throughput: 23819.331
    sample_time_ms: 6792.466
    update_time_ms: 38.705
  timestamp: 1602815756
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: 1bbc1_00000
  
2020-10-16 02:35:57,084	WARNING util.py:136 -- The `process_trial` operation took 0.5701205730438232 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    216 |          5568.24 | 34947072 |  298.192 |              322.949 |              165.677 |            787.144 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3071.0230525535308
    time_step_min: 2911
  date: 2020-10-16_02-36-22
  done: false
  episode_len_mean: 787.191871561693
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.2994784490236
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 220
  episodes_total: 44535
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.586917373610088e-38
        cur_lr: 5.0e-05
        entropy: 0.07487019958595435
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0068164903204888105
        total_loss: .inf
        vf_explained_var: 0.9990677833557129
        vf_loss: 0.4896731302142143
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.333333333333332
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146971324007226
    mean_env_wait_ms: 1.200185445810125
    mean_inference_ms: 4.328736691792898
    mean_raw_obs_processing_ms: 0.3793235405091583
  time_since_restore: 5593.940158605576
  time_this_iter_s: 25.698520183563232
  time_total_s: 5593.940158605576
  timers:
    learn_throughput: 8628.622
    learn_time_ms: 18750.618
    sample_throughput: 23846.271
    sample_time_ms: 6784.792
    update_time_ms: 32.652
  timestamp: 1602815782
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:36:23,687	WARNING util.py:136 -- The `process_trial` operation took 0.6009387969970703 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    217 |          5593.94 | 35108864 |  298.299 |              322.949 |              165.677 |            787.192 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3070.4136086995436
    time_step_min: 2911
  date: 2020-10-16_02-36-49
  done: false
  episode_len_mean: 787.2350849731664
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.39066085723044
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 44720
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.380376060415132e-38
        cur_lr: 5.0e-05
        entropy: 0.07501308185358842
        entropy_coeff: 0.0005000000000000001
        kl: 0.004657537948029737
        model: {}
        policy_loss: -0.007218635961180553
        total_loss: 0.27396052703261375
        vf_explained_var: 0.9993869662284851
        vf_loss: 0.28121666858593625
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.432258064516137
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696866501439806
    mean_env_wait_ms: 1.2001010404259305
    mean_inference_ms: 4.328571894720866
    mean_raw_obs_processing_ms: 0.3793126255972916
  time_since_restore: 5619.983203172684
  time_this_iter_s: 26.043044567108154
  time_total_s: 5619.983203172684
  timers:
    learn_throughput: 8634.243
    learn_time_ms: 18738.411
    sample_throughput: 23788.586
    sample_time_ms: 6801.245
    update_time_ms: 32.36
  timestamp: 1602815809
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: 1bbc1_00000
  
2020-10-16 02:36:50,581	WARNING util.py:136 -- The `process_trial` operation took 0.642871618270874 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    218 |          5619.98 | 35270656 |  298.391 |              322.949 |              165.677 |            787.235 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3069.8095365418894
    time_step_min: 2911
  date: 2020-10-16_02-37-16
  done: false
  episode_len_mean: 787.2814197915739
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.4845248152675
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 188
  episodes_total: 44908
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.190188030207566e-38
        cur_lr: 5.0e-05
        entropy: 0.07338412168125312
        entropy_coeff: 0.0005000000000000001
        kl: 0.004338512158331771
        model: {}
        policy_loss: -0.005338257469702512
        total_loss: 0.22324458261330923
        vf_explained_var: 0.9995181560516357
        vf_loss: 0.22861953328053156
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.51
    gpu_util_percent0: 0.33699999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469660009208544
    mean_env_wait_ms: 1.2000100392930912
    mean_inference_ms: 4.328403771079198
    mean_raw_obs_processing_ms: 0.37930162742847745
  time_since_restore: 5645.811888694763
  time_this_iter_s: 25.828685522079468
  time_total_s: 5645.811888694763
  timers:
    learn_throughput: 8633.741
    learn_time_ms: 18739.501
    sample_throughput: 23809.311
    sample_time_ms: 6795.325
    update_time_ms: 32.521
  timestamp: 1602815836
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: 1bbc1_00000
  
2020-10-16 02:37:17,272	WARNING util.py:136 -- The `process_trial` operation took 0.6495840549468994 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    219 |          5645.81 | 35432448 |  298.485 |              322.949 |              165.677 |            787.281 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3069.0908647558244
    time_step_min: 2911
  date: 2020-10-16_02-37-43
  done: false
  episode_len_mean: 787.3384656283923
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.5945059491879
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 231
  episodes_total: 45139
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.95094015103783e-39
        cur_lr: 5.0e-05
        entropy: 0.07589085772633553
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044176827650517225
        model: {}
        policy_loss: -0.008216859952275021
        total_loss: 0.4050924703478813
        vf_explained_var: 0.9992017149925232
        vf_loss: 0.4133472740650177
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.345161290322583
    gpu_util_percent0: 0.323225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696191393362334
    mean_env_wait_ms: 1.199898309829016
    mean_inference_ms: 4.328204189246401
    mean_raw_obs_processing_ms: 0.3792872701003477
  time_since_restore: 5671.552244186401
  time_this_iter_s: 25.740355491638184
  time_total_s: 5671.552244186401
  timers:
    learn_throughput: 8632.974
    learn_time_ms: 18741.167
    sample_throughput: 23832.646
    sample_time_ms: 6788.671
    update_time_ms: 34.299
  timestamp: 1602815863
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: 1bbc1_00000
  
2020-10-16 02:37:44,008	WARNING util.py:136 -- The `process_trial` operation took 0.5971527099609375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    220 |          5671.55 | 35594240 |  298.595 |              322.949 |              165.677 |            787.338 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3068.5231957625247
    time_step_min: 2911
  date: 2020-10-16_02-38-09
  done: false
  episode_len_mean: 787.3823282897349
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.6841619690663
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 45338
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.975470075518915e-39
        cur_lr: 5.0e-05
        entropy: 0.07522732578217983
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032054959253097572
        model: {}
        policy_loss: -0.007849280935867379
        total_loss: 0.3750024636586507
        vf_explained_var: 0.9992119669914246
        vf_loss: 0.3828893651564916
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.81333333333333
    gpu_util_percent0: 0.30800000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695892198550362
    mean_env_wait_ms: 1.1998034451227897
    mean_inference_ms: 4.328031091832769
    mean_raw_obs_processing_ms: 0.3792752445021635
  time_since_restore: 5697.534163951874
  time_this_iter_s: 25.981919765472412
  time_total_s: 5697.534163951874
  timers:
    learn_throughput: 8627.334
    learn_time_ms: 18753.419
    sample_throughput: 23834.101
    sample_time_ms: 6788.257
    update_time_ms: 32.576
  timestamp: 1602815889
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: 1bbc1_00000
  
2020-10-16 02:38:10,808	WARNING util.py:136 -- The `process_trial` operation took 0.5996482372283936 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    221 |          5697.53 | 35756032 |  298.684 |              322.949 |              165.677 |            787.382 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3067.947554676338
    time_step_min: 2911
  date: 2020-10-16_02-38-36
  done: false
  episode_len_mean: 787.422863168069
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.77178657830177
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 45523
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4877350377594576e-39
        cur_lr: 5.0e-05
        entropy: 0.07438995130360126
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047378177211309476
        model: {}
        policy_loss: -0.007852864859160036
        total_loss: 0.3171274612347285
        vf_explained_var: 0.9992765784263611
        vf_loss: 0.3250175242622693
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.587096774193554
    gpu_util_percent0: 0.3345161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695622995582772
    mean_env_wait_ms: 1.1997157522832722
    mean_inference_ms: 4.327871624718521
    mean_raw_obs_processing_ms: 0.37926481473995644
  time_since_restore: 5723.513164997101
  time_this_iter_s: 25.97900104522705
  time_total_s: 5723.513164997101
  timers:
    learn_throughput: 8625.344
    learn_time_ms: 18757.744
    sample_throughput: 23860.003
    sample_time_ms: 6780.888
    update_time_ms: 33.592
  timestamp: 1602815916
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: 1bbc1_00000
  
2020-10-16 02:38:37,598	WARNING util.py:136 -- The `process_trial` operation took 0.5957651138305664 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    222 |          5723.51 | 35917824 |  298.772 |              322.949 |              165.677 |            787.423 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3067.3275718724017
    time_step_min: 2911
  date: 2020-10-16_02-39-03
  done: false
  episode_len_mean: 787.4503870205974
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.8634625433941
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 211
  episodes_total: 45734
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.438675188797288e-40
        cur_lr: 5.0e-05
        entropy: 0.08350681699812412
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005667695309966803
        total_loss: .inf
        vf_explained_var: 0.9986863136291504
        vf_loss: 0.6671372900406519
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.110000000000007
    gpu_util_percent0: 0.3143333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695281721793024
    mean_env_wait_ms: 1.1996132218033777
    mean_inference_ms: 4.327693502728755
    mean_raw_obs_processing_ms: 0.37925255629723026
  time_since_restore: 5749.361221790314
  time_this_iter_s: 25.84805679321289
  time_total_s: 5749.361221790314
  timers:
    learn_throughput: 8620.027
    learn_time_ms: 18769.314
    sample_throughput: 23839.707
    sample_time_ms: 6786.661
    update_time_ms: 33.473
  timestamp: 1602815943
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:39:04,269	WARNING util.py:136 -- The `process_trial` operation took 0.6009640693664551 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    223 |          5749.36 | 36079616 |  298.863 |              322.949 |              165.677 |             787.45 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3066.6527684034054
    time_step_min: 2911
  date: 2020-10-16_02-39-30
  done: false
  episode_len_mean: 787.4912200535283
  episode_reward_max: 322.949494949495
  episode_reward_mean: 298.9649171392756
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 45957
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1158012783195936e-39
        cur_lr: 5.0e-05
        entropy: 0.0799137894064188
        entropy_coeff: 0.0005000000000000001
        kl: 0.009540176251903176
        model: {}
        policy_loss: -0.005875428630436848
        total_loss: 0.4832577233513196
        vf_explained_var: 0.9990188479423523
        vf_loss: 0.4891730969150861
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65483870967742
    gpu_util_percent0: 0.3541935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694952892373514
    mean_env_wait_ms: 1.199506206067959
    mean_inference_ms: 4.32750744372067
    mean_raw_obs_processing_ms: 0.37923927086131454
  time_since_restore: 5775.206748485565
  time_this_iter_s: 25.845526695251465
  time_total_s: 5775.206748485565
  timers:
    learn_throughput: 8612.123
    learn_time_ms: 18786.541
    sample_throughput: 23857.735
    sample_time_ms: 6781.532
    update_time_ms: 33.259
  timestamp: 1602815970
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: 1bbc1_00000
  
2020-10-16 02:39:31,110	WARNING util.py:136 -- The `process_trial` operation took 0.5915656089782715 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    224 |          5775.21 | 36241408 |  298.965 |              322.949 |              165.677 |            787.491 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3066.1896645197667
    time_step_min: 2911
  date: 2020-10-16_02-39-57
  done: false
  episode_len_mean: 787.5113673305736
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.02547877509386
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 184
  episodes_total: 46141
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1158012783195936e-39
        cur_lr: 5.0e-05
        entropy: 0.10141836168865363
        entropy_coeff: 0.0005000000000000001
        kl: 0.0059511565292874975
        model: {}
        policy_loss: -0.011043798838121196
        total_loss: 1.5129315952459972
        vf_explained_var: 0.9966171383857727
        vf_loss: 1.524026095867157
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09
    gpu_util_percent0: 0.2946666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694690845470657
    mean_env_wait_ms: 1.1994215437116977
    mean_inference_ms: 4.3273505437115665
    mean_raw_obs_processing_ms: 0.3792284547185664
  time_since_restore: 5801.106090545654
  time_this_iter_s: 25.89934206008911
  time_total_s: 5801.106090545654
  timers:
    learn_throughput: 8610.934
    learn_time_ms: 18789.135
    sample_throughput: 23825.192
    sample_time_ms: 6790.795
    update_time_ms: 34.133
  timestamp: 1602815997
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: 1bbc1_00000
  
2020-10-16 02:39:57,867	WARNING util.py:136 -- The `process_trial` operation took 0.6363098621368408 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    225 |          5801.11 | 36403200 |  299.025 |              322.949 |              165.677 |            787.511 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3065.93605165972
    time_step_min: 2911
  date: 2020-10-16_02-40-23
  done: false
  episode_len_mean: 787.5077809673868
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.0620133693236
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 190
  episodes_total: 46331
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1158012783195936e-39
        cur_lr: 5.0e-05
        entropy: 0.10119158712526162
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00979378996999003
        total_loss: .inf
        vf_explained_var: 0.9969356656074524
        vf_loss: 1.5078135132789612
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.535483870967745
    gpu_util_percent0: 0.3012903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469445056018961
    mean_env_wait_ms: 1.1993306907535632
    mean_inference_ms: 4.327193712421128
    mean_raw_obs_processing_ms: 0.379218443425375
  time_since_restore: 5826.99653339386
  time_this_iter_s: 25.890442848205566
  time_total_s: 5826.99653339386
  timers:
    learn_throughput: 8623.065
    learn_time_ms: 18762.703
    sample_throughput: 23806.656
    sample_time_ms: 6796.083
    update_time_ms: 33.874
  timestamp: 1602816023
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:40:24,589	WARNING util.py:136 -- The `process_trial` operation took 0.5897109508514404 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    226 |             5827 | 36564992 |  299.062 |              322.949 |              165.677 |            787.508 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3065.3115599484313
    time_step_min: 2911
  date: 2020-10-16_02-40-50
  done: false
  episode_len_mean: 787.5524823913416
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.16447935808856
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 237
  episodes_total: 46568
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6737019174793896e-39
        cur_lr: 5.0e-05
        entropy: 0.07988909383614858
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007639308588598699
        total_loss: .inf
        vf_explained_var: 0.999176025390625
        vf_loss: 0.42963657528162
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.983333333333334
    gpu_util_percent0: 0.32966666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694011175536098
    mean_env_wait_ms: 1.1992165246896933
    mean_inference_ms: 4.326992739780328
    mean_raw_obs_processing_ms: 0.37920355903127445
  time_since_restore: 5852.840587615967
  time_this_iter_s: 25.844054222106934
  time_total_s: 5852.840587615967
  timers:
    learn_throughput: 8620.692
    learn_time_ms: 18767.867
    sample_throughput: 23780.145
    sample_time_ms: 6803.659
    update_time_ms: 34.262
  timestamp: 1602816050
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:40:51,266	WARNING util.py:136 -- The `process_trial` operation took 0.6081833839416504 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    227 |          5852.84 | 36726784 |  299.164 |              322.949 |              165.677 |            787.552 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3064.732497432386
    time_step_min: 2911
  date: 2020-10-16_02-41-17
  done: false
  episode_len_mean: 787.5877598152425
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.2512959982165
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 46764
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5105528762190843e-39
        cur_lr: 5.0e-05
        entropy: 0.07358477575083573
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006801897960637386
        total_loss: .inf
        vf_explained_var: 0.99930340051651
        vf_loss: 0.323316457370917
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.148387096774194
    gpu_util_percent0: 0.3058064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469375855812663
    mean_env_wait_ms: 1.1991262399684783
    mean_inference_ms: 4.326834509707452
    mean_raw_obs_processing_ms: 0.3791928576225911
  time_since_restore: 5878.645789861679
  time_this_iter_s: 25.80520224571228
  time_total_s: 5878.645789861679
  timers:
    learn_throughput: 8618.496
    learn_time_ms: 18772.649
    sample_throughput: 23848.519
    sample_time_ms: 6784.153
    update_time_ms: 32.913
  timestamp: 1602816077
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:41:18,105	WARNING util.py:136 -- The `process_trial` operation took 0.6305856704711914 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    228 |          5878.65 | 36888576 |  299.251 |              322.949 |              165.677 |            787.588 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3064.1952472293265
    time_step_min: 2911
  date: 2020-10-16_02-41-44
  done: false
  episode_len_mean: 787.6207293175428
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.3334891042139
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 184
  episodes_total: 46948
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.765829314328628e-39
        cur_lr: 5.0e-05
        entropy: 0.07258747331798077
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007032028118070836
        total_loss: .inf
        vf_explained_var: 0.9993877410888672
        vf_loss: 0.27070023243625957
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333337
    gpu_util_percent0: 0.2893333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693522521538424
    mean_env_wait_ms: 1.19903973354598
    mean_inference_ms: 4.326689782683308
    mean_raw_obs_processing_ms: 0.37918353274528954
  time_since_restore: 5904.6884706020355
  time_this_iter_s: 26.042680740356445
  time_total_s: 5904.6884706020355
  timers:
    learn_throughput: 8611.027
    learn_time_ms: 18788.932
    sample_throughput: 23832.485
    sample_time_ms: 6788.717
    update_time_ms: 32.271
  timestamp: 1602816104
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:41:45,128	WARNING util.py:136 -- The `process_trial` operation took 0.6922323703765869 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    229 |          5904.69 | 37050368 |  299.333 |              322.949 |              165.677 |            787.621 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3063.571658922623
    time_step_min: 2911
  date: 2020-10-16_02-42-11
  done: false
  episode_len_mean: 787.6599944869702
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.4286447092153
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 213
  episodes_total: 47161
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6487439714929405e-39
        cur_lr: 5.0e-05
        entropy: 0.07474520988762379
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007064636544479678
        total_loss: .inf
        vf_explained_var: 0.9994878768920898
        vf_loss: 0.2651444983979066
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.661290322580648
    gpu_util_percent0: 0.35741935483870974
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693189772035617
    mean_env_wait_ms: 1.19893709700163
    mean_inference_ms: 4.326517349069828
    mean_raw_obs_processing_ms: 0.37917147162496656
  time_since_restore: 5930.694683790207
  time_this_iter_s: 26.006213188171387
  time_total_s: 5930.694683790207
  timers:
    learn_throughput: 8604.991
    learn_time_ms: 18802.112
    sample_throughput: 23823.061
    sample_time_ms: 6791.403
    update_time_ms: 32.273
  timestamp: 1602816131
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:42:12,044	WARNING util.py:136 -- The `process_trial` operation took 0.6929447650909424 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    230 |          5930.69 | 37212160 |  299.429 |              322.949 |              165.677 |             787.66 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3062.9104001689366
    time_step_min: 2911
  date: 2020-10-16_02-42-38
  done: false
  episode_len_mean: 787.7064770065214
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.52593298922125
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 47383
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.473115957239412e-39
        cur_lr: 5.0e-05
        entropy: 0.07120519069333871
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039554658384683234
        model: {}
        policy_loss: -0.006399275651953455
        total_loss: 0.452204334239165
        vf_explained_var: 0.9991055130958557
        vf_loss: 0.4586392194032669
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34666666666667
    gpu_util_percent0: 0.321
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692874845460602
    mean_env_wait_ms: 1.1988309928986443
    mean_inference_ms: 4.326340213052014
    mean_raw_obs_processing_ms: 0.37915887569848744
  time_since_restore: 5956.793418645859
  time_this_iter_s: 26.098734855651855
  time_total_s: 5956.793418645859
  timers:
    learn_throughput: 8596.755
    learn_time_ms: 18820.124
    sample_throughput: 23850.377
    sample_time_ms: 6783.624
    update_time_ms: 32.37
  timestamp: 1602816158
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: 1bbc1_00000
  
2020-10-16 02:42:39,149	WARNING util.py:136 -- The `process_trial` operation took 0.6666123867034912 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    231 |          5956.79 | 37373952 |  299.526 |              322.949 |              165.677 |            787.706 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3062.3754838438235
    time_step_min: 2911
  date: 2020-10-16_02-43-04
  done: false
  episode_len_mean: 787.7386679000925
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.6061241037061
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 47564
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.236557978619706e-39
        cur_lr: 5.0e-05
        entropy: 0.07062103350957234
        entropy_coeff: 0.0005000000000000001
        kl: 0.006808806637612482
        model: {}
        policy_loss: -0.006383033789461479
        total_loss: 0.3720623155434926
        vf_explained_var: 0.9991818070411682
        vf_loss: 0.37848066786925
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.158064516129038
    gpu_util_percent0: 0.3129032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692641360266448
    mean_env_wait_ms: 1.1987491568321709
    mean_inference_ms: 4.326197660307566
    mean_raw_obs_processing_ms: 0.37914925601605565
  time_since_restore: 5982.48934841156
  time_this_iter_s: 25.695929765701294
  time_total_s: 5982.48934841156
  timers:
    learn_throughput: 8610.233
    learn_time_ms: 18790.665
    sample_throughput: 23848.229
    sample_time_ms: 6784.235
    update_time_ms: 32.932
  timestamp: 1602816184
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: 1bbc1_00000
  
2020-10-16 02:43:05,771	WARNING util.py:136 -- The `process_trial` operation took 0.6271116733551025 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    232 |          5982.49 | 37535744 |  299.606 |              322.949 |              165.677 |            787.739 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3061.850387596899
    time_step_min: 2911
  date: 2020-10-16_02-43-31
  done: false
  episode_len_mean: 787.7656937057666
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.684634992667
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 47758
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.236557978619706e-39
        cur_lr: 5.0e-05
        entropy: 0.07303827131787936
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031962137824545303
        model: {}
        policy_loss: -0.007618495094296425
        total_loss: 0.8298342923323313
        vf_explained_var: 0.9983438849449158
        vf_loss: 0.8374893218278885
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.770000000000003
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692406077693962
    mean_env_wait_ms: 1.1986573869523107
    mean_inference_ms: 4.32604834923676
    mean_raw_obs_processing_ms: 0.37913978263740933
  time_since_restore: 6008.332323074341
  time_this_iter_s: 25.84297466278076
  time_total_s: 6008.332323074341
  timers:
    learn_throughput: 8614.294
    learn_time_ms: 18781.807
    sample_throughput: 23831.269
    sample_time_ms: 6789.063
    update_time_ms: 35.005
  timestamp: 1602816211
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: 1bbc1_00000
  
2020-10-16 02:43:32,492	WARNING util.py:136 -- The `process_trial` operation took 0.6515092849731445 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    233 |          6008.33 | 37697536 |  299.685 |              322.949 |              165.677 |            787.766 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3061.2204361604536
    time_step_min: 2911
  date: 2020-10-16_02-43-58
  done: false
  episode_len_mean: 787.8064052342057
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.78414100161456
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 234
  episodes_total: 47992
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.118278989309853e-39
        cur_lr: 5.0e-05
        entropy: 0.07152862474322319
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038802786730229855
        model: {}
        policy_loss: -0.006767610146198422
        total_loss: 0.8336335569620132
        vf_explained_var: 0.9985087513923645
        vf_loss: 0.8404369254906973
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993548387096777
    gpu_util_percent0: 0.32161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691994363808003
    mean_env_wait_ms: 1.1985453445703176
    mean_inference_ms: 4.325866170405947
    mean_raw_obs_processing_ms: 0.37912630190341134
  time_since_restore: 6034.203525066376
  time_this_iter_s: 25.871201992034912
  time_total_s: 6034.203525066376
  timers:
    learn_throughput: 8619.77
    learn_time_ms: 18769.874
    sample_throughput: 23786.407
    sample_time_ms: 6801.868
    update_time_ms: 34.53
  timestamp: 1602816238
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: 1bbc1_00000
  
2020-10-16 02:43:59,444	WARNING util.py:136 -- The `process_trial` operation took 0.6672251224517822 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    234 |           6034.2 | 37859328 |  299.784 |              322.949 |              165.677 |            787.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3060.6904084556763
    time_step_min: 2911
  date: 2020-10-16_02-44-25
  done: false
  episode_len_mean: 787.8329148075127
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.86469551801065
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 193
  episodes_total: 48185
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0591394946549265e-39
        cur_lr: 5.0e-05
        entropy: 0.0696276817470789
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033839979053785405
        model: {}
        policy_loss: -0.007159226163518421
        total_loss: 0.32452842344840366
        vf_explained_var: 0.9992778897285461
        vf_loss: 0.3317224631706874
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.983333333333338
    gpu_util_percent0: 0.33833333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691765836283918
    mean_env_wait_ms: 1.1984566545017754
    mean_inference_ms: 4.3257161192639675
    mean_raw_obs_processing_ms: 0.37911605208789995
  time_since_restore: 6060.008172035217
  time_this_iter_s: 25.804646968841553
  time_total_s: 6060.008172035217
  timers:
    learn_throughput: 8623.327
    learn_time_ms: 18762.132
    sample_throughput: 23799.449
    sample_time_ms: 6798.141
    update_time_ms: 34.533
  timestamp: 1602816265
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: 1bbc1_00000
  
2020-10-16 02:44:26,140	WARNING util.py:136 -- The `process_trial` operation took 0.6531233787536621 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    235 |          6060.01 | 38021120 |  299.865 |              322.949 |              165.677 |            787.833 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3060.200649525267
    time_step_min: 2911
  date: 2020-10-16_02-44-51
  done: false
  episode_len_mean: 787.8581588141656
  episode_reward_max: 322.949494949495
  episode_reward_mean: 299.93947705121735
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 186
  episodes_total: 48371
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.295697473274632e-40
        cur_lr: 5.0e-05
        entropy: 0.06976655746499698
        entropy_coeff: 0.0005000000000000001
        kl: 0.003976064831173669
        model: {}
        policy_loss: -0.009774611903897798
        total_loss: 0.46616274615128833
        vf_explained_var: 0.9989613890647888
        vf_loss: 0.4759722426533699
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.870967741935488
    gpu_util_percent0: 0.31516129032258056
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691529084435795
    mean_env_wait_ms: 1.1983694852410607
    mean_inference_ms: 4.325576169562022
    mean_raw_obs_processing_ms: 0.37910690636212485
  time_since_restore: 6085.834925889969
  time_this_iter_s: 25.826753854751587
  time_total_s: 6085.834925889969
  timers:
    learn_throughput: 8613.994
    learn_time_ms: 18782.46
    sample_throughput: 23859.622
    sample_time_ms: 6780.996
    update_time_ms: 35.31
  timestamp: 1602816291
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: 1bbc1_00000
  
2020-10-16 02:44:53,038	WARNING util.py:136 -- The `process_trial` operation took 0.6638500690460205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    236 |          6085.83 | 38182912 |  299.939 |              322.949 |              165.677 |            787.858 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3059.6065273345002
    time_step_min: 2911
  date: 2020-10-16_02-45-18
  done: false
  episode_len_mean: 787.8892021484576
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.03222104360106
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 48593
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.647848736637316e-40
        cur_lr: 5.0e-05
        entropy: 0.07386242846647899
        entropy_coeff: 0.0005000000000000001
        kl: 0.004638025032666822
        model: {}
        policy_loss: -0.00845400826074183
        total_loss: 0.38573093463977176
        vf_explained_var: 0.9992756247520447
        vf_loss: 0.3942218745748202
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.12333333333333
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691205458469245
    mean_env_wait_ms: 1.198263372758763
    mean_inference_ms: 4.325405297253918
    mean_raw_obs_processing_ms: 0.37909495514004354
  time_since_restore: 6111.581515550613
  time_this_iter_s: 25.74658966064453
  time_total_s: 6111.581515550613
  timers:
    learn_throughput: 8613.963
    learn_time_ms: 18782.527
    sample_throughput: 23891.186
    sample_time_ms: 6772.037
    update_time_ms: 33.437
  timestamp: 1602816318
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: 1bbc1_00000
  
2020-10-16 02:45:19,674	WARNING util.py:136 -- The `process_trial` operation took 0.6618480682373047 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    237 |          6111.58 | 38344704 |  300.032 |              322.949 |              165.677 |            787.889 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3059.0463509635097
    time_step_min: 2911
  date: 2020-10-16_02-45-45
  done: false
  episode_len_mean: 787.912862645468
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.1139966705241
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 48808
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.323924368318658e-40
        cur_lr: 5.0e-05
        entropy: 0.07559195533394814
        entropy_coeff: 0.0005000000000000001
        kl: 0.00490352138876915
        model: {}
        policy_loss: -0.008100793066356951
        total_loss: 0.7327623864014944
        vf_explained_var: 0.9985132813453674
        vf_loss: 0.7409009983142217
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.546666666666674
    gpu_util_percent0: 0.372
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469093020578874
    mean_env_wait_ms: 1.1981622952706847
    mean_inference_ms: 4.325246429871726
    mean_raw_obs_processing_ms: 0.37908353999960803
  time_since_restore: 6137.2552173137665
  time_this_iter_s: 25.673701763153076
  time_total_s: 6137.2552173137665
  timers:
    learn_throughput: 8617.293
    learn_time_ms: 18775.269
    sample_throughput: 23910.412
    sample_time_ms: 6766.592
    update_time_ms: 32.829
  timestamp: 1602816345
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: 1bbc1_00000
  
2020-10-16 02:45:46,323	WARNING util.py:136 -- The `process_trial` operation took 0.6641879081726074 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    238 |          6137.26 | 38506496 |  300.114 |              322.949 |              165.677 |            787.913 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3058.631931537346
    time_step_min: 2911
  date: 2020-10-16_02-46-12
  done: false
  episode_len_mean: 787.9272489742596
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.1803544436174
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 48989
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.61962184159329e-41
        cur_lr: 5.0e-05
        entropy: 0.0728306087354819
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051832586371650296
        model: {}
        policy_loss: -0.008300230760748187
        total_loss: 0.7723995844523112
        vf_explained_var: 0.9982762932777405
        vf_loss: 0.7807362327973048
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.68064516129032
    gpu_util_percent0: 0.3267741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146907019493708
    mean_env_wait_ms: 1.1980808614372005
    mean_inference_ms: 4.325113908428721
    mean_raw_obs_processing_ms: 0.37907470974757757
  time_since_restore: 6163.06715130806
  time_this_iter_s: 25.811933994293213
  time_total_s: 6163.06715130806
  timers:
    learn_throughput: 8622.9
    learn_time_ms: 18763.061
    sample_throughput: 23953.022
    sample_time_ms: 6754.555
    update_time_ms: 33.173
  timestamp: 1602816372
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: 1bbc1_00000
  
2020-10-16 02:46:13,236	WARNING util.py:136 -- The `process_trial` operation took 0.6978006362915039 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    239 |          6163.07 | 38668288 |   300.18 |              322.949 |              165.677 |            787.927 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3058.1244024005696
    time_step_min: 2911
  date: 2020-10-16_02-46-39
  done: false
  episode_len_mean: 787.9503486977208
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.2573446068351
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 49183
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.61962184159329e-41
        cur_lr: 5.0e-05
        entropy: 0.07012288769086202
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00833699247353555
        total_loss: .inf
        vf_explained_var: 0.9991521239280701
        vf_loss: 0.40645678838094074
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.520000000000007
    gpu_util_percent0: 0.31966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690463353891073
    mean_env_wait_ms: 1.1979900176837435
    mean_inference_ms: 4.324967364693697
    mean_raw_obs_processing_ms: 0.37906558429097753
  time_since_restore: 6188.95095705986
  time_this_iter_s: 25.883805751800537
  time_total_s: 6188.95095705986
  timers:
    learn_throughput: 8632.159
    learn_time_ms: 18742.937
    sample_throughput: 23891.584
    sample_time_ms: 6771.924
    update_time_ms: 31.515
  timestamp: 1602816399
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:46:40,013	WARNING util.py:136 -- The `process_trial` operation took 0.6641366481781006 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    240 |          6188.95 | 38830080 |  300.257 |              322.949 |              165.677 |             787.95 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3057.501427443358
    time_step_min: 2911
  date: 2020-10-16_02-47-05
  done: false
  episode_len_mean: 787.9907116984034
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.35430472848753
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 234
  episodes_total: 49417
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.929432762389937e-41
        cur_lr: 5.0e-05
        entropy: 0.07185195696850617
        entropy_coeff: 0.0005000000000000001
        kl: 0.007342742445568244
        model: {}
        policy_loss: -0.005996837319495778
        total_loss: 0.24019909898440042
        vf_explained_var: 0.9995264410972595
        vf_loss: 0.24623185768723488
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.610000000000003
    gpu_util_percent0: 0.3486666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690089761050185
    mean_env_wait_ms: 1.1978793441774864
    mean_inference_ms: 4.324800839899969
    mean_raw_obs_processing_ms: 0.3790532004703173
  time_since_restore: 6214.671000480652
  time_this_iter_s: 25.720043420791626
  time_total_s: 6214.671000480652
  timers:
    learn_throughput: 8645.039
    learn_time_ms: 18715.011
    sample_throughput: 23930.319
    sample_time_ms: 6760.963
    update_time_ms: 32.005
  timestamp: 1602816425
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: 1bbc1_00000
  
2020-10-16 02:47:06,720	WARNING util.py:136 -- The `process_trial` operation took 0.6636948585510254 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    241 |          6214.67 | 38991872 |  300.354 |              322.949 |              165.677 |            787.991 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3057.0022992214917
    time_step_min: 2911
  date: 2020-10-16_02-47-32
  done: false
  episode_len_mean: 788.0112678895384
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.42775874039705
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 193
  episodes_total: 49610
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.929432762389937e-41
        cur_lr: 5.0e-05
        entropy: 0.07176184405883153
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036580318119376898
        model: {}
        policy_loss: -0.00714991054701386
        total_loss: 0.8588201999664307
        vf_explained_var: 0.9981699585914612
        vf_loss: 0.8660059918959936
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.061290322580643
    gpu_util_percent0: 0.30419354838709683
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689880022066068
    mean_env_wait_ms: 1.1977918386408286
    mean_inference_ms: 4.324658910752895
    mean_raw_obs_processing_ms: 0.37904333419084557
  time_since_restore: 6240.517153978348
  time_this_iter_s: 25.846153497695923
  time_total_s: 6240.517153978348
  timers:
    learn_throughput: 8639.439
    learn_time_ms: 18727.142
    sample_throughput: 23919.821
    sample_time_ms: 6763.93
    update_time_ms: 31.734
  timestamp: 1602816452
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: 1bbc1_00000
  
2020-10-16 02:47:33,499	WARNING util.py:136 -- The `process_trial` operation took 0.6937470436096191 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    242 |          6240.52 | 39153664 |  300.428 |              322.949 |              165.677 |            788.011 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3056.5451367262062
    time_step_min: 2911
  date: 2020-10-16_02-47-59
  done: false
  episode_len_mean: 788.0273298660616
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.49499756698674
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 189
  episodes_total: 49799
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.964716381194969e-41
        cur_lr: 5.0e-05
        entropy: 0.07430351401368777
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010059338645078242
        total_loss: .inf
        vf_explained_var: 0.9987800121307373
        vf_loss: 0.5832084467013677
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49333333333334
    gpu_util_percent0: 0.313
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689665115199307
    mean_env_wait_ms: 1.1977042877275172
    mean_inference_ms: 4.32452503514698
    mean_raw_obs_processing_ms: 0.3790346042689147
  time_since_restore: 6266.520976305008
  time_this_iter_s: 26.003822326660156
  time_total_s: 6266.520976305008
  timers:
    learn_throughput: 8632.537
    learn_time_ms: 18742.116
    sample_throughput: 23917.857
    sample_time_ms: 6764.486
    update_time_ms: 31.578
  timestamp: 1602816479
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:48:00,392	WARNING util.py:136 -- The `process_trial` operation took 0.6477236747741699 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    243 |          6266.52 | 39315456 |  300.495 |              322.949 |              165.677 |            788.027 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3055.961677700662
    time_step_min: 2911
  date: 2020-10-16_02-48-26
  done: false
  episode_len_mean: 788.055932033983
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.5835536777064
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 226
  episodes_total: 50025
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.447074571792451e-41
        cur_lr: 5.0e-05
        entropy: 0.07216315468152364
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057853444789846735
        model: {}
        policy_loss: -0.005866110431573664
        total_loss: 0.48611287772655487
        vf_explained_var: 0.9991181492805481
        vf_loss: 0.4920150761802991
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.616129032258062
    gpu_util_percent0: 0.3490322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689331573044123
    mean_env_wait_ms: 1.1975989319397273
    mean_inference_ms: 4.324363769615456
    mean_raw_obs_processing_ms: 0.3790229341412502
  time_since_restore: 6292.352388143539
  time_this_iter_s: 25.831411838531494
  time_total_s: 6292.352388143539
  timers:
    learn_throughput: 8629.144
    learn_time_ms: 18749.485
    sample_throughput: 23956.015
    sample_time_ms: 6753.711
    update_time_ms: 31.104
  timestamp: 1602816506
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: 1bbc1_00000
  
2020-10-16 02:48:27,334	WARNING util.py:136 -- The `process_trial` operation took 0.6974771022796631 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    244 |          6292.35 | 39477248 |  300.584 |              322.949 |              165.677 |            788.056 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3055.4157155661787
    time_step_min: 2911
  date: 2020-10-16_02-48-52
  done: false
  episode_len_mean: 788.071128540999
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.66605999878925
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 208
  episodes_total: 50233
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.447074571792451e-41
        cur_lr: 5.0e-05
        entropy: 0.06867619603872299
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038389874001344046
        model: {}
        policy_loss: -0.007558659272035584
        total_loss: 0.3207550247510274
        vf_explained_var: 0.9993269443511963
        vf_loss: 0.3283480207125346
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.96333333333333
    gpu_util_percent0: 0.3416666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689082045440907
    mean_env_wait_ms: 1.197502591862487
    mean_inference_ms: 4.3242149541667825
    mean_raw_obs_processing_ms: 0.37901247923681547
  time_since_restore: 6317.9921543598175
  time_this_iter_s: 25.639766216278076
  time_total_s: 6317.9921543598175
  timers:
    learn_throughput: 8634.993
    learn_time_ms: 18736.783
    sample_throughput: 23964.402
    sample_time_ms: 6751.347
    update_time_ms: 29.632
  timestamp: 1602816532
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: 1bbc1_00000
  
2020-10-16 02:48:53,948	WARNING util.py:136 -- The `process_trial` operation took 0.7355914115905762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    245 |          6317.99 | 39639040 |  300.666 |              322.949 |              165.677 |            788.071 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3054.9511362508683
    time_step_min: 2911
  date: 2020-10-16_02-49-19
  done: false
  episode_len_mean: 788.0844623410628
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.73687122148806
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 50413
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7235372858962254e-41
        cur_lr: 5.0e-05
        entropy: 0.07019696508844693
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00811262741141642
        total_loss: .inf
        vf_explained_var: 0.9994284510612488
        vf_loss: 0.2527790193756421
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14666666666667
    gpu_util_percent0: 0.3126666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688855527393982
    mean_env_wait_ms: 1.1974215144326654
    mean_inference_ms: 4.324091796199153
    mean_raw_obs_processing_ms: 0.37900400230583736
  time_since_restore: 6343.724091291428
  time_this_iter_s: 25.731936931610107
  time_total_s: 6343.724091291428
  timers:
    learn_throughput: 8642.946
    learn_time_ms: 18719.543
    sample_throughput: 23942.078
    sample_time_ms: 6757.642
    update_time_ms: 29.936
  timestamp: 1602816559
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:49:20,577	WARNING util.py:136 -- The `process_trial` operation took 0.6579740047454834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    246 |          6343.72 | 39800832 |  300.737 |              322.949 |              165.677 |            788.084 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3054.439860844814
    time_step_min: 2911
  date: 2020-10-16_02-49-46
  done: false
  episode_len_mean: 788.0932456192339
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.81132309283777
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 206
  episodes_total: 50619
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.585305928844339e-41
        cur_lr: 5.0e-05
        entropy: 0.08426284914215405
        entropy_coeff: 0.0005000000000000001
        kl: 0.004327422667605181
        model: {}
        policy_loss: -0.008665393543196842
        total_loss: 0.7734628568092982
        vf_explained_var: 0.998393714427948
        vf_loss: 0.782170370221138
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.27096774193548
    gpu_util_percent0: 0.3190322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468861686080201
    mean_env_wait_ms: 1.1973250335976071
    mean_inference_ms: 4.323945511751623
    mean_raw_obs_processing_ms: 0.37899454745601996
  time_since_restore: 6369.705541849136
  time_this_iter_s: 25.98145055770874
  time_total_s: 6369.705541849136
  timers:
    learn_throughput: 8640.395
    learn_time_ms: 18725.069
    sample_throughput: 23920.735
    sample_time_ms: 6763.672
    update_time_ms: 31.737
  timestamp: 1602816586
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: 1bbc1_00000
  
2020-10-16 02:49:47,454	WARNING util.py:136 -- The `process_trial` operation took 0.6573593616485596 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    247 |          6369.71 | 39962624 |  300.811 |              322.949 |              165.677 |            788.093 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3053.9786308809353
    time_step_min: 2911
  date: 2020-10-16_02-50-13
  done: false
  episode_len_mean: 788.0902869279632
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.88269228897343
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 230
  episodes_total: 50849
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7926529644221695e-41
        cur_lr: 5.0e-05
        entropy: 0.08532803505659103
        entropy_coeff: 0.0005000000000000001
        kl: 0.004569496784824878
        model: {}
        policy_loss: -0.00596083401372501
        total_loss: 1.2947463194529216
        vf_explained_var: 0.9974839091300964
        vf_loss: 1.3007498184839885
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.470000000000002
    gpu_util_percent0: 0.327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688290928591763
    mean_env_wait_ms: 1.197220382743948
    mean_inference_ms: 4.323792159578777
    mean_raw_obs_processing_ms: 0.3789830052771105
  time_since_restore: 6395.619601726532
  time_this_iter_s: 25.91405987739563
  time_total_s: 6395.619601726532
  timers:
    learn_throughput: 8635.894
    learn_time_ms: 18734.83
    sample_throughput: 23884.948
    sample_time_ms: 6773.806
    update_time_ms: 34.12
  timestamp: 1602816613
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: 1bbc1_00000
  
2020-10-16 02:50:14,320	WARNING util.py:136 -- The `process_trial` operation took 0.6890945434570312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    248 |          6395.62 | 40124416 |  300.883 |              322.949 |              165.677 |             788.09 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3053.562833281054
    time_step_min: 2911
  date: 2020-10-16_02-50-40
  done: false
  episode_len_mean: 788.0990281370014
  episode_reward_max: 322.949494949495
  episode_reward_mean: 300.94807349298276
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 51036
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3963264822110847e-41
        cur_lr: 5.0e-05
        entropy: 0.07865231297910213
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008622396975018395
        total_loss: .inf
        vf_explained_var: 0.9988759160041809
        vf_loss: 0.5114175553123156
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.425806451612903
    gpu_util_percent0: 0.3341935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688083388078252
    mean_env_wait_ms: 1.1971375557628328
    mean_inference_ms: 4.323661259720064
    mean_raw_obs_processing_ms: 0.37897409814915534
  time_since_restore: 6421.577824115753
  time_this_iter_s: 25.95822238922119
  time_total_s: 6421.577824115753
  timers:
    learn_throughput: 8635.074
    learn_time_ms: 18736.608
    sample_throughput: 23874.775
    sample_time_ms: 6776.692
    update_time_ms: 41.814
  timestamp: 1602816640
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:50:41,248	WARNING util.py:136 -- The `process_trial` operation took 0.72444748878479 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    249 |          6421.58 | 40286208 |  300.948 |              322.949 |              165.677 |            788.099 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3053.087560303912
    time_step_min: 2911
  date: 2020-10-16_02-51-07
  done: false
  episode_len_mean: 788.1227868116423
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.0201974850302
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 191
  episodes_total: 51227
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0944897233166275e-41
        cur_lr: 5.0e-05
        entropy: 0.07245245079199474
        entropy_coeff: 0.0005000000000000001
        kl: 0.00434496133433034
        model: {}
        policy_loss: -0.007277095612759392
        total_loss: 0.31283790866533917
        vf_explained_var: 0.9993278384208679
        vf_loss: 0.32015122721592587
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74
    gpu_util_percent0: 0.349
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687885566673223
    mean_env_wait_ms: 1.197050059726401
    mean_inference_ms: 4.323530511306742
    mean_raw_obs_processing_ms: 0.3789655042792282
  time_since_restore: 6447.358994722366
  time_this_iter_s: 25.78117060661316
  time_total_s: 6447.358994722366
  timers:
    learn_throughput: 8632.989
    learn_time_ms: 18741.133
    sample_throughput: 23933.707
    sample_time_ms: 6760.006
    update_time_ms: 43.556
  timestamp: 1602816667
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: 1bbc1_00000
  
2020-10-16 02:51:07,943	WARNING util.py:136 -- The `process_trial` operation took 0.6768014430999756 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    250 |          6447.36 | 40448000 |   301.02 |              322.949 |              165.677 |            788.123 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3052.5029458087847
    time_step_min: 2911
  date: 2020-10-16_02-51-33
  done: false
  episode_len_mean: 788.1551781098782
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.107630515466
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 230
  episodes_total: 51457
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0472448616583137e-41
        cur_lr: 5.0e-05
        entropy: 0.07192403636872768
        entropy_coeff: 0.0005000000000000001
        kl: 0.004542506901392092
        model: {}
        policy_loss: -0.004144338433131149
        total_loss: 0.259403258562088
        vf_explained_var: 0.9994840621948242
        vf_loss: 0.26358356947700184
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.158064516129038
    gpu_util_percent0: 0.30774193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687546251538844
    mean_env_wait_ms: 1.1969450328049254
    mean_inference_ms: 4.323378913560818
    mean_raw_obs_processing_ms: 0.3789545023719386
  time_since_restore: 6473.278051376343
  time_this_iter_s: 25.91905665397644
  time_total_s: 6473.278051376343
  timers:
    learn_throughput: 8629.728
    learn_time_ms: 18748.216
    sample_throughput: 23926.438
    sample_time_ms: 6762.06
    update_time_ms: 44.546
  timestamp: 1602816693
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: 1bbc1_00000
  
2020-10-16 02:51:34,848	WARNING util.py:136 -- The `process_trial` operation took 0.6572105884552002 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    251 |          6473.28 | 40609792 |  301.108 |              322.949 |              165.677 |            788.155 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3051.9885148744966
    time_step_min: 2911
  date: 2020-10-16_02-52-00
  done: false
  episode_len_mean: 788.1841076267906
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.18526730721834
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 203
  episodes_total: 51660
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.236224308291569e-42
        cur_lr: 5.0e-05
        entropy: 0.06640605938931306
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0047315647098002955
        total_loss: .inf
        vf_explained_var: 0.9996795654296875
        vf_loss: 0.15462222571174303
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.073333333333334
    gpu_util_percent0: 0.2923333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687316179410417
    mean_env_wait_ms: 1.196853939468602
    mean_inference_ms: 4.323240288567541
    mean_raw_obs_processing_ms: 0.3789445901363846
  time_since_restore: 6499.217586994171
  time_this_iter_s: 25.93953561782837
  time_total_s: 6499.217586994171
  timers:
    learn_throughput: 8621.503
    learn_time_ms: 18766.102
    sample_throughput: 23927.499
    sample_time_ms: 6761.76
    update_time_ms: 44.355
  timestamp: 1602816720
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:52:01,829	WARNING util.py:136 -- The `process_trial` operation took 0.7047195434570312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    252 |          6499.22 | 40771584 |  301.185 |              322.949 |              165.677 |            788.184 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3051.5284978093455
    time_step_min: 2911
  date: 2020-10-16_02-52-27
  done: false
  episode_len_mean: 788.2119060938676
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.2537580905603
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 51839
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.85433646243735e-42
        cur_lr: 5.0e-05
        entropy: 0.06562024975816409
        entropy_coeff: 0.0005000000000000001
        kl: 0.00456874055089429
        model: {}
        policy_loss: -0.006148295952395226
        total_loss: 0.18244275574882826
        vf_explained_var: 0.9996242523193359
        vf_loss: 0.18862386420369148
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28387096774194
    gpu_util_percent0: 0.31999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687106382173556
    mean_env_wait_ms: 1.1967732993574438
    mean_inference_ms: 4.32312083964783
    mean_raw_obs_processing_ms: 0.37893642371754743
  time_since_restore: 6525.13844537735
  time_this_iter_s: 25.92085838317871
  time_total_s: 6525.13844537735
  timers:
    learn_throughput: 8627.181
    learn_time_ms: 18753.751
    sample_throughput: 23911.619
    sample_time_ms: 6766.25
    update_time_ms: 43.517
  timestamp: 1602816747
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: 1bbc1_00000
  
2020-10-16 02:52:28,708	WARNING util.py:136 -- The `process_trial` operation took 0.6777172088623047 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    253 |          6525.14 | 40933376 |  301.254 |              322.949 |              165.677 |            788.212 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3051.0168777993504
    time_step_min: 2911
  date: 2020-10-16_02-52-54
  done: false
  episode_len_mean: 788.2392168917751
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.3284379851075
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 210
  episodes_total: 52049
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.927168231218675e-42
        cur_lr: 5.0e-05
        entropy: 0.07380476780235767
        entropy_coeff: 0.0005000000000000001
        kl: 0.005277828856681784
        model: {}
        policy_loss: -0.007356394851134003
        total_loss: 0.4370995784799258
        vf_explained_var: 0.9991054534912109
        vf_loss: 0.4444928715626399
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.590000000000007
    gpu_util_percent0: 0.3226666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686847506812986
    mean_env_wait_ms: 1.1966765530713215
    mean_inference_ms: 4.3229835687667535
    mean_raw_obs_processing_ms: 0.3789268921968416
  time_since_restore: 6550.858793735504
  time_this_iter_s: 25.720348358154297
  time_total_s: 6550.858793735504
  timers:
    learn_throughput: 8635.64
    learn_time_ms: 18735.38
    sample_throughput: 23887.231
    sample_time_ms: 6773.158
    update_time_ms: 42.994
  timestamp: 1602816774
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: 1bbc1_00000
  
2020-10-16 02:52:55,411	WARNING util.py:136 -- The `process_trial` operation took 0.6644785404205322 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    254 |          6550.86 | 41095168 |  301.328 |              322.949 |              165.677 |            788.239 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3050.483625854116
    time_step_min: 2911
  date: 2020-10-16_02-53-21
  done: false
  episode_len_mean: 788.2725011956002
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.4084054316477
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 226
  episodes_total: 52275
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.927168231218675e-42
        cur_lr: 5.0e-05
        entropy: 0.07566056276361148
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043171421469499665
        model: {}
        policy_loss: -0.007438794199212377
        total_loss: 0.6787708600362142
        vf_explained_var: 0.9986443519592285
        vf_loss: 0.6862474779287974
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.140625
    gpu_util_percent0: 0.3096875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686565786480293
    mean_env_wait_ms: 1.1965768101320517
    mean_inference_ms: 4.32283964877685
    mean_raw_obs_processing_ms: 0.37891666538302327
  time_since_restore: 6577.172795057297
  time_this_iter_s: 26.314001321792603
  time_total_s: 6577.172795057297
  timers:
    learn_throughput: 8616.986
    learn_time_ms: 18775.938
    sample_throughput: 23802.028
    sample_time_ms: 6797.404
    update_time_ms: 44.633
  timestamp: 1602816801
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: 1bbc1_00000
  
2020-10-16 02:53:22,824	WARNING util.py:136 -- The `process_trial` operation took 0.6803712844848633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    255 |          6577.17 | 41256960 |  301.408 |              322.949 |              165.677 |            788.273 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3050.0528303326214
    time_step_min: 2911
  date: 2020-10-16_02-53-48
  done: false
  episode_len_mean: 788.2961303850553
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.47404179037784
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 52460
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9635841156093376e-42
        cur_lr: 5.0e-05
        entropy: 0.07216892515619595
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0069495806261935895
        total_loss: .inf
        vf_explained_var: 0.9993376731872559
        vf_loss: 0.2948884641130765
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.820000000000004
    gpu_util_percent0: 0.32400000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686374146404915
    mean_env_wait_ms: 1.19649521938888
    mean_inference_ms: 4.322717808012822
    mean_raw_obs_processing_ms: 0.37890819940132603
  time_since_restore: 6603.0767822265625
  time_this_iter_s: 25.903987169265747
  time_total_s: 6603.0767822265625
  timers:
    learn_throughput: 8607.586
    learn_time_ms: 18796.443
    sample_throughput: 23815.138
    sample_time_ms: 6793.662
    update_time_ms: 42.992
  timestamp: 1602816828
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:53:49,707	WARNING util.py:136 -- The `process_trial` operation took 0.7357778549194336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    256 |          6603.08 | 41418752 |  301.474 |              322.949 |              165.677 |            788.296 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3049.6449908786867
    time_step_min: 2911
  date: 2020-10-16_02-54-15
  done: false
  episode_len_mean: 788.3163222669605
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.5385525466621
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 192
  episodes_total: 52652
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.945376173414007e-42
        cur_lr: 5.0e-05
        entropy: 0.07493447388211887
        entropy_coeff: 0.0005000000000000001
        kl: 0.004303191012392442
        model: {}
        policy_loss: -0.009373692062884706
        total_loss: 0.4947909290591876
        vf_explained_var: 0.9989476799964905
        vf_loss: 0.5042020777861277
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.293548387096774
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686195465048368
    mean_env_wait_ms: 1.196409016972427
    mean_inference_ms: 4.322594098584319
    mean_raw_obs_processing_ms: 0.3789001274836648
  time_since_restore: 6629.007013320923
  time_this_iter_s: 25.93023109436035
  time_total_s: 6629.007013320923
  timers:
    learn_throughput: 8609.337
    learn_time_ms: 18792.621
    sample_throughput: 23823.995
    sample_time_ms: 6791.136
    update_time_ms: 43.351
  timestamp: 1602816855
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: 1bbc1_00000
  
2020-10-16 02:54:16,629	WARNING util.py:136 -- The `process_trial` operation took 0.7340006828308105 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    257 |          6629.01 | 41580544 |  301.539 |              322.949 |              165.677 |            788.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3049.135994702488
    time_step_min: 2911
  date: 2020-10-16_02-54-42
  done: false
  episode_len_mean: 788.340147117221
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.6155754164375
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 231
  episodes_total: 52883
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4726880867070035e-42
        cur_lr: 5.0e-05
        entropy: 0.07420648820698261
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007232263267117863
        total_loss: .inf
        vf_explained_var: 0.9989327788352966
        vf_loss: 0.553872138261795
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.053333333333335
    gpu_util_percent0: 0.3596666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685857602428976
    mean_env_wait_ms: 1.1963048456417775
    mean_inference_ms: 4.322448878428525
    mean_raw_obs_processing_ms: 0.3788890754714393
  time_since_restore: 6654.805671215057
  time_this_iter_s: 25.79865789413452
  time_total_s: 6654.805671215057
  timers:
    learn_throughput: 8609.91
    learn_time_ms: 18791.369
    sample_throughput: 23853.93
    sample_time_ms: 6782.614
    update_time_ms: 41.345
  timestamp: 1602816882
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:54:43,428	WARNING util.py:136 -- The `process_trial` operation took 0.7515261173248291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    258 |          6654.81 | 41742336 |  301.616 |              322.949 |              165.677 |             788.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3048.6924961830623
    time_step_min: 2911
  date: 2020-10-16_02-55-09
  done: false
  episode_len_mean: 788.368719504154
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.68499523978863
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 53081
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2090321300605053e-42
        cur_lr: 5.0e-05
        entropy: 0.0703418217599392
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008395893208216876
        total_loss: .inf
        vf_explained_var: 0.9993597865104675
        vf_loss: 0.3083875998854637
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.841935483870973
    gpu_util_percent0: 0.3129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468564812406541
    mean_env_wait_ms: 1.1962172635314885
    mean_inference_ms: 4.322321234379204
    mean_raw_obs_processing_ms: 0.37888010395352667
  time_since_restore: 6680.795508623123
  time_this_iter_s: 25.989837408065796
  time_total_s: 6680.795508623123
  timers:
    learn_throughput: 8611.127
    learn_time_ms: 18788.713
    sample_throughput: 23831.944
    sample_time_ms: 6788.871
    update_time_ms: 31.847
  timestamp: 1602816909
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:55:10,394	WARNING util.py:136 -- The `process_trial` operation took 0.707075834274292 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    259 |           6680.8 | 41904128 |  301.685 |              322.949 |              165.677 |            788.369 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3048.270855640086
    time_step_min: 2911
  date: 2020-10-16_02-55-36
  done: false
  episode_len_mean: 788.3944201415617
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.7472134179978
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 182
  episodes_total: 53263
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.313548195090757e-42
        cur_lr: 5.0e-05
        entropy: 0.06987964920699596
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007836210659055117
        total_loss: .inf
        vf_explained_var: 0.9992274641990662
        vf_loss: 0.3635381832718849
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.773333333333337
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685471466494202
    mean_env_wait_ms: 1.1961373308429182
    mean_inference_ms: 4.322207778197562
    mean_raw_obs_processing_ms: 0.3788723481736745
  time_since_restore: 6706.469789505005
  time_this_iter_s: 25.674280881881714
  time_total_s: 6706.469789505005
  timers:
    learn_throughput: 8613.826
    learn_time_ms: 18782.828
    sample_throughput: 23845.645
    sample_time_ms: 6784.971
    update_time_ms: 29.93
  timestamp: 1602816936
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:55:37,106	WARNING util.py:136 -- The `process_trial` operation took 0.7872524261474609 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    260 |          6706.47 | 42065920 |  301.747 |              322.949 |              165.677 |            788.394 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3047.786382516933
    time_step_min: 2911
  date: 2020-10-16_02-56-02
  done: false
  episode_len_mean: 788.4244305643864
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.8210479330461
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 211
  episodes_total: 53474
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.9703222926361374e-42
        cur_lr: 5.0e-05
        entropy: 0.0736902579665184
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00867642241549523
        total_loss: .inf
        vf_explained_var: 0.9995377659797668
        vf_loss: 0.24374699095884958
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35161290322581
    gpu_util_percent0: 0.31032258064516133
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685206983479687
    mean_env_wait_ms: 1.1960409496801117
    mean_inference_ms: 4.322078519443739
    mean_raw_obs_processing_ms: 0.3788632095677131
  time_since_restore: 6732.290620088577
  time_this_iter_s: 25.820830583572388
  time_total_s: 6732.290620088577
  timers:
    learn_throughput: 8619.997
    learn_time_ms: 18769.381
    sample_throughput: 23835.944
    sample_time_ms: 6787.732
    update_time_ms: 29.775
  timestamp: 1602816962
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:56:03,958	WARNING util.py:136 -- The `process_trial` operation took 0.7804450988769531 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    261 |          6732.29 | 42227712 |  301.821 |              322.949 |              165.677 |            788.424 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3047.2786152154877
    time_step_min: 2911
  date: 2020-10-16_02-56-29
  done: false
  episode_len_mean: 788.458535858614
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.8984571679133
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 53697
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.455483438954203e-42
        cur_lr: 5.0e-05
        entropy: 0.07541089629133542
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005737459384060155
        total_loss: .inf
        vf_explained_var: 0.9992527365684509
        vf_loss: 0.3754478891690572
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.136666666666667
    gpu_util_percent0: 0.3246666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684918460591534
    mean_env_wait_ms: 1.195943872631786
    mean_inference_ms: 4.321937744420289
    mean_raw_obs_processing_ms: 0.3788532616202169
  time_since_restore: 6758.079988718033
  time_this_iter_s: 25.789368629455566
  time_total_s: 6758.079988718033
  timers:
    learn_throughput: 8626.509
    learn_time_ms: 18755.211
    sample_throughput: 23842.657
    sample_time_ms: 6785.821
    update_time_ms: 29.519
  timestamp: 1602816989
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:56:30,817	WARNING util.py:136 -- The `process_trial` operation took 0.7159223556518555 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    262 |          6758.08 | 42389504 |  301.898 |              322.949 |              165.677 |            788.459 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3046.9757673667204
    time_step_min: 2911
  date: 2020-10-16_02-56-56
  done: false
  episode_len_mean: 788.4700729385127
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.9400257094804
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 184
  episodes_total: 53881
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1183225158431304e-41
        cur_lr: 5.0e-05
        entropy: 0.08802823101480801
        entropy_coeff: 0.0005000000000000001
        kl: 0.004361501623255511
        model: {}
        policy_loss: -0.012321775535989824
        total_loss: 1.1092246373494465
        vf_explained_var: 0.9975925087928772
        vf_loss: 1.1215904156366985
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.545161290322586
    gpu_util_percent0: 0.3441935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684748055724256
    mean_env_wait_ms: 1.1958648177635094
    mean_inference_ms: 4.32182898347879
    mean_raw_obs_processing_ms: 0.3788452098304094
  time_since_restore: 6783.9213399887085
  time_this_iter_s: 25.84135127067566
  time_total_s: 6783.9213399887085
  timers:
    learn_throughput: 8628.85
    learn_time_ms: 18750.124
    sample_throughput: 23859.38
    sample_time_ms: 6781.065
    update_time_ms: 29.639
  timestamp: 1602817016
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: 1bbc1_00000
  
2020-10-16 02:56:57,644	WARNING util.py:136 -- The `process_trial` operation took 0.6904840469360352 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    263 |          6783.92 | 42551296 |   301.94 |              322.949 |              165.677 |             788.47 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3046.6154330009435
    time_step_min: 2911
  date: 2020-10-16_02-57-23
  done: false
  episode_len_mean: 788.4857528522032
  episode_reward_max: 322.949494949495
  episode_reward_mean: 301.99573759450595
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 200
  episodes_total: 54081
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.591612579215652e-42
        cur_lr: 5.0e-05
        entropy: 0.08296401426196098
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007758317592864235
        total_loss: .inf
        vf_explained_var: 0.9984821677207947
        vf_loss: 0.7768841485182444
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.523333333333333
    gpu_util_percent0: 0.33566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684553242599035
    mean_env_wait_ms: 1.1957760684134981
    mean_inference_ms: 4.321705257092942
    mean_raw_obs_processing_ms: 0.37883676346800516
  time_since_restore: 6809.758305072784
  time_this_iter_s: 25.836965084075928
  time_total_s: 6809.758305072784
  timers:
    learn_throughput: 8623.488
    learn_time_ms: 18761.782
    sample_throughput: 23871.278
    sample_time_ms: 6777.685
    update_time_ms: 31.057
  timestamp: 1602817043
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:57:24,545	WARNING util.py:136 -- The `process_trial` operation took 0.7085816860198975 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    264 |          6809.76 | 42713088 |  301.996 |              322.949 |              165.677 |            788.486 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3046.159322033898
    time_step_min: 2911
  date: 2020-10-16_02-57-50
  done: false
  episode_len_mean: 788.5129999263461
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.06801033089954
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 227
  episodes_total: 54308
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.38741886882348e-42
        cur_lr: 5.0e-05
        entropy: 0.07372427483399709
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007294539478607476
        total_loss: .inf
        vf_explained_var: 0.9993823170661926
        vf_loss: 0.32328644891579944
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.048387096774196
    gpu_util_percent0: 0.307741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684244182428546
    mean_env_wait_ms: 1.1956763900671696
    mean_inference_ms: 4.321571782845085
    mean_raw_obs_processing_ms: 0.3788269748919148
  time_since_restore: 6835.7770256996155
  time_this_iter_s: 26.018720626831055
  time_total_s: 6835.7770256996155
  timers:
    learn_throughput: 8629.234
    learn_time_ms: 18749.288
    sample_throughput: 23939.354
    sample_time_ms: 6758.411
    update_time_ms: 31.516
  timestamp: 1602817070
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:57:51,594	WARNING util.py:136 -- The `process_trial` operation took 0.7413232326507568 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    265 |          6835.78 | 42874880 |  302.068 |              322.949 |              165.677 |            788.513 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3045.8231849472236
    time_step_min: 2911
  date: 2020-10-16_02-58-17
  done: false
  episode_len_mean: 788.5209071060309
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.1092131523848
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 195
  episodes_total: 54503
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2581128303235218e-41
        cur_lr: 5.0e-05
        entropy: 0.08921164957185586
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008603201708259197
        total_loss: .inf
        vf_explained_var: 0.9966201782226562
        vf_loss: 1.6694317261377971
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.076666666666668
    gpu_util_percent0: 0.3463333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684047515982135
    mean_env_wait_ms: 1.1955921467709898
    mean_inference_ms: 4.321451483130764
    mean_raw_obs_processing_ms: 0.3788182613923359
  time_since_restore: 6861.5591514110565
  time_this_iter_s: 25.78212571144104
  time_total_s: 6861.5591514110565
  timers:
    learn_throughput: 8637.024
    learn_time_ms: 18732.379
    sample_throughput: 23921.789
    sample_time_ms: 6763.374
    update_time_ms: 31.485
  timestamp: 1602817097
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:58:18,411	WARNING util.py:136 -- The `process_trial` operation took 0.7689881324768066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    266 |          6861.56 | 43036672 |  302.109 |              322.949 |              165.677 |            788.521 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3045.5297916323657
    time_step_min: 2911
  date: 2020-10-16_02-58-44
  done: false
  episode_len_mean: 788.5267594302536
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.1504853068911
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 188
  episodes_total: 54691
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8871692454852825e-41
        cur_lr: 5.0e-05
        entropy: 0.08766715663174789
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009348560444777831
        total_loss: .inf
        vf_explained_var: 0.9977816939353943
        vf_loss: 1.0422802070776622
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.480645161290326
    gpu_util_percent0: 0.3416129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468388292115877
    mean_env_wait_ms: 1.1955123021563123
    mean_inference_ms: 4.321344147876176
    mean_raw_obs_processing_ms: 0.3788111473402965
  time_since_restore: 6887.46110868454
  time_this_iter_s: 25.901957273483276
  time_total_s: 6887.46110868454
  timers:
    learn_throughput: 8638.903
    learn_time_ms: 18728.305
    sample_throughput: 23889.173
    sample_time_ms: 6772.608
    update_time_ms: 31.079
  timestamp: 1602817124
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:58:45,331	WARNING util.py:136 -- The `process_trial` operation took 0.7580630779266357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    267 |          6887.46 | 43198464 |   302.15 |              322.949 |              165.677 |            788.527 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3045.162995353922
    time_step_min: 2911
  date: 2020-10-16_02-59-11
  done: false
  episode_len_mean: 788.5383242583723
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.20753066328774
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 54913
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.830753868227925e-41
        cur_lr: 5.0e-05
        entropy: 0.08045129850506783
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0091488326555312
        total_loss: .inf
        vf_explained_var: 0.9983231425285339
        vf_loss: 0.8708376884460449
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50967741935484
    gpu_util_percent0: 0.31225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468358754524525
    mean_env_wait_ms: 1.1954121351953968
    mean_inference_ms: 4.3212102014685065
    mean_raw_obs_processing_ms: 0.37880088648788235
  time_since_restore: 6913.290647029877
  time_this_iter_s: 25.829538345336914
  time_total_s: 6913.290647029877
  timers:
    learn_throughput: 8642.821
    learn_time_ms: 18719.814
    sample_throughput: 23853.818
    sample_time_ms: 6782.646
    update_time_ms: 32.058
  timestamp: 1602817151
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 02:59:12,356	WARNING util.py:136 -- The `process_trial` operation took 0.7628934383392334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    268 |          6913.29 | 43360256 |  302.208 |              322.949 |              165.677 |            788.538 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3044.704174228675
    time_step_min: 2911
  date: 2020-10-16_02-59-37
  done: false
  episode_len_mean: 788.5683318821651
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.2776508005609
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 55128
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2461308023418864e-41
        cur_lr: 5.0e-05
        entropy: 0.06846655967334907
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642111995257437
        model: {}
        policy_loss: -0.008557990964618511
        total_loss: 0.1990675963461399
        vf_explained_var: 0.9995818138122559
        vf_loss: 0.20765981947382292
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468335281012079
    mean_env_wait_ms: 1.19532218736196
    mean_inference_ms: 4.321082649411274
    mean_raw_obs_processing_ms: 0.37879241602055
  time_since_restore: 6938.671983003616
  time_this_iter_s: 25.381335973739624
  time_total_s: 6938.671983003616
  timers:
    learn_throughput: 8669.422
    learn_time_ms: 18662.374
    sample_throughput: 23858.36
    sample_time_ms: 6781.355
    update_time_ms: 32.058
  timestamp: 1602817177
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: 1bbc1_00000
  
2020-10-16 02:59:38,899	WARNING util.py:136 -- The `process_trial` operation took 0.78011155128479 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    269 |          6938.67 | 43522048 |  302.278 |              322.949 |              165.677 |            788.568 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3044.3371565027046
    time_step_min: 2911
  date: 2020-10-16_03-00-04
  done: false
  episode_len_mean: 788.5904167796764
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.33024577206817
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 177
  episodes_total: 55305
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2461308023418864e-41
        cur_lr: 5.0e-05
        entropy: 0.07305771422882874
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010683585685910657
        total_loss: .inf
        vf_explained_var: 0.9985249638557434
        vf_loss: 0.663709337512652
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.889999999999997
    gpu_util_percent0: 0.345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468319981434302
    mean_env_wait_ms: 1.195248069146664
    mean_inference_ms: 4.320985840667264
    mean_raw_obs_processing_ms: 0.37878549473038625
  time_since_restore: 6964.0655863285065
  time_this_iter_s: 25.393603324890137
  time_total_s: 6964.0655863285065
  timers:
    learn_throughput: 8694.43
    learn_time_ms: 18608.696
    sample_throughput: 23797.65
    sample_time_ms: 6798.655
    update_time_ms: 32.025
  timestamp: 1602817204
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:00:05,341	WARNING util.py:136 -- The `process_trial` operation took 0.7635107040405273 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    270 |          6964.07 | 43683840 |   302.33 |              322.949 |              165.677 |             788.59 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3044.02447684793
    time_step_min: 2911
  date: 2020-10-16_03-00-31
  done: false
  episode_len_mean: 788.6075411194581
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.3746441336019
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 204
  episodes_total: 55509
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.36919620351283e-41
        cur_lr: 5.0e-05
        entropy: 0.078483151892821
        entropy_coeff: 0.0005000000000000001
        kl: 0.003577448194846511
        model: {}
        policy_loss: -0.009860293803891787
        total_loss: 1.3723692099253337
        vf_explained_var: 0.9972724914550781
        vf_loss: 1.382268746693929
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74
    gpu_util_percent0: 0.366
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683005072350974
    mean_env_wait_ms: 1.1951586162226646
    mean_inference_ms: 4.320865388804352
    mean_raw_obs_processing_ms: 0.37877709613753807
  time_since_restore: 6989.870260000229
  time_this_iter_s: 25.804673671722412
  time_total_s: 6989.870260000229
  timers:
    learn_throughput: 8694.153
    learn_time_ms: 18609.289
    sample_throughput: 23774.517
    sample_time_ms: 6805.27
    update_time_ms: 32.12
  timestamp: 1602817231
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: 1bbc1_00000
  
2020-10-16 03:00:32,190	WARNING util.py:136 -- The `process_trial` operation took 0.7866506576538086 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    271 |          6989.87 | 43845632 |  302.375 |              322.949 |              165.677 |            788.608 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3043.6987631715942
    time_step_min: 2911
  date: 2020-10-16_03-00-57
  done: false
  episode_len_mean: 788.6310038575401
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.43511548607063
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 226
  episodes_total: 55735
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.184598101756415e-41
        cur_lr: 5.0e-05
        entropy: 0.07346769608557224
        entropy_coeff: 0.0005000000000000001
        kl: 0.00415405014064163
        model: {}
        policy_loss: -0.009754505609938255
        total_loss: 0.8083672424157461
        vf_explained_var: 0.9984336495399475
        vf_loss: 0.8181584775447845
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.04193548387097
    gpu_util_percent0: 0.3029032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682713288101218
    mean_env_wait_ms: 1.195063596063013
    mean_inference_ms: 4.320742307014778
    mean_raw_obs_processing_ms: 0.3787682744174126
  time_since_restore: 7015.568287611008
  time_this_iter_s: 25.69802761077881
  time_total_s: 7015.568287611008
  timers:
    learn_throughput: 8700.756
    learn_time_ms: 18595.166
    sample_throughput: 23754.155
    sample_time_ms: 6811.103
    update_time_ms: 30.958
  timestamp: 1602817257
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: 1bbc1_00000
  
2020-10-16 03:00:59,091	WARNING util.py:136 -- The `process_trial` operation took 0.7676770687103271 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    272 |          7015.57 | 44007424 |  302.435 |              322.949 |              165.677 |            788.631 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3043.2996726355523
    time_step_min: 2911
  date: 2020-10-16_03-01-24
  done: false
  episode_len_mean: 788.6567612508717
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.49573783211054
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 55929
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5922990508782076e-41
        cur_lr: 5.0e-05
        entropy: 0.06448541767895222
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0086214902306286
        total_loss: .inf
        vf_explained_var: 0.9994975924491882
        vf_loss: 0.2298381651441256
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.440000000000005
    gpu_util_percent0: 0.28933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682534190572605
    mean_env_wait_ms: 1.1949820458528948
    mean_inference_ms: 4.3206312142782455
    mean_raw_obs_processing_ms: 0.37876028358120295
  time_since_restore: 7041.42772436142
  time_this_iter_s: 25.859436750411987
  time_total_s: 7041.42772436142
  timers:
    learn_throughput: 8700.832
    learn_time_ms: 18595.003
    sample_throughput: 23759.007
    sample_time_ms: 6809.712
    update_time_ms: 31.609
  timestamp: 1602817284
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:01:26,009	WARNING util.py:136 -- The `process_trial` operation took 0.7924866676330566 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    273 |          7041.43 | 44169216 |  302.496 |              322.949 |              165.677 |            788.657 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3042.922938397076
    time_step_min: 2911
  date: 2020-10-16_03-01-52
  done: false
  episode_len_mean: 788.6810364799601
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.5513967396594
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 184
  episodes_total: 56113
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.388448576317312e-41
        cur_lr: 5.0e-05
        entropy: 0.06881815753877163
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008860993727770014
        total_loss: .inf
        vf_explained_var: 0.9992311000823975
        vf_loss: 0.35212529947360355
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.806451612903224
    gpu_util_percent0: 0.32806451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682375828248362
    mean_env_wait_ms: 1.1949052229317019
    mean_inference_ms: 4.320530524370924
    mean_raw_obs_processing_ms: 0.3787537320506962
  time_since_restore: 7067.545698165894
  time_this_iter_s: 26.117973804473877
  time_total_s: 7067.545698165894
  timers:
    learn_throughput: 8689.503
    learn_time_ms: 18619.247
    sample_throughput: 23748.08
    sample_time_ms: 6812.846
    update_time_ms: 31.829
  timestamp: 1602817312
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:01:53,184	WARNING util.py:136 -- The `process_trial` operation took 0.7985336780548096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    274 |          7067.55 | 44331008 |  302.551 |              322.949 |              165.677 |            788.681 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3042.448738212364
    time_step_min: 2911
  date: 2020-10-16_03-02-18
  done: false
  episode_len_mean: 788.7131370147505
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.6212591864648
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 224
  episodes_total: 56337
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5826728644759666e-41
        cur_lr: 5.0e-05
        entropy: 0.07142200569311778
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008045268652494997
        total_loss: .inf
        vf_explained_var: 0.9991478323936462
        vf_loss: 0.46111338088909787
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33
    gpu_util_percent0: 0.32899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468209998572352
    mean_env_wait_ms: 1.194807994024873
    mean_inference_ms: 4.3204064803575895
    mean_raw_obs_processing_ms: 0.37874438066393656
  time_since_restore: 7093.262983322144
  time_this_iter_s: 25.71728515625
  time_total_s: 7093.262983322144
  timers:
    learn_throughput: 8703.278
    learn_time_ms: 18589.778
    sample_throughput: 23755.578
    sample_time_ms: 6810.695
    update_time_ms: 31.021
  timestamp: 1602817338
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:02:20,046	WARNING util.py:136 -- The `process_trial` operation took 0.8066537380218506 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    275 |          7093.26 | 44492800 |  302.621 |              322.949 |              165.677 |            788.713 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3042.0076254843334
    time_step_min: 2911
  date: 2020-10-16_03-02-45
  done: false
  episode_len_mean: 788.7412332667244
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.68674347142553
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 212
  episodes_total: 56549
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.374009296713951e-41
        cur_lr: 5.0e-05
        entropy: 0.07786776684224606
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00930876580241602
        total_loss: .inf
        vf_explained_var: 0.9993565678596497
        vf_loss: 0.3153404692808787
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.303225806451607
    gpu_util_percent0: 0.36258064516129035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681887919633027
    mean_env_wait_ms: 1.1947199616760555
    mean_inference_ms: 4.320286540881765
    mean_raw_obs_processing_ms: 0.3787364878318331
  time_since_restore: 7119.093159914017
  time_this_iter_s: 25.83017659187317
  time_total_s: 7119.093159914017
  timers:
    learn_throughput: 8705.464
    learn_time_ms: 18585.108
    sample_throughput: 23736.149
    sample_time_ms: 6816.27
    update_time_ms: 33.646
  timestamp: 1602817365
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:02:47,021	WARNING util.py:136 -- The `process_trial` operation took 0.8289861679077148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    276 |          7119.09 | 44654592 |  302.687 |              322.949 |              165.677 |            788.741 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3041.655391344221
    time_step_min: 2911
  date: 2020-10-16_03-03-12
  done: false
  episode_len_mean: 788.7433632998413
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.74087250078765
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 56730
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.061013945070927e-41
        cur_lr: 5.0e-05
        entropy: 0.07293015656371911
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008188514208692746
        total_loss: .inf
        vf_explained_var: 0.9992137551307678
        vf_loss: 0.34603284299373627
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.232258064516127
    gpu_util_percent0: 0.2861290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681712307510122
    mean_env_wait_ms: 1.194645167236328
    mean_inference_ms: 4.320186771793513
    mean_raw_obs_processing_ms: 0.37872918759599944
  time_since_restore: 7144.929395437241
  time_this_iter_s: 25.836235523223877
  time_total_s: 7144.929395437241
  timers:
    learn_throughput: 8704.346
    learn_time_ms: 18587.495
    sample_throughput: 23769.064
    sample_time_ms: 6806.831
    update_time_ms: 33.922
  timestamp: 1602817392
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:03:14,089	WARNING util.py:136 -- The `process_trial` operation took 0.7981283664703369 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    277 |          7144.93 | 44816384 |  302.741 |              322.949 |              165.677 |            788.743 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3041.239922684941
    time_step_min: 2911
  date: 2020-10-16_03-03-40
  done: false
  episode_len_mean: 788.7422810776634
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.80218692598805
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 208
  episodes_total: 56938
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2091520917606387e-40
        cur_lr: 5.0e-05
        entropy: 0.0793671514838934
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009108486236073077
        total_loss: .inf
        vf_explained_var: 0.9991297125816345
        vf_loss: 0.4368865465124448
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50967741935484
    gpu_util_percent0: 0.34483870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681509861359232
    mean_env_wait_ms: 1.1945555734613225
    mean_inference_ms: 4.32007153905233
    mean_raw_obs_processing_ms: 0.37872105848375537
  time_since_restore: 7170.846631288528
  time_this_iter_s: 25.917235851287842
  time_total_s: 7170.846631288528
  timers:
    learn_throughput: 8698.372
    learn_time_ms: 18600.262
    sample_throughput: 23790.042
    sample_time_ms: 6800.829
    update_time_ms: 34.289
  timestamp: 1602817420
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:03:41,193	WARNING util.py:136 -- The `process_trial` operation took 0.7497737407684326 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    278 |          7170.85 | 44978176 |  302.802 |              322.949 |              165.677 |            788.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3040.823225693533
    time_step_min: 2911
  date: 2020-10-16_03-04-07
  done: false
  episode_len_mean: 788.7496282560397
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.86814067233195
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 225
  episodes_total: 57163
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.813728137640958e-40
        cur_lr: 5.0e-05
        entropy: 0.07419742581744988
        entropy_coeff: 0.0005000000000000001
        kl: 0.005195180730273326
        model: {}
        policy_loss: -0.00803787245725592
        total_loss: 0.39745864272117615
        vf_explained_var: 0.9992191195487976
        vf_loss: 0.4055336192250252
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34
    gpu_util_percent0: 0.3116666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681244821895822
    mean_env_wait_ms: 1.1944624170136477
    mean_inference_ms: 4.319956502208349
    mean_raw_obs_processing_ms: 0.37871309673129216
  time_since_restore: 7196.766115427017
  time_this_iter_s: 25.91948413848877
  time_total_s: 7196.766115427017
  timers:
    learn_throughput: 8671.265
    learn_time_ms: 18658.408
    sample_throughput: 23787.005
    sample_time_ms: 6801.697
    update_time_ms: 34.783
  timestamp: 1602817447
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: 1bbc1_00000
  
2020-10-16 03:04:08,195	WARNING util.py:136 -- The `process_trial` operation took 0.8149890899658203 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    279 |          7196.77 | 45139968 |  302.868 |              322.949 |              165.677 |             788.75 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3040.468487798071
    time_step_min: 2911
  date: 2020-10-16_03-04-34
  done: false
  episode_len_mean: 788.7443466131984
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.920629008241
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 192
  episodes_total: 57355
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.813728137640958e-40
        cur_lr: 5.0e-05
        entropy: 0.07772817773123582
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0067503125659034895
        total_loss: .inf
        vf_explained_var: 0.9983887672424316
        vf_loss: 0.7382173389196396
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03870967741935
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681095768293898
    mean_env_wait_ms: 1.1943844854133536
    mean_inference_ms: 4.319852960632927
    mean_raw_obs_processing_ms: 0.37870570785244745
  time_since_restore: 7222.829678297043
  time_this_iter_s: 26.063562870025635
  time_total_s: 7222.829678297043
  timers:
    learn_throughput: 8630.002
    learn_time_ms: 18747.62
    sample_throughput: 23853.64
    sample_time_ms: 6782.697
    update_time_ms: 36.96
  timestamp: 1602817474
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:04:35,266	WARNING util.py:136 -- The `process_trial` operation took 0.7336668968200684 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    280 |          7222.83 | 45301760 |  302.921 |              322.949 |              165.677 |            788.744 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3040.0764804061337
    time_step_min: 2911
  date: 2020-10-16_03-05-01
  done: false
  episode_len_mean: 788.7491745733847
  episode_reward_max: 322.949494949495
  episode_reward_mean: 302.978183461136
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 191
  episodes_total: 57546
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.720592206461437e-40
        cur_lr: 5.0e-05
        entropy: 0.0737056868771712
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010366778005845845
        total_loss: .inf
        vf_explained_var: 0.9993963241577148
        vf_loss: 0.2885508139928182
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.161290322580644
    gpu_util_percent0: 0.2919354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680935737417422
    mean_env_wait_ms: 1.1943059412607047
    mean_inference_ms: 4.319751599003161
    mean_raw_obs_processing_ms: 0.378699266722143
  time_since_restore: 7248.922433853149
  time_this_iter_s: 26.092755556106567
  time_total_s: 7248.922433853149
  timers:
    learn_throughput: 8618.284
    learn_time_ms: 18773.11
    sample_throughput: 23876.152
    sample_time_ms: 6776.301
    update_time_ms: 35.202
  timestamp: 1602817501
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:05:02,392	WARNING util.py:136 -- The `process_trial` operation took 0.748157262802124 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    281 |          7248.92 | 45463552 |  302.978 |              322.949 |              165.677 |            788.749 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3039.609648819007
    time_step_min: 2911
  date: 2020-10-16_03-05-28
  done: false
  episode_len_mean: 788.7663735807256
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.04787122820534
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 230
  episodes_total: 57776
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.080888309692155e-40
        cur_lr: 5.0e-05
        entropy: 0.07661349388460319
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005921488375558208
        total_loss: .inf
        vf_explained_var: 0.9995078444480896
        vf_loss: 0.2535695359110832
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65666666666666
    gpu_util_percent0: 0.32399999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680657156517457
    mean_env_wait_ms: 1.1942093663565942
    mean_inference_ms: 4.3196318725897065
    mean_raw_obs_processing_ms: 0.37869049301446867
  time_since_restore: 7274.903417110443
  time_this_iter_s: 25.9809832572937
  time_total_s: 7274.903417110443
  timers:
    learn_throughput: 8606.695
    learn_time_ms: 18798.388
    sample_throughput: 23873.679
    sample_time_ms: 6777.003
    update_time_ms: 36.707
  timestamp: 1602817528
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:05:29,520	WARNING util.py:136 -- The `process_trial` operation took 0.7732939720153809 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    282 |           7274.9 | 45625344 |  303.048 |              322.949 |              165.677 |            788.766 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3039.215137448446
    time_step_min: 2911
  date: 2020-10-16_03-05-55
  done: false
  episode_len_mean: 788.7866222812495
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.10641192963465
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 201
  episodes_total: 57977
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.121332464538233e-40
        cur_lr: 5.0e-05
        entropy: 0.0781080350279808
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011128519276098814
        total_loss: .inf
        vf_explained_var: 0.9992268085479736
        vf_loss: 0.37612733741601306
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.948387096774194
    gpu_util_percent0: 0.32258064516129026
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146804829476495
    mean_env_wait_ms: 1.1941270721987307
    mean_inference_ms: 4.319523499555657
    mean_raw_obs_processing_ms: 0.37868268508361486
  time_since_restore: 7300.751802206039
  time_this_iter_s: 25.848385095596313
  time_total_s: 7300.751802206039
  timers:
    learn_throughput: 8607.716
    learn_time_ms: 18796.16
    sample_throughput: 23865.388
    sample_time_ms: 6779.358
    update_time_ms: 36.96
  timestamp: 1602817555
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:05:56,409	WARNING util.py:136 -- The `process_trial` operation took 0.7630681991577148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    283 |          7300.75 | 45787136 |  303.106 |              322.949 |              165.677 |            788.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3038.917771928919
    time_step_min: 2911
  date: 2020-10-16_03-06-22
  done: false
  episode_len_mean: 788.7980020289207
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.15141268077167
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 182
  episodes_total: 58159
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.181998696807349e-40
        cur_lr: 5.0e-05
        entropy: 0.08001125790178776
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006570836428788122
        total_loss: .inf
        vf_explained_var: 0.9983363747596741
        vf_loss: 0.7670094867547353
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.519354838709674
    gpu_util_percent0: 0.3580645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468033296905983
    mean_env_wait_ms: 1.1940545036931647
    mean_inference_ms: 4.3194335560742925
    mean_raw_obs_processing_ms: 0.37867658808360105
  time_since_restore: 7326.78315114975
  time_this_iter_s: 26.031348943710327
  time_total_s: 7326.78315114975
  timers:
    learn_throughput: 8612.02
    learn_time_ms: 18786.765
    sample_throughput: 23889.216
    sample_time_ms: 6772.596
    update_time_ms: 43.05
  timestamp: 1602817582
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:06:23,510	WARNING util.py:136 -- The `process_trial` operation took 0.7756221294403076 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    284 |          7326.78 | 45948928 |  303.151 |              322.949 |              165.677 |            788.798 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3038.5156049154198
    time_step_min: 2911
  date: 2020-10-16_03-06-49
  done: false
  episode_len_mean: 788.813807280514
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.2074431682995
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 58375
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.377299804521102e-39
        cur_lr: 5.0e-05
        entropy: 0.07954935356974602
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00733436439865424
        total_loss: .inf
        vf_explained_var: 0.999143123626709
        vf_loss: 0.4679352218906085
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.2741935483871
    gpu_util_percent0: 0.32967741935483874
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146801025675967
    mean_env_wait_ms: 1.193964114118034
    mean_inference_ms: 4.319318522995836
    mean_raw_obs_processing_ms: 0.3786687571344765
  time_since_restore: 7352.803263187408
  time_this_iter_s: 26.02011203765869
  time_total_s: 7352.803263187408
  timers:
    learn_throughput: 8602.42
    learn_time_ms: 18807.73
    sample_throughput: 23891.36
    sample_time_ms: 6771.988
    update_time_ms: 43.824
  timestamp: 1602817609
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:06:50,690	WARNING util.py:136 -- The `process_trial` operation took 0.8748259544372559 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    285 |           7352.8 | 46110720 |  303.207 |              322.949 |              165.677 |            788.814 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3038.119915992214
    time_step_min: 2911
  date: 2020-10-16_03-07-16
  done: false
  episode_len_mean: 788.8332423114995
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.2690539900832
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 219
  episodes_total: 58594
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.065949706781654e-39
        cur_lr: 5.0e-05
        entropy: 0.07367100194096565
        entropy_coeff: 0.0005000000000000001
        kl: 0.006090904663627346
        model: {}
        policy_loss: -0.008847087471319052
        total_loss: 0.2977414031823476
        vf_explained_var: 0.9993991851806641
        vf_loss: 0.3066253239909808
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.323333333333334
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679875045468901
    mean_env_wait_ms: 1.193876316388681
    mean_inference_ms: 4.3192130066827215
    mean_raw_obs_processing_ms: 0.3786614658758326
  time_since_restore: 7378.812618017197
  time_this_iter_s: 26.009354829788208
  time_total_s: 7378.812618017197
  timers:
    learn_throughput: 8599.755
    learn_time_ms: 18813.559
    sample_throughput: 23847.572
    sample_time_ms: 6784.422
    update_time_ms: 42.655
  timestamp: 1602817636
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: 1bbc1_00000
  
2020-10-16 03:07:17,722	WARNING util.py:136 -- The `process_trial` operation took 0.7364997863769531 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    286 |          7378.81 | 46272512 |  303.269 |              322.949 |              165.677 |            788.833 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3037.7567231753815
    time_step_min: 2911
  date: 2020-10-16_03-07-43
  done: false
  episode_len_mean: 788.8518543722355
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.3227485814247
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 186
  episodes_total: 58780
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.065949706781654e-39
        cur_lr: 5.0e-05
        entropy: 0.07078137248754501
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007273757301542598
        total_loss: .inf
        vf_explained_var: 0.999445915222168
        vf_loss: 0.2665189243853092
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.261290322580646
    gpu_util_percent0: 0.27935483870967737
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679718629274974
    mean_env_wait_ms: 1.193802094947924
    mean_inference_ms: 4.319115134256167
    mean_raw_obs_processing_ms: 0.3786544509018161
  time_since_restore: 7404.778244256973
  time_this_iter_s: 25.96562623977661
  time_total_s: 7404.778244256973
  timers:
    learn_throughput: 8595.977
    learn_time_ms: 18821.828
    sample_throughput: 23839.681
    sample_time_ms: 6786.668
    update_time_ms: 42.635
  timestamp: 1602817663
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:07:44,921	WARNING util.py:136 -- The `process_trial` operation took 0.7690410614013672 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    287 |          7404.78 | 46434304 |  303.323 |              322.949 |              165.677 |            788.852 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3037.3800193394068
    time_step_min: 2911
  date: 2020-10-16_03-08-11
  done: false
  episode_len_mean: 788.8741839762612
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.3796618152698
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 195
  episodes_total: 58975
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0989245601724803e-39
        cur_lr: 5.0e-05
        entropy: 0.07216091826558113
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00893531973027469
        total_loss: .inf
        vf_explained_var: 0.9995319247245789
        vf_loss: 0.22268063202500343
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.409677419354836
    gpu_util_percent0: 0.28645161290322585
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679564296253458
    mean_env_wait_ms: 1.1937226408506023
    mean_inference_ms: 4.319015951192885
    mean_raw_obs_processing_ms: 0.37864817597338246
  time_since_restore: 7430.857949495316
  time_this_iter_s: 26.079705238342285
  time_total_s: 7430.857949495316
  timers:
    learn_throughput: 8600.887
    learn_time_ms: 18811.083
    sample_throughput: 23789.443
    sample_time_ms: 6801.0
    update_time_ms: 42.935
  timestamp: 1602817691
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:08:12,165	WARNING util.py:136 -- The `process_trial` operation took 0.8080101013183594 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    288 |          7430.86 | 46596096 |   303.38 |              322.949 |              165.677 |            788.874 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3036.9400929446556
    time_step_min: 2911
  date: 2020-10-16_03-08-38
  done: false
  episode_len_mean: 788.9038562235022
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.44628079009755
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 228
  episodes_total: 59203
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6483868402587205e-39
        cur_lr: 5.0e-05
        entropy: 0.07875484166045983
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007055494463808524
        total_loss: .inf
        vf_explained_var: 0.9994599223136902
        vf_loss: 0.28896468381086987
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.330000000000005
    gpu_util_percent0: 0.3103333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679303175083455
    mean_env_wait_ms: 1.1936287773672998
    mean_inference_ms: 4.318902851392742
    mean_raw_obs_processing_ms: 0.37864002692441817
  time_since_restore: 7456.921885728836
  time_this_iter_s: 26.063936233520508
  time_total_s: 7456.921885728836
  timers:
    learn_throughput: 8604.376
    learn_time_ms: 18803.456
    sample_throughput: 23720.071
    sample_time_ms: 6820.89
    update_time_ms: 44.471
  timestamp: 1602817718
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:08:39,464	WARNING util.py:136 -- The `process_trial` operation took 0.8597085475921631 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    289 |          7456.92 | 46757888 |  303.446 |              322.949 |              165.677 |            788.904 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3036.562302095264
    time_step_min: 2911
  date: 2020-10-16_03-09-05
  done: false
  episode_len_mean: 788.928569023569
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.5009769411282
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 197
  episodes_total: 59400
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.972580260388082e-39
        cur_lr: 5.0e-05
        entropy: 0.07419098665316899
        entropy_coeff: 0.0005000000000000001
        kl: 0.005618499514336388
        model: {}
        policy_loss: -0.007315463528357213
        total_loss: 0.3867812156677246
        vf_explained_var: 0.9992071986198425
        vf_loss: 0.3941337739427884
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15625
    gpu_util_percent0: 0.309375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679135739295943
    mean_env_wait_ms: 1.1935506146114632
    mean_inference_ms: 4.318804030080838
    mean_raw_obs_processing_ms: 0.37863279784181814
  time_since_restore: 7483.224020957947
  time_this_iter_s: 26.302135229110718
  time_total_s: 7483.224020957947
  timers:
    learn_throughput: 8610.637
    learn_time_ms: 18789.784
    sample_throughput: 23627.86
    sample_time_ms: 6847.51
    update_time_ms: 44.56
  timestamp: 1602817745
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: 1bbc1_00000
  
2020-10-16 03:09:06,891	WARNING util.py:136 -- The `process_trial` operation took 0.7936668395996094 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    290 |          7483.22 | 46919680 |  303.501 |              322.949 |              165.677 |            788.929 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3036.2019175873997
    time_step_min: 2911
  date: 2020-10-16_03-09-32
  done: false
  episode_len_mean: 788.9512940149709
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.5541391220789
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 182
  episodes_total: 59582
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.972580260388082e-39
        cur_lr: 5.0e-05
        entropy: 0.07239740652342637
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0067343200207687914
        total_loss: .inf
        vf_explained_var: 0.9993929266929626
        vf_loss: 0.2969464523096879
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.353333333333335
    gpu_util_percent0: 0.2856666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467900440807342
    mean_env_wait_ms: 1.1934792532783443
    mean_inference_ms: 4.3187183781840295
    mean_raw_obs_processing_ms: 0.37862724547295423
  time_since_restore: 7509.16455245018
  time_this_iter_s: 25.940531492233276
  time_total_s: 7509.16455245018
  timers:
    learn_throughput: 8621.222
    learn_time_ms: 18766.714
    sample_throughput: 23574.927
    sample_time_ms: 6862.884
    update_time_ms: 46.469
  timestamp: 1602817772
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:09:34,038	WARNING util.py:136 -- The `process_trial` operation took 0.8338267803192139 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    291 |          7509.16 | 47081472 |  303.554 |              322.949 |              165.677 |            788.951 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3035.811159444537
    time_step_min: 2911
  date: 2020-10-16_03-10-00
  done: false
  episode_len_mean: 788.973828556139
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.6122058742545
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 59798
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0458870390582121e-38
        cur_lr: 5.0e-05
        entropy: 0.07507234066724777
        entropy_coeff: 0.0005000000000000001
        kl: 0.004126690056485434
        model: {}
        policy_loss: -0.006670759437838569
        total_loss: 0.3577288016676903
        vf_explained_var: 0.9992976188659668
        vf_loss: 0.3644370809197426
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.261290322580653
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678786067120805
    mean_env_wait_ms: 1.1933901877238469
    mean_inference_ms: 4.318609755695025
    mean_raw_obs_processing_ms: 0.37861987441541284
  time_since_restore: 7535.214693546295
  time_this_iter_s: 26.050141096115112
  time_total_s: 7535.214693546295
  timers:
    learn_throughput: 8621.034
    learn_time_ms: 18767.122
    sample_throughput: 23562.204
    sample_time_ms: 6866.59
    update_time_ms: 46.492
  timestamp: 1602817800
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: 1bbc1_00000
  
2020-10-16 03:10:01,268	WARNING util.py:136 -- The `process_trial` operation took 0.7707836627960205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    292 |          7535.21 | 47243264 |  303.612 |              322.949 |              165.677 |            788.974 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3035.390448408068
    time_step_min: 2911
  date: 2020-10-16_03-10-27
  done: false
  episode_len_mean: 789.0041654170416
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.6751989554647
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 220
  episodes_total: 60018
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.2294351952910606e-39
        cur_lr: 5.0e-05
        entropy: 0.06974237846831481
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0071426133217755705
        total_loss: .inf
        vf_explained_var: 0.999426543712616
        vf_loss: 0.2872786795099576
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10967741935484
    gpu_util_percent0: 0.29193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678548492473914
    mean_env_wait_ms: 1.1933027758531134
    mean_inference_ms: 4.318505104431048
    mean_raw_obs_processing_ms: 0.3786126229339614
  time_since_restore: 7561.286880016327
  time_this_iter_s: 26.07218647003174
  time_total_s: 7561.286880016327
  timers:
    learn_throughput: 8610.398
    learn_time_ms: 18790.305
    sample_throughput: 23570.416
    sample_time_ms: 6864.198
    update_time_ms: 46.481
  timestamp: 1602817827
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:10:28,521	WARNING util.py:136 -- The `process_trial` operation took 0.8187487125396729 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    293 |          7561.29 | 47405056 |  303.675 |              322.949 |              165.677 |            789.004 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3035.0338185927476
    time_step_min: 2911
  date: 2020-10-16_03-10-54
  done: false
  episode_len_mean: 789.0292847413707
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.7278718885474
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 184
  episodes_total: 60202
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.844152792936591e-39
        cur_lr: 5.0e-05
        entropy: 0.06936217534045379
        entropy_coeff: 0.0005000000000000001
        kl: 0.006162379713108142
        model: {}
        policy_loss: -0.008768625363397101
        total_loss: 0.16852931678295135
        vf_explained_var: 0.9996006488800049
        vf_loss: 0.17733262479305267
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12258064516129
    gpu_util_percent0: 0.33032258064516123
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678420441811801
    mean_env_wait_ms: 1.1932305781389159
    mean_inference_ms: 4.318415674150694
    mean_raw_obs_processing_ms: 0.37860620139633067
  time_since_restore: 7587.036273956299
  time_this_iter_s: 25.749393939971924
  time_total_s: 7587.036273956299
  timers:
    learn_throughput: 8624.484
    learn_time_ms: 18759.616
    sample_throughput: 23564.814
    sample_time_ms: 6865.83
    update_time_ms: 38.214
  timestamp: 1602817854
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: 1bbc1_00000
  
2020-10-16 03:10:55,378	WARNING util.py:136 -- The `process_trial` operation took 0.8049771785736084 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    294 |          7587.04 | 47566848 |  303.728 |              322.949 |              165.677 |            789.029 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3034.72887195627
    time_step_min: 2911
  date: 2020-10-16_03-11-21
  done: false
  episode_len_mean: 789.0460114573331
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.7699365254249
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 60398
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.844152792936591e-39
        cur_lr: 5.0e-05
        entropy: 0.08370465723176797
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00789730999773989
        total_loss: .inf
        vf_explained_var: 0.998387336730957
        vf_loss: 0.7997382978598276
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.112903225806456
    gpu_util_percent0: 0.32806451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467828287524426
    mean_env_wait_ms: 1.1931524932397348
    mean_inference_ms: 4.318324109222512
    mean_raw_obs_processing_ms: 0.37860053095122664
  time_since_restore: 7613.042236328125
  time_this_iter_s: 26.005962371826172
  time_total_s: 7613.042236328125
  timers:
    learn_throughput: 8623.78
    learn_time_ms: 18761.146
    sample_throughput: 23538.788
    sample_time_ms: 6873.421
    update_time_ms: 37.895
  timestamp: 1602817881
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:11:22,630	WARNING util.py:136 -- The `process_trial` operation took 0.775935173034668 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    295 |          7613.04 | 47728640 |   303.77 |              322.949 |              165.677 |            789.046 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3034.4872027591214
    time_step_min: 2911
  date: 2020-10-16_03-11-48
  done: false
  episode_len_mean: 789.0530126841177
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.8041271740611
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 229
  episodes_total: 60627
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1766229189404887e-38
        cur_lr: 5.0e-05
        entropy: 0.0944938895603021
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010477894498762907
        total_loss: .inf
        vf_explained_var: 0.9959307312965393
        vf_loss: 2.242187956968943
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.803225806451614
    gpu_util_percent0: 0.2887096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678028445928334
    mean_env_wait_ms: 1.1930603881446116
    mean_inference_ms: 4.3182154217996755
    mean_raw_obs_processing_ms: 0.3785927265477635
  time_since_restore: 7638.917591094971
  time_this_iter_s: 25.875354766845703
  time_total_s: 7638.917591094971
  timers:
    learn_throughput: 8625.353
    learn_time_ms: 18757.724
    sample_throughput: 23572.076
    sample_time_ms: 6863.714
    update_time_ms: 36.618
  timestamp: 1602817908
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:11:49,808	WARNING util.py:136 -- The `process_trial` operation took 0.8360676765441895 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    296 |          7638.92 | 47890432 |  303.804 |              322.949 |              165.677 |            789.053 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3034.263931837621
    time_step_min: 2911
  date: 2020-10-16_03-12-15
  done: false
  episode_len_mean: 789.053958963567
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.8379849394907
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 197
  episodes_total: 60824
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7649343784107332e-38
        cur_lr: 5.0e-05
        entropy: 0.08894698632260163
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008714941194436202
        total_loss: .inf
        vf_explained_var: 0.9965489506721497
        vf_loss: 1.7558635572592418
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.786666666666665
    gpu_util_percent0: 0.333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467787872545315
    mean_env_wait_ms: 1.1929839088316774
    mean_inference_ms: 4.31812134924098
    mean_raw_obs_processing_ms: 0.3785858434100463
  time_since_restore: 7664.657253742218
  time_this_iter_s: 25.739662647247314
  time_total_s: 7664.657253742218
  timers:
    learn_throughput: 8633.084
    learn_time_ms: 18740.928
    sample_throughput: 23584.19
    sample_time_ms: 6860.189
    update_time_ms: 34.477
  timestamp: 1602817935
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:12:16,757	WARNING util.py:136 -- The `process_trial` operation took 0.8371760845184326 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    297 |          7664.66 | 48052224 |  303.838 |              322.949 |              165.677 |            789.054 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3034.019989832899
    time_step_min: 2911
  date: 2020-10-16_03-12-42
  done: false
  episode_len_mean: 789.058270091298
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.8793950089494
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 61009
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6474015676160997e-38
        cur_lr: 5.0e-05
        entropy: 0.08208241934577624
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009467573079746217
        total_loss: .inf
        vf_explained_var: 0.9980373978614807
        vf_loss: 0.9247655471165975
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95806451612903
    gpu_util_percent0: 0.29709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677739125684178
    mean_env_wait_ms: 1.1929125069441233
    mean_inference_ms: 4.3180407018878375
    mean_raw_obs_processing_ms: 0.37858066619193653
  time_since_restore: 7690.550387144089
  time_this_iter_s: 25.893133401870728
  time_total_s: 7690.550387144089
  timers:
    learn_throughput: 8627.298
    learn_time_ms: 18753.495
    sample_throughput: 23658.331
    sample_time_ms: 6838.69
    update_time_ms: 33.836
  timestamp: 1602817962
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:12:43,733	WARNING util.py:136 -- The `process_trial` operation took 0.7892441749572754 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    298 |          7690.55 | 48214016 |  303.879 |              322.949 |              165.677 |            789.058 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3033.6550653594772
    time_step_min: 2911
  date: 2020-10-16_03-13-09
  done: false
  episode_len_mean: 789.077366564317
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.9361771170909
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 219
  episodes_total: 61228
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.971102351424149e-38
        cur_lr: 5.0e-05
        entropy: 0.07429660360018413
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048604126010711
        model: {}
        policy_loss: -0.008703251745828311
        total_loss: 0.4318993041912715
        vf_explained_var: 0.9991343021392822
        vf_loss: 0.4406396994988124
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.299999999999997
    gpu_util_percent0: 0.33290322580645165
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677523117847843
    mean_env_wait_ms: 1.1928249451626345
    mean_inference_ms: 4.317932359948612
    mean_raw_obs_processing_ms: 0.37857344459915954
  time_since_restore: 7716.610521554947
  time_this_iter_s: 26.060134410858154
  time_total_s: 7716.610521554947
  timers:
    learn_throughput: 8621.848
    learn_time_ms: 18765.35
    sample_throughput: 23734.941
    sample_time_ms: 6816.617
    update_time_ms: 33.9
  timestamp: 1602817989
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: 1bbc1_00000
  
2020-10-16 03:13:10,892	WARNING util.py:136 -- The `process_trial` operation took 0.7971374988555908 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    299 |          7716.61 | 48375808 |  303.936 |              322.949 |              165.677 |            789.077 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3033.291866807783
    time_step_min: 2911
  date: 2020-10-16_03-13-36
  done: false
  episode_len_mean: 789.1011994857022
  episode_reward_max: 322.949494949495
  episode_reward_mean: 303.99193931404255
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 61443
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9855511757120745e-38
        cur_lr: 5.0e-05
        entropy: 0.07214841557045777
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008605117317832386
        total_loss: .inf
        vf_explained_var: 0.9988288283348083
        vf_loss: 0.5847675427794456
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.2483870967742
    gpu_util_percent0: 0.31870967741935485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677319865712202
    mean_env_wait_ms: 1.1927408532533728
    mean_inference_ms: 4.317836866436063
    mean_raw_obs_processing_ms: 0.37856645510352416
  time_since_restore: 7742.577105283737
  time_this_iter_s: 25.966583728790283
  time_total_s: 7742.577105283737
  timers:
    learn_throughput: 8628.661
    learn_time_ms: 18750.534
    sample_throughput: 23786.5
    sample_time_ms: 6801.841
    update_time_ms: 31.711
  timestamp: 1602818016
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:13:38,042	WARNING util.py:136 -- The `process_trial` operation took 0.8498153686523438 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    300 |          7742.58 | 48537600 |  303.992 |              322.949 |              165.677 |            789.101 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3032.9621097745094
    time_step_min: 2911
  date: 2020-10-16_03-14-04
  done: false
  episode_len_mean: 789.1240689957324
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.04173315087337
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 184
  episodes_total: 61627
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9783267635681117e-38
        cur_lr: 5.0e-05
        entropy: 0.06902976396183173
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007211523373068
        total_loss: .inf
        vf_explained_var: 0.9994964003562927
        vf_loss: 0.2508523998161157
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280000000000005
    gpu_util_percent0: 0.3269999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467718745996021
    mean_env_wait_ms: 1.1926704844001053
    mean_inference_ms: 4.317751753963235
    mean_raw_obs_processing_ms: 0.37856047287821654
  time_since_restore: 7768.824683904648
  time_this_iter_s: 26.247578620910645
  time_total_s: 7768.824683904648
  timers:
    learn_throughput: 8620.002
    learn_time_ms: 18769.369
    sample_throughput: 23749.295
    sample_time_ms: 6812.497
    update_time_ms: 31.715
  timestamp: 1602818044
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:14:05,478	WARNING util.py:136 -- The `process_trial` operation took 0.7982876300811768 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    301 |          7768.82 | 48699392 |  304.042 |              322.949 |              165.677 |            789.124 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3032.633721682848
    time_step_min: 2911
  date: 2020-10-16_03-14-31
  done: false
  episode_len_mean: 789.1452901597982
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.0922791347515
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 201
  episodes_total: 61828
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.4674901453521686e-38
        cur_lr: 5.0e-05
        entropy: 0.07459521479904652
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00970433031761786
        total_loss: .inf
        vf_explained_var: 0.9990701675415039
        vf_loss: 0.45522452394167584
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.138709677419357
    gpu_util_percent0: 0.3170967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677049016608473
    mean_env_wait_ms: 1.1925918090040264
    mean_inference_ms: 4.317661170239659
    mean_raw_obs_processing_ms: 0.37855512591309537
  time_since_restore: 7794.706109046936
  time_this_iter_s: 25.881425142288208
  time_total_s: 7794.706109046936
  timers:
    learn_throughput: 8625.125
    learn_time_ms: 18758.221
    sample_throughput: 23762.078
    sample_time_ms: 6808.832
    update_time_ms: 30.024
  timestamp: 1602818071
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:14:32,479	WARNING util.py:136 -- The `process_trial` operation took 0.8103194236755371 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    302 |          7794.71 | 48861184 |  304.092 |              322.949 |              165.677 |            789.145 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3032.259935189514
    time_step_min: 2911
  date: 2020-10-16_03-14-58
  done: false
  episode_len_mean: 789.1695431472082
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.14960335772497
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 227
  episodes_total: 62055
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.701235218028252e-38
        cur_lr: 5.0e-05
        entropy: 0.07059666141867638
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00719802650079752
        total_loss: .inf
        vf_explained_var: 0.9991393685340881
        vf_loss: 0.44795603553454083
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.841935483870973
    gpu_util_percent0: 0.2738709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676806532834977
    mean_env_wait_ms: 1.192502429688952
    mean_inference_ms: 4.31756075022176
    mean_raw_obs_processing_ms: 0.3785475260709639
  time_since_restore: 7820.511764287949
  time_this_iter_s: 25.805655241012573
  time_total_s: 7820.511764287949
  timers:
    learn_throughput: 8632.704
    learn_time_ms: 18741.752
    sample_throughput: 23793.544
    sample_time_ms: 6799.828
    update_time_ms: 27.931
  timestamp: 1602818098
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:14:59,605	WARNING util.py:136 -- The `process_trial` operation took 0.8513832092285156 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    303 |          7820.51 | 49022976 |   304.15 |              322.949 |              165.677 |             789.17 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3031.9366592198526
    time_step_min: 2911
  date: 2020-10-16_03-15-25
  done: false
  episode_len_mean: 789.1901296448021
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.2005811646756
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 192
  episodes_total: 62247
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0051852827042377e-37
        cur_lr: 5.0e-05
        entropy: 0.06838119278351466
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007772739641950466
        total_loss: .inf
        vf_explained_var: 0.999517023563385
        vf_loss: 0.22649715219934782
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61612903225807
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676675946878476
    mean_env_wait_ms: 1.192429939636271
    mean_inference_ms: 4.31747435672411
    mean_raw_obs_processing_ms: 0.37854122546544017
  time_since_restore: 7846.678894281387
  time_this_iter_s: 26.16712999343872
  time_total_s: 7846.678894281387
  timers:
    learn_throughput: 8622.634
    learn_time_ms: 18763.639
    sample_throughput: 23733.408
    sample_time_ms: 6817.057
    update_time_ms: 29.325
  timestamp: 1602818125
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:15:26,943	WARNING util.py:136 -- The `process_trial` operation took 0.7981052398681641 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    304 |          7846.68 | 49184768 |  304.201 |              322.949 |              165.677 |             789.19 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3031.617800711516
    time_step_min: 2911
  date: 2020-10-16_03-15-52
  done: false
  episode_len_mean: 789.2132628543969
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.24947213606487
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 183
  episodes_total: 62430
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.507777924056357e-37
        cur_lr: 5.0e-05
        entropy: 0.06438505152861278
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005659055207312728
        total_loss: .inf
        vf_explained_var: 0.9997274279594421
        vf_loss: 0.13964086398482323
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.70333333333334
    gpu_util_percent0: 0.3299999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676530550409697
    mean_env_wait_ms: 1.192360342730297
    mean_inference_ms: 4.31739770717577
    mean_raw_obs_processing_ms: 0.37853619290436247
  time_since_restore: 7872.5056364536285
  time_this_iter_s: 25.82674217224121
  time_total_s: 7872.5056364536285
  timers:
    learn_throughput: 8628.682
    learn_time_ms: 18750.489
    sample_throughput: 23757.019
    sample_time_ms: 6810.282
    update_time_ms: 29.415
  timestamp: 1602818152
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:15:53,970	WARNING util.py:136 -- The `process_trial` operation took 0.8241512775421143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    305 |          7872.51 | 49346560 |  304.249 |              322.949 |              165.677 |            789.213 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3031.2107624750497
    time_step_min: 2911
  date: 2020-10-16_03-16-20
  done: false
  episode_len_mean: 789.2412174995611
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.30761270148025
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 62653
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2616668860845354e-37
        cur_lr: 5.0e-05
        entropy: 0.06586103265484174
        entropy_coeff: 0.0005000000000000001
        kl: 0.004834717682873209
        model: {}
        policy_loss: -0.007024278806056827
        total_loss: 0.47105679909388226
        vf_explained_var: 0.9990742206573486
        vf_loss: 0.4781140014529228
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.151612903225804
    gpu_util_percent0: 0.34903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467633726001296
    mean_env_wait_ms: 1.1922727674441032
    mean_inference_ms: 4.3172947788297105
    mean_raw_obs_processing_ms: 0.37852922714719334
  time_since_restore: 7898.6063487529755
  time_this_iter_s: 26.100712299346924
  time_total_s: 7898.6063487529755
  timers:
    learn_throughput: 8617.026
    learn_time_ms: 18775.851
    sample_throughput: 23772.744
    sample_time_ms: 6805.777
    update_time_ms: 30.826
  timestamp: 1602818180
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: 1bbc1_00000
  
2020-10-16 03:16:21,246	WARNING util.py:136 -- The `process_trial` operation took 0.8582024574279785 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    306 |          7898.61 | 49508352 |  304.308 |              322.949 |              165.677 |            789.241 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3030.8549013367283
    time_step_min: 2911
  date: 2020-10-16_03-16-47
  done: false
  episode_len_mean: 789.2642838964179
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.36187943570053
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 62868
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1308334430422677e-37
        cur_lr: 5.0e-05
        entropy: 0.07002023793756962
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0080308741889894
        total_loss: .inf
        vf_explained_var: 0.999447762966156
        vf_loss: 0.2685306929051876
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10967741935484
    gpu_util_percent0: 0.30096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676151923146258
    mean_env_wait_ms: 1.1921900080146572
    mean_inference_ms: 4.317203233332451
    mean_raw_obs_processing_ms: 0.3785224281884079
  time_since_restore: 7924.607849597931
  time_this_iter_s: 26.001500844955444
  time_total_s: 7924.607849597931
  timers:
    learn_throughput: 8612.655
    learn_time_ms: 18785.38
    sample_throughput: 23726.98
    sample_time_ms: 6818.904
    update_time_ms: 33.309
  timestamp: 1602818207
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:16:48,372	WARNING util.py:136 -- The `process_trial` operation took 0.7883374691009521 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    307 |          7924.61 | 49670144 |  304.362 |              322.949 |              165.677 |            789.264 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3030.5789114566805
    time_step_min: 2911
  date: 2020-10-16_03-17-14
  done: false
  episode_len_mean: 789.2820708031976
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.40349776793414
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 63048
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6962501645634017e-37
        cur_lr: 5.0e-05
        entropy: 0.07764504911998908
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010991709442654004
        total_loss: .inf
        vf_explained_var: 0.9984790682792664
        vf_loss: 0.6651125599940618
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.148387096774197
    gpu_util_percent0: 0.2970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676012979711464
    mean_env_wait_ms: 1.1921229983324177
    mean_inference_ms: 4.317123963019299
    mean_raw_obs_processing_ms: 0.3785171386020297
  time_since_restore: 7950.724169969559
  time_this_iter_s: 26.116320371627808
  time_total_s: 7950.724169969559
  timers:
    learn_throughput: 8610.379
    learn_time_ms: 18790.345
    sample_throughput: 23697.406
    sample_time_ms: 6827.414
    update_time_ms: 41.383
  timestamp: 1602818234
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:17:15,645	WARNING util.py:136 -- The `process_trial` operation took 0.8477437496185303 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    308 |          7950.72 | 49831936 |  304.403 |              322.949 |              165.677 |            789.282 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3030.38384765038
    time_step_min: 2911
  date: 2020-10-16_03-17-41
  done: false
  episode_len_mean: 789.295947890152
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.4300823925966
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 203
  episodes_total: 63251
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.544375246845102e-37
        cur_lr: 5.0e-05
        entropy: 0.0795327207694451
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011208621164162954
        total_loss: .inf
        vf_explained_var: 0.9976356625556946
        vf_loss: 1.2042679985364277
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.087096774193547
    gpu_util_percent0: 0.29129032258064513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467586794656899
    mean_env_wait_ms: 1.1920444101131047
    mean_inference_ms: 4.317034285086845
    mean_raw_obs_processing_ms: 0.378511477755582
  time_since_restore: 7976.88875579834
  time_this_iter_s: 26.164585828781128
  time_total_s: 7976.88875579834
  timers:
    learn_throughput: 8602.118
    learn_time_ms: 18808.392
    sample_throughput: 23694.741
    sample_time_ms: 6828.182
    update_time_ms: 40.008
  timestamp: 1602818261
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:17:42,919	WARNING util.py:136 -- The `process_trial` operation took 0.8094744682312012 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    309 |          7976.89 | 49993728 |   304.43 |              322.949 |              165.677 |            789.296 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3030.036298012514
    time_step_min: 2911
  date: 2020-10-16_03-18-08
  done: false
  episode_len_mean: 789.3280189050807
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.4867835503516
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 224
  episodes_total: 63475
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.816562870267652e-37
        cur_lr: 5.0e-05
        entropy: 0.06667930694917838
        entropy_coeff: 0.0005000000000000001
        kl: 0.004410715152819951
        model: {}
        policy_loss: -0.006980563222896308
        total_loss: 0.2663399887581666
        vf_explained_var: 0.9994757175445557
        vf_loss: 0.27335389455159503
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81612903225807
    gpu_util_percent0: 0.33161290322580644
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675652318535826
    mean_env_wait_ms: 1.1919586296533342
    mean_inference_ms: 4.316939617918315
    mean_raw_obs_processing_ms: 0.3785045401704716
  time_since_restore: 8002.964421987534
  time_this_iter_s: 26.075666189193726
  time_total_s: 8002.964421987534
  timers:
    learn_throughput: 8597.214
    learn_time_ms: 18819.121
    sample_throughput: 23714.087
    sample_time_ms: 6822.611
    update_time_ms: 42.137
  timestamp: 1602818288
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: 1bbc1_00000
  
2020-10-16 03:18:10,183	WARNING util.py:136 -- The `process_trial` operation took 0.8799910545349121 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    310 |          8002.96 | 50155520 |  304.487 |              322.949 |              165.677 |            789.328 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3029.711306998523
    time_step_min: 2911
  date: 2020-10-16_03-18-36
  done: false
  episode_len_mean: 789.3573739594785
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.53720335124416
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 195
  episodes_total: 63670
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.908281435133826e-37
        cur_lr: 5.0e-05
        entropy: 0.06243871835370859
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006787496424900989
        total_loss: .inf
        vf_explained_var: 0.999649703502655
        vf_loss: 0.16470513492822647
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.712903225806457
    gpu_util_percent0: 0.3387096774193547
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675524342052515
    mean_env_wait_ms: 1.1918861493300377
    mean_inference_ms: 4.316860007778195
    mean_raw_obs_processing_ms: 0.37849864408505396
  time_since_restore: 8028.888676643372
  time_this_iter_s: 25.924254655838013
  time_total_s: 8028.888676643372
  timers:
    learn_throughput: 8600.554
    learn_time_ms: 18811.811
    sample_throughput: 23807.564
    sample_time_ms: 6795.823
    update_time_ms: 42.14
  timestamp: 1602818316
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:18:37,488	WARNING util.py:136 -- The `process_trial` operation took 0.888314962387085 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    311 |          8028.89 | 50317312 |  304.537 |              322.949 |              165.677 |            789.357 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3029.4021371270996
    time_step_min: 2911
  date: 2020-10-16_03-19-03
  done: false
  episode_len_mean: 789.3854695232726
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.58456867111227
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 182
  episodes_total: 63852
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.862422152700739e-37
        cur_lr: 5.0e-05
        entropy: 0.06490140253057082
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007539828424341977
        total_loss: .inf
        vf_explained_var: 0.9995686411857605
        vf_loss: 0.20050683245062828
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380000000000006
    gpu_util_percent0: 0.329
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675396596667287
    mean_env_wait_ms: 1.191818419501868
    mean_inference_ms: 4.316785493006283
    mean_raw_obs_processing_ms: 0.3784936774581367
  time_since_restore: 8054.7117393016815
  time_this_iter_s: 25.823062658309937
  time_total_s: 8054.7117393016815
  timers:
    learn_throughput: 8601.842
    learn_time_ms: 18808.994
    sample_throughput: 23825.052
    sample_time_ms: 6790.835
    update_time_ms: 43.683
  timestamp: 1602818343
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:19:04,579	WARNING util.py:136 -- The `process_trial` operation took 0.8824689388275146 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    312 |          8054.71 | 50479104 |  304.585 |              322.949 |              165.677 |            789.385 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3029.044925748372
    time_step_min: 2911
  date: 2020-10-16_03-19-30
  done: false
  episode_len_mean: 789.4164546490393
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.6395414964099
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 64067
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.293633229051108e-37
        cur_lr: 5.0e-05
        entropy: 0.06217388280977806
        entropy_coeff: 0.0005000000000000001
        kl: 0.00544295075815171
        model: {}
        policy_loss: -0.005977152516910185
        total_loss: 0.14804654568433762
        vf_explained_var: 0.9997224807739258
        vf_loss: 0.1540547894934813
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.335483870967746
    gpu_util_percent0: 0.36645161290322587
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467520937355738
    mean_env_wait_ms: 1.1917348849518685
    mean_inference_ms: 4.316688898355372
    mean_raw_obs_processing_ms: 0.3784876291337574
  time_since_restore: 8080.5017948150635
  time_this_iter_s: 25.790055513381958
  time_total_s: 8080.5017948150635
  timers:
    learn_throughput: 8608.758
    learn_time_ms: 18793.883
    sample_throughput: 23789.35
    sample_time_ms: 6801.026
    update_time_ms: 45.678
  timestamp: 1602818370
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: 1bbc1_00000
  
2020-10-16 03:19:31,549	WARNING util.py:136 -- The `process_trial` operation took 0.8636775016784668 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    313 |           8080.5 | 50640896 |   304.64 |              322.949 |              165.677 |            789.416 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3028.6673721230604
    time_step_min: 2911
  date: 2020-10-16_03-19-57
  done: false
  episode_len_mean: 789.4510880554994
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.6967706274583
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 64289
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.293633229051108e-37
        cur_lr: 5.0e-05
        entropy: 0.06104960944503546
        entropy_coeff: 0.0005000000000000001
        kl: 0.004024918579186003
        model: {}
        policy_loss: -0.008439965167781338
        total_loss: 0.13555196610589823
        vf_explained_var: 0.9997120499610901
        vf_loss: 0.14402245978514353
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19677419354839
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675011948507877
    mean_env_wait_ms: 1.1916513760917433
    mean_inference_ms: 4.31659971000133
    mean_raw_obs_processing_ms: 0.3784807079105478
  time_since_restore: 8106.379952430725
  time_this_iter_s: 25.87815761566162
  time_total_s: 8106.379952430725
  timers:
    learn_throughput: 8619.098
    learn_time_ms: 18771.338
    sample_throughput: 23812.263
    sample_time_ms: 6794.482
    update_time_ms: 44.953
  timestamp: 1602818397
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: 1bbc1_00000
  
2020-10-16 03:19:58,630	WARNING util.py:136 -- The `process_trial` operation took 0.8641817569732666 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    314 |          8106.38 | 50802688 |  304.697 |              322.949 |              165.677 |            789.451 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3028.364779093406
    time_step_min: 2911
  date: 2020-10-16_03-20-24
  done: false
  episode_len_mean: 789.4797183054897
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.74206347527553
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 178
  episodes_total: 64467
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.146816614525554e-37
        cur_lr: 5.0e-05
        entropy: 0.05967277164260546
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005539242420733596
        total_loss: .inf
        vf_explained_var: 0.9995567798614502
        vf_loss: 0.2302729475001494
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.838709677419356
    gpu_util_percent0: 0.3529032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674905796624768
    mean_env_wait_ms: 1.1915863765798702
    mean_inference_ms: 4.316528752935004
    mean_raw_obs_processing_ms: 0.3784758065964728
  time_since_restore: 8132.491097450256
  time_this_iter_s: 26.11114501953125
  time_total_s: 8132.491097450256
  timers:
    learn_throughput: 8611.21
    learn_time_ms: 18788.533
    sample_throughput: 23816.682
    sample_time_ms: 6793.222
    update_time_ms: 44.591
  timestamp: 1602818424
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:20:25,980	WARNING util.py:136 -- The `process_trial` operation took 0.9060928821563721 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    315 |          8132.49 | 50964480 |  304.742 |              322.949 |              165.677 |             789.48 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3028.0386638611258
    time_step_min: 2911
  date: 2020-10-16_03-20-52
  done: false
  episode_len_mean: 789.5133617889951
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.79180690640254
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 195
  episodes_total: 64662
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2202249217883313e-37
        cur_lr: 5.0e-05
        entropy: 0.06295585849632819
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005845788803261105
        total_loss: .inf
        vf_explained_var: 0.9996245503425598
        vf_loss: 0.18199136977394423
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.738709677419358
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674770571251916
    mean_env_wait_ms: 1.191512430920894
    mean_inference_ms: 4.316450605829573
    mean_raw_obs_processing_ms: 0.3784708798018896
  time_since_restore: 8158.568818330765
  time_this_iter_s: 26.077720880508423
  time_total_s: 8158.568818330765
  timers:
    learn_throughput: 8613.509
    learn_time_ms: 18783.519
    sample_throughput: 23838.341
    sample_time_ms: 6787.05
    update_time_ms: 44.667
  timestamp: 1602818452
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:20:53,293	WARNING util.py:136 -- The `process_trial` operation took 0.9199979305267334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    316 |          8158.57 | 51126272 |  304.792 |              322.949 |              165.677 |            789.513 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3027.694084272521
    time_step_min: 2911
  date: 2020-10-16_03-21-19
  done: false
  episode_len_mean: 789.5487678959454
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.8454482721151
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 227
  episodes_total: 64889
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.830337382682496e-37
        cur_lr: 5.0e-05
        entropy: 0.06852883907655875
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0074640233457709355
        total_loss: .inf
        vf_explained_var: 0.9988407492637634
        vf_loss: 0.6219240427017212
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.196666666666662
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674539529969663
    mean_env_wait_ms: 1.1914250223456886
    mean_inference_ms: 4.316349871471615
    mean_raw_obs_processing_ms: 0.3784636715679941
  time_since_restore: 8184.559055089951
  time_this_iter_s: 25.99023675918579
  time_total_s: 8184.559055089951
  timers:
    learn_throughput: 8614.353
    learn_time_ms: 18781.678
    sample_throughput: 23829.593
    sample_time_ms: 6789.541
    update_time_ms: 42.572
  timestamp: 1602818479
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:21:20,540	WARNING util.py:136 -- The `process_trial` operation took 0.8539798259735107 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    317 |          8184.56 | 51288064 |  304.845 |              322.949 |              165.677 |            789.549 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3027.384531439726
    time_step_min: 2911
  date: 2020-10-16_03-21-46
  done: false
  episode_len_mean: 789.5790532962559
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.89206123519114
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 200
  episodes_total: 65089
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.245506074023744e-37
        cur_lr: 5.0e-05
        entropy: 0.0663844080020984
        entropy_coeff: 0.0005000000000000001
        kl: 0.00604818695380042
        model: {}
        policy_loss: -0.007294051647477318
        total_loss: 0.37491970012585324
        vf_explained_var: 0.9991838932037354
        vf_loss: 0.3822469487786293
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.477419354838712
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674424475059475
    mean_env_wait_ms: 1.191352112405165
    mean_inference_ms: 4.316273985527346
    mean_raw_obs_processing_ms: 0.37845789296822563
  time_since_restore: 8210.56864643097
  time_this_iter_s: 26.009591341018677
  time_total_s: 8210.56864643097
  timers:
    learn_throughput: 8616.843
    learn_time_ms: 18776.251
    sample_throughput: 23821.974
    sample_time_ms: 6791.713
    update_time_ms: 35.129
  timestamp: 1602818506
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: 1bbc1_00000
  
2020-10-16 03:21:47,825	WARNING util.py:136 -- The `process_trial` operation took 0.8698146343231201 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    318 |          8210.57 | 51449856 |  304.892 |              322.949 |              165.677 |            789.579 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3027.1086893211323
    time_step_min: 2911
  date: 2020-10-16_03-22-13
  done: false
  episode_len_mean: 789.6085584274311
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.9345270567135
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 65269
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.245506074023744e-37
        cur_lr: 5.0e-05
        entropy: 0.06817881887157758
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008271971989112595
        total_loss: .inf
        vf_explained_var: 0.9992008209228516
        vf_loss: 0.35987160603205365
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.409677419354836
    gpu_util_percent0: 0.3232258064516128
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467429233166575
    mean_env_wait_ms: 1.1912856997121868
    mean_inference_ms: 4.31620251769605
    mean_raw_obs_processing_ms: 0.3784532496026788
  time_since_restore: 8236.454709768295
  time_this_iter_s: 25.88606333732605
  time_total_s: 8236.454709768295
  timers:
    learn_throughput: 8631.771
    learn_time_ms: 18743.778
    sample_throughput: 23809.498
    sample_time_ms: 6795.271
    update_time_ms: 35.153
  timestamp: 1602818533
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:22:14,904	WARNING util.py:136 -- The `process_trial` operation took 0.8789324760437012 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    319 |          8236.45 | 51611648 |  304.935 |              322.949 |              165.677 |            789.609 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3026.8563025210083
    time_step_min: 2911
  date: 2020-10-16_03-22-40
  done: false
  episode_len_mean: 789.6427960536363
  episode_reward_max: 322.949494949495
  episode_reward_mean: 304.97594241693
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 209
  episodes_total: 65478
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0868259111035618e-36
        cur_lr: 5.0e-05
        entropy: 0.07218212075531483
        entropy_coeff: 0.0005000000000000001
        kl: 0.005619241662013034
        model: {}
        policy_loss: -0.009443661304734027
        total_loss: 0.7454402099053065
        vf_explained_var: 0.9985501170158386
        vf_loss: 0.7549199511607488
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44193548387097
    gpu_util_percent0: 0.30709677419354836
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674149839371012
    mean_env_wait_ms: 1.1912067263226664
    mean_inference_ms: 4.316114809803748
    mean_raw_obs_processing_ms: 0.37844765969552646
  time_since_restore: 8262.282137155533
  time_this_iter_s: 25.82742738723755
  time_total_s: 8262.282137155533
  timers:
    learn_throughput: 8640.019
    learn_time_ms: 18725.884
    sample_throughput: 23803.747
    sample_time_ms: 6796.913
    update_time_ms: 35.812
  timestamp: 1602818560
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: 1bbc1_00000
  
2020-10-16 03:22:41,926	WARNING util.py:136 -- The `process_trial` operation took 0.837338924407959 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    320 |          8262.28 | 51773440 |  304.976 |              322.949 |              165.677 |            789.643 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3026.510703736411
    time_step_min: 2911
  date: 2020-10-16_03-23-07
  done: false
  episode_len_mean: 789.6792986941832
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.02793619696155
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 228
  episodes_total: 65706
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0868259111035618e-36
        cur_lr: 5.0e-05
        entropy: 0.06514509953558445
        entropy_coeff: 0.0005000000000000001
        kl: 0.004234529294384022
        model: {}
        policy_loss: -0.00870622971948857
        total_loss: 0.23066350941856703
        vf_explained_var: 0.9995427131652832
        vf_loss: 0.2394023115436236
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.396774193548392
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467394586972067
    mean_env_wait_ms: 1.1911212146778458
    mean_inference_ms: 4.316028040827159
    mean_raw_obs_processing_ms: 0.37844090003895453
  time_since_restore: 8288.230057477951
  time_this_iter_s: 25.947920322418213
  time_total_s: 8288.230057477951
  timers:
    learn_throughput: 8641.17
    learn_time_ms: 18723.39
    sample_throughput: 23818.853
    sample_time_ms: 6792.602
    update_time_ms: 35.7
  timestamp: 1602818587
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: 1bbc1_00000
  
2020-10-16 03:23:09,025	WARNING util.py:136 -- The `process_trial` operation took 0.8344941139221191 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    321 |          8288.23 | 51935232 |  305.028 |              322.949 |              165.677 |            789.679 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3026.230917566316
    time_step_min: 2911
  date: 2020-10-16_03-23-35
  done: false
  episode_len_mean: 789.710504348354
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.06847183876005
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 65887
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.434129555517809e-37
        cur_lr: 5.0e-05
        entropy: 0.0647525464495023
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009094035107409582
        total_loss: .inf
        vf_explained_var: 0.9993410110473633
        vf_loss: 0.3072685127456983
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.483870967741943
    gpu_util_percent0: 0.2935483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673831714317043
    mean_env_wait_ms: 1.1910562337445474
    mean_inference_ms: 4.315958050268531
    mean_raw_obs_processing_ms: 0.3784359797085422
  time_since_restore: 8314.364449739456
  time_this_iter_s: 26.134392261505127
  time_total_s: 8314.364449739456
  timers:
    learn_throughput: 8634.412
    learn_time_ms: 18738.046
    sample_throughput: 23819.948
    sample_time_ms: 6792.29
    update_time_ms: 35.646
  timestamp: 1602818615
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:23:36,417	WARNING util.py:136 -- The `process_trial` operation took 0.8766982555389404 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    322 |          8314.36 | 52097024 |  305.068 |              322.949 |              165.677 |            789.711 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3025.9383667413554
    time_step_min: 2911
  date: 2020-10-16_03-24-02
  done: false
  episode_len_mean: 789.7397397094431
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.11175465306803
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 193
  episodes_total: 66080
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.151194333276712e-37
        cur_lr: 5.0e-05
        entropy: 0.06333692433933417
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038930133062725267
        model: {}
        policy_loss: -0.007866019402475407
        total_loss: 0.464669831097126
        vf_explained_var: 0.9990275502204895
        vf_loss: 0.47256751358509064
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11935483870968
    gpu_util_percent0: 0.3196774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673692350660655
    mean_env_wait_ms: 1.1909843730931804
    mean_inference_ms: 4.315885700824348
    mean_raw_obs_processing_ms: 0.3784312486779518
  time_since_restore: 8340.383446216583
  time_this_iter_s: 26.018996477127075
  time_total_s: 8340.383446216583
  timers:
    learn_throughput: 8623.696
    learn_time_ms: 18761.329
    sample_throughput: 23852.248
    sample_time_ms: 6783.092
    update_time_ms: 35.395
  timestamp: 1602818642
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: 1bbc1_00000
  
2020-10-16 03:24:03,714	WARNING util.py:136 -- The `process_trial` operation took 0.8965272903442383 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    323 |          8340.38 | 52258816 |  305.112 |              322.949 |              165.677 |             789.74 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3025.597199589595
    time_step_min: 2911
  date: 2020-10-16_03-24-29
  done: false
  episode_len_mean: 789.7749909507722
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.161878040784
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 224
  episodes_total: 66304
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.075597166638356e-37
        cur_lr: 5.0e-05
        entropy: 0.06501173134893179
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00847765026567989
        total_loss: .inf
        vf_explained_var: 0.9988718628883362
        vf_loss: 0.6119569465517998
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.141935483870974
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673501750747872
    mean_env_wait_ms: 1.1909001634661056
    mean_inference_ms: 4.315792569774504
    mean_raw_obs_processing_ms: 0.3784247259314091
  time_since_restore: 8366.376239299774
  time_this_iter_s: 25.992793083190918
  time_total_s: 8366.376239299774
  timers:
    learn_throughput: 8615.97
    learn_time_ms: 18778.153
    sample_throughput: 23880.568
    sample_time_ms: 6775.048
    update_time_ms: 36.944
  timestamp: 1602818669
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:24:30,900	WARNING util.py:136 -- The `process_trial` operation took 0.866112470626831 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    324 |          8366.38 | 52420608 |  305.162 |              322.949 |              165.677 |            789.775 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3025.289044990298
    time_step_min: 2911
  date: 2020-10-16_03-24-56
  done: false
  episode_len_mean: 789.8021470778391
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.2064303289398
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 205
  episodes_total: 66509
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.113395749957536e-37
        cur_lr: 5.0e-05
        entropy: 0.06469530301789443
        entropy_coeff: 0.0005000000000000001
        kl: 0.017546471984436113
        model: {}
        policy_loss: -0.008405866819278648
        total_loss: 0.3202722817659378
        vf_explained_var: 0.9993164539337158
        vf_loss: 0.3287104864915212
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993548387096777
    gpu_util_percent0: 0.3041935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673378722098038
    mean_env_wait_ms: 1.190826071842166
    mean_inference_ms: 4.315717530756626
    mean_raw_obs_processing_ms: 0.378418904253498
  time_since_restore: 8392.323258161545
  time_this_iter_s: 25.94701886177063
  time_total_s: 8392.323258161545
  timers:
    learn_throughput: 8616.055
    learn_time_ms: 18777.968
    sample_throughput: 23896.514
    sample_time_ms: 6770.527
    update_time_ms: 35.918
  timestamp: 1602818696
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: 1bbc1_00000
  
2020-10-16 03:24:58,266	WARNING util.py:136 -- The `process_trial` operation took 0.915717363357544 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    325 |          8392.32 | 52582400 |  305.206 |              322.949 |              165.677 |            789.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3025.03462294295
    time_step_min: 2911
  date: 2020-10-16_03-25-24
  done: false
  episode_len_mean: 789.8294621301864
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.24515317671586
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 66689
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.113395749957536e-37
        cur_lr: 5.0e-05
        entropy: 0.07007484324276447
        entropy_coeff: 0.0005000000000000001
        kl: 0.005095514158407847
        model: {}
        policy_loss: -0.008565381567071503
        total_loss: 0.5443178564310074
        vf_explained_var: 0.9987851977348328
        vf_loss: 0.5529182776808739
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29
    gpu_util_percent0: 0.3363333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673243380513298
    mean_env_wait_ms: 1.1907600819713555
    mean_inference_ms: 4.315647082742059
    mean_raw_obs_processing_ms: 0.3784141574531949
  time_since_restore: 8418.188731193542
  time_this_iter_s: 25.86547303199768
  time_total_s: 8418.188731193542
  timers:
    learn_throughput: 8621.967
    learn_time_ms: 18765.091
    sample_throughput: 23901.189
    sample_time_ms: 6769.203
    update_time_ms: 35.409
  timestamp: 1602818724
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: 1bbc1_00000
  
2020-10-16 03:25:25,417	WARNING util.py:136 -- The `process_trial` operation took 0.8776686191558838 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    326 |          8418.19 | 52744192 |  305.245 |              322.949 |              165.677 |            789.829 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3024.8149039899504
    time_step_min: 2911
  date: 2020-10-16_03-25-51
  done: false
  episode_len_mean: 789.8611426692179
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.2819667012143
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 207
  episodes_total: 66896
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.113395749957536e-37
        cur_lr: 5.0e-05
        entropy: 0.07009924141069253
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012473940131409714
        total_loss: .inf
        vf_explained_var: 0.9988861680030823
        vf_loss: 0.5943762163321177
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02903225806452
    gpu_util_percent0: 0.2848387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673095993441007
    mean_env_wait_ms: 1.19068247848957
    mean_inference_ms: 4.315565001504043
    mean_raw_obs_processing_ms: 0.3784087861163563
  time_since_restore: 8444.212128400803
  time_this_iter_s: 26.023397207260132
  time_total_s: 8444.212128400803
  timers:
    learn_throughput: 8615.889
    learn_time_ms: 18778.33
    sample_throughput: 23951.078
    sample_time_ms: 6755.103
    update_time_ms: 37.017
  timestamp: 1602818751
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:25:52,756	WARNING util.py:136 -- The `process_trial` operation took 0.8856813907623291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    327 |          8444.21 | 52905984 |  305.282 |              322.949 |              165.677 |            789.861 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3024.4990013712513
    time_step_min: 2911
  date: 2020-10-16_03-26-18
  done: false
  episode_len_mean: 789.9011620977354
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.33107746114274
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 224
  episodes_total: 67120
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.1700936249363e-37
        cur_lr: 5.0e-05
        entropy: 0.0695123461385568
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0069805273330227164
        total_loss: .inf
        vf_explained_var: 0.9990171790122986
        vf_loss: 0.5052748968203863
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.890322580645165
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467292299703297
    mean_env_wait_ms: 1.190600981070237
    mean_inference_ms: 4.315485968097662
    mean_raw_obs_processing_ms: 0.3784027906053796
  time_since_restore: 8470.178468227386
  time_this_iter_s: 25.966339826583862
  time_total_s: 8470.178468227386
  timers:
    learn_throughput: 8619.335
    learn_time_ms: 18770.822
    sample_throughput: 23940.179
    sample_time_ms: 6758.178
    update_time_ms: 36.165
  timestamp: 1602818778
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:26:19,928	WARNING util.py:136 -- The `process_trial` operation took 0.8863770961761475 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    328 |          8470.18 | 53067776 |  305.331 |              322.949 |              165.677 |            789.901 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3024.2325723118997
    time_step_min: 2911
  date: 2020-10-16_03-26-45
  done: false
  episode_len_mean: 789.932324012718
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.3714839837471
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 186
  episodes_total: 67306
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3755140437404455e-36
        cur_lr: 5.0e-05
        entropy: 0.06886812423666318
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009665505650142828
        total_loss: .inf
        vf_explained_var: 0.9990922808647156
        vf_loss: 0.4222667341430982
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.612903225806452
    gpu_util_percent0: 0.3396774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672795375068953
    mean_env_wait_ms: 1.1905345686834952
    mean_inference_ms: 4.315415896713323
    mean_raw_obs_processing_ms: 0.37839759592603106
  time_since_restore: 8496.032061100006
  time_this_iter_s: 25.85359287261963
  time_total_s: 8496.032061100006
  timers:
    learn_throughput: 8619.006
    learn_time_ms: 18771.538
    sample_throughput: 23960.032
    sample_time_ms: 6752.579
    update_time_ms: 36.973
  timestamp: 1602818805
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:26:47,066	WARNING util.py:136 -- The `process_trial` operation took 0.9417963027954102 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    329 |          8496.03 | 53229568 |  305.371 |              322.949 |              165.677 |            789.932 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3024.0653227599496
    time_step_min: 2911
  date: 2020-10-16_03-27-12
  done: false
  episode_len_mean: 789.9556250277807
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.3985435376984
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 67493
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.063271065610668e-36
        cur_lr: 5.0e-05
        entropy: 0.06895363579193751
        entropy_coeff: 0.0005000000000000001
        kl: 0.004547407501377165
        model: {}
        policy_loss: -0.010826807313909134
        total_loss: 0.9843021134535471
        vf_explained_var: 0.9979196190834045
        vf_loss: 0.9951633960008621
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.687096774193545
    gpu_util_percent0: 0.3325806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672672344779997
    mean_env_wait_ms: 1.190466671377235
    mean_inference_ms: 4.315345733509605
    mean_raw_obs_processing_ms: 0.37839290620401844
  time_since_restore: 8521.894607067108
  time_this_iter_s: 25.86254596710205
  time_total_s: 8521.894607067108
  timers:
    learn_throughput: 8618.363
    learn_time_ms: 18772.938
    sample_throughput: 23945.713
    sample_time_ms: 6756.616
    update_time_ms: 34.334
  timestamp: 1602818832
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: 1bbc1_00000
  
2020-10-16 03:27:14,148	WARNING util.py:136 -- The `process_trial` operation took 0.8693602085113525 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    330 |          8521.89 | 53391360 |  305.399 |              322.949 |              165.677 |            789.956 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3023.77352915103
    time_step_min: 2911
  date: 2020-10-16_03-27-40
  done: false
  episode_len_mean: 789.9943435238517
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.4472415783889
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 217
  episodes_total: 67710
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.031635532805334e-36
        cur_lr: 5.0e-05
        entropy: 0.061446634121239185
        entropy_coeff: 0.0005000000000000001
        kl: 0.004166185448411852
        model: {}
        policy_loss: -0.006662263655622762
        total_loss: 0.4113997742533684
        vf_explained_var: 0.999204158782959
        vf_loss: 0.4180927673975627
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.2
    gpu_util_percent0: 0.31419354838709673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672482279283516
    mean_env_wait_ms: 1.1903850827974798
    mean_inference_ms: 4.315258244826572
    mean_raw_obs_processing_ms: 0.3783867930706321
  time_since_restore: 8547.759397268295
  time_this_iter_s: 25.864790201187134
  time_total_s: 8547.759397268295
  timers:
    learn_throughput: 8623.675
    learn_time_ms: 18761.375
    sample_throughput: 23930.806
    sample_time_ms: 6760.825
    update_time_ms: 39.714
  timestamp: 1602818860
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: 1bbc1_00000
  
2020-10-16 03:27:41,364	WARNING util.py:136 -- The `process_trial` operation took 0.9904818534851074 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    331 |          8547.76 | 53553152 |  305.447 |              322.949 |              165.677 |            789.994 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3023.4836666764854
    time_step_min: 2911
  date: 2020-10-16_03-28-07
  done: false
  episode_len_mean: 790.0315490386597
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.49573510924074
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 67926
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.15817766402667e-37
        cur_lr: 5.0e-05
        entropy: 0.06106012035161257
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009009007442121705
        total_loss: .inf
        vf_explained_var: 0.9995281100273132
        vf_loss: 0.2560454358657201
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.329032258064515
    gpu_util_percent0: 0.2974193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146723574717975
    mean_env_wait_ms: 1.1903090497985336
    mean_inference_ms: 4.315186616583239
    mean_raw_obs_processing_ms: 0.37838139143183613
  time_since_restore: 8573.720464468002
  time_this_iter_s: 25.96106719970703
  time_total_s: 8573.720464468002
  timers:
    learn_throughput: 8633.299
    learn_time_ms: 18740.46
    sample_throughput: 23867.853
    sample_time_ms: 6778.657
    update_time_ms: 39.957
  timestamp: 1602818887
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:28:08,646	WARNING util.py:136 -- The `process_trial` operation took 0.9418854713439941 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    332 |          8573.72 | 53714944 |  305.496 |              322.949 |              165.677 |            790.032 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3023.2135638532272
    time_step_min: 2911
  date: 2020-10-16_03-28-34
  done: false
  episode_len_mean: 790.0637535606261
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.5363048153989
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 68106
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.737266496040005e-37
        cur_lr: 5.0e-05
        entropy: 0.06377208760629098
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006116505430933709
        total_loss: .inf
        vf_explained_var: 0.9994528293609619
        vf_loss: 0.2507497767607371
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08387096774194
    gpu_util_percent0: 0.3193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672238773255009
    mean_env_wait_ms: 1.1902446812522067
    mean_inference_ms: 4.315120231759354
    mean_raw_obs_processing_ms: 0.37837689246508405
  time_since_restore: 8599.770107269287
  time_this_iter_s: 26.04964280128479
  time_total_s: 8599.770107269287
  timers:
    learn_throughput: 8628.716
    learn_time_ms: 18750.414
    sample_throughput: 23865.178
    sample_time_ms: 6779.417
    update_time_ms: 40.034
  timestamp: 1602818914
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:28:35,993	WARNING util.py:136 -- The `process_trial` operation took 0.9051253795623779 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    333 |          8599.77 | 53876736 |  305.536 |              322.949 |              165.677 |            790.064 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3022.9334475738538
    time_step_min: 2911
  date: 2020-10-16_03-29-01
  done: false
  episode_len_mean: 790.0919698411536
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.57814363531344
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 68305
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1605899744060005e-36
        cur_lr: 5.0e-05
        entropy: 0.06420784319440524
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00810202998885264
        total_loss: .inf
        vf_explained_var: 0.9993153214454651
        vf_loss: 0.3331268827120463
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.796666666666667
    gpu_util_percent0: 0.3423333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672096716299377
    mean_env_wait_ms: 1.1901718298139727
    mean_inference_ms: 4.315043795376548
    mean_raw_obs_processing_ms: 0.378371977592037
  time_since_restore: 8625.097522258759
  time_this_iter_s: 25.327414989471436
  time_total_s: 8625.097522258759
  timers:
    learn_throughput: 8661.438
    learn_time_ms: 18679.577
    sample_throughput: 23812.922
    sample_time_ms: 6794.294
    update_time_ms: 38.465
  timestamp: 1602818941
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:29:02,701	WARNING util.py:136 -- The `process_trial` operation took 0.9546277523040771 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    334 |           8625.1 | 54038528 |  305.578 |              322.949 |              165.677 |            790.092 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3022.594330258087
    time_step_min: 2911
  date: 2020-10-16_03-29-28
  done: false
  episode_len_mean: 790.1283633922839
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.62856634989333
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 227
  episodes_total: 68532
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7408849616090008e-36
        cur_lr: 5.0e-05
        entropy: 0.06510085674623649
        entropy_coeff: 0.0005000000000000001
        kl: 0.005180697694110374
        model: {}
        policy_loss: -0.007695408285750697
        total_loss: 0.3240092148383458
        vf_explained_var: 0.9994181990623474
        vf_loss: 0.33173718055089313
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.87741935483871
    gpu_util_percent0: 0.29935483870967744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671904978250208
    mean_env_wait_ms: 1.1900891036015004
    mean_inference_ms: 4.314960301431701
    mean_raw_obs_processing_ms: 0.37836568327035736
  time_since_restore: 8651.103147029877
  time_this_iter_s: 26.005624771118164
  time_total_s: 8651.103147029877
  timers:
    learn_throughput: 8664.019
    learn_time_ms: 18674.012
    sample_throughput: 23785.97
    sample_time_ms: 6801.993
    update_time_ms: 40.962
  timestamp: 1602818968
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: 1bbc1_00000
  
2020-10-16 03:29:30,077	WARNING util.py:136 -- The `process_trial` operation took 0.9499335289001465 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    335 |           8651.1 | 54200320 |  305.629 |              322.949 |              165.677 |            790.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3022.298742321465
    time_step_min: 2911
  date: 2020-10-16_03-29-56
  done: false
  episode_len_mean: 790.1587317754562
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.67240692581873
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 68726
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7408849616090008e-36
        cur_lr: 5.0e-05
        entropy: 0.06115915315846602
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036522335916136703
        model: {}
        policy_loss: -0.0055891724380974965
        total_loss: 0.14393742630879083
        vf_explained_var: 0.9996762871742249
        vf_loss: 0.14955717449386915
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.767741935483873
    gpu_util_percent0: 0.3319354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671800075224195
    mean_env_wait_ms: 1.1900212686500329
    mean_inference_ms: 4.314894603244003
    mean_raw_obs_processing_ms: 0.3783606433928713
  time_since_restore: 8677.169605255127
  time_this_iter_s: 26.066458225250244
  time_total_s: 8677.169605255127
  timers:
    learn_throughput: 8658.347
    learn_time_ms: 18686.247
    sample_throughput: 23761.305
    sample_time_ms: 6809.054
    update_time_ms: 39.645
  timestamp: 1602818996
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: 1bbc1_00000
  
2020-10-16 03:29:57,480	WARNING util.py:136 -- The `process_trial` operation took 0.9441096782684326 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    336 |          8677.17 | 54362112 |  305.672 |              322.949 |              165.677 |            790.159 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3022.0309528303255
    time_step_min: 2911
  date: 2020-10-16_03-30-23
  done: false
  episode_len_mean: 790.1833050343216
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.71106291263874
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 68907
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.704424808045004e-37
        cur_lr: 5.0e-05
        entropy: 0.06446095276623964
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007810919128435974
        total_loss: .inf
        vf_explained_var: 0.9993780255317688
        vf_loss: 0.3039314846197764
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.416129032258066
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467166957541735
    mean_env_wait_ms: 1.189957049941673
    mean_inference_ms: 4.314826089110777
    mean_raw_obs_processing_ms: 0.37835615850512455
  time_since_restore: 8703.044526100159
  time_this_iter_s: 25.87492084503174
  time_total_s: 8703.044526100159
  timers:
    learn_throughput: 8665.249
    learn_time_ms: 18671.363
    sample_throughput: 23762.566
    sample_time_ms: 6808.692
    update_time_ms: 40.193
  timestamp: 1602819023
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:30:24,655	WARNING util.py:136 -- The `process_trial` operation took 0.9698543548583984 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    337 |          8703.04 | 54523904 |  305.711 |              322.949 |              165.677 |            790.183 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3021.725189955858
    time_step_min: 2911
  date: 2020-10-16_03-30-50
  done: false
  episode_len_mean: 790.2097565209843
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.75712508970594
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 69123
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3056637212067508e-36
        cur_lr: 5.0e-05
        entropy: 0.0675124494979779
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005921309594365691
        total_loss: .inf
        vf_explained_var: 0.9994788765907288
        vf_loss: 0.2688363902270794
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.706451612903226
    gpu_util_percent0: 0.2699999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671510548577174
    mean_env_wait_ms: 1.1898774512767
    mean_inference_ms: 4.314747292300161
    mean_raw_obs_processing_ms: 0.37835051987671126
  time_since_restore: 8729.380812644958
  time_this_iter_s: 26.336286544799805
  time_total_s: 8729.380812644958
  timers:
    learn_throughput: 8651.017
    learn_time_ms: 18702.08
    sample_throughput: 23738.025
    sample_time_ms: 6815.731
    update_time_ms: 39.07
  timestamp: 1602819050
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:30:52,381	WARNING util.py:136 -- The `process_trial` operation took 0.9626779556274414 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    338 |          8729.38 | 54685696 |  305.757 |              322.949 |              165.677 |             790.21 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3021.4142910727683
    time_step_min: 2911
  date: 2020-10-16_03-31-18
  done: false
  episode_len_mean: 790.239674665436
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.8018795476684
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 69344
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9584955818101263e-36
        cur_lr: 5.0e-05
        entropy: 0.06591279494265716
        entropy_coeff: 0.0005000000000000001
        kl: 0.005220691363016765
        model: {}
        policy_loss: -0.008178784264600836
        total_loss: 0.36718593289454776
        vf_explained_var: 0.9992530345916748
        vf_loss: 0.37539765735467273
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.296774193548384
    gpu_util_percent0: 0.29838709677419356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671377344548997
    mean_env_wait_ms: 1.1898003612479884
    mean_inference_ms: 4.314674320601083
    mean_raw_obs_processing_ms: 0.37834534870444547
  time_since_restore: 8755.29232954979
  time_this_iter_s: 25.911516904830933
  time_total_s: 8755.29232954979
  timers:
    learn_throughput: 8651.168
    learn_time_ms: 18701.752
    sample_throughput: 23713.246
    sample_time_ms: 6822.853
    update_time_ms: 37.542
  timestamp: 1602819078
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: 1bbc1_00000
  
2020-10-16 03:31:19,622	WARNING util.py:136 -- The `process_trial` operation took 0.8941426277160645 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    339 |          8755.29 | 54847488 |  305.802 |              322.949 |              165.677 |             790.24 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3021.149118641629
    time_step_min: 2911
  date: 2020-10-16_03-31-45
  done: false
  episode_len_mean: 790.2659263840743
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.83963507752725
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 69523
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9584955818101263e-36
        cur_lr: 5.0e-05
        entropy: 0.06614255905151367
        entropy_coeff: 0.0005000000000000001
        kl: 0.005271527295311292
        model: {}
        policy_loss: -0.008816676097922027
        total_loss: 0.28160983696579933
        vf_explained_var: 0.9994053244590759
        vf_loss: 0.29045958692828816
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.606250000000003
    gpu_util_percent0: 0.26625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671245230066174
    mean_env_wait_ms: 1.1897370363731372
    mean_inference_ms: 4.314610037356598
    mean_raw_obs_processing_ms: 0.37834042153496905
  time_since_restore: 8781.512464284897
  time_this_iter_s: 26.220134735107422
  time_total_s: 8781.512464284897
  timers:
    learn_throughput: 8638.115
    learn_time_ms: 18730.013
    sample_throughput: 23723.174
    sample_time_ms: 6819.998
    update_time_ms: 38.676
  timestamp: 1602819105
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: 1bbc1_00000
  
2020-10-16 03:31:47,176	WARNING util.py:136 -- The `process_trial` operation took 0.9032382965087891 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    340 |          8781.51 | 55009280 |   305.84 |              322.949 |              165.677 |            790.266 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3020.891060410389
    time_step_min: 2911
  date: 2020-10-16_03-32-13
  done: false
  episode_len_mean: 790.2895235089934
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.8802345437214
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 195
  episodes_total: 69718
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9584955818101263e-36
        cur_lr: 5.0e-05
        entropy: 0.06328275830795367
        entropy_coeff: 0.0005000000000000001
        kl: 0.005347611304993431
        model: {}
        policy_loss: -0.008110729807716174
        total_loss: 0.25975307698051137
        vf_explained_var: 0.9995379447937012
        vf_loss: 0.26789545143644017
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10666666666667
    gpu_util_percent0: 0.3383333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467113832433833
    mean_env_wait_ms: 1.189667768732108
    mean_inference_ms: 4.314544868738306
    mean_raw_obs_processing_ms: 0.37833612696002483
  time_since_restore: 8807.351108789444
  time_this_iter_s: 25.83864450454712
  time_total_s: 8807.351108789444
  timers:
    learn_throughput: 8635.633
    learn_time_ms: 18735.396
    sample_throughput: 23726.997
    sample_time_ms: 6818.899
    update_time_ms: 32.765
  timestamp: 1602819133
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: 1bbc1_00000
  
2020-10-16 03:32:14,406	WARNING util.py:136 -- The `process_trial` operation took 0.9799244403839111 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    341 |          8807.35 | 55171072 |   305.88 |              322.949 |              165.677 |             790.29 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3020.554563917732
    time_step_min: 2911
  date: 2020-10-16_03-32-40
  done: false
  episode_len_mean: 790.3180310525262
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.9305608626796
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 228
  episodes_total: 69946
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9584955818101263e-36
        cur_lr: 5.0e-05
        entropy: 0.06858490904172261
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0074469432001933455
        total_loss: .inf
        vf_explained_var: 0.9996030330657959
        vf_loss: 0.2148857501645883
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.677419354838708
    gpu_util_percent0: 0.30451612903225805
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670946512486482
    mean_env_wait_ms: 1.189585444078091
    mean_inference_ms: 4.314459718435641
    mean_raw_obs_processing_ms: 0.3783296073624599
  time_since_restore: 8833.194634437561
  time_this_iter_s: 25.843525648117065
  time_total_s: 8833.194634437561
  timers:
    learn_throughput: 8632.642
    learn_time_ms: 18741.887
    sample_throughput: 23793.954
    sample_time_ms: 6799.711
    update_time_ms: 32.744
  timestamp: 1602819160
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:32:41,659	WARNING util.py:136 -- The `process_trial` operation took 0.9694709777832031 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    342 |          8833.19 | 55332864 |  305.931 |              322.949 |              165.677 |            790.318 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3020.2847241039976
    time_step_min: 2911
  date: 2020-10-16_03-33-07
  done: false
  episode_len_mean: 790.3402380782665
  episode_reward_max: 322.949494949495
  episode_reward_mean: 305.9690669615822
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 70145
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.937743372715189e-36
        cur_lr: 5.0e-05
        entropy: 0.06710711556176345
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008176313693790386
        total_loss: .inf
        vf_explained_var: 0.9992074966430664
        vf_loss: 0.38865246375401813
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23870967741936
    gpu_util_percent0: 0.3058064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670841589951888
    mean_env_wait_ms: 1.1895161633248184
    mean_inference_ms: 4.314394918004074
    mean_raw_obs_processing_ms: 0.3783248409480191
  time_since_restore: 8859.166401147842
  time_this_iter_s: 25.971766710281372
  time_total_s: 8859.166401147842
  timers:
    learn_throughput: 8640.594
    learn_time_ms: 18724.639
    sample_throughput: 23770.478
    sample_time_ms: 6806.426
    update_time_ms: 33.04
  timestamp: 1602819187
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:33:08,965	WARNING util.py:136 -- The `process_trial` operation took 0.9047493934631348 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    343 |          8859.17 | 55494656 |  305.969 |              322.949 |              165.677 |             790.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3020.070886976314
    time_step_min: 2911
  date: 2020-10-16_03-33-34
  done: false
  episode_len_mean: 790.3622712341623
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.0034119618606
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 178
  episodes_total: 70323
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.406615059072784e-36
        cur_lr: 5.0e-05
        entropy: 0.06532223212222259
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007784503958343218
        total_loss: .inf
        vf_explained_var: 0.9993174076080322
        vf_loss: 0.34000324457883835
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36451612903226
    gpu_util_percent0: 0.31870967741935485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467071822821475
    mean_env_wait_ms: 1.1894539879625332
    mean_inference_ms: 4.314331990807035
    mean_raw_obs_processing_ms: 0.3783204992853404
  time_since_restore: 8885.022026062012
  time_this_iter_s: 25.85562491416931
  time_total_s: 8885.022026062012
  timers:
    learn_throughput: 8612.636
    learn_time_ms: 18785.422
    sample_throughput: 23804.885
    sample_time_ms: 6796.588
    update_time_ms: 33.291
  timestamp: 1602819214
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:33:36,116	WARNING util.py:136 -- The `process_trial` operation took 0.9532082080841064 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    344 |          8885.02 | 55656448 |  306.003 |              322.949 |              165.677 |            790.362 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3019.807068300431
    time_step_min: 2911
  date: 2020-10-16_03-34-02
  done: false
  episode_len_mean: 790.3840090728664
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.04318990872684
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 217
  episodes_total: 70540
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.609922588609175e-36
        cur_lr: 5.0e-05
        entropy: 0.06796834990382195
        entropy_coeff: 0.0005000000000000001
        kl: 0.004693116410635412
        model: {}
        policy_loss: -0.006600938732541787
        total_loss: 0.7708214620749155
        vf_explained_var: 0.9985299110412598
        vf_loss: 0.7774563829104105
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380645161290328
    gpu_util_percent0: 0.3393548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467057645297426
    mean_env_wait_ms: 1.189375932670543
    mean_inference_ms: 4.314256499182061
    mean_raw_obs_processing_ms: 0.37831563055857054
  time_since_restore: 8910.95916724205
  time_this_iter_s: 25.937141180038452
  time_total_s: 8910.95916724205
  timers:
    learn_throughput: 8614.925
    learn_time_ms: 18780.43
    sample_throughput: 23809.272
    sample_time_ms: 6795.336
    update_time_ms: 32.156
  timestamp: 1602819242
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: 1bbc1_00000
  
2020-10-16 03:34:03,326	WARNING util.py:136 -- The `process_trial` operation took 0.9314873218536377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    345 |          8910.96 | 55818240 |  306.043 |              322.949 |              165.677 |            790.384 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3019.5079807162147
    time_step_min: 2911
  date: 2020-10-16_03-34-29
  done: false
  episode_len_mean: 790.4107488588347
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.08646947706575
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 70761
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.304961294304588e-36
        cur_lr: 5.0e-05
        entropy: 0.06623722178240617
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007436111181353529
        total_loss: .inf
        vf_explained_var: 0.9994320869445801
        vf_loss: 0.2850457777579625
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.435483870967747
    gpu_util_percent0: 0.32903225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670424923839345
    mean_env_wait_ms: 1.1892992936524567
    mean_inference_ms: 4.3141810582340625
    mean_raw_obs_processing_ms: 0.3783099874719076
  time_since_restore: 8936.773831367493
  time_this_iter_s: 25.814664125442505
  time_total_s: 8936.773831367493
  timers:
    learn_throughput: 8626.909
    learn_time_ms: 18754.341
    sample_throughput: 23815.009
    sample_time_ms: 6793.699
    update_time_ms: 34.222
  timestamp: 1602819269
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:34:30,401	WARNING util.py:136 -- The `process_trial` operation took 0.9117100238800049 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    346 |          8936.77 | 55980032 |  306.086 |              322.949 |              165.677 |            790.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3019.2564973982203
    time_step_min: 2911
  date: 2020-10-16_03-34-56
  done: false
  episode_len_mean: 790.4352490097405
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.1248459560716
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 70941
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.9574419414568826e-36
        cur_lr: 5.0e-05
        entropy: 0.06455963477492332
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007190112111857161
        total_loss: .inf
        vf_explained_var: 0.9997164607048035
        vf_loss: 0.12659849971532822
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96129032258065
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670307694980697
    mean_env_wait_ms: 1.1892375302364746
    mean_inference_ms: 4.314123542597412
    mean_raw_obs_processing_ms: 0.3783053307336051
  time_since_restore: 8962.516537427902
  time_this_iter_s: 25.742706060409546
  time_total_s: 8962.516537427902
  timers:
    learn_throughput: 8632.272
    learn_time_ms: 18742.69
    sample_throughput: 23838.415
    sample_time_ms: 6787.029
    update_time_ms: 31.651
  timestamp: 1602819296
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:34:57,540	WARNING util.py:136 -- The `process_trial` operation took 0.9620778560638428 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    347 |          8962.52 | 56141824 |  306.125 |              322.949 |              165.677 |            790.435 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3018.9907602733947
    time_step_min: 2911
  date: 2020-10-16_03-35-23
  done: false
  episode_len_mean: 790.4634773807181
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.1653121310666
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 193
  episodes_total: 71134
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.436162912185323e-36
        cur_lr: 5.0e-05
        entropy: 0.06401225520918767
        entropy_coeff: 0.0005000000000000001
        kl: 0.004530657626067598
        model: {}
        policy_loss: -0.008420106304887062
        total_loss: 0.21367212012410164
        vf_explained_var: 0.9996251463890076
        vf_loss: 0.2221242276330789
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.664516129032258
    gpu_util_percent0: 0.35096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670192744225216
    mean_env_wait_ms: 1.1891697242636874
    mean_inference_ms: 4.314058304114135
    mean_raw_obs_processing_ms: 0.3783010597182385
  time_since_restore: 8988.471198558807
  time_this_iter_s: 25.95466113090515
  time_total_s: 8988.471198558807
  timers:
    learn_throughput: 8641.089
    learn_time_ms: 18723.565
    sample_throughput: 23915.14
    sample_time_ms: 6765.254
    update_time_ms: 31.992
  timestamp: 1602819323
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: 1bbc1_00000
  
2020-10-16 03:35:25,053	WARNING util.py:136 -- The `process_trial` operation took 1.0348501205444336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    348 |          8988.47 | 56303616 |  306.165 |              322.949 |              165.677 |            790.463 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3018.6820176360907
    time_step_min: 2911
  date: 2020-10-16_03-35-50
  done: false
  episode_len_mean: 790.4967978811362
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.2125838040996
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 225
  episodes_total: 71359
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.718081456092662e-36
        cur_lr: 5.0e-05
        entropy: 0.06324216226736705
        entropy_coeff: 0.0005000000000000001
        kl: 0.005253338216183086
        model: {}
        policy_loss: -0.0064108472361112945
        total_loss: 0.1871348830560843
        vf_explained_var: 0.9996512532234192
        vf_loss: 0.1935773454606533
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.432258064516123
    gpu_util_percent0: 0.31354838709677424
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8709677419354844
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670005953058854
    mean_env_wait_ms: 1.189089157838886
    mean_inference_ms: 4.313976329371484
    mean_raw_obs_processing_ms: 0.3782951458306086
  time_since_restore: 9014.4153778553
  time_this_iter_s: 25.94417929649353
  time_total_s: 9014.4153778553
  timers:
    learn_throughput: 8643.047
    learn_time_ms: 18719.325
    sample_throughput: 23937.453
    sample_time_ms: 6758.948
    update_time_ms: 34.175
  timestamp: 1602819350
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: 1bbc1_00000
  
2020-10-16 03:35:52,346	WARNING util.py:136 -- The `process_trial` operation took 0.9900708198547363 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    349 |          9014.42 | 56465408 |  306.213 |              322.949 |              165.677 |            790.497 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3018.41030587709
    time_step_min: 2911
  date: 2020-10-16_03-36-18
  done: false
  episode_len_mean: 790.5313862493012
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.2538443405545
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 201
  episodes_total: 71560
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.718081456092662e-36
        cur_lr: 5.0e-05
        entropy: 0.061593105706075825
        entropy_coeff: 0.0005000000000000001
        kl: 0.004958677222020924
        model: {}
        policy_loss: -0.007493278904197116
        total_loss: 0.10741023160517216
        vf_explained_var: 0.9997580051422119
        vf_loss: 0.11493430659174919
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.935483870967737
    gpu_util_percent0: 0.31741935483870976
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669918891784117
    mean_env_wait_ms: 1.1890214583600038
    mean_inference_ms: 4.3139178792078825
    mean_raw_obs_processing_ms: 0.37829061760235355
  time_since_restore: 9040.505664348602
  time_this_iter_s: 26.09028649330139
  time_total_s: 9040.505664348602
  timers:
    learn_throughput: 8644.981
    learn_time_ms: 18715.136
    sample_throughput: 23971.098
    sample_time_ms: 6749.461
    update_time_ms: 34.804
  timestamp: 1602819378
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: 1bbc1_00000
  
2020-10-16 03:36:19,735	WARNING util.py:136 -- The `process_trial` operation took 0.9540927410125732 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    350 |          9040.51 | 56627200 |  306.254 |              322.949 |              165.677 |            790.531 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3018.1636477724323
    time_step_min: 2911
  date: 2020-10-16_03-36-45
  done: false
  episode_len_mean: 790.5476631866524
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.28983984218615
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 183
  episodes_total: 71743
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.859040728046331e-36
        cur_lr: 5.0e-05
        entropy: 0.06274725372592609
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006776872750682135
        total_loss: .inf
        vf_explained_var: 0.9993525147438049
        vf_loss: 0.3108671059211095
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.154838709677417
    gpu_util_percent0: 0.3190322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466979476085932
    mean_env_wait_ms: 1.1889584571225449
    mean_inference_ms: 4.313857302558096
    mean_raw_obs_processing_ms: 0.3782864134228193
  time_since_restore: 9066.502214193344
  time_this_iter_s: 25.99654984474182
  time_total_s: 9066.502214193344
  timers:
    learn_throughput: 8642.729
    learn_time_ms: 18720.013
    sample_throughput: 23972.145
    sample_time_ms: 6749.167
    update_time_ms: 35.353
  timestamp: 1602819405
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:36:47,026	WARNING util.py:136 -- The `process_trial` operation took 0.9515392780303955 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    351 |           9066.5 | 56788992 |   306.29 |              322.949 |              165.677 |            790.548 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3017.894016072074
    time_step_min: 2911
  date: 2020-10-16_03-37-13
  done: false
  episode_len_mean: 790.570003057509
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.33148731667205
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 211
  episodes_total: 71954
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7885610920694958e-36
        cur_lr: 5.0e-05
        entropy: 0.0619171109671394
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050609445897862315
        model: {}
        policy_loss: -0.008506728050027354
        total_loss: 0.2965182935198148
        vf_explained_var: 0.999413013458252
        vf_loss: 0.30505597094694775
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.84193548387097
    gpu_util_percent0: 0.30612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669687974317877
    mean_env_wait_ms: 1.1888838713837377
    mean_inference_ms: 4.313788825960575
    mean_raw_obs_processing_ms: 0.37828188691361303
  time_since_restore: 9092.503833055496
  time_this_iter_s: 26.0016188621521
  time_total_s: 9092.503833055496
  timers:
    learn_throughput: 8638.444
    learn_time_ms: 18729.298
    sample_throughput: 23981.851
    sample_time_ms: 6746.435
    update_time_ms: 33.973
  timestamp: 1602819433
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: 1bbc1_00000
  
2020-10-16 03:37:14,349	WARNING util.py:136 -- The `process_trial` operation took 0.9381630420684814 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    352 |           9092.5 | 56950784 |  306.331 |              322.949 |              165.677 |             790.57 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3017.611734863606
    time_step_min: 2911
  date: 2020-10-16_03-37-40
  done: false
  episode_len_mean: 790.6020478856066
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.37490839783953
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 218
  episodes_total: 72172
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7885610920694958e-36
        cur_lr: 5.0e-05
        entropy: 0.06211204081773758
        entropy_coeff: 0.0005000000000000001
        kl: 0.006502186957125862
        model: {}
        policy_loss: -0.008438337751916455
        total_loss: 0.1992665355404218
        vf_explained_var: 0.9996212124824524
        vf_loss: 0.2077359358469645
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.509677419354844
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466950722419139
    mean_env_wait_ms: 1.188808311841152
    mean_inference_ms: 4.313713056839664
    mean_raw_obs_processing_ms: 0.3782760479754113
  time_since_restore: 9118.568988323212
  time_this_iter_s: 26.065155267715454
  time_total_s: 9118.568988323212
  timers:
    learn_throughput: 8634.767
    learn_time_ms: 18737.275
    sample_throughput: 24011.877
    sample_time_ms: 6737.999
    update_time_ms: 34.07
  timestamp: 1602819460
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: 1bbc1_00000
  
2020-10-16 03:37:41,843	WARNING util.py:136 -- The `process_trial` operation took 0.9935140609741211 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    353 |          9118.57 | 57112576 |  306.375 |              322.949 |              165.677 |            790.602 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3017.376489617607
    time_step_min: 2911
  date: 2020-10-16_03-38-07
  done: false
  episode_len_mean: 790.6312014593295
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.41229938477085
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 190
  episodes_total: 72362
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7885610920694958e-36
        cur_lr: 5.0e-05
        entropy: 0.06209659390151501
        entropy_coeff: 0.0005000000000000001
        kl: 0.004317862059300144
        model: {}
        policy_loss: -0.006922178290551528
        total_loss: 0.5691508377591769
        vf_explained_var: 0.9988099932670593
        vf_loss: 0.57610405733188
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11935483870968
    gpu_util_percent0: 0.29193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669407060203835
    mean_env_wait_ms: 1.188744594839509
    mean_inference_ms: 4.313656196423651
    mean_raw_obs_processing_ms: 0.3782715772917616
  time_since_restore: 9144.300641536713
  time_this_iter_s: 25.731653213500977
  time_total_s: 9144.300641536713
  timers:
    learn_throughput: 8636.332
    learn_time_ms: 18733.879
    sample_throughput: 24051.512
    sample_time_ms: 6726.895
    update_time_ms: 35.267
  timestamp: 1602819487
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: 1bbc1_00000
  
2020-10-16 03:38:09,040	WARNING util.py:136 -- The `process_trial` operation took 0.9377143383026123 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    354 |           9144.3 | 57274368 |  306.412 |              322.949 |              165.677 |            790.631 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3017.149901407868
    time_step_min: 2911
  date: 2020-10-16_03-38-34
  done: false
  episode_len_mean: 790.6531861224827
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.44721039113773
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 72549
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3942805460347479e-36
        cur_lr: 5.0e-05
        entropy: 0.06777458575864632
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009597537321193764
        total_loss: .inf
        vf_explained_var: 0.9994678497314453
        vf_loss: 0.26170532902081806
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809677419354845
    gpu_util_percent0: 0.32935483870967747
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466930413737605
    mean_env_wait_ms: 1.1886808723073818
    mean_inference_ms: 4.313596123624827
    mean_raw_obs_processing_ms: 0.3782678665151448
  time_since_restore: 9170.15958070755
  time_this_iter_s: 25.858939170837402
  time_total_s: 9170.15958070755
  timers:
    learn_throughput: 8636.896
    learn_time_ms: 18732.655
    sample_throughput: 24070.242
    sample_time_ms: 6721.661
    update_time_ms: 33.875
  timestamp: 1602819514
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:38:36,438	WARNING util.py:136 -- The `process_trial` operation took 1.001654863357544 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    355 |          9170.16 | 57436160 |  306.447 |              322.949 |              165.677 |            790.653 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3016.8689872286604
    time_step_min: 2911
  date: 2020-10-16_03-39-02
  done: false
  episode_len_mean: 790.6833129491954
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.48625531656745
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 220
  episodes_total: 72769
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0914208190521216e-36
        cur_lr: 5.0e-05
        entropy: 0.07243639975786209
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009421380828522766
        total_loss: .inf
        vf_explained_var: 0.9986994862556458
        vf_loss: 0.6844395101070404
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.806451612903224
    gpu_util_percent0: 0.27741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669126747401948
    mean_env_wait_ms: 1.1886023112718263
    mean_inference_ms: 4.313520118535328
    mean_raw_obs_processing_ms: 0.37826188916268855
  time_since_restore: 9196.20023560524
  time_this_iter_s: 26.04065489768982
  time_total_s: 9196.20023560524
  timers:
    learn_throughput: 8628.846
    learn_time_ms: 18750.132
    sample_throughput: 24091.066
    sample_time_ms: 6715.851
    update_time_ms: 33.735
  timestamp: 1602819542
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:39:03,828	WARNING util.py:136 -- The `process_trial` operation took 0.9862396717071533 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    356 |           9196.2 | 57597952 |  306.486 |              322.949 |              165.677 |            790.683 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3016.6214615690415
    time_step_min: 2911
  date: 2020-10-16_03-39-29
  done: false
  episode_len_mean: 790.7132795264262
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.5211163113102
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 208
  episodes_total: 72977
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.137131228578183e-36
        cur_lr: 5.0e-05
        entropy: 0.06528707904120286
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008200612869889786
        total_loss: .inf
        vf_explained_var: 0.9989919066429138
        vf_loss: 0.5407980158925056
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83225806451613
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669040060846733
    mean_env_wait_ms: 1.1885333037627352
    mean_inference_ms: 4.313458618379246
    mean_raw_obs_processing_ms: 0.3782576554624809
  time_since_restore: 9222.211984872818
  time_this_iter_s: 26.011749267578125
  time_total_s: 9222.211984872818
  timers:
    learn_throughput: 8619.633
    learn_time_ms: 18770.173
    sample_throughput: 24089.331
    sample_time_ms: 6716.334
    update_time_ms: 36.047
  timestamp: 1602819569
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:39:31,210	WARNING util.py:136 -- The `process_trial` operation took 0.9889264106750488 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    357 |          9222.21 | 57759744 |  306.521 |              322.949 |              165.677 |            790.713 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3016.38631298284
    time_step_min: 2911
  date: 2020-10-16_03-39-57
  done: false
  episode_len_mean: 790.7414950179735
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.55708928327573
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 186
  episodes_total: 73163
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.705696842867275e-36
        cur_lr: 5.0e-05
        entropy: 0.0621847960477074
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006924376044480596
        total_loss: .inf
        vf_explained_var: 0.9994600415229797
        vf_loss: 0.2600783494611581
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.256666666666668
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668914344154277
    mean_env_wait_ms: 1.188469824438213
    mean_inference_ms: 4.31339928210746
    mean_raw_obs_processing_ms: 0.37825317557234045
  time_since_restore: 9248.037960290909
  time_this_iter_s: 25.82597541809082
  time_total_s: 9248.037960290909
  timers:
    learn_throughput: 8627.018
    learn_time_ms: 18754.105
    sample_throughput: 24082.626
    sample_time_ms: 6718.204
    update_time_ms: 37.534
  timestamp: 1602819597
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:39:58,473	WARNING util.py:136 -- The `process_trial` operation took 0.9971933364868164 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    358 |          9248.04 | 57921536 |  306.557 |              322.949 |              165.677 |            790.741 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3016.1378703792084
    time_step_min: 2911
  date: 2020-10-16_03-40-24
  done: false
  episode_len_mean: 790.771035234785
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.59440035191386
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 202
  episodes_total: 73365
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.058545264300914e-36
        cur_lr: 5.0e-05
        entropy: 0.0684346320728461
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008495489251799881
        total_loss: .inf
        vf_explained_var: 0.9991259574890137
        vf_loss: 0.453615481654803
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.529032258064518
    gpu_util_percent0: 0.3248387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668812725620542
    mean_env_wait_ms: 1.1884005021561275
    mean_inference_ms: 4.3133364046281315
    mean_raw_obs_processing_ms: 0.378249052461037
  time_since_restore: 9273.95045542717
  time_this_iter_s: 25.912495136260986
  time_total_s: 9273.95045542717
  timers:
    learn_throughput: 8622.952
    learn_time_ms: 18762.947
    sample_throughput: 24091.595
    sample_time_ms: 6715.703
    update_time_ms: 37.305
  timestamp: 1602819624
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:40:25,861	WARNING util.py:136 -- The `process_trial` operation took 1.0278825759887695 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    359 |          9273.95 | 58083328 |  306.594 |              322.949 |              165.677 |            790.771 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3015.8641597672554
    time_step_min: 2911
  date: 2020-10-16_03-40-51
  done: false
  episode_len_mean: 790.8030984575661
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.635294303365
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 220
  episodes_total: 73585
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0587817896451371e-35
        cur_lr: 5.0e-05
        entropy: 0.06529451409975688
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009178172714503793
        total_loss: .inf
        vf_explained_var: 0.9995601177215576
        vf_loss: 0.2463274635374546
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.348387096774193
    gpu_util_percent0: 0.3212903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466862763384896
    mean_env_wait_ms: 1.188324674210369
    mean_inference_ms: 4.313261291786143
    mean_raw_obs_processing_ms: 0.37824339920767247
  time_since_restore: 9299.847817897797
  time_this_iter_s: 25.89736247062683
  time_total_s: 9299.847817897797
  timers:
    learn_throughput: 8631.095
    learn_time_ms: 18745.247
    sample_throughput: 24079.209
    sample_time_ms: 6719.158
    update_time_ms: 37.663
  timestamp: 1602819651
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:40:53,159	WARNING util.py:136 -- The `process_trial` operation took 0.9459655284881592 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    360 |          9299.85 | 58245120 |  306.635 |              322.949 |              165.677 |            790.803 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3015.618239258064
    time_step_min: 2911
  date: 2020-10-16_03-41-19
  done: false
  episode_len_mean: 790.8342120600155
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.67307000146053
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 73781
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.588172684467705e-35
        cur_lr: 5.0e-05
        entropy: 0.06324732645104329
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007822855492122471
        total_loss: .inf
        vf_explained_var: 0.9996709227561951
        vf_loss: 0.16709677626689276
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.803125
    gpu_util_percent0: 0.33437500000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668535757234985
    mean_env_wait_ms: 1.188259788838282
    mean_inference_ms: 4.313207308446701
    mean_raw_obs_processing_ms: 0.3782390298994846
  time_since_restore: 9325.843253135681
  time_this_iter_s: 25.99543523788452
  time_total_s: 9325.843253135681
  timers:
    learn_throughput: 8630.012
    learn_time_ms: 18747.598
    sample_throughput: 24058.403
    sample_time_ms: 6724.968
    update_time_ms: 37.407
  timestamp: 1602819679
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:41:20,706	WARNING util.py:136 -- The `process_trial` operation took 1.0163519382476807 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    361 |          9325.84 | 58406912 |  306.673 |              322.949 |              165.677 |            790.834 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3015.411556252874
    time_step_min: 2911
  date: 2020-10-16_03-41-46
  done: false
  episode_len_mean: 790.8628890511344
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.70547980002806
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 73962
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.382259026701558e-35
        cur_lr: 5.0e-05
        entropy: 0.06493112320701282
        entropy_coeff: 0.0005000000000000001
        kl: 0.005427637331498166
        model: {}
        policy_loss: -0.009427838556500015
        total_loss: 0.2387081185976664
        vf_explained_var: 0.9994760155677795
        vf_loss: 0.24816841880480447
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33870967741936
    gpu_util_percent0: 0.32225806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668440863615334
    mean_env_wait_ms: 1.1881987092029642
    mean_inference_ms: 4.3131523792302975
    mean_raw_obs_processing_ms: 0.37823527810639906
  time_since_restore: 9352.089344263077
  time_this_iter_s: 26.24609112739563
  time_total_s: 9352.089344263077
  timers:
    learn_throughput: 8617.567
    learn_time_ms: 18774.673
    sample_throughput: 24039.672
    sample_time_ms: 6730.208
    update_time_ms: 38.948
  timestamp: 1602819706
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: 1bbc1_00000
  
2020-10-16 03:41:48,409	WARNING util.py:136 -- The `process_trial` operation took 1.0420455932617188 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    362 |          9352.09 | 58568704 |  306.705 |              322.949 |              165.677 |            790.863 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3015.1481726230613
    time_step_min: 2911
  date: 2020-10-16_03-42-14
  done: false
  episode_len_mean: 790.8977594435007
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.74595465289445
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 74178
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.382259026701558e-35
        cur_lr: 5.0e-05
        entropy: 0.06736721346775691
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006619279971346259
        total_loss: .inf
        vf_explained_var: 0.9995148181915283
        vf_loss: 0.2531753033399582
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64516129032258
    gpu_util_percent0: 0.3387096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466830807097009
    mean_env_wait_ms: 1.1881237327983223
    mean_inference_ms: 4.313084992199546
    mean_raw_obs_processing_ms: 0.3782305784102461
  time_since_restore: 9377.908644914627
  time_this_iter_s: 25.819300651550293
  time_total_s: 9377.908644914627
  timers:
    learn_throughput: 8624.544
    learn_time_ms: 18759.484
    sample_throughput: 24029.82
    sample_time_ms: 6732.968
    update_time_ms: 36.689
  timestamp: 1602819734
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:42:15,741	WARNING util.py:136 -- The `process_trial` operation took 1.0648930072784424 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    363 |          9377.91 | 58730496 |  306.746 |              322.949 |              165.677 |            790.898 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3014.8831621127365
    time_step_min: 2911
  date: 2020-10-16_03-42-41
  done: false
  episode_len_mean: 790.9311253293188
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.78533167580935
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 218
  episodes_total: 74396
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.573388540052337e-35
        cur_lr: 5.0e-05
        entropy: 0.06549424926439922
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009224426745883344
        total_loss: .inf
        vf_explained_var: 0.9993183612823486
        vf_loss: 0.34117335081100464
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.145161290322584
    gpu_util_percent0: 0.34161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466818199748378
    mean_env_wait_ms: 1.188050587684181
    mean_inference_ms: 4.313017040086886
    mean_raw_obs_processing_ms: 0.37822552700033346
  time_since_restore: 9403.668723344803
  time_this_iter_s: 25.76007843017578
  time_total_s: 9403.668723344803
  timers:
    learn_throughput: 8625.929
    learn_time_ms: 18756.473
    sample_throughput: 24004.093
    sample_time_ms: 6740.184
    update_time_ms: 34.879
  timestamp: 1602819761
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:42:43,056	WARNING util.py:136 -- The `process_trial` operation took 0.9644608497619629 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    364 |          9403.67 | 58892288 |  306.785 |              322.949 |              165.677 |            790.931 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3014.6997987927566
    time_step_min: 2911
  date: 2020-10-16_03-43-09
  done: false
  episode_len_mean: 790.9553755799298
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.81446000133786
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 182
  episodes_total: 74578
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.360082810078505e-35
        cur_lr: 5.0e-05
        entropy: 0.06392804719507694
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009809529983613174
        total_loss: .inf
        vf_explained_var: 0.9992227554321289
        vf_loss: 0.3757939661542575
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.945161290322584
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668059170145026
    mean_env_wait_ms: 1.187990310883073
    mean_inference_ms: 4.312964413876201
    mean_raw_obs_processing_ms: 0.3782214614444257
  time_since_restore: 9429.701478242874
  time_this_iter_s: 26.03275489807129
  time_total_s: 9429.701478242874
  timers:
    learn_throughput: 8619.949
    learn_time_ms: 18769.485
    sample_throughput: 23993.705
    sample_time_ms: 6743.102
    update_time_ms: 34.957
  timestamp: 1602819789
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:43:10,577	WARNING util.py:136 -- The `process_trial` operation took 1.0348994731903076 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    365 |           9429.7 | 59054080 |  306.814 |              322.949 |              165.677 |            790.955 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3014.459101490381
    time_step_min: 2911
  date: 2020-10-16_03-43-36
  done: false
  episode_len_mean: 790.9845802016744
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.8500040391069
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 74774
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.040124215117759e-35
        cur_lr: 5.0e-05
        entropy: 0.06723917461931705
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009365666369073248
        total_loss: .inf
        vf_explained_var: 0.9993841648101807
        vf_loss: 0.3067277669906616
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.54193548387097
    gpu_util_percent0: 0.29193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466796578714767
    mean_env_wait_ms: 1.1879240250457577
    mean_inference_ms: 4.312906739966425
    mean_raw_obs_processing_ms: 0.37821759297441715
  time_since_restore: 9455.546560525894
  time_this_iter_s: 25.84508228302002
  time_total_s: 9455.546560525894
  timers:
    learn_throughput: 8624.843
    learn_time_ms: 18758.833
    sample_throughput: 24020.369
    sample_time_ms: 6735.617
    update_time_ms: 34.455
  timestamp: 1602819816
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:43:37,756	WARNING util.py:136 -- The `process_trial` operation took 0.9587187767028809 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    366 |          9455.55 | 59215872 |   306.85 |              322.949 |              165.677 |            790.985 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3014.2150517554155
    time_step_min: 2911
  date: 2020-10-16_03-44-03
  done: false
  episode_len_mean: 791.0143340978185
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.8850785846622
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 74996
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2060186322676639e-34
        cur_lr: 5.0e-05
        entropy: 0.07023573356370132
        entropy_coeff: 0.0005000000000000001
        kl: 0.0067635611242925124
        model: {}
        policy_loss: -0.009910946996872857
        total_loss: 0.710472822189331
        vf_explained_var: 0.9986626505851746
        vf_loss: 0.7204188853502274
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.354838709677423
    gpu_util_percent0: 0.2841935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466779167420136
    mean_env_wait_ms: 1.1878476101702533
    mean_inference_ms: 4.312832286243774
    mean_raw_obs_processing_ms: 0.37821186357706627
  time_since_restore: 9481.606576442719
  time_this_iter_s: 26.06001591682434
  time_total_s: 9481.606576442719
  timers:
    learn_throughput: 8624.231
    learn_time_ms: 18760.166
    sample_throughput: 24005.03
    sample_time_ms: 6739.921
    update_time_ms: 33.637
  timestamp: 1602819843
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: 1bbc1_00000
  
2020-10-16 03:44:05,213	WARNING util.py:136 -- The `process_trial` operation took 1.0388810634613037 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    367 |          9481.61 | 59377664 |  306.885 |              322.949 |              165.677 |            791.014 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3013.99392012346
    time_step_min: 2911
  date: 2020-10-16_03-44-31
  done: false
  episode_len_mean: 791.0405218501476
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.9161678223303
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 75194
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2060186322676639e-34
        cur_lr: 5.0e-05
        entropy: 0.06952020960549514
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009120030086099481
        total_loss: .inf
        vf_explained_var: 0.9990822672843933
        vf_loss: 0.4729053005576134
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58709677419355
    gpu_util_percent0: 0.3274193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466772215146353
    mean_env_wait_ms: 1.187782834880301
    mean_inference_ms: 4.312783874164347
    mean_raw_obs_processing_ms: 0.37820826205421476
  time_since_restore: 9507.471228599548
  time_this_iter_s: 25.864652156829834
  time_total_s: 9507.471228599548
  timers:
    learn_throughput: 8629.436
    learn_time_ms: 18748.85
    sample_throughput: 23983.083
    sample_time_ms: 6746.089
    update_time_ms: 33.778
  timestamp: 1602819871
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:44:32,474	WARNING util.py:136 -- The `process_trial` operation took 1.0422735214233398 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    368 |          9507.47 | 59539456 |  306.916 |              322.949 |              165.677 |            791.041 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3013.8609573861
    time_step_min: 2911
  date: 2020-10-16_03-44-58
  done: false
  episode_len_mean: 791.0597115907614
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.93723046139473
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 75379
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8090279484014956e-34
        cur_lr: 5.0e-05
        entropy: 0.07797875814139843
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010856268752831966
        total_loss: .inf
        vf_explained_var: 0.9974110126495361
        vf_loss: 1.2243152409791946
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.135483870967747
    gpu_util_percent0: 0.27290322580645165
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667604216659452
    mean_env_wait_ms: 1.1877207882157093
    mean_inference_ms: 4.31272652040989
    mean_raw_obs_processing_ms: 0.37820407679092816
  time_since_restore: 9533.611396074295
  time_this_iter_s: 26.140167474746704
  time_total_s: 9533.611396074295
  timers:
    learn_throughput: 8620.85
    learn_time_ms: 18767.524
    sample_throughput: 23979.942
    sample_time_ms: 6746.972
    update_time_ms: 34.185
  timestamp: 1602819898
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:45:00,021	WARNING util.py:136 -- The `process_trial` operation took 1.0436882972717285 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    369 |          9533.61 | 59701248 |  306.937 |              322.949 |              165.677 |             791.06 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3013.7473135355463
    time_step_min: 2911
  date: 2020-10-16_03-45-26
  done: false
  episode_len_mean: 791.0727722510319
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.9487958214805
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 213
  episodes_total: 75592
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7135419226022434e-34
        cur_lr: 5.0e-05
        entropy: 0.08069673801461856
        entropy_coeff: 0.0005000000000000001
        kl: 0.004844478137480716
        model: {}
        policy_loss: -0.01185156933934195
        total_loss: 1.700413187344869
        vf_explained_var: 0.9967475533485413
        vf_loss: 1.7123051782449086
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.238709677419358
    gpu_util_percent0: 0.2909677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466749053862261
    mean_env_wait_ms: 1.18764899513995
    mean_inference_ms: 4.312664427340324
    mean_raw_obs_processing_ms: 0.3781998465474861
  time_since_restore: 9559.702338933945
  time_this_iter_s: 26.090942859649658
  time_total_s: 9559.702338933945
  timers:
    learn_throughput: 8617.518
    learn_time_ms: 18774.78
    sample_throughput: 23931.403
    sample_time_ms: 6760.657
    update_time_ms: 33.362
  timestamp: 1602819926
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: 1bbc1_00000
  
2020-10-16 03:45:27,538	WARNING util.py:136 -- The `process_trial` operation took 0.9685702323913574 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    370 |           9559.7 | 59863040 |  306.949 |              322.949 |              165.677 |            791.073 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3013.5487761430363
    time_step_min: 2911
  date: 2020-10-16_03-45-53
  done: false
  episode_len_mean: 791.0992837639983
  episode_reward_max: 322.949494949495
  episode_reward_mean: 306.98022606660936
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 75813
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3567709613011217e-34
        cur_lr: 5.0e-05
        entropy: 0.06880473407606284
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007173550123601065
        total_loss: .inf
        vf_explained_var: 0.9983487129211426
        vf_loss: 0.8664796749750773
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.241935483870968
    gpu_util_percent0: 0.38064516129032255
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667348382086967
    mean_env_wait_ms: 1.1875755880887915
    mean_inference_ms: 4.312598191835009
    mean_raw_obs_processing_ms: 0.378194859133695
  time_since_restore: 9585.572757959366
  time_this_iter_s: 25.870419025421143
  time_total_s: 9585.572757959366
  timers:
    learn_throughput: 8626.298
    learn_time_ms: 18755.671
    sample_throughput: 23910.616
    sample_time_ms: 6766.534
    update_time_ms: 33.604
  timestamp: 1602819953
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:45:54,870	WARNING util.py:136 -- The `process_trial` operation took 0.9968559741973877 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    371 |          9585.57 | 60024832 |   306.98 |              322.949 |              165.677 |            791.099 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3013.3348163748847
    time_step_min: 2911
  date: 2020-10-16_03-46-20
  done: false
  episode_len_mean: 791.1215953051396
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.01298705096144
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 75998
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.035156441951683e-34
        cur_lr: 5.0e-05
        entropy: 0.06150625925511122
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007394993117486592
        total_loss: .inf
        vf_explained_var: 0.9995014071464539
        vf_loss: 0.22858222077290216
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93225806451613
    gpu_util_percent0: 0.32161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667244357061157
    mean_env_wait_ms: 1.187515586414101
    mean_inference_ms: 4.312547201119676
    mean_raw_obs_processing_ms: 0.3781908269748562
  time_since_restore: 9611.433020591736
  time_this_iter_s: 25.860262632369995
  time_total_s: 9611.433020591736
  timers:
    learn_throughput: 8644.405
    learn_time_ms: 18716.384
    sample_throughput: 23908.009
    sample_time_ms: 6767.272
    update_time_ms: 31.183
  timestamp: 1602819980
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:46:22,157	WARNING util.py:136 -- The `process_trial` operation took 0.9795749187469482 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    372 |          9611.43 | 60186624 |  307.013 |              322.949 |              165.677 |            791.122 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3013.116958833957
    time_step_min: 2911
  date: 2020-10-16_03-46-48
  done: false
  episode_len_mean: 791.1462531010856
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.0459986234631
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 76183
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.052734662927524e-34
        cur_lr: 5.0e-05
        entropy: 0.06329976487904787
        entropy_coeff: 0.0005000000000000001
        kl: 0.005311879989070197
        model: {}
        policy_loss: -0.007558476691580533
        total_loss: 0.20810727526744208
        vf_explained_var: 0.9995428919792175
        vf_loss: 0.21569739654660225
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.729032258064517
    gpu_util_percent0: 0.29935483870967744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667166195379575
    mean_env_wait_ms: 1.1874545676130552
    mean_inference_ms: 4.312494746851682
    mean_raw_obs_processing_ms: 0.37818713166944046
  time_since_restore: 9637.425370693207
  time_this_iter_s: 25.992350101470947
  time_total_s: 9637.425370693207
  timers:
    learn_throughput: 8639.488
    learn_time_ms: 18727.036
    sample_throughput: 23889.303
    sample_time_ms: 6772.571
    update_time_ms: 31.112
  timestamp: 1602820008
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: 1bbc1_00000
  
2020-10-16 03:46:49,557	WARNING util.py:136 -- The `process_trial` operation took 1.036574363708496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    373 |          9637.43 | 60348416 |  307.046 |              322.949 |              165.677 |            791.146 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3012.8789033347734
    time_step_min: 2911
  date: 2020-10-16_03-47-15
  done: false
  episode_len_mean: 791.1740331130162
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.083515608939
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 76405
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.052734662927524e-34
        cur_lr: 5.0e-05
        entropy: 0.06573066146423419
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008088945527561009
        total_loss: .inf
        vf_explained_var: 0.9993383884429932
        vf_loss: 0.3424763505657514
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.635483870967743
    gpu_util_percent0: 0.29645161290322586
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667013084830485
    mean_env_wait_ms: 1.1873787100152458
    mean_inference_ms: 4.312425481536059
    mean_raw_obs_processing_ms: 0.37818198522404134
  time_since_restore: 9663.397356748581
  time_this_iter_s: 25.971986055374146
  time_total_s: 9663.397356748581
  timers:
    learn_throughput: 8634.658
    learn_time_ms: 18737.511
    sample_throughput: 23865.219
    sample_time_ms: 6779.405
    update_time_ms: 33.178
  timestamp: 1602820035
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:47:16,978	WARNING util.py:136 -- The `process_trial` operation took 0.9935784339904785 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    374 |           9663.4 | 60510208 |  307.084 |              322.949 |              165.677 |            791.174 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3012.6376230642677
    time_step_min: 2911
  date: 2020-10-16_03-47-42
  done: false
  episode_len_mean: 791.2013339598507
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.1184952350663
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 209
  episodes_total: 76614
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.579101994391287e-34
        cur_lr: 5.0e-05
        entropy: 0.07117812956372897
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006101678746442
        total_loss: .inf
        vf_explained_var: 0.999302089214325
        vf_loss: 0.34660738706588745
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.674193548387098
    gpu_util_percent0: 0.32903225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466691407114232
    mean_env_wait_ms: 1.1873113661281718
    mean_inference_ms: 4.312373441916142
    mean_raw_obs_processing_ms: 0.37817800233259036
  time_since_restore: 9689.283843755722
  time_this_iter_s: 25.886487007141113
  time_total_s: 9689.283843755722
  timers:
    learn_throughput: 8643.787
    learn_time_ms: 18717.722
    sample_throughput: 23852.643
    sample_time_ms: 6782.98
    update_time_ms: 32.982
  timestamp: 1602820062
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:47:44,288	WARNING util.py:136 -- The `process_trial` operation took 0.9831352233886719 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    375 |          9689.28 | 60672000 |  307.118 |              322.949 |              165.677 |            791.201 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3012.4489559315853
    time_step_min: 2911
  date: 2020-10-16_03-48-10
  done: false
  episode_len_mean: 791.2230874405885
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.1455008447648
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 76795
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.868652991586929e-34
        cur_lr: 5.0e-05
        entropy: 0.07228361380596955
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008987027065207561
        total_loss: .inf
        vf_explained_var: 0.9989683032035828
        vf_loss: 0.4821588223179181
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19375
    gpu_util_percent0: 0.31187499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666792950825938
    mean_env_wait_ms: 1.187252207649007
    mean_inference_ms: 4.312318954702693
    mean_raw_obs_processing_ms: 0.3781739003099452
  time_since_restore: 9715.31230211258
  time_this_iter_s: 26.0284583568573
  time_total_s: 9715.31230211258
  timers:
    learn_throughput: 8636.267
    learn_time_ms: 18734.021
    sample_throughput: 23819.86
    sample_time_ms: 6792.315
    update_time_ms: 33.535
  timestamp: 1602820090
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:48:11,908	WARNING util.py:136 -- The `process_trial` operation took 1.0423030853271484 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    376 |          9715.31 | 60833792 |  307.146 |              322.949 |              165.677 |            791.223 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3012.2459595697137
    time_step_min: 2911
  date: 2020-10-16_03-48-37
  done: false
  episode_len_mean: 791.244012987013
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.17500262363865
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 205
  episodes_total: 77000
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0302979487380394e-33
        cur_lr: 5.0e-05
        entropy: 0.07184645719826221
        entropy_coeff: 0.0005000000000000001
        kl: 0.004243108890174578
        model: {}
        policy_loss: -0.007668345303197081
        total_loss: 0.4759902358055115
        vf_explained_var: 0.9990527629852295
        vf_loss: 0.4836945061882337
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.341935483870973
    gpu_util_percent0: 0.29516129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666711483303949
    mean_env_wait_ms: 1.1871844017188011
    mean_inference_ms: 4.312263403348376
    mean_raw_obs_processing_ms: 0.37817030549917124
  time_since_restore: 9741.280350208282
  time_this_iter_s: 25.968048095703125
  time_total_s: 9741.280350208282
  timers:
    learn_throughput: 8640.329
    learn_time_ms: 18725.212
    sample_throughput: 23788.79
    sample_time_ms: 6801.187
    update_time_ms: 32.275
  timestamp: 1602820117
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: 1bbc1_00000
  
2020-10-16 03:48:39,463	WARNING util.py:136 -- The `process_trial` operation took 1.027944803237915 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    377 |          9741.28 | 60995584 |  307.175 |              322.949 |              165.677 |            791.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3012.0161798043914
    time_step_min: 2911
  date: 2020-10-16_03-49-05
  done: false
  episode_len_mean: 791.2768346218096
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.2111237074522
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 77223
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.151489743690197e-34
        cur_lr: 5.0e-05
        entropy: 0.06725240126252174
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0070026257502225535
        total_loss: .inf
        vf_explained_var: 0.9993100762367249
        vf_loss: 0.37807948142290115
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925806451612907
    gpu_util_percent0: 0.26161290322580644
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466655711793793
    mean_env_wait_ms: 1.1871109259141466
    mean_inference_ms: 4.312198937846459
    mean_raw_obs_processing_ms: 0.37816507157039214
  time_since_restore: 9767.144984483719
  time_this_iter_s: 25.8646342754364
  time_total_s: 9767.144984483719
  timers:
    learn_throughput: 8639.62
    learn_time_ms: 18726.75
    sample_throughput: 23797.087
    sample_time_ms: 6798.815
    update_time_ms: 30.909
  timestamp: 1602820145
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:49:06,784	WARNING util.py:136 -- The `process_trial` operation took 1.0316386222839355 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    378 |          9767.14 | 61157376 |  307.211 |              322.949 |              165.677 |            791.277 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3011.787589807205
    time_step_min: 2911
  date: 2020-10-16_03-49-32
  done: false
  episode_len_mean: 791.3090704763873
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.2465708025797
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 193
  episodes_total: 77416
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.727234615535294e-34
        cur_lr: 5.0e-05
        entropy: 0.06555107608437538
        entropy_coeff: 0.0005000000000000001
        kl: 0.00622628132502238
        model: {}
        policy_loss: -0.006595247124399369
        total_loss: 0.09964095739026864
        vf_explained_var: 0.9997865557670593
        vf_loss: 0.10626897712548573
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.335483870967746
    gpu_util_percent0: 0.2770967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666459836441248
    mean_env_wait_ms: 1.1870496438777902
    mean_inference_ms: 4.312151055943894
    mean_raw_obs_processing_ms: 0.3781612993674394
  time_since_restore: 9793.062542676926
  time_this_iter_s: 25.917558193206787
  time_total_s: 9793.062542676926
  timers:
    learn_throughput: 8653.2
    learn_time_ms: 18697.36
    sample_throughput: 23791.393
    sample_time_ms: 6800.443
    update_time_ms: 28.201
  timestamp: 1602820172
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: 1bbc1_00000
  
2020-10-16 03:49:34,093	WARNING util.py:136 -- The `process_trial` operation took 1.0185654163360596 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    379 |          9793.06 | 61319168 |  307.247 |              322.949 |              165.677 |            791.309 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3011.660371521019
    time_step_min: 2911
  date: 2020-10-16_03-50-00
  done: false
  episode_len_mean: 791.3260138400278
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.25993807483707
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 185
  episodes_total: 77601
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.727234615535294e-34
        cur_lr: 5.0e-05
        entropy: 0.10033312750359376
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011113274105203649
        total_loss: .inf
        vf_explained_var: 0.995561420917511
        vf_loss: 2.094510078430176
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.987096774193553
    gpu_util_percent0: 0.3048387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666376620502486
    mean_env_wait_ms: 1.186989127510612
    mean_inference_ms: 4.3121010536473054
    mean_raw_obs_processing_ms: 0.3781578275165008
  time_since_restore: 9819.132276296616
  time_this_iter_s: 26.06973361968994
  time_total_s: 9819.132276296616
  timers:
    learn_throughput: 8656.082
    learn_time_ms: 18691.135
    sample_throughput: 23807.269
    sample_time_ms: 6795.908
    update_time_ms: 26.776
  timestamp: 1602820200
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:50:01,544	WARNING util.py:136 -- The `process_trial` operation took 1.011587142944336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    380 |          9819.13 | 61480960 |   307.26 |              322.949 |              165.677 |            791.326 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3011.7759409707937
    time_step_min: 2911
  date: 2020-10-16_03-50-27
  done: false
  episode_len_mean: 791.3200848111026
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.24237751454376
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 219
  episodes_total: 77820
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1590851923302944e-33
        cur_lr: 5.0e-05
        entropy: 0.1066354364156723
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010027175662495816
        total_loss: .inf
        vf_explained_var: 0.9944650530815125
        vf_loss: 3.1392609675725303
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.954838709677418
    gpu_util_percent0: 0.32451612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146662253329228
    mean_env_wait_ms: 1.1869155648403211
    mean_inference_ms: 4.31203148021942
    mean_raw_obs_processing_ms: 0.37815280561396997
  time_since_restore: 9845.221777200699
  time_this_iter_s: 26.089500904083252
  time_total_s: 9845.221777200699
  timers:
    learn_throughput: 8648.819
    learn_time_ms: 18706.832
    sample_throughput: 23783.56
    sample_time_ms: 6802.682
    update_time_ms: 24.882
  timestamp: 1602820227
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:50:29,215	WARNING util.py:136 -- The `process_trial` operation took 1.1139957904815674 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    381 |          9845.22 | 61642752 |  307.242 |              322.949 |              165.677 |             791.32 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3011.6944159573377
    time_step_min: 2911
  date: 2020-10-16_03-50-55
  done: false
  episode_len_mean: 791.3343969449998
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.25821532253156
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 78036
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7386277884954418e-33
        cur_lr: 5.0e-05
        entropy: 0.08163026161491871
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012205093681889897
        total_loss: .inf
        vf_explained_var: 0.9979825615882874
        vf_loss: 1.0341143260399501
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.015625
    gpu_util_percent0: 0.296875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666138041121543
    mean_env_wait_ms: 1.1868480558190921
    mean_inference_ms: 4.311980315507289
    mean_raw_obs_processing_ms: 0.3781488133025188
  time_since_restore: 9871.177068710327
  time_this_iter_s: 25.955291509628296
  time_total_s: 9871.177068710327
  timers:
    learn_throughput: 8649.613
    learn_time_ms: 18705.115
    sample_throughput: 23784.157
    sample_time_ms: 6802.511
    update_time_ms: 26.945
  timestamp: 1602820255
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:50:56,664	WARNING util.py:136 -- The `process_trial` operation took 1.0719397068023682 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    382 |          9871.18 | 61804544 |  307.258 |              322.949 |              165.677 |            791.334 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3011.510890557254
    time_step_min: 2911
  date: 2020-10-16_03-51-22
  done: false
  episode_len_mean: 791.3554433292846
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.285949051339
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 78215
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.607941682743162e-33
        cur_lr: 5.0e-05
        entropy: 0.06956563952068488
        entropy_coeff: 0.0005000000000000001
        kl: 0.005288331924627225
        model: {}
        policy_loss: -0.009907572670878531
        total_loss: 0.30065538237492245
        vf_explained_var: 0.9993212819099426
        vf_loss: 0.3105977301796277
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95483870967742
    gpu_util_percent0: 0.30774193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666019464823465
    mean_env_wait_ms: 1.1867911900051966
    mean_inference_ms: 4.3119315466543595
    mean_raw_obs_processing_ms: 0.3781452979028981
  time_since_restore: 9897.106420516968
  time_this_iter_s: 25.929351806640625
  time_total_s: 9897.106420516968
  timers:
    learn_throughput: 8654.547
    learn_time_ms: 18694.45
    sample_throughput: 23810.691
    sample_time_ms: 6794.931
    update_time_ms: 28.661
  timestamp: 1602820282
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: 1bbc1_00000
  
2020-10-16 03:51:24,053	WARNING util.py:136 -- The `process_trial` operation took 1.0777881145477295 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    383 |          9897.11 | 61966336 |  307.286 |              322.949 |              165.677 |            791.355 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3011.2830845771146
    time_step_min: 2911
  date: 2020-10-16_03-51-49
  done: false
  episode_len_mean: 791.3762018924226
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.3211991887038
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 203
  episodes_total: 78418
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.607941682743162e-33
        cur_lr: 5.0e-05
        entropy: 0.07308647160728772
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006804542674217373
        total_loss: .inf
        vf_explained_var: 0.9993279576301575
        vf_loss: 0.35300852606693905
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.09677419354839
    gpu_util_percent0: 0.2803225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665937978294946
    mean_env_wait_ms: 1.1867251296316013
    mean_inference_ms: 4.3118811476331524
    mean_raw_obs_processing_ms: 0.3781418263650955
  time_since_restore: 9923.02475643158
  time_this_iter_s: 25.918335914611816
  time_total_s: 9923.02475643158
  timers:
    learn_throughput: 8656.182
    learn_time_ms: 18690.919
    sample_throughput: 23853.331
    sample_time_ms: 6782.784
    update_time_ms: 28.246
  timestamp: 1602820309
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:51:51,444	WARNING util.py:136 -- The `process_trial` operation took 1.0917894840240479 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    384 |          9923.02 | 62128128 |  307.321 |              322.949 |              165.677 |            791.376 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3011.0364198394627
    time_step_min: 2911
  date: 2020-10-16_03-52-17
  done: false
  episode_len_mean: 791.3981103523697
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.357227715294
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 78639
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.911912524114743e-33
        cur_lr: 5.0e-05
        entropy: 0.07143988025685151
        entropy_coeff: 0.0005000000000000001
        kl: 0.005941039067693055
        model: {}
        policy_loss: -0.006510319256146128
        total_loss: 0.4125284180045128
        vf_explained_var: 0.9991984963417053
        vf_loss: 0.4190744608640671
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.690322580645166
    gpu_util_percent0: 0.314516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466579361580595
    mean_env_wait_ms: 1.1866533228883223
    mean_inference_ms: 4.311813629441512
    mean_raw_obs_processing_ms: 0.37813703872854487
  time_since_restore: 9948.957317590714
  time_this_iter_s: 25.93256115913391
  time_total_s: 9948.957317590714
  timers:
    learn_throughput: 8656.502
    learn_time_ms: 18690.229
    sample_throughput: 23865.514
    sample_time_ms: 6779.322
    update_time_ms: 27.641
  timestamp: 1602820337
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: 1bbc1_00000
  
2020-10-16 03:52:18,872	WARNING util.py:136 -- The `process_trial` operation took 1.0879168510437012 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    385 |          9948.96 | 62289920 |  307.357 |              322.949 |              165.677 |            791.398 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3010.829232701849
    time_step_min: 2911
  date: 2020-10-16_03-52-44
  done: false
  episode_len_mean: 791.4100105280515
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.3891308534176
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 78837
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.911912524114743e-33
        cur_lr: 5.0e-05
        entropy: 0.07263815278808276
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00852885958738625
        total_loss: .inf
        vf_explained_var: 0.9992926716804504
        vf_loss: 0.33152610808610916
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.896774193548392
    gpu_util_percent0: 0.3112903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665711847587715
    mean_env_wait_ms: 1.1865920518769069
    mean_inference_ms: 4.311769439430986
    mean_raw_obs_processing_ms: 0.3781332536875557
  time_since_restore: 9975.077486515045
  time_this_iter_s: 26.120168924331665
  time_total_s: 9975.077486515045
  timers:
    learn_throughput: 8659.129
    learn_time_ms: 18684.559
    sample_throughput: 23850.244
    sample_time_ms: 6783.662
    update_time_ms: 27.794
  timestamp: 1602820364
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:52:46,455	WARNING util.py:136 -- The `process_trial` operation took 1.091102123260498 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    386 |          9975.08 | 62451712 |  307.389 |              322.949 |              165.677 |             791.41 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3010.62605228179
    time_step_min: 2911
  date: 2020-10-16_03-53-12
  done: false
  episode_len_mean: 791.4260152107614
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.4201036215384
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 186
  episodes_total: 79023
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.8678687861721144e-33
        cur_lr: 5.0e-05
        entropy: 0.07521333980063598
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008251331331848633
        total_loss: .inf
        vf_explained_var: 0.9992063641548157
        vf_loss: 0.37600210060675937
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806453
    gpu_util_percent0: 0.30677419354838714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466562006613136
    mean_env_wait_ms: 1.1865328953343162
    mean_inference_ms: 4.311720028304358
    mean_raw_obs_processing_ms: 0.37812982671827144
  time_since_restore: 10000.936845064163
  time_this_iter_s: 25.859358549118042
  time_total_s: 10000.936845064163
  timers:
    learn_throughput: 8662.379
    learn_time_ms: 18677.548
    sample_throughput: 23864.824
    sample_time_ms: 6779.518
    update_time_ms: 27.777
  timestamp: 1602820392
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:53:13,887	WARNING util.py:136 -- The `process_trial` operation took 1.1059532165527344 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    387 |          10000.9 | 62613504 |   307.42 |              322.949 |              165.677 |            791.426 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3010.4285570003785
    time_step_min: 2911
  date: 2020-10-16_03-53-39
  done: false
  episode_len_mean: 791.4418208435346
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.45271195510946
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 79238
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.801803179258173e-33
        cur_lr: 5.0e-05
        entropy: 0.07297782537837823
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008788598310881449
        total_loss: .inf
        vf_explained_var: 0.9991412162780762
        vf_loss: 0.4374001274506251
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.018749999999997
    gpu_util_percent0: 0.31843750000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665500840321166
    mean_env_wait_ms: 1.186462369138942
    mean_inference_ms: 4.311660402436605
    mean_raw_obs_processing_ms: 0.3781255465306964
  time_since_restore: 10026.796073436737
  time_this_iter_s: 25.859228372573853
  time_total_s: 10026.796073436737
  timers:
    learn_throughput: 8658.558
    learn_time_ms: 18685.791
    sample_throughput: 23871.481
    sample_time_ms: 6777.627
    update_time_ms: 28.653
  timestamp: 1602820419
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:53:41,415	WARNING util.py:136 -- The `process_trial` operation took 1.0962626934051514 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    388 |          10026.8 | 62775296 |  307.453 |              322.949 |              165.677 |            791.442 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3010.1987636290382
    time_step_min: 2911
  date: 2020-10-16_03-54-07
  done: false
  episode_len_mean: 791.4650615450449
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.4883418981007
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 79454
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3202704768887258e-32
        cur_lr: 5.0e-05
        entropy: 0.07118080866833527
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008252374575628588
        total_loss: .inf
        vf_explained_var: 0.9995127320289612
        vf_loss: 0.24886703242858252
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870967
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665392029501947
    mean_env_wait_ms: 1.1863954834928983
    mean_inference_ms: 4.311608165392096
    mean_raw_obs_processing_ms: 0.37812137899308085
  time_since_restore: 10052.76455450058
  time_this_iter_s: 25.968481063842773
  time_total_s: 10052.76455450058
  timers:
    learn_throughput: 8656.257
    learn_time_ms: 18690.758
    sample_throughput: 23879.387
    sample_time_ms: 6775.383
    update_time_ms: 28.872
  timestamp: 1602820447
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:54:08,923	WARNING util.py:136 -- The `process_trial` operation took 1.1495630741119385 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    389 |          10052.8 | 62937088 |  307.488 |              322.949 |              165.677 |            791.465 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3010.0044091046125
    time_step_min: 2911
  date: 2020-10-16_03-54-34
  done: false
  episode_len_mean: 791.4846802953438
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.5184827074296
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 182
  episodes_total: 79636
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9804057153330887e-32
        cur_lr: 5.0e-05
        entropy: 0.06963859622677167
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007804923424070391
        total_loss: .inf
        vf_explained_var: 0.9995193481445312
        vf_loss: 0.25519708544015884
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.570967741935487
    gpu_util_percent0: 0.31387096774193546
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665282014841877
    mean_env_wait_ms: 1.1863382877462756
    mean_inference_ms: 4.311560123792184
    mean_raw_obs_processing_ms: 0.37811778548360386
  time_since_restore: 10078.668437242508
  time_this_iter_s: 25.9038827419281
  time_total_s: 10078.668437242508
  timers:
    learn_throughput: 8659.386
    learn_time_ms: 18684.003
    sample_throughput: 23894.935
    sample_time_ms: 6770.975
    update_time_ms: 30.597
  timestamp: 1602820474
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:54:36,460	WARNING util.py:136 -- The `process_trial` operation took 1.1380469799041748 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    390 |          10078.7 | 63098880 |  307.518 |              322.949 |              165.677 |            791.485 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3009.7850573848546
    time_step_min: 2911
  date: 2020-10-16_03-55-02
  done: false
  episode_len_mean: 791.5021417835671
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.55122492459634
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 204
  episodes_total: 79840
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9706085729996325e-32
        cur_lr: 5.0e-05
        entropy: 0.07117714857061704
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007601281475217547
        total_loss: .inf
        vf_explained_var: 0.9993659853935242
        vf_loss: 0.31520605832338333
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.365625
    gpu_util_percent0: 0.326875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665196148726983
    mean_env_wait_ms: 1.1862728080197402
    mean_inference_ms: 4.311509286024145
    mean_raw_obs_processing_ms: 0.37811431446781163
  time_since_restore: 10105.00966000557
  time_this_iter_s: 26.341222763061523
  time_total_s: 10105.00966000557
  timers:
    learn_throughput: 8647.624
    learn_time_ms: 18709.416
    sample_throughput: 23946.468
    sample_time_ms: 6756.404
    update_time_ms: 32.519
  timestamp: 1602820502
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:55:04,303	WARNING util.py:136 -- The `process_trial` operation took 1.104457139968872 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    391 |            10105 | 63260672 |  307.551 |              322.949 |              165.677 |            791.502 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3009.55651217714
    time_step_min: 2911
  date: 2020-10-16_03-55-30
  done: false
  episode_len_mean: 791.5288614077822
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.5862390061376
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 80055
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.4559128594994485e-32
        cur_lr: 5.0e-05
        entropy: 0.06721807767947514
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005778669845312834
        total_loss: .inf
        vf_explained_var: 0.9994316697120667
        vf_loss: 0.29447075227896374
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.377419354838715
    gpu_util_percent0: 0.30258064516129035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665073094060121
    mean_env_wait_ms: 1.1862040767215623
    mean_inference_ms: 4.311450613674932
    mean_raw_obs_processing_ms: 0.37810997688080994
  time_since_restore: 10131.193537712097
  time_this_iter_s: 26.18387770652771
  time_total_s: 10131.193537712097
  timers:
    learn_throughput: 8639.661
    learn_time_ms: 18726.662
    sample_throughput: 23896.277
    sample_time_ms: 6770.595
    update_time_ms: 32.628
  timestamp: 1602820530
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:55:32,047	WARNING util.py:136 -- The `process_trial` operation took 1.0802569389343262 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    392 |          10131.2 | 63422464 |  307.586 |              322.949 |              165.677 |            791.529 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3009.334716872951
    time_step_min: 2911
  date: 2020-10-16_03-55-58
  done: false
  episode_len_mean: 791.556001345677
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.6197059119293
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 202
  episodes_total: 80257
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.683869289249174e-32
        cur_lr: 5.0e-05
        entropy: 0.0652796955158313
        entropy_coeff: 0.0005000000000000001
        kl: 0.004785986267961562
        model: {}
        policy_loss: -0.007175201433710754
        total_loss: 0.17278344929218292
        vf_explained_var: 0.9996288418769836
        vf_loss: 0.17999129121502241
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.396875
    gpu_util_percent0: 0.3103125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664985041616319
    mean_env_wait_ms: 1.1861423835154685
    mean_inference_ms: 4.311402453819037
    mean_raw_obs_processing_ms: 0.3781059930018121
  time_since_restore: 10157.214220762253
  time_this_iter_s: 26.02068305015564
  time_total_s: 10157.214220762253
  timers:
    learn_throughput: 8631.886
    learn_time_ms: 18743.527
    sample_throughput: 23891.884
    sample_time_ms: 6771.839
    update_time_ms: 32.777
  timestamp: 1602820558
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: 1bbc1_00000
  
2020-10-16 03:55:59,730	WARNING util.py:136 -- The `process_trial` operation took 1.0922982692718506 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    393 |          10157.2 | 63584256 |   307.62 |              322.949 |              165.677 |            791.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3009.162765825146
    time_step_min: 2911
  date: 2020-10-16_03-56-25
  done: false
  episode_len_mean: 791.5746786344762
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.6467754448432
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 80438
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.341934644624587e-32
        cur_lr: 5.0e-05
        entropy: 0.06470947774748008
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007828271477289187
        total_loss: .inf
        vf_explained_var: 0.999162495136261
        vf_loss: 0.398640679816405
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.238709677419358
    gpu_util_percent0: 0.31451612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664898258715173
    mean_env_wait_ms: 1.1860863638366956
    mean_inference_ms: 4.311358194980514
    mean_raw_obs_processing_ms: 0.37810297741676935
  time_since_restore: 10183.260904312134
  time_this_iter_s: 26.04668354988098
  time_total_s: 10183.260904312134
  timers:
    learn_throughput: 8627.634
    learn_time_ms: 18752.766
    sample_throughput: 23875.226
    sample_time_ms: 6776.564
    update_time_ms: 40.922
  timestamp: 1602820585
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:56:27,268	WARNING util.py:136 -- The `process_trial` operation took 1.1050188541412354 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    394 |          10183.3 | 63746048 |  307.647 |              322.949 |              165.677 |            791.575 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3008.947952715861
    time_step_min: 2911
  date: 2020-10-16_03-56-53
  done: false
  episode_len_mean: 791.6033206442893
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.6816216024615
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 209
  episodes_total: 80647
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.01290196693688e-32
        cur_lr: 5.0e-05
        entropy: 0.0632454976439476
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008209751875256188
        total_loss: .inf
        vf_explained_var: 0.9995713233947754
        vf_loss: 0.21553393205006918
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93225806451613
    gpu_util_percent0: 0.2729032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664781539041474
    mean_env_wait_ms: 1.1860183115403073
    mean_inference_ms: 4.311302942500384
    mean_raw_obs_processing_ms: 0.37809890908356436
  time_since_restore: 10209.418506145477
  time_this_iter_s: 26.157601833343506
  time_total_s: 10209.418506145477
  timers:
    learn_throughput: 8617.676
    learn_time_ms: 18774.435
    sample_throughput: 23843.11
    sample_time_ms: 6785.692
    update_time_ms: 41.449
  timestamp: 1602820613
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:56:54,968	WARNING util.py:136 -- The `process_trial` operation took 1.0572235584259033 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    395 |          10209.4 | 63907840 |  307.682 |              322.949 |              165.677 |            791.603 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3008.7204532521832
    time_step_min: 2911
  date: 2020-10-16_03-57-21
  done: false
  episode_len_mean: 791.6313036381174
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.71750060644007
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 219
  episodes_total: 80866
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.519352950405322e-32
        cur_lr: 5.0e-05
        entropy: 0.06298697739839554
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008002918058385452
        total_loss: .inf
        vf_explained_var: 0.9995881915092468
        vf_loss: 0.21410322189331055
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.596875
    gpu_util_percent0: 0.25625000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664672331363768
    mean_env_wait_ms: 1.1859509097607017
    mean_inference_ms: 4.311248948669664
    mean_raw_obs_processing_ms: 0.37809478543877006
  time_since_restore: 10235.704922914505
  time_this_iter_s: 26.28641676902771
  time_total_s: 10235.704922914505
  timers:
    learn_throughput: 8610.203
    learn_time_ms: 18790.73
    sample_throughput: 23850.778
    sample_time_ms: 6783.51
    update_time_ms: 41.372
  timestamp: 1602820641
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:57:22,711	WARNING util.py:136 -- The `process_trial` operation took 1.058875560760498 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    396 |          10235.7 | 64069632 |  307.718 |              322.949 |              165.677 |            791.631 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3008.523412238047
    time_step_min: 2911
  date: 2020-10-16_03-57-49
  done: false
  episode_len_mean: 791.6553902336714
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.74647092735006
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 188
  episodes_total: 81054
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1279029425607983e-31
        cur_lr: 5.0e-05
        entropy: 0.06386866513639688
        entropy_coeff: 0.0005000000000000001
        kl: 0.005662234383635223
        model: {}
        policy_loss: -0.008883705411183959
        total_loss: 0.3696958323319753
        vf_explained_var: 0.9991917610168457
        vf_loss: 0.3786114652951558
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.516129032258068
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466457798798578
    mean_env_wait_ms: 1.1858932334698915
    mean_inference_ms: 4.311205675410036
    mean_raw_obs_processing_ms: 0.3780913367218815
  time_since_restore: 10262.034090042114
  time_this_iter_s: 26.329167127609253
  time_total_s: 10262.034090042114
  timers:
    learn_throughput: 8597.68
    learn_time_ms: 18818.1
    sample_throughput: 23802.43
    sample_time_ms: 6797.289
    update_time_ms: 44.513
  timestamp: 1602820669
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: 1bbc1_00000
  
2020-10-16 03:57:50,476	WARNING util.py:136 -- The `process_trial` operation took 1.037053108215332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    397 |            10262 | 64231424 |  307.746 |              322.949 |              165.677 |            791.655 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3008.351009603546
    time_step_min: 2911
  date: 2020-10-16_03-58-16
  done: false
  episode_len_mean: 791.6739488972037
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.7738598569386
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 81248
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1279029425607983e-31
        cur_lr: 5.0e-05
        entropy: 0.06590996868908405
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010257520363666117
        total_loss: .inf
        vf_explained_var: 0.9992066025733948
        vf_loss: 0.3844748412569364
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48387096774194
    gpu_util_percent0: 0.3341935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664487753113925
    mean_env_wait_ms: 1.185831543827664
    mean_inference_ms: 4.311155984713603
    mean_raw_obs_processing_ms: 0.37808758279613663
  time_since_restore: 10287.871896028519
  time_this_iter_s: 25.83780598640442
  time_total_s: 10287.871896028519
  timers:
    learn_throughput: 8604.074
    learn_time_ms: 18804.114
    sample_throughput: 23760.522
    sample_time_ms: 6809.278
    update_time_ms: 44.111
  timestamp: 1602820696
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:58:17,777	WARNING util.py:136 -- The `process_trial` operation took 1.074385404586792 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    398 |          10287.9 | 64393216 |  307.774 |              322.949 |              165.677 |            791.674 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3008.1313579094726
    time_step_min: 2911
  date: 2020-10-16_03-58-43
  done: false
  episode_len_mean: 791.6973558223466
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.80760503317003
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 214
  episodes_total: 81462
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6918544138411976e-31
        cur_lr: 5.0e-05
        entropy: 0.06680384650826454
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007260619691805914
        total_loss: .inf
        vf_explained_var: 0.9995172619819641
        vf_loss: 0.2688194625079632
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.500000000000007
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664369615572911
    mean_env_wait_ms: 1.1857641050377017
    mean_inference_ms: 4.311099294516041
    mean_raw_obs_processing_ms: 0.378084021545389
  time_since_restore: 10313.833477258682
  time_this_iter_s: 25.961581230163574
  time_total_s: 10313.833477258682
  timers:
    learn_throughput: 8603.132
    learn_time_ms: 18806.174
    sample_throughput: 23742.329
    sample_time_ms: 6814.496
    update_time_ms: 44.424
  timestamp: 1602820723
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:58:45,277	WARNING util.py:136 -- The `process_trial` operation took 1.0548350811004639 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    399 |          10313.8 | 64555008 |  307.808 |              322.949 |              165.677 |            791.697 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.9155980158002
    time_step_min: 2911
  date: 2020-10-16_03-59-11
  done: false
  episode_len_mean: 791.7240948661124
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.8409449261013
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 211
  episodes_total: 81673
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.537781620761796e-31
        cur_lr: 5.0e-05
        entropy: 0.06384735430280368
        entropy_coeff: 0.0005000000000000001
        kl: 0.004998661150845389
        model: {}
        policy_loss: -0.00721775004058145
        total_loss: 0.2841413269440333
        vf_explained_var: 0.9994101524353027
        vf_loss: 0.2913909964263439
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.216129032258067
    gpu_util_percent0: 0.3570967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664298513712168
    mean_env_wait_ms: 1.1857002443637117
    mean_inference_ms: 4.311052771351522
    mean_raw_obs_processing_ms: 0.37807998374109003
  time_since_restore: 10339.588620901108
  time_this_iter_s: 25.755143642425537
  time_total_s: 10339.588620901108
  timers:
    learn_throughput: 8607.648
    learn_time_ms: 18796.308
    sample_throughput: 23760.579
    sample_time_ms: 6809.262
    update_time_ms: 44.671
  timestamp: 1602820751
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: 1bbc1_00000
  
2020-10-16 03:59:12,670	WARNING util.py:136 -- The `process_trial` operation took 1.1375672817230225 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    400 |          10339.6 | 64716800 |  307.841 |              322.949 |              165.677 |            791.724 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.7305323361197
    time_step_min: 2911
  date: 2020-10-16_03-59-39
  done: false
  episode_len_mean: 791.7416560789679
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.8686582399441
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 183
  episodes_total: 81856
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.268890810380898e-31
        cur_lr: 5.0e-05
        entropy: 0.07047267941137154
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008282067955102926
        total_loss: .inf
        vf_explained_var: 0.9991998672485352
        vf_loss: 0.3752811973293622
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.771875
    gpu_util_percent0: 0.331875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466418837128543
    mean_env_wait_ms: 1.1856443451064442
    mean_inference_ms: 4.311010455104768
    mean_raw_obs_processing_ms: 0.3780767846047712
  time_since_restore: 10365.96095919609
  time_this_iter_s: 26.37233829498291
  time_total_s: 10365.96095919609
  timers:
    learn_throughput: 8609.736
    learn_time_ms: 18791.749
    sample_throughput: 23718.67
    sample_time_ms: 6821.293
    update_time_ms: 43.153
  timestamp: 1602820779
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 03:59:40,632	WARNING util.py:136 -- The `process_trial` operation took 1.134404182434082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    401 |            10366 | 64878592 |  307.869 |              322.949 |              165.677 |            791.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.5599912234725
    time_step_min: 2911
  date: 2020-10-16_04-00-06
  done: false
  episode_len_mean: 791.7462011819899
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.89241159539057
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 209
  episodes_total: 82065
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9033362155713472e-31
        cur_lr: 5.0e-05
        entropy: 0.0769629788895448
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008232126400495568
        total_loss: .inf
        vf_explained_var: 0.9983156323432922
        vf_loss: 0.8773807386557261
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.690322580645166
    gpu_util_percent0: 0.3051612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664103917159002
    mean_env_wait_ms: 1.1855779058716072
    mean_inference_ms: 4.310958814434186
    mean_raw_obs_processing_ms: 0.3780730895281358
  time_since_restore: 10392.21072268486
  time_this_iter_s: 26.24976348876953
  time_total_s: 10392.21072268486
  timers:
    learn_throughput: 8610.275
    learn_time_ms: 18790.573
    sample_throughput: 23702.557
    sample_time_ms: 6825.93
    update_time_ms: 44.453
  timestamp: 1602820806
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:00:08,427	WARNING util.py:136 -- The `process_trial` operation took 1.1473534107208252 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    402 |          10392.2 | 65040384 |  307.892 |              322.949 |              165.677 |            791.746 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.40071482409
    time_step_min: 2911
  date: 2020-10-16_04-00-34
  done: false
  episode_len_mean: 791.7490703157281
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.91834896126
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 82286
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.855004323357021e-31
        cur_lr: 5.0e-05
        entropy: 0.07977267975608508
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007989124981880499
        total_loss: .inf
        vf_explained_var: 0.9984976649284363
        vf_loss: 0.7829727083444595
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.151612903225807
    gpu_util_percent0: 0.32064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663989844328415
    mean_env_wait_ms: 1.185510712314315
    mean_inference_ms: 4.310903664603914
    mean_raw_obs_processing_ms: 0.3780690260555704
  time_since_restore: 10418.263668775558
  time_this_iter_s: 26.052946090698242
  time_total_s: 10418.263668775558
  timers:
    learn_throughput: 8614.017
    learn_time_ms: 18782.411
    sample_throughput: 23677.276
    sample_time_ms: 6833.218
    update_time_ms: 44.286
  timestamp: 1602820834
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:00:36,169	WARNING util.py:136 -- The `process_trial` operation took 1.1744041442871094 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    403 |          10418.3 | 65202176 |  307.918 |              322.949 |              165.677 |            791.749 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.299882351514
    time_step_min: 2911
  date: 2020-10-16_04-01-02
  done: false
  episode_len_mean: 791.7559925797495
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.9312731568994
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 191
  episodes_total: 82477
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2825064850355305e-31
        cur_lr: 5.0e-05
        entropy: 0.09585650886098544
        entropy_coeff: 0.0005000000000000001
        kl: 0.012255099912484487
        model: {}
        policy_loss: -0.011924497669194048
        total_loss: 1.6423001686731975
        vf_explained_var: 0.9964057803153992
        vf_loss: 1.6542725364367168
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.590625
    gpu_util_percent0: 0.30781250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466388662539517
    mean_env_wait_ms: 1.1854529133120413
    mean_inference_ms: 4.3108605746447175
    mean_raw_obs_processing_ms: 0.3780653410648969
  time_since_restore: 10444.35455083847
  time_this_iter_s: 26.090882062911987
  time_total_s: 10444.35455083847
  timers:
    learn_throughput: 8611.945
    learn_time_ms: 18786.929
    sample_throughput: 23683.672
    sample_time_ms: 6831.373
    update_time_ms: 34.994
  timestamp: 1602820862
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: 1bbc1_00000
  
2020-10-16 04:01:03,882	WARNING util.py:136 -- The `process_trial` operation took 1.2248501777648926 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    404 |          10444.4 | 65363968 |  307.931 |              322.949 |              165.677 |            791.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.479164650082
    time_step_min: 2911
  date: 2020-10-16_04-01-30
  done: false
  episode_len_mean: 791.7352073153031
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.90101801800455
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 82676
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2825064850355305e-31
        cur_lr: 5.0e-05
        entropy: 0.139341339468956
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01328297956691434
        total_loss: .inf
        vf_explained_var: 0.9915666580200195
        vf_loss: 4.085738182067871
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612904
    gpu_util_percent0: 0.32000000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663790526403409
    mean_env_wait_ms: 1.1853912257899917
    mean_inference_ms: 4.310811583806023
    mean_raw_obs_processing_ms: 0.37806180808470935
  time_since_restore: 10470.546818733215
  time_this_iter_s: 26.192267894744873
  time_total_s: 10470.546818733215
  timers:
    learn_throughput: 8611.763
    learn_time_ms: 18787.326
    sample_throughput: 23696.536
    sample_time_ms: 6827.665
    update_time_ms: 37.019
  timestamp: 1602820890
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:01:31,736	WARNING util.py:136 -- The `process_trial` operation took 1.2014844417572021 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    405 |          10470.5 | 65525760 |  307.901 |              322.949 |              165.677 |            791.735 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.9441783012153
    time_step_min: 2911
  date: 2020-10-16_04-01-57
  done: false
  episode_len_mean: 791.696775594398
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.83380469918274
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 82899
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.423759727553295e-31
        cur_lr: 5.0e-05
        entropy: 0.14092272520065308
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01325957135607799
        total_loss: .inf
        vf_explained_var: 0.9884300231933594
        vf_loss: 6.913722038269043
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.884375
    gpu_util_percent0: 0.2628125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663707251295266
    mean_env_wait_ms: 1.1853236534310077
    mean_inference_ms: 4.3107602327088586
    mean_raw_obs_processing_ms: 0.3780579593679152
  time_since_restore: 10496.558934211731
  time_this_iter_s: 26.012115478515625
  time_total_s: 10496.558934211731
  timers:
    learn_throughput: 8619.417
    learn_time_ms: 18770.643
    sample_throughput: 23690.599
    sample_time_ms: 6829.376
    update_time_ms: 34.777
  timestamp: 1602820917
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:01:59,473	WARNING util.py:136 -- The `process_trial` operation took 1.1468422412872314 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    406 |          10496.6 | 65687552 |  307.834 |              322.949 |              165.677 |            791.697 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3008.1530303942222
    time_step_min: 2911
  date: 2020-10-16_04-02-25
  done: false
  episode_len_mean: 791.6778696316619
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.80805856478185
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 204
  episodes_total: 83103
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.635639591329943e-31
        cur_lr: 5.0e-05
        entropy: 0.11236484100421269
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011697180685587227
        total_loss: .inf
        vf_explained_var: 0.9937595725059509
        vf_loss: 3.1395411292711892
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.409677419354843
    gpu_util_percent0: 0.28354838709677416
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663606556635156
    mean_env_wait_ms: 1.1852643853333944
    mean_inference_ms: 4.310714726261679
    mean_raw_obs_processing_ms: 0.3780544843684988
  time_since_restore: 10522.694625139236
  time_this_iter_s: 26.135690927505493
  time_total_s: 10522.694625139236
  timers:
    learn_throughput: 8625.899
    learn_time_ms: 18756.538
    sample_throughput: 23704.962
    sample_time_ms: 6825.238
    update_time_ms: 33.572
  timestamp: 1602820945
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:02:27,232	WARNING util.py:136 -- The `process_trial` operation took 1.1024377346038818 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    407 |          10522.7 | 65849344 |  307.808 |              322.949 |              165.677 |            791.678 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3008.2126514778474
    time_step_min: 2911
  date: 2020-10-16_04-02-53
  done: false
  episode_len_mean: 791.6717412445522
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.79977161731466
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 188
  episodes_total: 83291
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4453459386994917e-30
        cur_lr: 5.0e-05
        entropy: 0.09504453279078007
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012190670163060227
        total_loss: .inf
        vf_explained_var: 0.9955076575279236
        vf_loss: 2.224833200375239
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05483870967742
    gpu_util_percent0: 0.3080645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663519687329862
    mean_env_wait_ms: 1.185208512324632
    mean_inference_ms: 4.310670020399734
    mean_raw_obs_processing_ms: 0.37805141773620193
  time_since_restore: 10548.668673753738
  time_this_iter_s: 25.974048614501953
  time_total_s: 10548.668673753738
  timers:
    learn_throughput: 8617.303
    learn_time_ms: 18775.247
    sample_throughput: 23735.279
    sample_time_ms: 6816.52
    update_time_ms: 34.448
  timestamp: 1602820973
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:02:54,894	WARNING util.py:136 -- The `process_trial` operation took 1.1845922470092773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    408 |          10548.7 | 66011136 |    307.8 |              322.949 |              165.677 |            791.672 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3008.1207968471113
    time_step_min: 2911
  date: 2020-10-16_04-03-21
  done: false
  episode_len_mean: 791.6816913552158
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.8134784079672
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 83507
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1680189080492373e-30
        cur_lr: 5.0e-05
        entropy: 0.07449386144677798
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009969327346576998
        total_loss: .inf
        vf_explained_var: 0.9983325600624084
        vf_loss: 0.8641502807537714
    num_steps_sampled: 66172928
    num_steps_trained: 66172928
  iterations_since_restore: 409
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.875
    gpu_util_percent0: 0.2871875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466341731620288
    mean_env_wait_ms: 1.1851435196068658
    mean_inference_ms: 4.310619719923298
    mean_raw_obs_processing_ms: 0.378047490503932
  time_since_restore: 10574.79476904869
  time_this_iter_s: 26.126095294952393
  time_total_s: 10574.79476904869
  timers:
    learn_throughput: 8612.314
    learn_time_ms: 18786.123
    sample_throughput: 23750.575
    sample_time_ms: 6812.13
    update_time_ms: 36.201
  timestamp: 1602821001
  timesteps_since_restore: 0
  timesteps_total: 66172928
  training_iteration: 409
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:03:22,632	WARNING util.py:136 -- The `process_trial` operation took 1.2062675952911377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    409 |          10574.8 | 66172928 |  307.813 |              322.949 |              165.677 |            791.682 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.942911762949
    time_step_min: 2911
  date: 2020-10-16_04-03-48
  done: false
  episode_len_mean: 791.7052781195132
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.84416565111206
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 83723
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.252028362073857e-30
        cur_lr: 5.0e-05
        entropy: 0.0622104179734985
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041016342778069275
        model: {}
        policy_loss: -0.008588243877359977
        total_loss: 0.662292925020059
        vf_explained_var: 0.9987334609031677
        vf_loss: 0.6709122732281685
    num_steps_sampled: 66334720
    num_steps_trained: 66334720
  iterations_since_restore: 410
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64193548387097
    gpu_util_percent0: 0.3122580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663323813432322
    mean_env_wait_ms: 1.185080435540861
    mean_inference_ms: 4.310567612299631
    mean_raw_obs_processing_ms: 0.3780437528586187
  time_since_restore: 10600.519268751144
  time_this_iter_s: 25.724499702453613
  time_total_s: 10600.519268751144
  timers:
    learn_throughput: 8622.087
    learn_time_ms: 18764.829
    sample_throughput: 23725.677
    sample_time_ms: 6819.278
    update_time_ms: 36.075
  timestamp: 1602821028
  timesteps_since_restore: 0
  timesteps_total: 66334720
  training_iteration: 410
  trial_id: 1bbc1_00000
  
2020-10-16 04:03:49,948	WARNING util.py:136 -- The `process_trial` operation took 1.1825921535491943 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    410 |          10600.5 | 66334720 |  307.844 |              322.949 |              165.677 |            791.705 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.7521012911766
    time_step_min: 2911
  date: 2020-10-16_04-04-16
  done: false
  episode_len_mean: 791.727417913116
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.872537423577
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 182
  episodes_total: 83905
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6260141810369284e-30
        cur_lr: 5.0e-05
        entropy: 0.06217920873314142
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006616246651295417
        total_loss: .inf
        vf_explained_var: 0.9995811581611633
        vf_loss: 0.19365122665961584
    num_steps_sampled: 66496512
    num_steps_trained: 66496512
  iterations_since_restore: 411
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.590322580645164
    gpu_util_percent0: 0.33225806451612905
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663220819821476
    mean_env_wait_ms: 1.1850279639239178
    mean_inference_ms: 4.31052866426709
    mean_raw_obs_processing_ms: 0.37804081274981927
  time_since_restore: 10626.560954093933
  time_this_iter_s: 26.041685342788696
  time_total_s: 10626.560954093933
  timers:
    learn_throughput: 8632.102
    learn_time_ms: 18743.059
    sample_throughput: 23748.104
    sample_time_ms: 6812.839
    update_time_ms: 38.034
  timestamp: 1602821056
  timesteps_since_restore: 0
  timesteps_total: 66496512
  training_iteration: 411
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:04:17,688	WARNING util.py:136 -- The `process_trial` operation took 1.1606342792510986 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    411 |          10626.6 | 66496512 |  307.873 |              322.949 |              165.677 |            791.727 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.5606936416184
    time_step_min: 2911
  date: 2020-10-16_04-04-43
  done: false
  episode_len_mean: 791.7451905928233
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.8986662333508
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 201
  episodes_total: 84106
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4390212715553926e-30
        cur_lr: 5.0e-05
        entropy: 0.06609877447287242
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520900275868674
        model: {}
        policy_loss: -0.009671826960887605
        total_loss: 0.7464259068171183
        vf_explained_var: 0.9984772801399231
        vf_loss: 0.7561307946840922
    num_steps_sampled: 66658304
    num_steps_trained: 66658304
  iterations_since_restore: 412
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.13225806451613
    gpu_util_percent0: 0.3438709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663155727550536
    mean_env_wait_ms: 1.1849672190543372
    mean_inference_ms: 4.310481459118015
    mean_raw_obs_processing_ms: 0.37803737297265116
  time_since_restore: 10652.466837644577
  time_this_iter_s: 25.90588355064392
  time_total_s: 10652.466837644577
  timers:
    learn_throughput: 8637.445
    learn_time_ms: 18731.466
    sample_throughput: 23818.177
    sample_time_ms: 6792.795
    update_time_ms: 35.061
  timestamp: 1602821083
  timesteps_since_restore: 0
  timesteps_total: 66658304
  training_iteration: 412
  trial_id: 1bbc1_00000
  
2020-10-16 04:04:45,215	WARNING util.py:136 -- The `process_trial` operation took 1.0971505641937256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    412 |          10652.5 | 66658304 |  307.899 |              322.949 |              165.677 |            791.745 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.367058572293
    time_step_min: 2911
  date: 2020-10-16_04-05-11
  done: false
  episode_len_mean: 791.7655039908915
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.9278648170328
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 211
  episodes_total: 84317
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2195106357776963e-30
        cur_lr: 5.0e-05
        entropy: 0.06386579386889935
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011251340925809927
        total_loss: .inf
        vf_explained_var: 0.9992117881774902
        vf_loss: 0.40273771931727725
    num_steps_sampled: 66820096
    num_steps_trained: 66820096
  iterations_since_restore: 413
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.409374999999997
    gpu_util_percent0: 0.3090625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466305920650081
    mean_env_wait_ms: 1.1849041327745655
    mean_inference_ms: 4.310431716497681
    mean_raw_obs_processing_ms: 0.3780337299074623
  time_since_restore: 10678.56552696228
  time_this_iter_s: 26.098689317703247
  time_total_s: 10678.56552696228
  timers:
    learn_throughput: 8634.651
    learn_time_ms: 18737.527
    sample_throughput: 23851.616
    sample_time_ms: 6783.272
    update_time_ms: 34.463
  timestamp: 1602821111
  timesteps_since_restore: 0
  timesteps_total: 66820096
  training_iteration: 413
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:05:12,908	WARNING util.py:136 -- The `process_trial` operation took 1.190406084060669 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    413 |          10678.6 | 66820096 |  307.928 |              322.949 |              165.677 |            791.766 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.1768075461873
    time_step_min: 2911
  date: 2020-10-16_04-05-39
  done: false
  episode_len_mean: 791.7872008139989
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.9561940197992
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 204
  episodes_total: 84521
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8292659536665445e-30
        cur_lr: 5.0e-05
        entropy: 0.06389717602481444
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009423556473241964
        total_loss: .inf
        vf_explained_var: 0.9993558526039124
        vf_loss: 0.32007697969675064
    num_steps_sampled: 66981888
    num_steps_trained: 66981888
  iterations_since_restore: 414
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20645161290323
    gpu_util_percent0: 0.3258064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662963330338746
    mean_env_wait_ms: 1.1848460270644738
    mean_inference_ms: 4.310387478197863
    mean_raw_obs_processing_ms: 0.3780302316788781
  time_since_restore: 10704.800899982452
  time_this_iter_s: 26.23537302017212
  time_total_s: 10704.800899982452
  timers:
    learn_throughput: 8635.718
    learn_time_ms: 18735.212
    sample_throughput: 23763.669
    sample_time_ms: 6808.376
    update_time_ms: 35.709
  timestamp: 1602821139
  timesteps_since_restore: 0
  timesteps_total: 66981888
  training_iteration: 414
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:05:40,767	WARNING util.py:136 -- The `process_trial` operation took 1.1269094944000244 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    414 |          10704.8 | 66981888 |  307.956 |              322.949 |              165.677 |            791.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3007.0106989761575
    time_step_min: 2911
  date: 2020-10-16_04-06-06
  done: false
  episode_len_mean: 791.8071751525812
  episode_reward_max: 322.949494949495
  episode_reward_mean: 307.98168715689843
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 188
  episodes_total: 84709
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7438989304998168e-30
        cur_lr: 5.0e-05
        entropy: 0.065917508987089
        entropy_coeff: 0.0005000000000000001
        kl: 0.00421331514371559
        model: {}
        policy_loss: -0.009954598538267115
        total_loss: 0.4745134934782982
        vf_explained_var: 0.9989755153656006
        vf_loss: 0.48450104892253876
    num_steps_sampled: 67143680
    num_steps_trained: 67143680
  iterations_since_restore: 415
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.596875000000004
    gpu_util_percent0: 0.34187500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662870246816648
    mean_env_wait_ms: 1.1847913071506777
    mean_inference_ms: 4.310345110927938
    mean_raw_obs_processing_ms: 0.37802711760199015
  time_since_restore: 10730.987487077713
  time_this_iter_s: 26.18658709526062
  time_total_s: 10730.987487077713
  timers:
    learn_throughput: 8636.7
    learn_time_ms: 18733.081
    sample_throughput: 23781.358
    sample_time_ms: 6803.312
    update_time_ms: 33.876
  timestamp: 1602821166
  timesteps_since_restore: 0
  timesteps_total: 67143680
  training_iteration: 415
  trial_id: 1bbc1_00000
  
2020-10-16 04:06:08,512	WARNING util.py:136 -- The `process_trial` operation took 1.1527502536773682 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    415 |            10731 | 67143680 |  307.982 |              322.949 |              165.677 |            791.807 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3006.834631106504
    time_step_min: 2911
  date: 2020-10-16_04-06-34
  done: false
  episode_len_mean: 791.8253824322574
  episode_reward_max: 322.949494949495
  episode_reward_mean: 308.0053944535023
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 208
  episodes_total: 84917
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3719494652499084e-30
        cur_lr: 5.0e-05
        entropy: 0.06522077694535255
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038872323154161372
        model: {}
        policy_loss: -0.008578757212186853
        total_loss: 0.882705936829249
        vf_explained_var: 0.9983193874359131
        vf_loss: 0.8913173576196035
    num_steps_sampled: 67305472
    num_steps_trained: 67305472
  iterations_since_restore: 416
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.112903225806456
    gpu_util_percent0: 0.2951612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662782254716142
    mean_env_wait_ms: 1.184728830169694
    mean_inference_ms: 4.310296817198455
    mean_raw_obs_processing_ms: 0.37802364651809367
  time_since_restore: 10757.04046702385
  time_this_iter_s: 26.052979946136475
  time_total_s: 10757.04046702385
  timers:
    learn_throughput: 8635.608
    learn_time_ms: 18735.449
    sample_throughput: 23780.417
    sample_time_ms: 6803.581
    update_time_ms: 33.86
  timestamp: 1602821194
  timesteps_since_restore: 0
  timesteps_total: 67305472
  training_iteration: 416
  trial_id: 1bbc1_00000
  
2020-10-16 04:06:36,172	WARNING util.py:136 -- The `process_trial` operation took 1.0913195610046387 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    416 |            10757 | 67305472 |  308.005 |              322.949 |              165.677 |            791.825 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3006.6361030172006
    time_step_min: 2911
  date: 2020-10-16_04-07-02
  done: false
  episode_len_mean: 791.8489194268265
  episode_reward_max: 322.949494949495
  episode_reward_mean: 308.0375068514601
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 85140
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.859747326249542e-31
        cur_lr: 5.0e-05
        entropy: 0.0597851158430179
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009539275684801396
        total_loss: .inf
        vf_explained_var: 0.9995151162147522
        vf_loss: 0.24669024224082628
    num_steps_sampled: 67467264
    num_steps_trained: 67467264
  iterations_since_restore: 417
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.24838709677419
    gpu_util_percent0: 0.32290322580645164
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662676513588813
    mean_env_wait_ms: 1.1846646713040085
    mean_inference_ms: 4.310246847686019
    mean_raw_obs_processing_ms: 0.3780195836784571
  time_since_restore: 10783.199356079102
  time_this_iter_s: 26.158889055252075
  time_total_s: 10783.199356079102
  timers:
    learn_throughput: 8629.489
    learn_time_ms: 18748.736
    sample_throughput: 23816.239
    sample_time_ms: 6793.348
    update_time_ms: 33.364
  timestamp: 1602821222
  timesteps_since_restore: 0
  timesteps_total: 67467264
  training_iteration: 417
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:07:03,975	WARNING util.py:136 -- The `process_trial` operation took 1.1501803398132324 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | RUNNING  | 172.17.0.4:80043 |    417 |          10783.2 | 67467264 |  308.038 |              322.949 |              165.677 |            791.849 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_1bbc1_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3006.45324407344
    time_step_min: 2911
  date: 2020-10-16_04-07-29
  done: true
  episode_len_mean: 791.8698694357845
  episode_reward_max: 322.949494949495
  episode_reward_mean: 308.06549046878587
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 182
  episodes_total: 85322
  experiment_id: f8ac0c5901c342e0bbadebc707c5c22a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0289620989374312e-30
        cur_lr: 5.0e-05
        entropy: 0.058181174409886204
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008898461431575319
        total_loss: .inf
        vf_explained_var: 0.999596118927002
        vf_loss: 0.1878378689289093
    num_steps_sampled: 67629056
    num_steps_trained: 67629056
  iterations_since_restore: 418
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.71875
    gpu_util_percent0: 0.31249999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 80043
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662590922076563
    mean_env_wait_ms: 1.1846130778720547
    mean_inference_ms: 4.3102087955107695
    mean_raw_obs_processing_ms: 0.3780169033080108
  time_since_restore: 10809.153240680695
  time_this_iter_s: 25.953884601593018
  time_total_s: 10809.153240680695
  timers:
    learn_throughput: 8630.658
    learn_time_ms: 18746.195
    sample_throughput: 23836.791
    sample_time_ms: 6787.491
    update_time_ms: 30.919
  timestamp: 1602821249
  timesteps_since_restore: 0
  timesteps_total: 67629056
  training_iteration: 418
  trial_id: 1bbc1_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-16 04:07:31,792	WARNING util.py:136 -- The `process_trial` operation took 1.3958709239959717 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 25.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | TERMINATED |       |    418 |          10809.2 | 67629056 |  308.065 |              322.949 |              165.677 |             791.87 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_1bbc1_00000 | TERMINATED |       |    418 |          10809.2 | 67629056 |  308.065 |              322.949 |              165.677 |             791.87 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


