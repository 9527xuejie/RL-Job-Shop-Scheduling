2020-10-11 14:57:55,872	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_25e0a_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=79049)[0m 2020-10-11 14:57:58,580	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=79037)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79037)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79006)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79006)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78986)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78986)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79032)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79032)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79047)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79047)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79081)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79081)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79071)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79071)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79029)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79029)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79079)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79079)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79035)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79035)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79092)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79092)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79043)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79043)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78972)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78972)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79075)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79075)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79094)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79094)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79056)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79056)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78974)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78974)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79004)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79004)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78965)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78965)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79099)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79099)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78988)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78988)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79045)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79045)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78990)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78990)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79053)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79053)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78993)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78993)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79074)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79074)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78985)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78985)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78963)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78963)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78975)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78975)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79057)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79057)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78981)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78981)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79089)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79089)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79087)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79087)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79009)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79009)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79048)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79048)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79082)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79082)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79085)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79085)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79031)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79031)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79011)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79011)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79067)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79067)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78956)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78956)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78958)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78958)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78982)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78982)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79050)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79050)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79076)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79076)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78987)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78987)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79036)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79036)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78971)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78971)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79034)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79034)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78960)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78960)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78957)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78957)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79014)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79014)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78968)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78968)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79055)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79055)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78955)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78955)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79102)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79102)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79096)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79096)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79023)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79023)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78973)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78973)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79042)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79042)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78979)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78979)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78967)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78967)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79054)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79054)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79028)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79028)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79083)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79083)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79026)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79026)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79007)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79007)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78966)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78966)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79063)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79063)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78983)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78983)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79046)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79046)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79020)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79020)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78954)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78954)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79030)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79030)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78970)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78970)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=79024)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=79024)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78961)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78961)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78962)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78962)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=78959)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=78959)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4070
    time_step_mean: 3604.8101265822784
    time_step_min: 3251
  date: 2020-10-11_14-58-23
  done: false
  episode_len_mean: 891.0759493670886
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 219.83684950773548
  episode_reward_min: 149.35353535353508
  episodes_this_iter: 79
  episodes_total: 79
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1821814060211182
        entropy_coeff: 0.0001
        kl: 0.005178337823599577
        model: {}
        policy_loss: -0.008954001311212777
        total_loss: 598.7196044921875
        vf_explained_var: 0.23389902710914612
        vf_loss: 598.7276489257813
    num_steps_sampled: 80896
    num_steps_trained: 80896
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.00869565217391
    gpu_util_percent0: 0.28130434782608693
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3260869565217397
    vram_util_percent0: 0.08697434654270111
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1405497560091834
    mean_env_wait_ms: 0.6502708059359789
    mean_inference_ms: 5.734528894592265
    mean_raw_obs_processing_ms: 0.3117136883124704
  time_since_restore: 19.361814737319946
  time_this_iter_s: 19.361814737319946
  time_total_s: 19.361814737319946
  timers:
    learn_throughput: 7390.735
    learn_time_ms: 10945.596
    sample_throughput: 9696.284
    sample_time_ms: 8342.99
    update_time_ms: 38.486
  timestamp: 1602428303
  timesteps_since_restore: 0
  timesteps_total: 80896
  training_iteration: 1
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      1 |          19.3618 | 80896 |  219.837 |              273.444 |              149.354 |            891.076 |
+-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4070
    time_step_mean: 3613.8734177215188
    time_step_min: 3251
  date: 2020-10-11_14-58-41
  done: false
  episode_len_mean: 890.2088607594936
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 218.46362357754748
  episode_reward_min: 149.35353535353508
  episodes_this_iter: 79
  episodes_total: 158
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1610076427459717
        entropy_coeff: 0.0001
        kl: 0.005721182934939862
        model: {}
        policy_loss: -0.010106014460325241
        total_loss: 247.57249755859374
        vf_explained_var: 0.652772068977356
        vf_loss: 247.58157653808593
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.18571428571429
    gpu_util_percent0: 0.3076190476190476
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4666666666666672
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.136365787780534
    mean_env_wait_ms: 0.6471527892022945
    mean_inference_ms: 5.543648386724649
    mean_raw_obs_processing_ms: 0.30285141011200317
  time_since_restore: 37.04883170127869
  time_this_iter_s: 17.68701696395874
  time_total_s: 37.04883170127869
  timers:
    learn_throughput: 7469.926
    learn_time_ms: 10829.558
    sample_throughput: 10613.329
    sample_time_ms: 7622.113
    update_time_ms: 37.121
  timestamp: 1602428321
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 2
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      2 |          37.0488 | 161792 |  218.464 |              273.444 |              149.354 |            890.209 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3614.9113924050635
    time_step_min: 3251
  date: 2020-10-11_14-58-58
  done: false
  episode_len_mean: 888.590717299578
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 218.30635468610134
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 237
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.159032130241394
        entropy_coeff: 0.0001
        kl: 0.0057226565666496755
        model: {}
        policy_loss: -0.01224328400567174
        total_loss: 104.11964721679688
        vf_explained_var: 0.8229959607124329
        vf_loss: 104.13086395263672
    num_steps_sampled: 242688
    num_steps_trained: 242688
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.3
    gpu_util_percent0: 0.31315789473684214
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13348297467848136
    mean_env_wait_ms: 0.6450977775609045
    mean_inference_ms: 5.3758693242674385
    mean_raw_obs_processing_ms: 0.2962846117850065
  time_since_restore: 53.9220027923584
  time_this_iter_s: 16.873171091079712
  time_total_s: 53.9220027923584
  timers:
    learn_throughput: 7508.142
    learn_time_ms: 10774.437
    sample_throughput: 11342.181
    sample_time_ms: 7132.314
    update_time_ms: 32.058
  timestamp: 1602428338
  timesteps_since_restore: 0
  timesteps_total: 242688
  training_iteration: 3
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      3 |           53.922 | 242688 |  218.306 |              273.444 |               146.02 |            888.591 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3604.6075949367087
    time_step_min: 3251
  date: 2020-10-11_14-59-15
  done: false
  episode_len_mean: 886.3354430379746
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 219.86753612070044
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 316
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.142055583000183
        entropy_coeff: 0.0001
        kl: 0.006965293735265732
        model: {}
        policy_loss: -0.013879792392253875
        total_loss: 68.49144897460937
        vf_explained_var: 0.8709942698478699
        vf_loss: 68.504052734375
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.6
    gpu_util_percent0: 0.30350000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.131492585828327
    mean_env_wait_ms: 0.6436242496916351
    mean_inference_ms: 5.253438706090032
    mean_raw_obs_processing_ms: 0.29135991605414235
  time_since_restore: 70.88383412361145
  time_this_iter_s: 16.96183133125305
  time_total_s: 70.88383412361145
  timers:
    learn_throughput: 7529.185
    learn_time_ms: 10744.323
    sample_throughput: 11702.96
    sample_time_ms: 6912.439
    update_time_ms: 28.741
  timestamp: 1602428355
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 4
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      4 |          70.8838 | 323584 |  219.868 |              273.444 |               146.02 |            886.335 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3599.4151898734176
    time_step_min: 3251
  date: 2020-10-11_14-59-32
  done: false
  episode_len_mean: 884.0708860759494
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 220.6542641605931
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 395
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1255623817443847
        entropy_coeff: 0.0001
        kl: 0.006865937076508999
        model: {}
        policy_loss: -0.0142045047134161
        total_loss: 57.66709518432617
        vf_explained_var: 0.8982939720153809
        vf_loss: 57.68003768920899
    num_steps_sampled: 404480
    num_steps_trained: 404480
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.765000000000008
    gpu_util_percent0: 0.33649999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13000907587399763
    mean_env_wait_ms: 0.6429696938711666
    mean_inference_ms: 5.160756409573132
    mean_raw_obs_processing_ms: 0.28765939963701187
  time_since_restore: 87.75734639167786
  time_this_iter_s: 16.873512268066406
  time_total_s: 87.75734639167786
  timers:
    learn_throughput: 7544.609
    learn_time_ms: 10722.358
    sample_throughput: 11960.587
    sample_time_ms: 6763.548
    update_time_ms: 29.055
  timestamp: 1602428372
  timesteps_since_restore: 0
  timesteps_total: 404480
  training_iteration: 5
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      5 |          87.7573 | 404480 |  220.654 |              273.444 |               146.02 |            884.071 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3588.4762808349146
    time_step_min: 3251
  date: 2020-10-11_14-59-48
  done: false
  episode_len_mean: 876.9468690702088
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 222.31167462097235
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 132
  episodes_total: 527
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1013608694076538
        entropy_coeff: 0.0001
        kl: 0.006304158177226782
        model: {}
        policy_loss: -0.016743747983127832
        total_loss: 61.22104263305664
        vf_explained_var: 0.9264847636222839
        vf_loss: 61.23663177490234
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.31
    gpu_util_percent0: 0.29600000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.475
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12829810238146183
    mean_env_wait_ms: 0.6432681475441565
    mean_inference_ms: 5.050175191309327
    mean_raw_obs_processing_ms: 0.2834020367570414
  time_since_restore: 104.622731924057
  time_this_iter_s: 16.86538553237915
  time_total_s: 104.622731924057
  timers:
    learn_throughput: 7563.922
    learn_time_ms: 10694.98
    sample_throughput: 12132.549
    sample_time_ms: 6667.684
    update_time_ms: 26.851
  timestamp: 1602428388
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 6
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      6 |          104.623 | 485376 |  222.312 |              273.444 |               146.02 |            876.947 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3579.4794303797466
    time_step_min: 3251
  date: 2020-10-11_15-00-05
  done: false
  episode_len_mean: 872.9651898734177
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 223.67483378084626
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 105
  episodes_total: 632
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1148772478103637
        entropy_coeff: 0.0001
        kl: 0.00640232590958476
        model: {}
        policy_loss: -0.01414772029966116
        total_loss: 40.958674621582034
        vf_explained_var: 0.9319775700569153
        vf_loss: 40.97165451049805
    num_steps_sampled: 566272
    num_steps_trained: 566272
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.463157894736845
    gpu_util_percent0: 0.3436842105263158
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.478947368421052
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12731261365806423
    mean_env_wait_ms: 0.6432604682631025
    mean_inference_ms: 4.986517843354875
    mean_raw_obs_processing_ms: 0.28088197454716307
  time_since_restore: 121.37148237228394
  time_this_iter_s: 16.74875044822693
  time_total_s: 121.37148237228394
  timers:
    learn_throughput: 7575.314
    learn_time_ms: 10678.897
    sample_throughput: 12278.622
    sample_time_ms: 6588.361
    update_time_ms: 25.698
  timestamp: 1602428405
  timesteps_since_restore: 0
  timesteps_total: 566272
  training_iteration: 7
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      7 |          121.371 | 566272 |  223.675 |              273.444 |               146.02 |            872.965 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3571.4486638537273
    time_step_min: 3251
  date: 2020-10-11_15-00-22
  done: false
  episode_len_mean: 869.395218002813
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 224.891616587819
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 711
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0891266345977784
        entropy_coeff: 0.0001
        kl: 0.006864890549331903
        model: {}
        policy_loss: -0.012392168352380394
        total_loss: 29.909460067749023
        vf_explained_var: 0.9454657435417175
        vf_loss: 29.92058868408203
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.905
    gpu_util_percent0: 0.316
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12671245780329993
    mean_env_wait_ms: 0.6434006338525907
    mean_inference_ms: 4.94725769618157
    mean_raw_obs_processing_ms: 0.2793458640278308
  time_since_restore: 138.17620182037354
  time_this_iter_s: 16.8047194480896
  time_total_s: 138.17620182037354
  timers:
    learn_throughput: 7579.728
    learn_time_ms: 10672.678
    sample_throughput: 12388.977
    sample_time_ms: 6529.675
    update_time_ms: 25.023
  timestamp: 1602428422
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 8
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      8 |          138.176 | 647168 |  224.892 |              273.444 |               146.02 |            869.395 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3564.345569620253
    time_step_min: 3251
  date: 2020-10-11_15-00-39
  done: false
  episode_len_mean: 866.1582278481013
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 225.96784298683022
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 790
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0771050453186035
        entropy_coeff: 0.0001
        kl: 0.006526902969926596
        model: {}
        policy_loss: -0.0148372957482934
        total_loss: 26.008992385864257
        vf_explained_var: 0.9503160715103149
        vf_loss: 26.0226318359375
    num_steps_sampled: 728064
    num_steps_trained: 728064
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.395
    gpu_util_percent0: 0.3565
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12618623072965274
    mean_env_wait_ms: 0.6435788954968152
    mean_inference_ms: 4.912974030587078
    mean_raw_obs_processing_ms: 0.2779692896540916
  time_since_restore: 155.11791062355042
  time_this_iter_s: 16.94170880317688
  time_total_s: 155.11791062355042
  timers:
    learn_throughput: 7578.867
    learn_time_ms: 10673.891
    sample_throughput: 12458.636
    sample_time_ms: 6493.167
    update_time_ms: 24.348
  timestamp: 1602428439
  timesteps_since_restore: 0
  timesteps_total: 728064
  training_iteration: 9
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |      9 |          155.118 | 728064 |  225.968 |              273.444 |               146.02 |            866.158 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3557.8855835240274
    time_step_min: 3251
  date: 2020-10-11_15-00-56
  done: false
  episode_len_mean: 862.9096109839817
  episode_reward_max: 273.4444444444444
  episode_reward_mean: 226.94662875898564
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 84
  episodes_total: 874
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0427801609039307
        entropy_coeff: 0.0001
        kl: 0.006021166313439607
        model: {}
        policy_loss: -0.010975334793329239
        total_loss: 26.823484039306642
        vf_explained_var: 0.9570068120956421
        vf_loss: 26.833358764648438
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.36
    gpu_util_percent0: 0.31799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4850000000000003
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1257158946165379
    mean_env_wait_ms: 0.6439133970724744
    mean_inference_ms: 4.881427813012746
    mean_raw_obs_processing_ms: 0.2766901551134662
  time_since_restore: 172.2000482082367
  time_this_iter_s: 17.08213758468628
  time_total_s: 172.2000482082367
  timers:
    learn_throughput: 7565.115
    learn_time_ms: 10693.295
    sample_throughput: 12526.945
    sample_time_ms: 6457.76
    update_time_ms: 26.007
  timestamp: 1602428456
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 10
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     10 |            172.2 | 808960 |  226.947 |              273.444 |               146.02 |             862.91 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3546.17738791423
    time_step_min: 3229
  date: 2020-10-11_15-01-13
  done: false
  episode_len_mean: 856.958089668616
  episode_reward_max: 276.77777777777794
  episode_reward_mean: 228.72059779077313
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 152
  episodes_total: 1026
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.047643208503723
        entropy_coeff: 0.0001
        kl: 0.0061458229087293145
        model: {}
        policy_loss: -0.012535271141678095
        total_loss: 28.427783584594728
        vf_explained_var: 0.9625579118728638
        vf_loss: 28.439194107055663
    num_steps_sampled: 889856
    num_steps_trained: 889856
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.095
    gpu_util_percent0: 0.34249999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.475
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1250004727198809
    mean_env_wait_ms: 0.6445938363621434
    mean_inference_ms: 4.833793420266109
    mean_raw_obs_processing_ms: 0.2748012933950252
  time_since_restore: 189.14513874053955
  time_this_iter_s: 16.945090532302856
  time_total_s: 189.14513874053955
  timers:
    learn_throughput: 7584.813
    learn_time_ms: 10665.524
    sample_throughput: 12965.305
    sample_time_ms: 6239.421
    update_time_ms: 24.27
  timestamp: 1602428473
  timesteps_since_restore: 0
  timesteps_total: 889856
  training_iteration: 11
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     11 |          189.145 | 889856 |  228.721 |              276.778 |               146.02 |            856.958 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3537.3589511754067
    time_step_min: 3229
  date: 2020-10-11_15-01-30
  done: false
  episode_len_mean: 853.8372513562387
  episode_reward_max: 276.77777777777794
  episode_reward_mean: 230.0567245693827
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 80
  episodes_total: 1106
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.029042935371399
        entropy_coeff: 0.0001
        kl: 0.006281163450330496
        model: {}
        policy_loss: -0.015307414811104536
        total_loss: 15.060502815246583
        vf_explained_var: 0.9696897268295288
        vf_loss: 15.074656867980957
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.14736842105263
    gpu_util_percent0: 0.3373684210526316
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12468306089991366
    mean_env_wait_ms: 0.6449567343788762
    mean_inference_ms: 4.812627363701398
    mean_raw_obs_processing_ms: 0.273944758139881
  time_since_restore: 206.12778520584106
  time_this_iter_s: 16.982646465301514
  time_total_s: 206.12778520584106
  timers:
    learn_throughput: 7578.509
    learn_time_ms: 10674.395
    sample_throughput: 13133.471
    sample_time_ms: 6159.53
    update_time_ms: 24.381
  timestamp: 1602428490
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 12
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     12 |          206.128 | 970752 |  230.057 |              276.778 |               146.02 |            853.837 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3530.604219409283
    time_step_min: 3225
  date: 2020-10-11_15-01-47
  done: false
  episode_len_mean: 851.1873417721519
  episode_reward_max: 277.38383838383817
  episode_reward_mean: 231.0801687763712
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 1185
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.020499587059021
        entropy_coeff: 0.0001
        kl: 0.0058583361096680164
        model: {}
        policy_loss: -0.014010852668434381
        total_loss: 17.26035079956055
        vf_explained_var: 0.9643712043762207
        vf_loss: 17.273291778564452
    num_steps_sampled: 1051648
    num_steps_trained: 1051648
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.305
    gpu_util_percent0: 0.29900000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12439936516116261
    mean_env_wait_ms: 0.6453060533097584
    mean_inference_ms: 4.793489559966399
    mean_raw_obs_processing_ms: 0.2731737658194734
  time_since_restore: 222.88168025016785
  time_this_iter_s: 16.753895044326782
  time_total_s: 222.88168025016785
  timers:
    learn_throughput: 7573.566
    learn_time_ms: 10681.362
    sample_throughput: 13175.11
    sample_time_ms: 6140.063
    update_time_ms: 24.318
  timestamp: 1602428507
  timesteps_since_restore: 0
  timesteps_total: 1051648
  training_iteration: 13
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     13 |          222.882 | 1051648 |   231.08 |              277.384 |               146.02 |            851.187 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3522.7099296325255
    time_step_min: 3184
  date: 2020-10-11_15-02-04
  done: false
  episode_len_mean: 848.0969507427678
  episode_reward_max: 283.59595959595947
  episode_reward_mean: 232.27627328800114
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 94
  episodes_total: 1279
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.988058340549469
        entropy_coeff: 0.0001
        kl: 0.006182871200144291
        model: {}
        policy_loss: -0.014513058867305518
        total_loss: 22.752033996582032
        vf_explained_var: 0.9629007577896118
        vf_loss: 22.76541061401367
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.820000000000004
    gpu_util_percent0: 0.3165
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12410530343173547
    mean_env_wait_ms: 0.6458437721239955
    mean_inference_ms: 4.772863207242403
    mean_raw_obs_processing_ms: 0.27235669150720343
  time_since_restore: 240.00171875953674
  time_this_iter_s: 17.120038509368896
  time_total_s: 240.00171875953674
  timers:
    learn_throughput: 7563.78
    learn_time_ms: 10695.181
    sample_throughput: 13172.637
    sample_time_ms: 6141.215
    update_time_ms: 24.555
  timestamp: 1602428524
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 14
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     14 |          240.002 | 1132544 |  232.276 |              283.596 |               146.02 |            848.097 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3513.834739803094
    time_step_min: 3184
  date: 2020-10-11_15-02-22
  done: false
  episode_len_mean: 843.6962025316456
  episode_reward_max: 283.59595959595947
  episode_reward_mean: 233.62099901973312
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 143
  episodes_total: 1422
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9878270983695984
        entropy_coeff: 0.0001
        kl: 0.005273265577852726
        model: {}
        policy_loss: -0.01443924605846405
        total_loss: 19.540371704101563
        vf_explained_var: 0.9709212183952332
        vf_loss: 19.55385627746582
    num_steps_sampled: 1213440
    num_steps_trained: 1213440
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.02500000000001
    gpu_util_percent0: 0.33049999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4799999999999995
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12368752424523184
    mean_env_wait_ms: 0.6465292187640248
    mean_inference_ms: 4.745004298337707
    mean_raw_obs_processing_ms: 0.2712266339697401
  time_since_restore: 256.9493980407715
  time_this_iter_s: 16.94767928123474
  time_total_s: 256.9493980407715
  timers:
    learn_throughput: 7564.837
    learn_time_ms: 10693.687
    sample_throughput: 13155.529
    sample_time_ms: 6149.202
    update_time_ms: 25.398
  timestamp: 1602428542
  timesteps_since_restore: 0
  timesteps_total: 1213440
  training_iteration: 15
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     15 |          256.949 | 1213440 |  233.621 |              283.596 |               146.02 |            843.696 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3507.775483011326
    time_step_min: 3184
  date: 2020-10-11_15-02-38
  done: false
  episode_len_mean: 841.0646235842771
  episode_reward_max: 283.59595959595947
  episode_reward_mean: 234.53906823060714
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 1501
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.97277330160141
        entropy_coeff: 0.0001
        kl: 0.005540623422712087
        model: {}
        policy_loss: -0.01531378725776449
        total_loss: 12.622322082519531
        vf_explained_var: 0.9739503860473633
        vf_loss: 12.636624717712403
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.76842105263158
    gpu_util_percent0: 0.3163157894736843
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12348587446834983
    mean_env_wait_ms: 0.6469145712639862
    mean_inference_ms: 4.731359925325055
    mean_raw_obs_processing_ms: 0.27067393672678736
  time_since_restore: 273.7271740436554
  time_this_iter_s: 16.77777600288391
  time_total_s: 273.7271740436554
  timers:
    learn_throughput: 7561.237
    learn_time_ms: 10698.778
    sample_throughput: 13174.224
    sample_time_ms: 6140.476
    update_time_ms: 26.374
  timestamp: 1602428558
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 16
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     16 |          273.727 | 1294336 |  234.539 |              283.596 |               146.02 |            841.065 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3502.0474383301707
    time_step_min: 3167
  date: 2020-10-11_15-02-55
  done: false
  episode_len_mean: 838.7457305502846
  episode_reward_max: 286.1717171717169
  episode_reward_mean: 235.40695378835792
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 80
  episodes_total: 1581
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9531836867332458
        entropy_coeff: 0.0001
        kl: 0.005672357883304358
        model: {}
        policy_loss: -0.013997910264879464
        total_loss: 14.515528297424316
        vf_explained_var: 0.9702242016792297
        vf_loss: 14.528486824035644
    num_steps_sampled: 1375232
    num_steps_trained: 1375232
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.085000000000008
    gpu_util_percent0: 0.346
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12329679443462903
    mean_env_wait_ms: 0.6473200822086033
    mean_inference_ms: 4.7185145686739975
    mean_raw_obs_processing_ms: 0.27014612389586146
  time_since_restore: 290.4690568447113
  time_this_iter_s: 16.741882801055908
  time_total_s: 290.4690568447113
  timers:
    learn_throughput: 7556.782
    learn_time_ms: 10705.086
    sample_throughput: 13191.474
    sample_time_ms: 6132.446
    update_time_ms: 26.165
  timestamp: 1602428575
  timesteps_since_restore: 0
  timesteps_total: 1375232
  training_iteration: 17
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     17 |          290.469 | 1375232 |  235.407 |              286.172 |               146.02 |            838.746 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3492.5957200694043
    time_step_min: 3149
  date: 2020-10-11_15-03-12
  done: false
  episode_len_mean: 834.5801041064199
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 236.83903231271648
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 148
  episodes_total: 1729
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.926102340221405
        entropy_coeff: 0.0001
        kl: 0.005256259627640247
        model: {}
        policy_loss: -0.011589129082858562
        total_loss: 17.027903366088868
        vf_explained_var: 0.976334273815155
        vf_loss: 17.03853416442871
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.45263157894736
    gpu_util_percent0: 0.3294736842105263
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12297569960283086
    mean_env_wait_ms: 0.6481164637801422
    mean_inference_ms: 4.697031733472803
    mean_raw_obs_processing_ms: 0.26926794617915933
  time_since_restore: 307.1953341960907
  time_this_iter_s: 16.726277351379395
  time_total_s: 307.1953341960907
  timers:
    learn_throughput: 7552.699
    learn_time_ms: 10710.874
    sample_throughput: 13221.798
    sample_time_ms: 6118.381
    update_time_ms: 26.134
  timestamp: 1602428592
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 18
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     18 |          307.195 | 1456128 |  236.839 |              288.899 |               146.02 |             834.58 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3488.034672537149
    time_step_min: 3149
  date: 2020-10-11_15-03-29
  done: false
  episode_len_mean: 832.3307649972483
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 237.53010012063393
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 88
  episodes_total: 1817
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9310240507125854
        entropy_coeff: 0.0001
        kl: 0.005109604261815548
        model: {}
        policy_loss: -0.01098379292525351
        total_loss: 14.388118743896484
        vf_explained_var: 0.9739601016044617
        vf_loss: 14.39817409515381
    num_steps_sampled: 1537024
    num_steps_trained: 1537024
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.140000000000004
    gpu_util_percent0: 0.3485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12280228009136415
    mean_env_wait_ms: 0.6485211065686625
    mean_inference_ms: 4.685172010692531
    mean_raw_obs_processing_ms: 0.26877865575122734
  time_since_restore: 323.9889533519745
  time_this_iter_s: 16.79361915588379
  time_total_s: 323.9889533519745
  timers:
    learn_throughput: 7558.797
    learn_time_ms: 10702.233
    sample_throughput: 13249.245
    sample_time_ms: 6105.707
    update_time_ms: 26.106
  timestamp: 1602428609
  timesteps_since_restore: 0
  timesteps_total: 1537024
  training_iteration: 19
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     19 |          323.989 | 1537024 |   237.53 |              288.899 |               146.02 |            832.331 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3484.7273206751056
    time_step_min: 3149
  date: 2020-10-11_15-03-46
  done: false
  episode_len_mean: 830.3449367088608
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 238.03121403912542
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 1896
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9225889205932617
        entropy_coeff: 0.0001
        kl: 0.005407916381955147
        model: {}
        policy_loss: -0.014131060149520636
        total_loss: 14.700571441650391
        vf_explained_var: 0.9705582857131958
        vf_loss: 14.713712692260742
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.6
    gpu_util_percent0: 0.35157894736842105
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12265549039934973
    mean_env_wait_ms: 0.6488958537198457
    mean_inference_ms: 4.67537146231669
    mean_raw_obs_processing_ms: 0.2683676603373263
  time_since_restore: 340.80086636543274
  time_this_iter_s: 16.811913013458252
  time_total_s: 340.80086636543274
  timers:
    learn_throughput: 7570.044
    learn_time_ms: 10686.331
    sample_throughput: 13271.987
    sample_time_ms: 6095.244
    update_time_ms: 24.189
  timestamp: 1602428626
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 20
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     20 |          340.801 | 1617920 |  238.031 |              288.899 |               146.02 |            830.345 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3479.526524541398
    time_step_min: 3149
  date: 2020-10-11_15-04-03
  done: false
  episode_len_mean: 827.4918195339613
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 238.81921345332347
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 121
  episodes_total: 2017
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8949668288230896
        entropy_coeff: 0.0001
        kl: 0.00522464383393526
        model: {}
        policy_loss: -0.015539329266175628
        total_loss: 16.214202308654784
        vf_explained_var: 0.9754984974861145
        vf_loss: 16.228786277770997
    num_steps_sampled: 1698816
    num_steps_trained: 1698816
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.51
    gpu_util_percent0: 0.34750000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12244846300265959
    mean_env_wait_ms: 0.6495053775860202
    mean_inference_ms: 4.661203272043386
    mean_raw_obs_processing_ms: 0.2677822534401562
  time_since_restore: 357.5641474723816
  time_this_iter_s: 16.763281106948853
  time_total_s: 357.5641474723816
  timers:
    learn_throughput: 7567.063
    learn_time_ms: 10690.541
    sample_throughput: 13311.187
    sample_time_ms: 6077.294
    update_time_ms: 25.046
  timestamp: 1602428643
  timesteps_since_restore: 0
  timesteps_total: 1698816
  training_iteration: 21
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     21 |          357.564 | 1698816 |  238.819 |              288.899 |               146.02 |            827.492 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3474.1439287388653
    time_step_min: 3149
  date: 2020-10-11_15-04-19
  done: false
  episode_len_mean: 825.413033286451
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 239.63475827188907
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 116
  episodes_total: 2133
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8959186911582947
        entropy_coeff: 0.0001
        kl: 0.00528133912011981
        model: {}
        policy_loss: -0.011150027438998223
        total_loss: 11.039695358276367
        vf_explained_var: 0.9806930422782898
        vf_loss: 11.04987850189209
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.978947368421046
    gpu_util_percent0: 0.35105263157894734
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4842105263157896
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12226120558461344
    mean_env_wait_ms: 0.6500578173986041
    mean_inference_ms: 4.649194209681777
    mean_raw_obs_processing_ms: 0.2672738214303179
  time_since_restore: 374.14853858947754
  time_this_iter_s: 16.584391117095947
  time_total_s: 374.14853858947754
  timers:
    learn_throughput: 7584.128
    learn_time_ms: 10666.487
    sample_throughput: 13343.395
    sample_time_ms: 6062.625
    update_time_ms: 23.786
  timestamp: 1602428659
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 22
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     22 |          374.149 | 1779712 |  239.635 |              288.899 |               146.02 |            825.413 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3471.4570524412297
    time_step_min: 3149
  date: 2020-10-11_15-04-36
  done: false
  episode_len_mean: 824.0126582278481
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 240.0418607412278
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 79
  episodes_total: 2212
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8983033895492554
        entropy_coeff: 0.0001
        kl: 0.005707137286663055
        model: {}
        policy_loss: -0.011461784783750772
        total_loss: 10.742603302001953
        vf_explained_var: 0.9777244329452515
        vf_loss: 10.753013420104981
    num_steps_sampled: 1860608
    num_steps_trained: 1860608
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.78947368421052
    gpu_util_percent0: 0.35052631578947374
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12214379206448586
    mean_env_wait_ms: 0.6504145672652913
    mean_inference_ms: 4.641409733081465
    mean_raw_obs_processing_ms: 0.2669458875142305
  time_since_restore: 390.91758847236633
  time_this_iter_s: 16.769049882888794
  time_total_s: 390.91758847236633
  timers:
    learn_throughput: 7581.401
    learn_time_ms: 10670.323
    sample_throughput: 13349.552
    sample_time_ms: 6059.829
    update_time_ms: 23.955
  timestamp: 1602428676
  timesteps_since_restore: 0
  timesteps_total: 1860608
  training_iteration: 23
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     23 |          390.918 | 1860608 |  240.042 |              288.899 |               146.02 |            824.013 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3468.180911062907
    time_step_min: 3149
  date: 2020-10-11_15-04-53
  done: false
  episode_len_mean: 822.7310195227766
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 240.53824579854947
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 93
  episodes_total: 2305
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8726944923400879
        entropy_coeff: 0.0001
        kl: 0.005827944166958332
        model: {}
        policy_loss: -0.012724796333350242
        total_loss: 12.167643547058105
        vf_explained_var: 0.9788719415664673
        vf_loss: 12.179289817810059
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.015
    gpu_util_percent0: 0.3365
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12201712317018855
    mean_env_wait_ms: 0.6508549566801675
    mean_inference_ms: 4.632800196509853
    mean_raw_obs_processing_ms: 0.26659025294640726
  time_since_restore: 407.82813453674316
  time_this_iter_s: 16.91054606437683
  time_total_s: 407.82813453674316
  timers:
    learn_throughput: 7580.439
    learn_time_ms: 10671.677
    sample_throughput: 13399.299
    sample_time_ms: 6037.331
    update_time_ms: 23.73
  timestamp: 1602428693
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 24
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     24 |          407.828 | 1941504 |  240.538 |              288.899 |               146.02 |            822.731 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3462.297794117647
    time_step_min: 3149
  date: 2020-10-11_15-05-10
  done: false
  episode_len_mean: 820.6352124183006
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 241.42962715389186
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 143
  episodes_total: 2448
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8506251096725463
        entropy_coeff: 0.0001
        kl: 0.004865732230246067
        model: {}
        policy_loss: -0.011699284473434091
        total_loss: 13.54094352722168
        vf_explained_var: 0.9798793792724609
        vf_loss: 13.551755332946778
    num_steps_sampled: 2022400
    num_steps_trained: 2022400
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.76000000000001
    gpu_util_percent0: 0.28049999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4799999999999995
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1218280208816506
    mean_env_wait_ms: 0.651444924363909
    mean_inference_ms: 4.620390322632076
    mean_raw_obs_processing_ms: 0.2660637639309074
  time_since_restore: 424.82299304008484
  time_this_iter_s: 16.994858503341675
  time_total_s: 424.82299304008484
  timers:
    learn_throughput: 7564.308
    learn_time_ms: 10694.435
    sample_throughput: 13439.475
    sample_time_ms: 6019.283
    update_time_ms: 23.251
  timestamp: 1602428710
  timesteps_since_restore: 0
  timesteps_total: 2022400
  training_iteration: 25
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     25 |          424.823 | 2022400 |   241.43 |              288.899 |               146.02 |            820.635 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3460.0423259493673
    time_step_min: 3149
  date: 2020-10-11_15-05-27
  done: false
  episode_len_mean: 819.539161392405
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 241.7713647551464
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 80
  episodes_total: 2528
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.8542515873908997
        entropy_coeff: 0.0001
        kl: 0.005588684789836406
        model: {}
        policy_loss: -0.013171911868266762
        total_loss: 15.62488899230957
        vf_explained_var: 0.9691354632377625
        vf_loss: 15.637587738037109
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.52631578947369
    gpu_util_percent0: 0.3205263157894737
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12173182767832617
    mean_env_wait_ms: 0.6517732236838806
    mean_inference_ms: 4.613996682870115
    mean_raw_obs_processing_ms: 0.26579729603070834
  time_since_restore: 441.42351174354553
  time_this_iter_s: 16.600518703460693
  time_total_s: 441.42351174354553
  timers:
    learn_throughput: 7565.413
    learn_time_ms: 10692.873
    sample_throughput: 13476.302
    sample_time_ms: 6002.834
    update_time_ms: 23.495
  timestamp: 1602428727
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 26
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     26 |          441.424 | 2103296 |  241.771 |              288.899 |               146.02 |            819.539 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3456.8169984686065
    time_step_min: 3149
  date: 2020-10-11_15-05-44
  done: false
  episode_len_mean: 818.4391271056661
  episode_reward_max: 288.89898989899024
  episode_reward_mean: 242.26005073707984
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 84
  episodes_total: 2612
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.8317709803581238
        entropy_coeff: 0.0001
        kl: 0.00648640925064683
        model: {}
        policy_loss: -0.016052717715501784
        total_loss: 10.588752174377442
        vf_explained_var: 0.9785087704658508
        vf_loss: 10.60423927307129
    num_steps_sampled: 2184192
    num_steps_trained: 2184192
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.810000000000002
    gpu_util_percent0: 0.318
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12163409969565207
    mean_env_wait_ms: 0.6521124443046049
    mean_inference_ms: 4.6076088059101075
    mean_raw_obs_processing_ms: 0.2655282922306935
  time_since_restore: 458.18903517723083
  time_this_iter_s: 16.765523433685303
  time_total_s: 458.18903517723083
  timers:
    learn_throughput: 7569.025
    learn_time_ms: 10687.771
    sample_throughput: 13459.131
    sample_time_ms: 6010.492
    update_time_ms: 23.601
  timestamp: 1602428744
  timesteps_since_restore: 0
  timesteps_total: 2184192
  training_iteration: 27
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     27 |          458.189 | 2184192 |   242.26 |              288.899 |               146.02 |            818.439 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3452.055938975663
    time_step_min: 3111
  date: 2020-10-11_15-06-01
  done: false
  episode_len_mean: 816.755539411551
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 242.98142338752584
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 141
  episodes_total: 2753
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.8073569655418396
        entropy_coeff: 0.0001
        kl: 0.007314516603946686
        model: {}
        policy_loss: -0.012918723002076148
        total_loss: 14.855539894104004
        vf_explained_var: 0.9783770442008972
        vf_loss: 14.867807388305664
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.61052631578947
    gpu_util_percent0: 0.34789473684210526
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.121486994856473
    mean_env_wait_ms: 0.652656269670233
    mean_inference_ms: 4.597419578814093
    mean_raw_obs_processing_ms: 0.2650989345344607
  time_since_restore: 474.9974591732025
  time_this_iter_s: 16.80842399597168
  time_total_s: 474.9974591732025
  timers:
    learn_throughput: 7570.675
    learn_time_ms: 10685.441
    sample_throughput: 13437.573
    sample_time_ms: 6020.135
    update_time_ms: 23.905
  timestamp: 1602428761
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 28
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     28 |          474.997 | 2265088 |  242.981 |              294.657 |               146.02 |            816.756 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3449.462728551336
    time_step_min: 3111
  date: 2020-10-11_15-06-17
  done: false
  episode_len_mean: 815.731364275668
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 243.3743340578784
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 91
  episodes_total: 2844
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.8145095586776734
        entropy_coeff: 0.0001
        kl: 0.005709170270711184
        model: {}
        policy_loss: -0.011662486474961042
        total_loss: 12.014441299438477
        vf_explained_var: 0.9769560098648071
        vf_loss: 12.025614356994629
    num_steps_sampled: 2345984
    num_steps_trained: 2345984
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.90500000000001
    gpu_util_percent0: 0.352
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12139221106336566
    mean_env_wait_ms: 0.6529852958216937
    mean_inference_ms: 4.591341456669196
    mean_raw_obs_processing_ms: 0.2648428504848152
  time_since_restore: 491.6463711261749
  time_this_iter_s: 16.648911952972412
  time_total_s: 491.6463711261749
  timers:
    learn_throughput: 7568.49
    learn_time_ms: 10688.526
    sample_throughput: 13464.931
    sample_time_ms: 6007.903
    update_time_ms: 23.866
  timestamp: 1602428777
  timesteps_since_restore: 0
  timesteps_total: 2345984
  training_iteration: 29
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     29 |          491.646 | 2345984 |  243.374 |              294.657 |               146.02 |            815.731 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3446.7897435897435
    time_step_min: 3111
  date: 2020-10-11_15-06-34
  done: false
  episode_len_mean: 814.8417094017094
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 243.7793317793318
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 81
  episodes_total: 2925
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.8143168568611145
        entropy_coeff: 0.0001
        kl: 0.006050251703709364
        model: {}
        policy_loss: -0.014920068671926856
        total_loss: 10.06276626586914
        vf_explained_var: 0.9785787463188171
        vf_loss: 10.07716236114502
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.18947368421053
    gpu_util_percent0: 0.32473684210526316
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12131394765638424
    mean_env_wait_ms: 0.6532775137196953
    mean_inference_ms: 4.58615184577209
    mean_raw_obs_processing_ms: 0.26462242488560034
  time_since_restore: 508.3601791858673
  time_this_iter_s: 16.713808059692383
  time_total_s: 508.3601791858673
  timers:
    learn_throughput: 7570.724
    learn_time_ms: 10685.372
    sample_throughput: 13479.185
    sample_time_ms: 6001.55
    update_time_ms: 23.677
  timestamp: 1602428794
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 30
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     30 |           508.36 | 2426880 |  243.779 |              294.657 |               146.02 |            814.842 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3443.8039408866994
    time_step_min: 3111
  date: 2020-10-11_15-06-52
  done: false
  episode_len_mean: 813.6032840722496
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 244.2317261282779
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 120
  episodes_total: 3045
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7847271919250488
        entropy_coeff: 0.0001
        kl: 0.006011144630610943
        model: {}
        policy_loss: -0.0123886376619339
        total_loss: 13.336679077148437
        vf_explained_var: 0.9797344207763672
        vf_loss: 13.348544883728028
    num_steps_sampled: 2507776
    num_steps_trained: 2507776
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.325
    gpu_util_percent0: 0.326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12120461289636575
    mean_env_wait_ms: 0.6537134773825316
    mean_inference_ms: 4.578829606903599
    mean_raw_obs_processing_ms: 0.26431758902828895
  time_since_restore: 525.2894396781921
  time_this_iter_s: 16.92926049232483
  time_total_s: 525.2894396781921
  timers:
    learn_throughput: 7559.066
    learn_time_ms: 10701.852
    sample_throughput: 13477.547
    sample_time_ms: 6002.279
    update_time_ms: 22.455
  timestamp: 1602428812
  timesteps_since_restore: 0
  timesteps_total: 2507776
  training_iteration: 31
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     31 |          525.289 | 2507776 |  244.232 |              294.657 |               146.02 |            813.603 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3441.074050632911
    time_step_min: 3111
  date: 2020-10-11_15-07-09
  done: false
  episode_len_mean: 812.5060126582279
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 244.64534586370036
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 115
  episodes_total: 3160
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7759140849113464
        entropy_coeff: 0.0001
        kl: 0.005504181887954473
        model: {}
        policy_loss: -0.011239721812307835
        total_loss: 11.656171417236328
        vf_explained_var: 0.9791274070739746
        vf_loss: 11.666938591003419
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.285000000000004
    gpu_util_percent0: 0.3175
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12110333400562719
    mean_env_wait_ms: 0.6540812881827459
    mean_inference_ms: 4.57217500758545
    mean_raw_obs_processing_ms: 0.2640325453469295
  time_since_restore: 542.2914707660675
  time_this_iter_s: 17.002031087875366
  time_total_s: 542.2914707660675
  timers:
    learn_throughput: 7540.968
    learn_time_ms: 10727.535
    sample_throughput: 13444.222
    sample_time_ms: 6017.157
    update_time_ms: 23.317
  timestamp: 1602428829
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 32
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     32 |          542.291 | 2588672 |  244.645 |              294.657 |               146.02 |            812.506 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3439.0583333333334
    time_step_min: 3111
  date: 2020-10-11_15-07-25
  done: false
  episode_len_mean: 811.7601851851852
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 244.95075757575762
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 80
  episodes_total: 3240
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7811835169792175
        entropy_coeff: 0.0001
        kl: 0.006416494492441416
        model: {}
        policy_loss: -0.012559976987540722
        total_loss: 8.990324211120605
        vf_explained_var: 0.9806587100028992
        vf_loss: 9.002320289611816
    num_steps_sampled: 2669568
    num_steps_trained: 2669568
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.37894736842105
    gpu_util_percent0: 0.2831578947368421
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12103828068124152
    mean_env_wait_ms: 0.6543448696043557
    mean_inference_ms: 4.567834982455508
    mean_raw_obs_processing_ms: 0.2638492420763033
  time_since_restore: 558.9726655483246
  time_this_iter_s: 16.68119478225708
  time_total_s: 558.9726655483246
  timers:
    learn_throughput: 7553.285
    learn_time_ms: 10710.042
    sample_throughput: 13425.57
    sample_time_ms: 6025.517
    update_time_ms: 23.002
  timestamp: 1602428845
  timesteps_since_restore: 0
  timesteps_total: 2669568
  training_iteration: 33
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     33 |          558.973 | 2669568 |  244.951 |              294.657 |               146.02 |             811.76 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3436.664176874813
    time_step_min: 3111
  date: 2020-10-11_15-07-42
  done: false
  episode_len_mean: 810.7051090528831
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 245.3135085543213
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 107
  episodes_total: 3347
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7612020254135132
        entropy_coeff: 0.0001
        kl: 0.005618926044553519
        model: {}
        policy_loss: -0.014426779658242595
        total_loss: 10.50188980102539
        vf_explained_var: 0.9819744229316711
        vf_loss: 10.515831184387206
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.050000000000004
    gpu_util_percent0: 0.3135
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12095196212371193
    mean_env_wait_ms: 0.6546898335950064
    mean_inference_ms: 4.562119786136694
    mean_raw_obs_processing_ms: 0.2636064951710148
  time_since_restore: 575.9109237194061
  time_this_iter_s: 16.938258171081543
  time_total_s: 575.9109237194061
  timers:
    learn_throughput: 7548.387
    learn_time_ms: 10716.991
    sample_throughput: 13440.954
    sample_time_ms: 6018.62
    update_time_ms: 25.267
  timestamp: 1602428862
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 34
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     34 |          575.911 | 2750464 |  245.314 |              294.657 |               146.02 |            810.705 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3433.486323063634
    time_step_min: 3111
  date: 2020-10-11_15-08-00
  done: false
  episode_len_mean: 809.8491217967176
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 245.79500155601514
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 126
  episodes_total: 3473
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.755747628211975
        entropy_coeff: 0.0001
        kl: 0.00591852879151702
        model: {}
        policy_loss: -0.012422900833189487
        total_loss: 9.482335662841797
        vf_explained_var: 0.9842199087142944
        vf_loss: 9.494241714477539
    num_steps_sampled: 2831360
    num_steps_trained: 2831360
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.169999999999998
    gpu_util_percent0: 0.313
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715647
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12085747040716455
    mean_env_wait_ms: 0.6550729696743465
    mean_inference_ms: 4.555915738902905
    mean_raw_obs_processing_ms: 0.26334772572820075
  time_since_restore: 592.8397195339203
  time_this_iter_s: 16.92879581451416
  time_total_s: 592.8397195339203
  timers:
    learn_throughput: 7550.241
    learn_time_ms: 10714.361
    sample_throughput: 13452.948
    sample_time_ms: 6013.254
    update_time_ms: 25.59
  timestamp: 1602428880
  timesteps_since_restore: 0
  timesteps_total: 2831360
  training_iteration: 35
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | RUNNING  | 172.17.0.4:79049 |     35 |           592.84 | 2831360 |  245.795 |              294.657 |               146.02 |            809.849 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_25e0a_00000:
  custom_metrics:
    time_step_max: 4092
    time_step_mean: 3432.0928270042195
    time_step_min: 3111
  date: 2020-10-11_15-08-17
  done: true
  episode_len_mean: 809.4239099859353
  episode_reward_max: 294.65656565656593
  episode_reward_mean: 246.00613732259308
  episode_reward_min: 146.02020202020225
  episodes_this_iter: 82
  episodes_total: 3555
  experiment_id: 53c1d7d85d994ed59577abcd9844eca4
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.762268352508545
        entropy_coeff: 0.0001
        kl: 0.005749249923974275
        model: {}
        policy_loss: -0.014044645662215772
        total_loss: 8.698633766174316
        vf_explained_var: 0.9826677441596985
        vf_loss: 8.712179946899415
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.48947368421053
    gpu_util_percent0: 0.3121052631578947
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5
    vram_util_percent0: 0.11634962282715645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 79049
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12080047013051126
    mean_env_wait_ms: 0.6553066919371578
    mean_inference_ms: 4.552032024188442
    mean_raw_obs_processing_ms: 0.263186835860797
  time_since_restore: 609.7753958702087
  time_this_iter_s: 16.935676336288452
  time_total_s: 609.7753958702087
  timers:
    learn_throughput: 7537.787
    learn_time_ms: 10732.062
    sample_throughput: 13421.011
    sample_time_ms: 6027.564
    update_time_ms: 26.437
  timestamp: 1602428897
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 36
  trial_id: 25e0a_00000
  
== Status ==
Memory usage on this node: 26.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | TERMINATED |       |     36 |          609.775 | 2912256 |  246.006 |              294.657 |               146.02 |            809.424 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 26.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_25e0a_00000 | TERMINATED |       |     36 |          609.775 | 2912256 |  246.006 |              294.657 |               146.02 |            809.424 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


