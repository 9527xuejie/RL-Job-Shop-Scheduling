2020-10-12 00:56:05,405	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_b5b60_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=70242)[0m 2020-10-12 00:56:08,169	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=70134)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70134)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70131)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70131)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70218)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70218)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70203)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70203)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70193)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70193)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70139)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70139)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70142)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70142)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70117)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70117)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70229)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70229)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70113)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70113)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70214)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70214)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70198)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70198)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70233)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70233)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70176)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70176)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70115)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70115)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70209)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70209)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70208)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70208)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70236)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70236)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70216)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70216)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70219)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70219)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70211)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70211)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70205)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70205)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70248)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70248)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70210)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70210)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70201)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70201)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70240)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70240)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70235)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70235)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70197)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70197)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70223)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70223)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70145)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70145)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70188)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70188)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70149)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70149)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70213)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70213)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70199)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70199)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70179)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70179)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70244)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70244)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70181)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70181)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70234)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70234)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70175)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70175)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70112)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70112)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70116)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70116)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70231)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70231)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70195)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70195)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70126)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70126)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70228)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70228)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70194)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70194)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70125)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70125)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70224)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70224)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70136)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70136)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70226)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70226)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70128)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70128)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70196)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70196)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70127)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70127)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70120)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70120)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70189)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70189)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70191)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70191)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70172)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70172)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70123)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70123)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70238)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70238)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70221)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70221)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70133)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70133)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70138)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70138)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70118)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70118)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70129)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70129)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70252)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70252)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70183)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70183)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70143)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70143)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70190)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70190)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70148)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70148)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70132)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70132)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70202)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70202)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70178)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70178)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70114)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70114)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70141)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70141)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70200)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70200)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70192)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70192)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70130)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70130)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70121)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70121)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70124)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70124)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-12_00-56-49
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.1818222403526306
        entropy_coeff: 0.0005000000000000001
        kl: 0.007154064990269641
        model: {}
        policy_loss: -0.013903728065391382
        total_loss: 500.4119695027669
        vf_explained_var: 0.5819632411003113
        vf_loss: 500.42430623372394
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.764285714285716
    gpu_util_percent0: 0.3466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5904761904761893
    vram_util_percent0: 0.08979915350856645
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17175475400551926
    mean_env_wait_ms: 1.1755354782062093
    mean_inference_ms: 5.977909309549735
    mean_raw_obs_processing_ms: 0.4632059870475823
  time_since_restore: 35.96728754043579
  time_this_iter_s: 35.96728754043579
  time_total_s: 35.96728754043579
  timers:
    learn_throughput: 6106.761
    learn_time_ms: 26493.913
    sample_throughput: 17218.769
    sample_time_ms: 9396.258
    update_time_ms: 39.687
  timestamp: 1602464209
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      1 |          35.9673 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3628.3819444444443
    time_step_min: 3310
  date: 2020-10-12_00-57-23
  done: false
  episode_len_mean: 891.0854430379746
  episode_reward_max: 264.50505050505035
  episode_reward_mean: 215.95978775092678
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.1480142672856648
        entropy_coeff: 0.0005000000000000001
        kl: 0.009108403542389473
        model: {}
        policy_loss: -0.01838825355904798
        total_loss: 129.07871627807617
        vf_explained_var: 0.8130671977996826
        vf_loss: 129.09494400024414
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.9125
    gpu_util_percent0: 0.30900000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7674999999999996
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1669472100676312
    mean_env_wait_ms: 1.1696362740515263
    mean_inference_ms: 5.70164367946033
    mean_raw_obs_processing_ms: 0.44969586330255223
  time_since_restore: 70.19521570205688
  time_this_iter_s: 34.227928161621094
  time_total_s: 70.19521570205688
  timers:
    learn_throughput: 6119.319
    learn_time_ms: 26439.544
    sample_throughput: 18860.304
    sample_time_ms: 8578.441
    update_time_ms: 36.322
  timestamp: 1602464243
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      2 |          70.1952 | 323584 |   215.96 |              264.505 |              119.505 |            891.085 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3627.3206278026905
    time_step_min: 3295
  date: 2020-10-12_00-57-57
  done: false
  episode_len_mean: 890.2257383966245
  episode_reward_max: 266.77777777777754
  episode_reward_mean: 216.98267484976327
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.1395687758922577
        entropy_coeff: 0.0005000000000000001
        kl: 0.01083881144101421
        model: {}
        policy_loss: -0.018865213457805414
        total_loss: 44.83079751332601
        vf_explained_var: 0.9206477999687195
        vf_loss: 44.846981366475426
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.88
    gpu_util_percent0: 0.40374999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7825
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16378402142593632
    mean_env_wait_ms: 1.1672259679168866
    mean_inference_ms: 5.497577140884755
    mean_raw_obs_processing_ms: 0.44045127959237207
  time_since_restore: 104.00969576835632
  time_this_iter_s: 33.81448006629944
  time_total_s: 104.00969576835632
  timers:
    learn_throughput: 6113.912
    learn_time_ms: 26462.928
    sample_throughput: 19918.8
    sample_time_ms: 8122.578
    update_time_ms: 37.125
  timestamp: 1602464277
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      3 |           104.01 | 485376 |  216.983 |              266.778 |              119.505 |            890.226 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3618.3112582781455
    time_step_min: 3295
  date: 2020-10-12_00-58-31
  done: false
  episode_len_mean: 891.882911392405
  episode_reward_max: 272.8383838383836
  episode_reward_mean: 218.3818725226951
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.1154198348522186
        entropy_coeff: 0.0005000000000000001
        kl: 0.011366037031014761
        model: {}
        policy_loss: -0.021963271622856457
        total_loss: 26.04944149653117
        vf_explained_var: 0.9526428580284119
        vf_loss: 26.068552652994793
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.866666666666667
    gpu_util_percent0: 0.3346153846153847
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7769230769230786
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1615641817231825
    mean_env_wait_ms: 1.1649727866640938
    mean_inference_ms: 5.349151217712109
    mean_raw_obs_processing_ms: 0.433465804206278
  time_since_restore: 137.45350241661072
  time_this_iter_s: 33.443806648254395
  time_total_s: 137.45350241661072
  timers:
    learn_throughput: 6126.325
    learn_time_ms: 26409.308
    sample_throughput: 20558.882
    sample_time_ms: 7869.689
    update_time_ms: 37.383
  timestamp: 1602464311
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      4 |          137.454 | 647168 |  218.382 |              272.838 |              119.505 |            891.883 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3608.8215223097113
    time_step_min: 3295
  date: 2020-10-12_00-59-04
  done: false
  episode_len_mean: 892.7392405063291
  episode_reward_max: 272.8383838383836
  episode_reward_mean: 219.23846055491603
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.0904381076494853
        entropy_coeff: 0.0005000000000000001
        kl: 0.012052082611868778
        model: {}
        policy_loss: -0.019467259427377332
        total_loss: 21.272361119588215
        vf_explained_var: 0.9621942639350891
        vf_loss: 21.288758118947346
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.982051282051284
    gpu_util_percent0: 0.3225641025641025
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779487179487181
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15989844175836182
    mean_env_wait_ms: 1.1634064304305094
    mean_inference_ms: 5.236753692960018
    mean_raw_obs_processing_ms: 0.4279690688023438
  time_since_restore: 170.9908845424652
  time_this_iter_s: 33.53738212585449
  time_total_s: 170.9908845424652
  timers:
    learn_throughput: 6124.45
    learn_time_ms: 26417.391
    sample_throughput: 21024.902
    sample_time_ms: 7695.256
    update_time_ms: 38.477
  timestamp: 1602464344
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      5 |          170.991 | 808960 |  219.238 |              272.838 |              119.505 |            892.739 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3604.651515151515
    time_step_min: 3295
  date: 2020-10-12_00-59-38
  done: false
  episode_len_mean: 891.6292016806723
  episode_reward_max: 279.2020202020193
  episode_reward_mean: 219.97121424327284
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 162
  episodes_total: 952
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.057053456703822
        entropy_coeff: 0.0005000000000000001
        kl: 0.010474394153182706
        model: {}
        policy_loss: -0.01984739606268704
        total_loss: 21.618106365203857
        vf_explained_var: 0.9685359597206116
        vf_loss: 21.635340372721355
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.01025641025641
    gpu_util_percent0: 0.3123076923076923
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7743589743589756
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15855617454232815
    mean_env_wait_ms: 1.1627494855337062
    mean_inference_ms: 5.145606342236573
    mean_raw_obs_processing_ms: 0.42329064235228
  time_since_restore: 204.48685789108276
  time_this_iter_s: 33.495973348617554
  time_total_s: 204.48685789108276
  timers:
    learn_throughput: 6124.904
    learn_time_ms: 26415.436
    sample_throughput: 21341.875
    sample_time_ms: 7580.965
    update_time_ms: 36.925
  timestamp: 1602464378
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      6 |          204.487 | 970752 |  219.971 |              279.202 |              119.505 |            891.629 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3592.766096169519
    time_step_min: 3240
  date: 2020-10-12_01-00-12
  done: false
  episode_len_mean: 885.7681274900399
  episode_reward_max: 279.2020202020193
  episode_reward_mean: 221.79431767877966
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 303
  episodes_total: 1255
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.0639467239379883
        entropy_coeff: 0.0005000000000000001
        kl: 0.009970864901940027
        model: {}
        policy_loss: -0.018240987168004114
        total_loss: 23.48925193150838
        vf_explained_var: 0.9710940718650818
        vf_loss: 23.505034287770588
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.474358974358974
    gpu_util_percent0: 0.35384615384615387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7820512820512837
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15679669090153517
    mean_env_wait_ms: 1.1629543565718965
    mean_inference_ms: 5.023995056237114
    mean_raw_obs_processing_ms: 0.4172743566991716
  time_since_restore: 238.1854374408722
  time_this_iter_s: 33.69857954978943
  time_total_s: 238.1854374408722
  timers:
    learn_throughput: 6121.077
    learn_time_ms: 26431.949
    sample_throughput: 21545.16
    sample_time_ms: 7509.436
    update_time_ms: 36.62
  timestamp: 1602464412
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      7 |          238.185 | 1132544 |  221.794 |              279.202 |              119.505 |            885.768 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3586.576757532281
    time_step_min: 3240
  date: 2020-10-12_01-00-45
  done: false
  episode_len_mean: 881.9099859353024
  episode_reward_max: 279.2020202020193
  episode_reward_mean: 222.61473383625264
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 167
  episodes_total: 1422
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.0595303773880005
        entropy_coeff: 0.0005000000000000001
        kl: 0.010472465539351106
        model: {}
        policy_loss: -0.020001635731508333
        total_loss: 15.92150886853536
        vf_explained_var: 0.9738628268241882
        vf_loss: 15.938898404439291
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.525641025641022
    gpu_util_percent0: 0.3851282051282051
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7871794871794884
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15606545947580586
    mean_env_wait_ms: 1.1634809782886413
    mean_inference_ms: 4.9732706511403135
    mean_raw_obs_processing_ms: 0.41475663662412215
  time_since_restore: 271.3770980834961
  time_this_iter_s: 33.1916606426239
  time_total_s: 271.3770980834961
  timers:
    learn_throughput: 6126.339
    learn_time_ms: 26409.248
    sample_throughput: 21788.113
    sample_time_ms: 7425.701
    update_time_ms: 37.271
  timestamp: 1602464445
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      8 |          271.377 | 1294336 |  222.615 |              279.202 |              119.505 |             881.91 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3579.402706185567
    time_step_min: 3240
  date: 2020-10-12_01-01-18
  done: false
  episode_len_mean: 877.9335443037975
  episode_reward_max: 279.2020202020193
  episode_reward_mean: 224.02490090781214
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.035999337832133
        entropy_coeff: 0.0005000000000000001
        kl: 0.01094190520234406
        model: {}
        policy_loss: -0.020443368547906477
        total_loss: 13.034753243128458
        vf_explained_var: 0.9764907956123352
        vf_loss: 13.05243213971456
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.52820512820513
    gpu_util_percent0: 0.3817948717948718
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7871794871794893
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15546391342096866
    mean_env_wait_ms: 1.1640974450561448
    mean_inference_ms: 4.931402792695413
    mean_raw_obs_processing_ms: 0.4126344490684239
  time_since_restore: 304.7150387763977
  time_this_iter_s: 33.33794069290161
  time_total_s: 304.7150387763977
  timers:
    learn_throughput: 6123.675
    learn_time_ms: 26420.738
    sample_throughput: 22016.81
    sample_time_ms: 7348.567
    update_time_ms: 37.742
  timestamp: 1602464478
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |      9 |          304.715 | 1456128 |  224.025 |              279.202 |              119.505 |            877.934 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3569.9619883040937
    time_step_min: 3211
  date: 2020-10-12_01-01-52
  done: false
  episode_len_mean: 874.4010356731876
  episode_reward_max: 279.5050505050503
  episode_reward_mean: 225.35660401483167
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 1738
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.0142365197340648
        entropy_coeff: 0.0005000000000000001
        kl: 0.010611492597187558
        model: {}
        policy_loss: -0.02072665700688958
        total_loss: 11.785370826721191
        vf_explained_var: 0.9780337810516357
        vf_loss: 11.803421099980673
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.60526315789474
    gpu_util_percent0: 0.3602631578947369
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7842105263157904
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15493128777729273
    mean_env_wait_ms: 1.1648040149997074
    mean_inference_ms: 4.894247226660159
    mean_raw_obs_processing_ms: 0.4106865050065554
  time_since_restore: 338.10313296318054
  time_this_iter_s: 33.38809418678284
  time_total_s: 338.10313296318054
  timers:
    learn_throughput: 6122.687
    learn_time_ms: 26425.001
    sample_throughput: 22175.668
    sample_time_ms: 7295.925
    update_time_ms: 38.329
  timestamp: 1602464512
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |     10 |          338.103 | 1617920 |  225.357 |              279.505 |              119.505 |            874.401 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3558.6770886724757
    time_step_min: 3211
  date: 2020-10-12_01-02-25
  done: false
  episode_len_mean: 868.2693279434058
  episode_reward_max: 279.5050505050503
  episode_reward_mean: 227.09604381357775
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 241
  episodes_total: 1979
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 0.9745247761408488
        entropy_coeff: 0.0005000000000000001
        kl: 0.010795687014857927
        model: {}
        policy_loss: -0.020097777635479968
        total_loss: 16.544294516245525
        vf_explained_var: 0.9785184264183044
        vf_loss: 16.56164042154948
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.325641025641023
    gpu_util_percent0: 0.3743589743589744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7769230769230786
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15422873533216327
    mean_env_wait_ms: 1.1662261342453708
    mean_inference_ms: 4.845834114583803
    mean_raw_obs_processing_ms: 0.40810973404083845
  time_since_restore: 371.42678570747375
  time_this_iter_s: 33.32365274429321
  time_total_s: 371.42678570747375
  timers:
    learn_throughput: 6126.936
    learn_time_ms: 26406.673
    sample_throughput: 22955.707
    sample_time_ms: 7048.008
    update_time_ms: 38.519
  timestamp: 1602464545
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |     11 |          371.427 | 1779712 |  227.096 |              279.505 |              119.505 |            868.269 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3547.037087912088
    time_step_min: 3211
  date: 2020-10-12_01-02-59
  done: false
  episode_len_mean: 863.619349005425
  episode_reward_max: 279.5050505050503
  episode_reward_mean: 228.6358247940525
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 233
  episodes_total: 2212
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 0.9877658486366272
        entropy_coeff: 0.0005000000000000001
        kl: 0.009924562027057013
        model: {}
        policy_loss: -0.0200248621404171
        total_loss: 13.261031548182169
        vf_explained_var: 0.9796447157859802
        vf_loss: 13.278573195139566
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.048717948717954
    gpu_util_percent0: 0.4141025641025641
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779487179487181
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15367421865970865
    mean_env_wait_ms: 1.1677030872910223
    mean_inference_ms: 4.806895154001889
    mean_raw_obs_processing_ms: 0.4060944925247294
  time_since_restore: 404.873140335083
  time_this_iter_s: 33.44635462760925
  time_total_s: 404.873140335083
  timers:
    learn_throughput: 6128.195
    learn_time_ms: 26401.247
    sample_throughput: 23200.1
    sample_time_ms: 6973.763
    update_time_ms: 39.474
  timestamp: 1602464579
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |     12 |          404.873 | 1941504 |  228.636 |              279.505 |              119.505 |            863.619 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3539.621263877028
    time_step_min: 3211
  date: 2020-10-12_01-03-32
  done: false
  episode_len_mean: 860.828270042194
  episode_reward_max: 279.5050505050503
  episode_reward_mean: 229.74530111238957
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 2370
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 0.9634115596612295
        entropy_coeff: 0.0005000000000000001
        kl: 0.010928460862487555
        model: {}
        policy_loss: -0.020925682542535167
        total_loss: 9.976799170176188
        vf_explained_var: 0.9820815920829773
        vf_loss: 9.994928201039633
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.494736842105265
    gpu_util_percent0: 0.3857894736842106
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7842105263157904
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1533391267431134
    mean_env_wait_ms: 1.168608685483693
    mean_inference_ms: 4.783691579938285
    mean_raw_obs_processing_ms: 0.40485882787071975
  time_since_restore: 438.2347252368927
  time_this_iter_s: 33.36158490180969
  time_total_s: 438.2347252368927
  timers:
    learn_throughput: 6133.972
    learn_time_ms: 26376.385
    sample_throughput: 23271.922
    sample_time_ms: 6952.24
    update_time_ms: 39.516
  timestamp: 1602464612
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |     13 |          438.235 | 2103296 |  229.745 |              279.505 |              119.505 |            860.828 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3532.0348
    time_step_min: 3211
  date: 2020-10-12_01-04-06
  done: false
  episode_len_mean: 857.9200949367089
  episode_reward_max: 279.5050505050503
  episode_reward_mean: 230.8380002557216
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 2528
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 0.9278096904357275
        entropy_coeff: 0.0005000000000000001
        kl: 0.01057234825566411
        model: {}
        policy_loss: -0.020143959516038496
        total_loss: 9.476422468821207
        vf_explained_var: 0.9823716282844543
        vf_loss: 9.493858734766642
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.24615384615385
    gpu_util_percent0: 0.3676923076923076
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7820512820512837
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15303236272841575
    mean_env_wait_ms: 1.169491079365702
    mean_inference_ms: 4.76242209843245
    mean_raw_obs_processing_ms: 0.40369607445461714
  time_since_restore: 471.8302457332611
  time_this_iter_s: 33.59552049636841
  time_total_s: 471.8302457332611
  timers:
    learn_throughput: 6123.115
    learn_time_ms: 26423.154
    sample_throughput: 23383.678
    sample_time_ms: 6919.014
    update_time_ms: 39.764
  timestamp: 1602464646
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |     14 |           471.83 | 2265088 |  230.838 |              279.505 |              119.505 |             857.92 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3521.0654069767443
    time_step_min: 3152
  date: 2020-10-12_01-04-40
  done: false
  episode_len_mean: 853.2219424460432
  episode_reward_max: 288.44444444444446
  episode_reward_mean: 232.56680110457074
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 252
  episodes_total: 2780
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 0.9003855437040329
        entropy_coeff: 0.0005000000000000001
        kl: 0.01034098615248998
        model: {}
        policy_loss: -0.018298421210298937
        total_loss: 12.77210815747579
        vf_explained_var: 0.982788622379303
        vf_loss: 12.78775461514791
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.130000000000003
    gpu_util_percent0: 0.36574999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7775
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15258297680573862
    mean_env_wait_ms: 1.1709236593556978
    mean_inference_ms: 4.731990605257719
    mean_raw_obs_processing_ms: 0.4020542091538528
  time_since_restore: 505.55780243873596
  time_this_iter_s: 33.72755670547485
  time_total_s: 505.55780243873596
  timers:
    learn_throughput: 6117.176
    learn_time_ms: 26448.805
    sample_throughput: 23435.896
    sample_time_ms: 6903.598
    update_time_ms: 39.493
  timestamp: 1602464680
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |     15 |          505.558 | 2426880 |  232.567 |              288.444 |              119.505 |            853.222 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3512.2199058507063
    time_step_min: 3152
  date: 2020-10-12_01-05-13
  done: false
  episode_len_mean: 850.4123917388408
  episode_reward_max: 288.44444444444446
  episode_reward_mean: 233.8752683396253
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 222
  episodes_total: 3002
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 0.9069168319304785
        entropy_coeff: 0.0005000000000000001
        kl: 0.009634924198811253
        model: {}
        policy_loss: -0.020494397341584165
        total_loss: 9.3593057791392
        vf_explained_var: 0.9852428436279297
        vf_loss: 9.377363363901773
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.563157894736843
    gpu_util_percent0: 0.33657894736842103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.778947368421054
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15225680432883576
    mean_env_wait_ms: 1.1721503863614784
    mean_inference_ms: 4.708857073912601
    mean_raw_obs_processing_ms: 0.40079476680200554
  time_since_restore: 538.8480432033539
  time_this_iter_s: 33.29024076461792
  time_total_s: 538.8480432033539
  timers:
    learn_throughput: 6117.113
    learn_time_ms: 26449.08
    sample_throughput: 23512.193
    sample_time_ms: 6881.196
    update_time_ms: 40.409
  timestamp: 1602464713
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |     16 |          538.848 | 2588672 |  233.875 |              288.444 |              119.505 |            850.412 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3506.5098978288634
    time_step_min: 3152
  date: 2020-10-12_01-05-47
  done: false
  episode_len_mean: 848.1174050632911
  episode_reward_max: 288.44444444444446
  episode_reward_mean: 234.742440225035
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 158
  episodes_total: 3160
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 0.888948326309522
        entropy_coeff: 0.0005000000000000001
        kl: 0.010426684437940517
        model: {}
        policy_loss: -0.019620118546299636
        total_loss: 9.738235394159952
        vf_explained_var: 0.9822618365287781
        vf_loss: 9.75517233212789
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.515384615384612
    gpu_util_percent0: 0.36974358974358973
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.792307692307694
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520377509993611
    mean_env_wait_ms: 1.1729626020331954
    mean_inference_ms: 4.69373127116815
    mean_raw_obs_processing_ms: 0.3999697716583076
  time_since_restore: 572.1661431789398
  time_this_iter_s: 33.31809997558594
  time_total_s: 572.1661431789398
  timers:
    learn_throughput: 6120.203
    learn_time_ms: 26435.723
    sample_throughput: 23604.099
    sample_time_ms: 6854.403
    update_time_ms: 41.01
  timestamp: 1602464747
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | RUNNING  | 172.17.0.4:70242 |     17 |          572.166 | 2750464 |  234.742 |              288.444 |              119.505 |            848.117 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b5b60_00000:
  custom_metrics:
    time_step_max: 4267
    time_step_mean: 3500.5568698817106
    time_step_min: 3152
  date: 2020-10-12_01-06-20
  done: true
  episode_len_mean: 845.6427067669173
  episode_reward_max: 288.44444444444446
  episode_reward_mean: 235.7232323232322
  episode_reward_min: 119.50505050505036
  episodes_this_iter: 165
  episodes_total: 3325
  experiment_id: 5149c6e8359d450a930804b017afd7ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 0.8549627164999644
        entropy_coeff: 0.0005000000000000001
        kl: 0.011315142658228675
        model: {}
        policy_loss: -0.018999382581872244
        total_loss: 10.610961437225342
        vf_explained_var: 0.9813644289970398
        vf_loss: 10.626994053522745
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 18.961538461538463
    gpu_util_percent0: 0.3633333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.784615384615386
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70242
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1518211695590136
    mean_env_wait_ms: 1.1738143138985686
    mean_inference_ms: 4.679057174318945
    mean_raw_obs_processing_ms: 0.39914702464806334
  time_since_restore: 605.2813889980316
  time_this_iter_s: 33.1152458190918
  time_total_s: 605.2813889980316
  timers:
    learn_throughput: 6119.608
    learn_time_ms: 26438.295
    sample_throughput: 23638.476
    sample_time_ms: 6844.434
    update_time_ms: 40.866
  timestamp: 1602464780
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: b5b60_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | TERMINATED |       |     18 |          605.281 | 2912256 |  235.723 |              288.444 |              119.505 |            845.643 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.96 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b5b60_00000 | TERMINATED |       |     18 |          605.281 | 2912256 |  235.723 |              288.444 |              119.505 |            845.643 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


2020-10-12 01:06:20,721	WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffffccf343cc01000000.
[2m[36m(pid=70133)[0m 2020-10-12 01:06:20,691	ERROR worker.py:372 -- SystemExit was raised from the worker
[2m[36m(pid=70133)[0m Traceback (most recent call last):
[2m[36m(pid=70133)[0m   File "python/ray/_raylet.pyx", line 553, in ray._raylet.task_execution_handler
[2m[36m(pid=70133)[0m   File "python/ray/_raylet.pyx", line 440, in ray._raylet.execute_task
[2m[36m(pid=70133)[0m   File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
[2m[36m(pid=70133)[0m   File "python/ray/_raylet.pyx", line 483, in ray._raylet.execute_task
[2m[36m(pid=70133)[0m   File "python/ray/_raylet.pyx", line 484, in ray._raylet.execute_task
[2m[36m(pid=70133)[0m   File "python/ray/_raylet.pyx", line 438, in ray._raylet.execute_task.function_executor
[2m[36m(pid=70133)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/function_manager.py", line 553, in actor_method_executor
[2m[36m(pid=70133)[0m     return method(actor, *args, **kwargs)
[2m[36m(pid=70133)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 929, in __ray_terminate__
[2m[36m(pid=70133)[0m     ray.actor.exit_actor()
[2m[36m(pid=70133)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 991, in exit_actor
[2m[36m(pid=70133)[0m     ray.state.state.disconnect()
[2m[36m(pid=70133)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/state.py", line 61, in disconnect
[2m[36m(pid=70133)[0m     self.global_state_accessor = None
[2m[36m(pid=70133)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/worker.py", line 369, in sigterm_handler
[2m[36m(pid=70133)[0m     sys.exit(1)
[2m[36m(pid=70133)[0m SystemExit: 1
