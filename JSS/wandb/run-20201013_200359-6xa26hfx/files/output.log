2020-10-13 20:04:03,020	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_3e643_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=427)[0m 2020-10-13 20:04:05,694	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=411)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=411)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=368)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=368)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=412)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=412)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=394)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=394)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=371)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=371)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=362)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=362)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=377)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=377)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=357)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=357)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=360)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=360)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=304)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=304)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=379)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=379)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=365)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=365)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=420)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=420)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=356)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=356)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=392)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=392)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=419)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=419)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=320)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=320)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=384)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=384)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=414)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=414)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=382)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=382)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=287)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=287)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=409)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=409)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=361)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=361)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=359)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=359)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=404)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=404)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=291)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=291)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=312)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=312)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=395)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=395)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=407)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=407)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=369)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=369)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=315)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=315)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=354)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=354)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=417)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=417)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=303)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=303)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=358)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=358)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=309)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=309)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=293)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=293)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=295)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=295)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=373)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=373)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=367)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=367)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=378)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=378)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=374)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=374)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=406)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=406)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=298)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=298)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=348)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=348)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=310)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=310)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=318)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=318)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=319)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=319)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=355)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=355)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=321)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=321)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=366)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=366)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=364)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=364)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=300)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=300)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=352)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=352)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=292)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=292)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=387)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=387)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=294)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=294)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=306)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=306)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=307)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=307)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=288)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=288)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=363)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=363)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=372)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=372)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=305)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=305)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=391)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=391)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=349)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=349)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=308)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=308)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=397)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=397)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=316)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=316)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=290)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=290)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=370)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=370)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=289)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=289)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=381)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=381)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=408)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=408)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=380)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=380)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=317)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=317)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=296)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=296)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=297)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=297)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=385)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=385)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=301)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=301)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-13_20-04-39
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1851047078768413
        entropy_coeff: 0.0005000000000000001
        kl: 0.004071502441850801
        model: {}
        policy_loss: -0.00785889983914482
        total_loss: 507.07567087809247
        vf_explained_var: 0.540532648563385
        vf_loss: 507.0832926432292
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.26666666666667
    gpu_util_percent0: 0.3290909090909091
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.518181818181818
    vram_util_percent0: 0.08750757824224535
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1664086365707129
    mean_env_wait_ms: 1.1600885453066663
    mean_inference_ms: 5.576722786065125
    mean_raw_obs_processing_ms: 0.4417715899993624
  time_since_restore: 28.753361463546753
  time_this_iter_s: 28.753361463546753
  time_total_s: 28.753361463546753
  timers:
    learn_throughput: 8239.455
    learn_time_ms: 19636.249
    sample_throughput: 17877.346
    sample_time_ms: 9050.113
    update_time_ms: 28.303
  timestamp: 1602619479
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      1 |          28.7534 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3614.4305555555557
    time_step_min: 3250
  date: 2020-10-13_20-05-07
  done: false
  episode_len_mean: 890.8607594936709
  episode_reward_max: 273.5959595959592
  episode_reward_mean: 217.6365234624726
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1561074058214824
        entropy_coeff: 0.0005000000000000001
        kl: 0.007923512797181806
        model: {}
        policy_loss: -0.010965243893830726
        total_loss: 127.46906661987305
        vf_explained_var: 0.8076093792915344
        vf_loss: 127.47981770833333
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.8125
    gpu_util_percent0: 0.3309375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.753125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16228866429830452
    mean_env_wait_ms: 1.1570990400005077
    mean_inference_ms: 5.364518877861221
    mean_raw_obs_processing_ms: 0.43185363723879344
  time_since_restore: 55.907390832901
  time_this_iter_s: 27.154029369354248
  time_total_s: 55.907390832901
  timers:
    learn_throughput: 8277.12
    learn_time_ms: 19546.896
    sample_throughput: 19406.529
    sample_time_ms: 8336.988
    update_time_ms: 26.81
  timestamp: 1602619507
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      2 |          55.9074 | 323584 |  217.637 |              273.596 |              145.717 |            890.861 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3601.8677130044844
    time_step_min: 3250
  date: 2020-10-13_20-05-33
  done: false
  episode_len_mean: 885.132911392405
  episode_reward_max: 273.5959595959592
  episode_reward_mean: 219.87009333844756
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1456398169199626
        entropy_coeff: 0.0005000000000000001
        kl: 0.008224547879459957
        model: {}
        policy_loss: -0.013529085864623388
        total_loss: 61.275455474853516
        vf_explained_var: 0.8916645646095276
        vf_loss: 61.28873507181803
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43548387096774
    gpu_util_percent0: 0.31193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15974476898657008
    mean_env_wait_ms: 1.1569916808841463
    mean_inference_ms: 5.204129379825419
    mean_raw_obs_processing_ms: 0.4244393492562123
  time_since_restore: 82.51634931564331
  time_this_iter_s: 26.60895848274231
  time_total_s: 82.51634931564331
  timers:
    learn_throughput: 8303.157
    learn_time_ms: 19485.601
    sample_throughput: 20362.388
    sample_time_ms: 7945.63
    update_time_ms: 29.463
  timestamp: 1602619533
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 27.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      3 |          82.5163 | 485376 |   219.87 |              273.596 |              145.717 |            885.133 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3596.0099337748343
    time_step_min: 3231
  date: 2020-10-13_20-06-00
  done: false
  episode_len_mean: 878.7689873417721
  episode_reward_max: 276.47474747474763
  episode_reward_mean: 220.6047340493541
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1263898611068726
        entropy_coeff: 0.0005000000000000001
        kl: 0.008100568510902425
        model: {}
        policy_loss: -0.013406771836647144
        total_loss: 47.16934140523275
        vf_explained_var: 0.9198758602142334
        vf_loss: 47.18250052134196
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.510000000000005
    gpu_util_percent0: 0.33066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15796368727184648
    mean_env_wait_ms: 1.1581941999006098
    mean_inference_ms: 5.08738965887223
    mean_raw_obs_processing_ms: 0.41876346398342434
  time_since_restore: 109.07175135612488
  time_this_iter_s: 26.555402040481567
  time_total_s: 109.07175135612488
  timers:
    learn_throughput: 8309.746
    learn_time_ms: 19470.151
    sample_throughput: 20952.372
    sample_time_ms: 7721.894
    update_time_ms: 30.392
  timestamp: 1602619560
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      4 |          109.072 | 647168 |  220.605 |              276.475 |              145.717 |            878.769 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3581.6985583224114
    time_step_min: 3204
  date: 2020-10-13_20-06-27
  done: false
  episode_len_mean: 872.4867256637168
  episode_reward_max: 280.5656565656565
  episode_reward_mean: 222.48133675567283
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 159
  episodes_total: 791
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0873714486757915
        entropy_coeff: 0.0005000000000000001
        kl: 0.006956188706681132
        model: {}
        policy_loss: -0.011262792395427823
        total_loss: 34.19948164621989
        vf_explained_var: 0.9459590911865234
        vf_loss: 34.2105925877889
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.83225806451613
    gpu_util_percent0: 0.3396774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15662158293646897
    mean_env_wait_ms: 1.1605512431818636
    mean_inference_ms: 4.997536099507661
    mean_raw_obs_processing_ms: 0.41421386572879043
  time_since_restore: 135.85302448272705
  time_this_iter_s: 26.781273126602173
  time_total_s: 135.85302448272705
  timers:
    learn_throughput: 8292.886
    learn_time_ms: 19509.733
    sample_throughput: 21329.153
    sample_time_ms: 7585.486
    update_time_ms: 29.433
  timestamp: 1602619587
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      5 |          135.853 | 808960 |  222.481 |              280.566 |              145.717 |            872.487 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3564.887755102041
    time_step_min: 3204
  date: 2020-10-13_20-06-53
  done: false
  episode_len_mean: 860.6943942133815
  episode_reward_max: 280.5656565656565
  episode_reward_mean: 225.7112809834327
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 315
  episodes_total: 1106
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0886386533578236
        entropy_coeff: 0.0005000000000000001
        kl: 0.007588425030310948
        model: {}
        policy_loss: -0.01092883839737624
        total_loss: 30.730765342712402
        vf_explained_var: 0.9611188769340515
        vf_loss: 30.741480032602947
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.948387096774194
    gpu_util_percent0: 0.317741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7548387096774185
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15484837705770382
    mean_env_wait_ms: 1.1659963168233263
    mean_inference_ms: 4.877469596330384
    mean_raw_obs_processing_ms: 0.4083938590993221
  time_since_restore: 162.4215853214264
  time_this_iter_s: 26.56856083869934
  time_total_s: 162.4215853214264
  timers:
    learn_throughput: 8298.844
    learn_time_ms: 19495.728
    sample_throughput: 21578.996
    sample_time_ms: 7497.661
    update_time_ms: 31.117
  timestamp: 1602619613
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      6 |          162.422 | 970752 |  225.711 |              280.566 |              145.717 |            860.694 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3552.704692556634
    time_step_min: 3179
  date: 2020-10-13_20-07-20
  done: false
  episode_len_mean: 855.0387658227849
  episode_reward_max: 284.35353535353545
  episode_reward_mean: 227.46978487405684
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0706470410029094
        entropy_coeff: 0.0005000000000000001
        kl: 0.007387861027382314
        model: {}
        policy_loss: -0.013462736414415607
        total_loss: 19.742233912150066
        vf_explained_var: 0.9632093906402588
        vf_loss: 19.75549300511678
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.820000000000004
    gpu_util_percent0: 0.3363333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15420381363172034
    mean_env_wait_ms: 1.168441094943792
    mean_inference_ms: 4.833841934497637
    mean_raw_obs_processing_ms: 0.4062603961485774
  time_since_restore: 188.55974888801575
  time_this_iter_s: 26.138163566589355
  time_total_s: 188.55974888801575
  timers:
    learn_throughput: 8313.842
    learn_time_ms: 19460.559
    sample_throughput: 21861.611
    sample_time_ms: 7400.735
    update_time_ms: 29.922
  timestamp: 1602619640
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      7 |           188.56 | 1132544 |   227.47 |              284.354 |              145.717 |            855.039 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3540.6398852223815
    time_step_min: 3179
  date: 2020-10-13_20-07-46
  done: false
  episode_len_mean: 850.7552742616034
  episode_reward_max: 284.35353535353545
  episode_reward_mean: 229.23441162681647
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0475670397281647
        entropy_coeff: 0.0005000000000000001
        kl: 0.007399068369219701
        model: {}
        policy_loss: -0.009183195322596779
        total_loss: 16.652963479359943
        vf_explained_var: 0.9667003154754639
        vf_loss: 16.661930561065674
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62258064516129
    gpu_util_percent0: 0.3045161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1536551328765953
    mean_env_wait_ms: 1.1706496188779516
    mean_inference_ms: 4.79624410152827
    mean_raw_obs_processing_ms: 0.4043706525642614
  time_since_restore: 214.88071632385254
  time_this_iter_s: 26.320967435836792
  time_total_s: 214.88071632385254
  timers:
    learn_throughput: 8317.58
    learn_time_ms: 19451.812
    sample_throughput: 22064.335
    sample_time_ms: 7332.738
    update_time_ms: 28.538
  timestamp: 1602619666
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      8 |          214.881 | 1294336 |  229.234 |              284.354 |              145.717 |            850.755 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3526.8721374045804
    time_step_min: 3179
  date: 2020-10-13_20-08-13
  done: false
  episode_len_mean: 845.9875
  episode_reward_max: 285.7171717171716
  episode_reward_mean: 231.28724747474732
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 178
  episodes_total: 1600
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9999773452679316
        entropy_coeff: 0.0005000000000000001
        kl: 0.007928823702968657
        model: {}
        policy_loss: -0.011458211791856835
        total_loss: 16.58501172065735
        vf_explained_var: 0.9729644656181335
        vf_loss: 16.596176783243816
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.209677419354843
    gpu_util_percent0: 0.3412903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15310486296356154
    mean_env_wait_ms: 1.173190314692454
    mean_inference_ms: 4.759119213601895
    mean_raw_obs_processing_ms: 0.40239381029391647
  time_since_restore: 241.6717405319214
  time_this_iter_s: 26.791024208068848
  time_total_s: 241.6717405319214
  timers:
    learn_throughput: 8310.899
    learn_time_ms: 19467.45
    sample_throughput: 22138.851
    sample_time_ms: 7308.058
    update_time_ms: 29.431
  timestamp: 1602619693
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |      9 |          241.672 | 1456128 |  231.287 |              285.717 |              145.717 |            845.987 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3508.0337259100643
    time_step_min: 3173
  date: 2020-10-13_20-08-40
  done: false
  episode_len_mean: 839.0052742616034
  episode_reward_max: 285.7171717171716
  episode_reward_mean: 234.27066551591852
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 296
  episodes_total: 1896
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9948871235052744
        entropy_coeff: 0.0005000000000000001
        kl: 0.006681857941051324
        model: {}
        policy_loss: -0.011002168253374597
        total_loss: 15.828110535939535
        vf_explained_var: 0.9757750630378723
        vf_loss: 15.838941733042398
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.274193548387096
    gpu_util_percent0: 0.2841935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15240754990713165
    mean_env_wait_ms: 1.1769862347158584
    mean_inference_ms: 4.710674265387487
    mean_raw_obs_processing_ms: 0.39999416675392635
  time_since_restore: 268.45834279060364
  time_this_iter_s: 26.78660225868225
  time_total_s: 268.45834279060364
  timers:
    learn_throughput: 8307.288
    learn_time_ms: 19475.911
    sample_throughput: 22186.6
    sample_time_ms: 7292.33
    update_time_ms: 28.913
  timestamp: 1602619720
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     10 |          268.458 | 1617920 |  234.271 |              285.717 |              145.717 |            839.005 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3499.19842053307
    time_step_min: 3171
  date: 2020-10-13_20-09-06
  done: false
  episode_len_mean: 835.7263875365142
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 235.87525203347977
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.975288137793541
        entropy_coeff: 0.0005000000000000001
        kl: 0.007294710182274382
        model: {}
        policy_loss: -0.012727556153549813
        total_loss: 11.962000767389933
        vf_explained_var: 0.9750909805297852
        vf_loss: 11.974486589431763
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.780645161290327
    gpu_util_percent0: 0.25903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520928372551677
    mean_env_wait_ms: 1.1787591008572416
    mean_inference_ms: 4.688925228370066
    mean_raw_obs_processing_ms: 0.39889213588140776
  time_since_restore: 294.8222498893738
  time_this_iter_s: 26.36390709877014
  time_total_s: 294.8222498893738
  timers:
    learn_throughput: 8316.911
    learn_time_ms: 19453.376
    sample_throughput: 22872.528
    sample_time_ms: 7073.639
    update_time_ms: 28.434
  timestamp: 1602619746
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     11 |          294.822 | 1779712 |  235.875 |              294.202 |              145.717 |            835.726 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3490.923534798535
    time_step_min: 3159
  date: 2020-10-13_20-09-33
  done: false
  episode_len_mean: 832.6595840867993
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 237.1319752680511
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9521185209353765
        entropy_coeff: 0.0005000000000000001
        kl: 0.006661186693236232
        model: {}
        policy_loss: -0.013089668517447231
        total_loss: 12.603836615880331
        vf_explained_var: 0.9737562537193298
        vf_loss: 12.61673672993978
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.553333333333335
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1518103486196889
    mean_env_wait_ms: 1.1804379397059988
    mean_inference_ms: 4.669256683654472
    mean_raw_obs_processing_ms: 0.39785923875645596
  time_since_restore: 321.2838063240051
  time_this_iter_s: 26.461556434631348
  time_total_s: 321.2838063240051
  timers:
    learn_throughput: 8318.587
    learn_time_ms: 19449.456
    sample_throughput: 23087.154
    sample_time_ms: 7007.88
    update_time_ms: 27.796
  timestamp: 1602619773
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     12 |          321.284 | 1941504 |  237.132 |              294.202 |              145.717 |             832.66 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3477.030998389694
    time_step_min: 3151
  date: 2020-10-13_20-10-00
  done: false
  episode_len_mean: 827.7671178343949
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 239.24816235604442
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 300
  episodes_total: 2512
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9211943199237188
        entropy_coeff: 0.0005000000000000001
        kl: 0.0069003046955913305
        model: {}
        policy_loss: -0.011187698284629732
        total_loss: 15.527917702992758
        vf_explained_var: 0.9792836308479309
        vf_loss: 15.538876056671143
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5741935483871
    gpu_util_percent0: 0.3064516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7548387096774185
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1513431855022594
    mean_env_wait_ms: 1.1834704239539333
    mean_inference_ms: 4.636837245807527
    mean_raw_obs_processing_ms: 0.3962203615019328
  time_since_restore: 348.23021721839905
  time_this_iter_s: 26.94641089439392
  time_total_s: 348.23021721839905
  timers:
    learn_throughput: 8298.652
    learn_time_ms: 19496.178
    sample_throughput: 23134.367
    sample_time_ms: 6993.578
    update_time_ms: 28.364
  timestamp: 1602619800
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     13 |           348.23 | 2103296 |  239.248 |              294.202 |              145.717 |            827.767 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3468.7953348382243
    time_step_min: 3151
  date: 2020-10-13_20-10-27
  done: false
  episode_len_mean: 825.1623231571109
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 240.4743300465563
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 174
  episodes_total: 2686
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9143994450569153
        entropy_coeff: 0.0005000000000000001
        kl: 0.0060155229875817895
        model: {}
        policy_loss: -0.012139652118397256
        total_loss: 10.54153060913086
        vf_explained_var: 0.979185163974762
        vf_loss: 10.553526004155477
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.932258064516137
    gpu_util_percent0: 0.3148387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1511170375866378
    mean_env_wait_ms: 1.185045622771163
    mean_inference_ms: 4.621175503537845
    mean_raw_obs_processing_ms: 0.39541586807833484
  time_since_restore: 374.96545243263245
  time_this_iter_s: 26.7352352142334
  time_total_s: 374.96545243263245
  timers:
    learn_throughput: 8288.116
    learn_time_ms: 19520.961
    sample_throughput: 23163.088
    sample_time_ms: 6984.906
    update_time_ms: 29.031
  timestamp: 1602619827
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     14 |          374.965 | 2265088 |  240.474 |              294.202 |              145.717 |            825.162 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3462.4602272727275
    time_step_min: 3131
  date: 2020-10-13_20-10-53
  done: false
  episode_len_mean: 822.9542897327707
  episode_reward_max: 294.20202020201987
  episode_reward_mean: 241.45540851553497
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9019962549209595
        entropy_coeff: 0.0005000000000000001
        kl: 0.006133316123547654
        model: {}
        policy_loss: -0.012229806568939239
        total_loss: 9.555021127065023
        vf_explained_var: 0.9795403480529785
        vf_loss: 9.567088762919107
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.232258064516135
    gpu_util_percent0: 0.2896774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15092531421061056
    mean_env_wait_ms: 1.1863556760603955
    mean_inference_ms: 4.607798785493067
    mean_raw_obs_processing_ms: 0.394718287241015
  time_since_restore: 401.53789925575256
  time_this_iter_s: 26.572446823120117
  time_total_s: 401.53789925575256
  timers:
    learn_throughput: 8297.933
    learn_time_ms: 19497.867
    sample_throughput: 23161.771
    sample_time_ms: 6985.304
    update_time_ms: 28.819
  timestamp: 1602619853
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     15 |          401.538 | 2426880 |  241.455 |              294.202 |              145.717 |            822.954 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3453.8821989528797
    time_step_min: 3083
  date: 2020-10-13_20-11-20
  done: false
  episode_len_mean: 819.9792477302204
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 242.75903981448718
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 240
  episodes_total: 3084
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8659086326758066
        entropy_coeff: 0.0005000000000000001
        kl: 0.00634678197093308
        model: {}
        policy_loss: -0.012134946203635385
        total_loss: 13.195513248443604
        vf_explained_var: 0.980156421661377
        vf_loss: 13.207446098327637
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.31612903225807
    gpu_util_percent0: 0.39225806451612905
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15066109760456928
    mean_env_wait_ms: 1.1883161715727024
    mean_inference_ms: 4.5890624630397125
    mean_raw_obs_processing_ms: 0.39375995340075964
  time_since_restore: 427.9896581172943
  time_this_iter_s: 26.451758861541748
  time_total_s: 427.9896581172943
  timers:
    learn_throughput: 8303.848
    learn_time_ms: 19483.98
    sample_throughput: 23156.592
    sample_time_ms: 6986.866
    update_time_ms: 27.501
  timestamp: 1602619880
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     16 |           427.99 | 2588672 |  242.759 |              298.899 |              145.717 |            819.979 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3445.377811550152
    time_step_min: 3083
  date: 2020-10-13_20-11-46
  done: false
  episode_len_mean: 817.4445449065702
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 243.9846110289147
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 234
  episodes_total: 3318
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8606445689996084
        entropy_coeff: 0.0005000000000000001
        kl: 0.005582816433161497
        model: {}
        policy_loss: -0.011729711521184072
        total_loss: 9.780861934026083
        vf_explained_var: 0.9827695488929749
        vf_loss: 9.792463779449463
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.016129032258068
    gpu_util_percent0: 0.356774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15043496741876733
    mean_env_wait_ms: 1.1900486341966663
    mean_inference_ms: 4.573505259854426
    mean_raw_obs_processing_ms: 0.3929702729542611
  time_since_restore: 454.35273933410645
  time_this_iter_s: 26.363081216812134
  time_total_s: 454.35273933410645
  timers:
    learn_throughput: 8298.823
    learn_time_ms: 19495.777
    sample_throughput: 23127.118
    sample_time_ms: 6995.77
    update_time_ms: 28.375
  timestamp: 1602619906
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     17 |          454.353 | 2750464 |  243.985 |              298.899 |              145.717 |            817.445 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3440.181554524362
    time_step_min: 3083
  date: 2020-10-13_20-12-13
  done: false
  episode_len_mean: 815.873417721519
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 244.73267194383405
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8570553759733835
        entropy_coeff: 0.0005000000000000001
        kl: 0.00693794801676025
        model: {}
        policy_loss: -0.012595690621916825
        total_loss: 9.302302360534668
        vf_explained_var: 0.9800246357917786
        vf_loss: 9.314632733662924
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.258064516129032
    gpu_util_percent0: 0.2793548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1502956602743105
    mean_env_wait_ms: 1.191104348685869
    mean_inference_ms: 4.563708210685664
    mean_raw_obs_processing_ms: 0.3924672335669029
  time_since_restore: 480.8537907600403
  time_this_iter_s: 26.501051425933838
  time_total_s: 480.8537907600403
  timers:
    learn_throughput: 8295.925
    learn_time_ms: 19502.586
    sample_throughput: 23094.053
    sample_time_ms: 7005.786
    update_time_ms: 28.623
  timestamp: 1602619933
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     18 |          480.854 | 2912256 |  244.733 |              298.899 |              145.717 |            815.873 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3435.160891089109
    time_step_min: 3083
  date: 2020-10-13_20-12-40
  done: false
  episode_len_mean: 814.1853165938865
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 245.55176216311577
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 3664
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8260929683844248
        entropy_coeff: 0.0005000000000000001
        kl: 0.00627721450291574
        model: {}
        policy_loss: -0.010709817167177485
        total_loss: 11.524338483810425
        vf_explained_var: 0.9801642894744873
        vf_loss: 11.534833749135336
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.454838709677418
    gpu_util_percent0: 0.23709677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15013673654422427
    mean_env_wait_ms: 1.1923211635776865
    mean_inference_ms: 4.552793738796966
    mean_raw_obs_processing_ms: 0.3918953681986802
  time_since_restore: 507.6680727005005
  time_this_iter_s: 26.814281940460205
  time_total_s: 507.6680727005005
  timers:
    learn_throughput: 8296.244
    learn_time_ms: 19501.837
    sample_throughput: 23086.979
    sample_time_ms: 7007.933
    update_time_ms: 28.863
  timestamp: 1602619960
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     19 |          507.668 | 3074048 |  245.552 |              298.899 |              145.717 |            814.185 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3428.4441611422744
    time_step_min: 3083
  date: 2020-10-13_20-13-07
  done: false
  episode_len_mean: 812.0630379746835
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 246.60976857179378
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 286
  episodes_total: 3950
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8063790599505106
        entropy_coeff: 0.0005000000000000001
        kl: 0.00581474454763035
        model: {}
        policy_loss: -0.010059737542178482
        total_loss: 10.378987232844034
        vf_explained_var: 0.9842923283576965
        vf_loss: 10.388868490854898
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.748387096774195
    gpu_util_percent0: 0.3648387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14992723197534163
    mean_env_wait_ms: 1.19406329871298
    mean_inference_ms: 4.537944341035613
    mean_raw_obs_processing_ms: 0.3911546266567258
  time_since_restore: 534.6695826053619
  time_this_iter_s: 27.00150990486145
  time_total_s: 534.6695826053619
  timers:
    learn_throughput: 8286.876
    learn_time_ms: 19523.883
    sample_throughput: 23097.63
    sample_time_ms: 7004.701
    update_time_ms: 31.202
  timestamp: 1602619987
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     20 |           534.67 | 3235840 |   246.61 |              298.899 |              145.717 |            812.063 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3424.633333333333
    time_step_min: 3083
  date: 2020-10-13_20-13-34
  done: false
  episode_len_mean: 811.1747809152872
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 247.171761431255
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 4108
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8050975054502487
        entropy_coeff: 0.0005000000000000001
        kl: 0.006411496355819206
        model: {}
        policy_loss: -0.0109325938198405
        total_loss: 8.397321462631226
        vf_explained_var: 0.9822394847869873
        vf_loss: 8.408015330632528
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.55161290322581
    gpu_util_percent0: 0.3190322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14982027946490634
    mean_env_wait_ms: 1.194926670074653
    mean_inference_ms: 4.530434463727461
    mean_raw_obs_processing_ms: 0.39077441907554605
  time_since_restore: 561.2095220088959
  time_this_iter_s: 26.539939403533936
  time_total_s: 561.2095220088959
  timers:
    learn_throughput: 8284.3
    learn_time_ms: 19529.955
    sample_throughput: 23060.11
    sample_time_ms: 7016.098
    update_time_ms: 31.403
  timestamp: 1602620014
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     21 |           561.21 | 3397632 |  247.172 |              298.899 |              145.717 |            811.175 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3421.5082547169814
    time_step_min: 3083
  date: 2020-10-13_20-14-01
  done: false
  episode_len_mean: 810.3659793814433
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 247.6299262541061
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 160
  episodes_total: 4268
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7943403224150339
        entropy_coeff: 0.0005000000000000001
        kl: 0.006092905105712513
        model: {}
        policy_loss: -0.012508889408006022
        total_loss: 10.069480101267496
        vf_explained_var: 0.9799533486366272
        vf_loss: 10.08177661895752
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.980645161290326
    gpu_util_percent0: 0.3396774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14971862438993364
    mean_env_wait_ms: 1.1957592444533478
    mean_inference_ms: 4.5232605067829255
    mean_raw_obs_processing_ms: 0.39040236500777303
  time_since_restore: 587.733895778656
  time_this_iter_s: 26.524373769760132
  time_total_s: 587.733895778656
  timers:
    learn_throughput: 8281.085
    learn_time_ms: 19537.536
    sample_throughput: 23067.481
    sample_time_ms: 7013.856
    update_time_ms: 31.798
  timestamp: 1602620041
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     22 |          587.734 | 3559424 |   247.63 |              298.899 |              145.717 |            810.366 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3415.407570422535
    time_step_min: 3083
  date: 2020-10-13_20-14-28
  done: false
  episode_len_mean: 808.5879265091863
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 248.62850287653427
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 304
  episodes_total: 4572
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7644506990909576
        entropy_coeff: 0.0005000000000000001
        kl: 0.005771325357879202
        model: {}
        policy_loss: -0.009549629636846172
        total_loss: 10.615382512410482
        vf_explained_var: 0.9846494197845459
        vf_loss: 10.624737024307251
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.090322580645164
    gpu_util_percent0: 0.2938709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1495377822913058
    mean_env_wait_ms: 1.1972899182809897
    mean_inference_ms: 4.51061035721675
    mean_raw_obs_processing_ms: 0.38977447268831633
  time_since_restore: 614.6514933109283
  time_this_iter_s: 26.91759753227234
  time_total_s: 614.6514933109283
  timers:
    learn_throughput: 8282.013
    learn_time_ms: 19535.348
    sample_throughput: 23073.961
    sample_time_ms: 7011.887
    update_time_ms: 32.083
  timestamp: 1602620068
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     23 |          614.651 | 3721216 |  248.629 |              298.899 |              145.717 |            808.588 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3412.3495331069607
    time_step_min: 3083
  date: 2020-10-13_20-14-54
  done: false
  episode_len_mean: 807.6881856540084
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 249.11699271192933
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 168
  episodes_total: 4740
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7512289037307104
        entropy_coeff: 0.0005000000000000001
        kl: 0.006618439607943098
        model: {}
        policy_loss: -0.011942399355272451
        total_loss: 7.106495062510173
        vf_explained_var: 0.9852812886238098
        vf_loss: 7.118151148160298
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970967741935485
    gpu_util_percent0: 0.28225806451612906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14944845699084452
    mean_env_wait_ms: 1.1980653483733485
    mean_inference_ms: 4.5043390758480895
    mean_raw_obs_processing_ms: 0.389461164438326
  time_since_restore: 641.2195816040039
  time_this_iter_s: 26.56808829307556
  time_total_s: 641.2195816040039
  timers:
    learn_throughput: 8290.806
    learn_time_ms: 19514.629
    sample_throughput: 23057.002
    sample_time_ms: 7017.044
    update_time_ms: 30.192
  timestamp: 1602620094
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     24 |           641.22 | 3883008 |  249.117 |              298.899 |              145.717 |            807.688 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3409.523613963039
    time_step_min: 3083
  date: 2020-10-13_20-15-21
  done: false
  episode_len_mean: 806.9183340138832
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 249.55621548271597
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 4898
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7577964961528778
        entropy_coeff: 0.0005000000000000001
        kl: 0.005755245918408036
        model: {}
        policy_loss: -0.00993577616463881
        total_loss: 7.848556240399678
        vf_explained_var: 0.9825068116188049
        vf_loss: 7.858295281728108
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90967741935484
    gpu_util_percent0: 0.2735483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14936806601898428
    mean_env_wait_ms: 1.1987554105975042
    mean_inference_ms: 4.498661952674449
    mean_raw_obs_processing_ms: 0.3891729686367362
  time_since_restore: 668.0133476257324
  time_this_iter_s: 26.793766021728516
  time_total_s: 668.0133476257324
  timers:
    learn_throughput: 8285.36
    learn_time_ms: 19527.456
    sample_throughput: 23024.958
    sample_time_ms: 7026.81
    update_time_ms: 29.784
  timestamp: 1602620121
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     25 |          668.013 | 4044800 |  249.556 |              298.899 |              145.717 |            806.918 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3405.1021997274675
    time_step_min: 3083
  date: 2020-10-13_20-15-48
  done: false
  episode_len_mean: 805.7744433688287
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 250.1943931082362
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 267
  episodes_total: 5165
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.734145000576973
        entropy_coeff: 0.0005000000000000001
        kl: 0.00586831111771365
        model: {}
        policy_loss: -0.009772113577734368
        total_loss: 11.755430380503336
        vf_explained_var: 0.9823409914970398
        vf_loss: 11.76498262087504
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.967741935483875
    gpu_util_percent0: 0.2951612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492421189402878
    mean_env_wait_ms: 1.1999082542447606
    mean_inference_ms: 4.489555076979593
    mean_raw_obs_processing_ms: 0.3887191826007824
  time_since_restore: 694.6627025604248
  time_this_iter_s: 26.649354934692383
  time_total_s: 694.6627025604248
  timers:
    learn_throughput: 8277.794
    learn_time_ms: 19545.305
    sample_throughput: 23019.721
    sample_time_ms: 7028.408
    update_time_ms: 29.732
  timestamp: 1602620148
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     26 |          694.663 | 4206592 |  250.194 |              298.899 |              145.717 |            805.774 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3401.4015718562873
    time_step_min: 3083
  date: 2020-10-13_20-16-15
  done: false
  episode_len_mean: 804.8773268801191
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 250.75784276119336
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 207
  episodes_total: 5372
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7132567266623179
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055438703469311195
        model: {}
        policy_loss: -0.011437343899160624
        total_loss: 6.541075627009074
        vf_explained_var: 0.9872210621833801
        vf_loss: 6.5523152351379395
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.854838709677423
    gpu_util_percent0: 0.31548387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14914914247283464
    mean_env_wait_ms: 1.2006944435704725
    mean_inference_ms: 4.483253733754734
    mean_raw_obs_processing_ms: 0.3883944095392934
  time_since_restore: 721.2284338474274
  time_this_iter_s: 26.565731287002563
  time_total_s: 721.2284338474274
  timers:
    learn_throughput: 8275.319
    learn_time_ms: 19551.15
    sample_throughput: 22986.493
    sample_time_ms: 7038.568
    update_time_ms: 32.868
  timestamp: 1602620175
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     27 |          721.228 | 4368384 |  250.758 |              298.899 |              145.717 |            804.877 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3398.81697564522
    time_step_min: 3083
  date: 2020-10-13_20-16-42
  done: false
  episode_len_mean: 804.1676311030741
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 251.131851973624
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 5530
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7216055691242218
        entropy_coeff: 0.0005000000000000001
        kl: 0.005685570455777149
        model: {}
        policy_loss: -0.011318465374642983
        total_loss: 6.673397620519002
        vf_explained_var: 0.9849072098731995
        vf_loss: 6.684508363405864
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.129032258064523
    gpu_util_percent0: 0.28225806451612906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14908242113123552
    mean_env_wait_ms: 1.2012747818031209
    mean_inference_ms: 4.478587548631685
    mean_raw_obs_processing_ms: 0.38815609953403163
  time_since_restore: 747.7852442264557
  time_this_iter_s: 26.55681037902832
  time_total_s: 747.7852442264557
  timers:
    learn_throughput: 8274.671
    learn_time_ms: 19552.682
    sample_throughput: 22981.45
    sample_time_ms: 7040.113
    update_time_ms: 34.611
  timestamp: 1602620202
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     28 |          747.785 | 4530176 |  251.132 |              298.899 |              145.717 |            804.168 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3395.749869178441
    time_step_min: 3083
  date: 2020-10-13_20-17-08
  done: false
  episode_len_mean: 803.1786148238153
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 251.58607600041373
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 231
  episodes_total: 5761
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6937208970387777
        entropy_coeff: 0.0005000000000000001
        kl: 0.005959675957759221
        model: {}
        policy_loss: -0.011802961448362717
        total_loss: 10.309924920399984
        vf_explained_var: 0.983536958694458
        vf_loss: 10.321478684743246
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43548387096775
    gpu_util_percent0: 0.32451612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7612903225806447
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14899017285972635
    mean_env_wait_ms: 1.202124812724087
    mean_inference_ms: 4.472038674610832
    mean_raw_obs_processing_ms: 0.38781819613113516
  time_since_restore: 774.4331998825073
  time_this_iter_s: 26.647955656051636
  time_total_s: 774.4331998825073
  timers:
    learn_throughput: 8278.544
    learn_time_ms: 19543.533
    sample_throughput: 23031.914
    sample_time_ms: 7024.688
    update_time_ms: 33.637
  timestamp: 1602620228
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     29 |          774.433 | 4691968 |  251.586 |              298.899 |              145.717 |            803.179 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3392.510374832664
    time_step_min: 3083
  date: 2020-10-13_20-17-35
  done: false
  episode_len_mean: 802.1863757495004
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 252.1170717837939
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 243
  episodes_total: 6004
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6723145395517349
        entropy_coeff: 0.0005000000000000001
        kl: 0.005279912458111842
        model: {}
        policy_loss: -0.011053321950991327
        total_loss: 7.6361691157023115
        vf_explained_var: 0.9863631129264832
        vf_loss: 7.647030512491862
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.24516129032258
    gpu_util_percent0: 0.27903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14889951503421475
    mean_env_wait_ms: 1.2029519916072202
    mean_inference_ms: 4.46575959710534
    mean_raw_obs_processing_ms: 0.3875021385960958
  time_since_restore: 801.1446940898895
  time_this_iter_s: 26.711494207382202
  time_total_s: 801.1446940898895
  timers:
    learn_throughput: 8284.654
    learn_time_ms: 19529.119
    sample_throughput: 23081.294
    sample_time_ms: 7009.659
    update_time_ms: 33.171
  timestamp: 1602620255
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     30 |          801.145 | 4853760 |  252.117 |              298.899 |              145.717 |            802.186 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3390.042386697098
    time_step_min: 3083
  date: 2020-10-13_20-18-02
  done: false
  episode_len_mean: 801.4599156118144
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 252.45022769073393
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 6162
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6794097969929377
        entropy_coeff: 0.0005000000000000001
        kl: 0.005776377705236276
        model: {}
        policy_loss: -0.011071474878311468
        total_loss: 6.609892050425212
        vf_explained_var: 0.9848344326019287
        vf_loss: 6.620725552241008
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.380645161290325
    gpu_util_percent0: 0.2954838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14884372326123518
    mean_env_wait_ms: 1.203469882568465
    mean_inference_ms: 4.461845729394711
    mean_raw_obs_processing_ms: 0.38730276508584366
  time_since_restore: 827.7272968292236
  time_this_iter_s: 26.582602739334106
  time_total_s: 827.7272968292236
  timers:
    learn_throughput: 8283.013
    learn_time_ms: 19532.989
    sample_throughput: 23089.522
    sample_time_ms: 7007.161
    update_time_ms: 35.584
  timestamp: 1602620282
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     31 |          827.727 | 5015552 |   252.45 |              298.899 |              145.717 |             801.46 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3386.696788413098
    time_step_min: 3083
  date: 2020-10-13_20-18-29
  done: false
  episode_len_mean: 800.385893416928
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 252.93216649251121
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 218
  episodes_total: 6380
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6597986618677775
        entropy_coeff: 0.0005000000000000001
        kl: 0.005507051052215199
        model: {}
        policy_loss: -0.01063559478886115
        total_loss: 8.718894402186075
        vf_explained_var: 0.9843838214874268
        vf_loss: 8.729309240976969
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.68666666666667
    gpu_util_percent0: 0.31366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14877216457073314
    mean_env_wait_ms: 1.2041977647231028
    mean_inference_ms: 4.456698682323604
    mean_raw_obs_processing_ms: 0.3870410736885443
  time_since_restore: 854.1232621669769
  time_this_iter_s: 26.395965337753296
  time_total_s: 854.1232621669769
  timers:
    learn_throughput: 8288.049
    learn_time_ms: 19521.121
    sample_throughput: 23095.241
    sample_time_ms: 7005.426
    update_time_ms: 35.304
  timestamp: 1602620309
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     32 |          854.123 | 5177344 |  252.932 |              298.899 |              145.717 |            800.386 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3383.374754048736
    time_step_min: 3083
  date: 2020-10-13_20-18-56
  done: false
  episode_len_mean: 799.2391861341372
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 253.41443827879388
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 255
  episodes_total: 6635
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6365775416294733
        entropy_coeff: 0.0005000000000000001
        kl: 0.005616956856101751
        model: {}
        policy_loss: -0.012718923583937189
        total_loss: 7.9217236042022705
        vf_explained_var: 0.9864194393157959
        vf_loss: 7.934199412663777
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.987096774193553
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14868980723191585
    mean_env_wait_ms: 1.2049879460481647
    mean_inference_ms: 4.451042723360223
    mean_raw_obs_processing_ms: 0.38675696376308816
  time_since_restore: 880.8052606582642
  time_this_iter_s: 26.68199849128723
  time_total_s: 880.8052606582642
  timers:
    learn_throughput: 8303.056
    learn_time_ms: 19485.839
    sample_throughput: 23052.425
    sample_time_ms: 7018.437
    update_time_ms: 33.287
  timestamp: 1602620336
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     33 |          880.805 | 5339136 |  253.414 |              298.899 |              145.717 |            799.239 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3381.266627253917
    time_step_min: 3083
  date: 2020-10-13_20-19-22
  done: false
  episode_len_mean: 798.6435089785105
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 253.7262884957909
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 159
  episodes_total: 6794
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6548023074865341
        entropy_coeff: 0.0005000000000000001
        kl: 0.005482170420388381
        model: {}
        policy_loss: -0.012428849945232892
        total_loss: 6.324912707010905
        vf_explained_var: 0.9852306842803955
        vf_loss: 6.337120532989502
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46129032258065
    gpu_util_percent0: 0.3277419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486422809640388
    mean_env_wait_ms: 1.2054563264035387
    mean_inference_ms: 4.44767787930299
    mean_raw_obs_processing_ms: 0.38658671448567267
  time_since_restore: 907.3872518539429
  time_this_iter_s: 26.58199119567871
  time_total_s: 907.3872518539429
  timers:
    learn_throughput: 8304.585
    learn_time_ms: 19482.25
    sample_throughput: 23039.736
    sample_time_ms: 7022.303
    update_time_ms: 33.073
  timestamp: 1602620362
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     34 |          907.387 | 5500928 |  253.726 |              298.899 |              145.717 |            798.644 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3378.6727925340992
    time_step_min: 3083
  date: 2020-10-13_20-19-49
  done: false
  episode_len_mean: 797.975975975976
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 254.1101534434867
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 6993
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6353265345096588
        entropy_coeff: 0.0005000000000000001
        kl: 0.005509571443932752
        model: {}
        policy_loss: -0.009562680769401291
        total_loss: 8.754625161488852
        vf_explained_var: 0.9838367104530334
        vf_loss: 8.763954242070517
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.3225806451613
    gpu_util_percent0: 0.34483870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485862047077988
    mean_env_wait_ms: 1.206052865180669
    mean_inference_ms: 4.443683948289975
    mean_raw_obs_processing_ms: 0.3863817565676252
  time_since_restore: 933.9656798839569
  time_this_iter_s: 26.578428030014038
  time_total_s: 933.9656798839569
  timers:
    learn_throughput: 8309.513
    learn_time_ms: 19470.695
    sample_throughput: 23077.008
    sample_time_ms: 7010.961
    update_time_ms: 33.0
  timestamp: 1602620389
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     35 |          933.966 | 5662720 |   254.11 |              298.899 |              145.717 |            797.976 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3375.4936446532192
    time_step_min: 3083
  date: 2020-10-13_20-20-16
  done: false
  episode_len_mean: 797.1249655931737
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 254.59083819199418
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 273
  episodes_total: 7266
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6047424674034119
        entropy_coeff: 0.0005000000000000001
        kl: 0.005252860602922738
        model: {}
        policy_loss: -0.011149611789733171
        total_loss: 9.254522800445557
        vf_explained_var: 0.9846187233924866
        vf_loss: 9.265449444452921
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.330000000000002
    gpu_util_percent0: 0.298
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485104206361561
    mean_env_wait_ms: 1.2068107651442888
    mean_inference_ms: 4.438417607950807
    mean_raw_obs_processing_ms: 0.38612490621800105
  time_since_restore: 960.3167681694031
  time_this_iter_s: 26.351088285446167
  time_total_s: 960.3167681694031
  timers:
    learn_throughput: 8313.411
    learn_time_ms: 19461.567
    sample_throughput: 23147.422
    sample_time_ms: 6989.634
    update_time_ms: 32.934
  timestamp: 1602620416
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     36 |          960.317 | 5824512 |  254.591 |              298.899 |              145.717 |            797.125 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3373.9128142741283
    time_step_min: 3083
  date: 2020-10-13_20-20-43
  done: false
  episode_len_mean: 796.6110961486669
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 254.86798499402855
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 160
  episodes_total: 7426
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.623786672949791
        entropy_coeff: 0.0005000000000000001
        kl: 0.005110279773361981
        model: {}
        policy_loss: -0.010506908385044275
        total_loss: 6.253582954406738
        vf_explained_var: 0.9853312373161316
        vf_loss: 6.263890822728475
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.759375000000002
    gpu_util_percent0: 0.3021875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14846971929083058
    mean_env_wait_ms: 1.207238405210291
    mean_inference_ms: 4.435509183326326
    mean_raw_obs_processing_ms: 0.38598068198598584
  time_since_restore: 987.1051871776581
  time_this_iter_s: 26.788419008255005
  time_total_s: 987.1051871776581
  timers:
    learn_throughput: 8311.029
    learn_time_ms: 19467.144
    sample_throughput: 23085.011
    sample_time_ms: 7008.53
    update_time_ms: 29.764
  timestamp: 1602620443
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     37 |          987.105 | 5986304 |  254.868 |              298.899 |              145.717 |            796.611 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3371.3291606272237
    time_step_min: 3083
  date: 2020-10-13_20-21-09
  done: false
  episode_len_mean: 796.0093212550873
  episode_reward_max: 298.8989898989898
  episode_reward_mean: 255.2320527050735
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 191
  episodes_total: 7617
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6084752033154169
        entropy_coeff: 0.0005000000000000001
        kl: 0.005453399266116321
        model: {}
        policy_loss: -0.012583952009057006
        total_loss: 7.687996904055278
        vf_explained_var: 0.9848602414131165
        vf_loss: 7.7003395557403564
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.245161290322585
    gpu_util_percent0: 0.3738709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14842295302126005
    mean_env_wait_ms: 1.2077514482640628
    mean_inference_ms: 4.432202666702219
    mean_raw_obs_processing_ms: 0.3858154679877812
  time_since_restore: 1013.6275017261505
  time_this_iter_s: 26.52231454849243
  time_total_s: 1013.6275017261505
  timers:
    learn_throughput: 8312.418
    learn_time_ms: 19463.89
    sample_throughput: 23103.624
    sample_time_ms: 7002.884
    update_time_ms: 34.059
  timestamp: 1602620469
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     38 |          1013.63 | 6148096 |  255.232 |              298.899 |              145.717 |            796.009 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3367.1486142893464
    time_step_min: 3079
  date: 2020-10-13_20-21-36
  done: false
  episode_len_mean: 795.201418799088
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 255.79623061115328
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 277
  episodes_total: 7894
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5781072477499644
        entropy_coeff: 0.0005000000000000001
        kl: 0.004660504714896281
        model: {}
        policy_loss: -0.009350795717182336
        total_loss: 7.616626938184102
        vf_explained_var: 0.9870114326477051
        vf_loss: 7.625800848007202
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.606451612903225
    gpu_util_percent0: 0.292258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14835548202008628
    mean_env_wait_ms: 1.2084460150514555
    mean_inference_ms: 4.4274899501035305
    mean_raw_obs_processing_ms: 0.3855871010892694
  time_since_restore: 1040.2780101299286
  time_this_iter_s: 26.650508403778076
  time_total_s: 1040.2780101299286
  timers:
    learn_throughput: 8314.441
    learn_time_ms: 19459.154
    sample_throughput: 23063.682
    sample_time_ms: 7015.012
    update_time_ms: 33.452
  timestamp: 1602620496
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     39 |          1040.28 | 6309888 |  255.796 |              299.505 |              145.717 |            795.201 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3365.2104607721044
    time_step_min: 3079
  date: 2020-10-13_20-22-03
  done: false
  episode_len_mean: 794.7603623727972
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 256.0866896816263
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 8058
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5902702659368515
        entropy_coeff: 0.0005000000000000001
        kl: 0.005777042941190302
        model: {}
        policy_loss: -0.01098569861399786
        total_loss: 5.731792251269023
        vf_explained_var: 0.9865006804466248
        vf_loss: 5.742784023284912
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.467741935483875
    gpu_util_percent0: 0.3096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14831930706394442
    mean_env_wait_ms: 1.2088427697698705
    mean_inference_ms: 4.424923757100713
    mean_raw_obs_processing_ms: 0.3854629833236848
  time_since_restore: 1067.0676140785217
  time_this_iter_s: 26.78960394859314
  time_total_s: 1067.0676140785217
  timers:
    learn_throughput: 8318.245
    learn_time_ms: 19450.256
    sample_throughput: 23014.531
    sample_time_ms: 7029.993
    update_time_ms: 34.061
  timestamp: 1602620523
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     40 |          1067.07 | 6471680 |  256.087 |              299.505 |              145.717 |             794.76 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3363.1631136557435
    time_step_min: 3079
  date: 2020-10-13_20-22-30
  done: false
  episode_len_mean: 794.3325239771762
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 256.39864960151465
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 8237
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.582388644417127
        entropy_coeff: 0.0005000000000000001
        kl: 0.005592259811237454
        model: {}
        policy_loss: -0.008816406540669655
        total_loss: 6.511826952298482
        vf_explained_var: 0.9866585731506348
        vf_loss: 6.520654956499736
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43666666666667
    gpu_util_percent0: 0.2843333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14828078985611992
    mean_env_wait_ms: 1.2092697886006278
    mean_inference_ms: 4.422214155267549
    mean_raw_obs_processing_ms: 0.3853263043314993
  time_since_restore: 1093.5339365005493
  time_this_iter_s: 26.466322422027588
  time_total_s: 1093.5339365005493
  timers:
    learn_throughput: 8321.951
    learn_time_ms: 19441.595
    sample_throughput: 23020.496
    sample_time_ms: 7028.172
    update_time_ms: 31.967
  timestamp: 1602620550
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     41 |          1093.53 | 6633472 |  256.399 |              299.505 |              145.717 |            794.333 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3359.921030952101
    time_step_min: 3079
  date: 2020-10-13_20-22-57
  done: false
  episode_len_mean: 793.6757771260997
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 256.8849847448087
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 288
  episodes_total: 8525
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5509877155224482
        entropy_coeff: 0.0005000000000000001
        kl: 0.00548382840740184
        model: {}
        policy_loss: -0.011438940273365006
        total_loss: 7.636974573135376
        vf_explained_var: 0.9875645637512207
        vf_loss: 7.648415048917134
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.38387096774193
    gpu_util_percent0: 0.3051612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14821962606562747
    mean_env_wait_ms: 1.2099149963886047
    mean_inference_ms: 4.417920030331045
    mean_raw_obs_processing_ms: 0.3851192669556511
  time_since_restore: 1119.9810991287231
  time_this_iter_s: 26.447162628173828
  time_total_s: 1119.9810991287231
  timers:
    learn_throughput: 8317.0
    learn_time_ms: 19453.167
    sample_throughput: 23047.077
    sample_time_ms: 7020.066
    update_time_ms: 32.683
  timestamp: 1602620577
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     42 |          1119.98 | 6795264 |  256.885 |              299.505 |              145.717 |            793.676 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3358.1212191179866
    time_step_min: 3079
  date: 2020-10-13_20-23-23
  done: false
  episode_len_mean: 793.313003452244
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 257.1631737396984
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 165
  episodes_total: 8690
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5553108304738998
        entropy_coeff: 0.0005000000000000001
        kl: 0.005491969912933807
        model: {}
        policy_loss: -0.011925454396987334
        total_loss: 5.948849519093831
        vf_explained_var: 0.9862935543060303
        vf_loss: 5.960778037707011
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.203225806451616
    gpu_util_percent0: 0.3332258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148185519009385
    mean_env_wait_ms: 1.2102661961992744
    mean_inference_ms: 4.415594748672097
    mean_raw_obs_processing_ms: 0.3850048744122204
  time_since_restore: 1146.609711408615
  time_this_iter_s: 26.628612279891968
  time_total_s: 1146.609711408615
  timers:
    learn_throughput: 8313.586
    learn_time_ms: 19461.157
    sample_throughput: 23094.706
    sample_time_ms: 7005.588
    update_time_ms: 32.386
  timestamp: 1602620603
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     43 |          1146.61 | 6957056 |  257.163 |              299.505 |              145.717 |            793.313 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3356.3764292992187
    time_step_min: 3079
  date: 2020-10-13_20-23-50
  done: false
  episode_len_mean: 792.9971786480081
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 257.43694819769746
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 171
  episodes_total: 8861
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5548702428738276
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055360287660732865
        model: {}
        policy_loss: -0.009155239444226027
        total_loss: 6.339707454045613
        vf_explained_var: 0.9862532615661621
        vf_loss: 6.3488632043202715
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.422580645161293
    gpu_util_percent0: 0.2987096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14815343114751206
    mean_env_wait_ms: 1.2106245874214205
    mean_inference_ms: 4.413278584779173
    mean_raw_obs_processing_ms: 0.38489052166174553
  time_since_restore: 1173.0087852478027
  time_this_iter_s: 26.399073839187622
  time_total_s: 1173.0087852478027
  timers:
    learn_throughput: 8317.217
    learn_time_ms: 19452.661
    sample_throughput: 23129.327
    sample_time_ms: 6995.102
    update_time_ms: 32.367
  timestamp: 1602620630
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     44 |          1173.01 | 7118848 |  257.437 |              299.505 |              145.717 |            792.997 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3353.178888523512
    time_step_min: 3079
  date: 2020-10-13_20-24-17
  done: false
  episode_len_mean: 792.5362255491203
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 257.9257231919235
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 290
  episodes_total: 9151
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5242825845877329
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052440856816247106
        model: {}
        policy_loss: -0.008842121120930338
        total_loss: 7.925516088803609
        vf_explained_var: 0.9871301651000977
        vf_loss: 7.9343580802281695
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44193548387097
    gpu_util_percent0: 0.34451612903225803
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14809647865086384
    mean_env_wait_ms: 1.211197621273303
    mean_inference_ms: 4.409412206156688
    mean_raw_obs_processing_ms: 0.3847030520382267
  time_since_restore: 1199.5548231601715
  time_this_iter_s: 26.546037912368774
  time_total_s: 1199.5548231601715
  timers:
    learn_throughput: 8315.487
    learn_time_ms: 19456.707
    sample_throughput: 23160.638
    sample_time_ms: 6985.645
    update_time_ms: 34.17
  timestamp: 1602620657
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     45 |          1199.55 | 7280640 |  257.926 |              299.505 |              145.717 |            792.536 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3351.434366257801
    time_step_min: 3079
  date: 2020-10-13_20-24-43
  done: false
  episode_len_mean: 792.2524136451406
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 258.21556262041133
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 171
  episodes_total: 9322
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5219554007053375
        entropy_coeff: 0.0005000000000000001
        kl: 0.005586102333230277
        model: {}
        policy_loss: -0.012437824868053818
        total_loss: 5.265158772468567
        vf_explained_var: 0.9879205822944641
        vf_loss: 5.277578234672546
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.40333333333334
    gpu_util_percent0: 0.2606666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480655304770875
    mean_env_wait_ms: 1.2115203416763625
    mean_inference_ms: 4.407279628005173
    mean_raw_obs_processing_ms: 0.38459856499070766
  time_since_restore: 1225.8152656555176
  time_this_iter_s: 26.26044249534607
  time_total_s: 1225.8152656555176
  timers:
    learn_throughput: 8316.618
    learn_time_ms: 19454.062
    sample_throughput: 23188.712
    sample_time_ms: 6977.188
    update_time_ms: 35.203
  timestamp: 1602620683
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     46 |          1225.82 | 7442432 |  258.216 |              299.505 |              145.717 |            792.252 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3349.8470078240643
    time_step_min: 3079
  date: 2020-10-13_20-25-10
  done: false
  episode_len_mean: 791.9538266919672
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 258.45257444783056
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 9486
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5329876442750295
        entropy_coeff: 0.0005000000000000001
        kl: 0.005586201014618079
        model: {}
        policy_loss: -0.011357899541811397
        total_loss: 6.645129680633545
        vf_explained_var: 0.9848142266273499
        vf_loss: 6.656474828720093
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.890322580645158
    gpu_util_percent0: 0.31935483870967746
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14803678222799457
    mean_env_wait_ms: 1.2118235990651878
    mean_inference_ms: 4.405271657882961
    mean_raw_obs_processing_ms: 0.38449919318003695
  time_since_restore: 1252.012216091156
  time_this_iter_s: 26.196950435638428
  time_total_s: 1252.012216091156
  timers:
    learn_throughput: 8326.311
    learn_time_ms: 19431.415
    sample_throughput: 23314.245
    sample_time_ms: 6939.62
    update_time_ms: 34.743
  timestamp: 1602620710
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     47 |          1252.01 | 7604224 |  258.453 |              299.505 |              145.717 |            791.954 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3346.939020634432
    time_step_min: 3079
  date: 2020-10-13_20-25-36
  done: false
  episode_len_mean: 791.4166240147405
  episode_reward_max: 299.5050505050505
  episode_reward_mean: 258.9031775426493
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 283
  episodes_total: 9769
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5053925216197968
        entropy_coeff: 0.0005000000000000001
        kl: 0.005304940083685021
        model: {}
        policy_loss: -0.008086977589603824
        total_loss: 8.415432214736938
        vf_explained_var: 0.9861140847206116
        vf_loss: 8.423506577809652
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.950000000000003
    gpu_util_percent0: 0.3446666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14798706973641843
    mean_env_wait_ms: 1.2123292656800955
    mean_inference_ms: 4.401805212540055
    mean_raw_obs_processing_ms: 0.3843322496855005
  time_since_restore: 1278.4684545993805
  time_this_iter_s: 26.456238508224487
  time_total_s: 1278.4684545993805
  timers:
    learn_throughput: 8323.078
    learn_time_ms: 19438.963
    sample_throughput: 23343.42
    sample_time_ms: 6930.947
    update_time_ms: 28.278
  timestamp: 1602620736
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     48 |          1278.47 | 7766016 |  258.903 |              299.505 |              145.717 |            791.417 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3345.2163006246224
    time_step_min: 3079
  date: 2020-10-13_20-26-03
  done: false
  episode_len_mean: 791.089813140446
  episode_reward_max: 299.8080808080811
  episode_reward_mean: 259.18265841050646
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 185
  episodes_total: 9954
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49424417813618976
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056748971886311965
        model: {}
        policy_loss: -0.011210046359337866
        total_loss: 5.939767042795817
        vf_explained_var: 0.9866703152656555
        vf_loss: 5.950940489768982
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.541935483870976
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14795764838966047
    mean_env_wait_ms: 1.2126480832390343
    mean_inference_ms: 4.39978378521729
    mean_raw_obs_processing_ms: 0.38423632641234007
  time_since_restore: 1304.9721167087555
  time_this_iter_s: 26.503662109375
  time_total_s: 1304.9721167087555
  timers:
    learn_throughput: 8323.938
    learn_time_ms: 19436.955
    sample_throughput: 23388.904
    sample_time_ms: 6917.468
    update_time_ms: 28.226
  timestamp: 1602620763
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     49 |          1304.97 | 7927808 |  259.183 |              299.808 |              145.717 |             791.09 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3343.438398255526
    time_step_min: 3079
  date: 2020-10-13_20-26-30
  done: false
  episode_len_mean: 790.8114065434418
  episode_reward_max: 299.8080808080811
  episode_reward_mean: 259.45167499847736
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 163
  episodes_total: 10117
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5094549457232157
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052692491638784604
        model: {}
        policy_loss: -0.011700866136076607
        total_loss: 5.405965685844421
        vf_explained_var: 0.9866686463356018
        vf_loss: 5.417657653490703
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.28333333333334
    gpu_util_percent0: 0.31999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147931912734316
    mean_env_wait_ms: 1.212917608124059
    mean_inference_ms: 4.397970161513645
    mean_raw_obs_processing_ms: 0.3841475859436674
  time_since_restore: 1331.2691097259521
  time_this_iter_s: 26.296993017196655
  time_total_s: 1331.2691097259521
  timers:
    learn_throughput: 8330.724
    learn_time_ms: 19421.121
    sample_throughput: 23494.183
    sample_time_ms: 6886.471
    update_time_ms: 24.971
  timestamp: 1602620790
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     50 |          1331.27 | 8089600 |  259.452 |              299.808 |              145.717 |            790.811 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3340.86744545279
    time_step_min: 3054
  date: 2020-10-13_20-26-56
  done: false
  episode_len_mean: 790.3745426535721
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 259.8196931767122
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 269
  episodes_total: 10386
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4893743023276329
        entropy_coeff: 0.0005000000000000001
        kl: 0.005739350298730035
        model: {}
        policy_loss: -0.009993007969266424
        total_loss: 9.506619215011597
        vf_explained_var: 0.9842750430107117
        vf_loss: 9.51656993230184
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.164516129032258
    gpu_util_percent0: 0.33967741935483875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788967665407177
    mean_env_wait_ms: 1.2133571629784137
    mean_inference_ms: 4.394996538882445
    mean_raw_obs_processing_ms: 0.38400429961120597
  time_since_restore: 1357.8114771842957
  time_this_iter_s: 26.542367458343506
  time_total_s: 1357.8114771842957
  timers:
    learn_throughput: 8328.361
    learn_time_ms: 19426.632
    sample_throughput: 23498.283
    sample_time_ms: 6885.269
    update_time_ms: 26.675
  timestamp: 1602620816
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     51 |          1357.81 | 8251392 |   259.82 |              303.293 |              145.717 |            790.375 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3339.231315714692
    time_step_min: 3054
  date: 2020-10-13_20-27-23
  done: false
  episode_len_mean: 790.126594237128
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 260.0842148456697
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 10585
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47421347349882126
        entropy_coeff: 0.0005000000000000001
        kl: 0.004842397950900097
        model: {}
        policy_loss: -0.00872566036802406
        total_loss: 5.855418086051941
        vf_explained_var: 0.9877085089683533
        vf_loss: 5.864138642946879
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.283333333333335
    gpu_util_percent0: 0.3696666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14786021354230192
    mean_env_wait_ms: 1.2136612039122636
    mean_inference_ms: 4.393010948858505
    mean_raw_obs_processing_ms: 0.3839101406312578
  time_since_restore: 1384.109564781189
  time_this_iter_s: 26.29808759689331
  time_total_s: 1384.109564781189
  timers:
    learn_throughput: 8336.109
    learn_time_ms: 19408.575
    sample_throughput: 23496.912
    sample_time_ms: 6885.671
    update_time_ms: 27.334
  timestamp: 1602620843
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     52 |          1384.11 | 8413184 |  260.084 |              303.293 |              145.717 |            790.127 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3337.8276891501073
    time_step_min: 3054
  date: 2020-10-13_20-27-50
  done: false
  episode_len_mean: 789.9024844142551
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 260.2794174178746
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 162
  episodes_total: 10747
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4887526035308838
        entropy_coeff: 0.0005000000000000001
        kl: 0.005664059310220182
        model: {}
        policy_loss: -0.01032462390139699
        total_loss: 6.339387933413188
        vf_explained_var: 0.9850199818611145
        vf_loss: 6.349815567334493
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.829032258064515
    gpu_util_percent0: 0.3629032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478365583310644
    mean_env_wait_ms: 1.2139019987429782
    mean_inference_ms: 4.39138412125097
    mean_raw_obs_processing_ms: 0.3838306335021706
  time_since_restore: 1410.4234747886658
  time_this_iter_s: 26.313910007476807
  time_total_s: 1410.4234747886658
  timers:
    learn_throughput: 8342.072
    learn_time_ms: 19394.701
    sample_throughput: 23560.496
    sample_time_ms: 6867.088
    update_time_ms: 27.884
  timestamp: 1602620870
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     53 |          1410.42 | 8574976 |  260.279 |              303.293 |              145.717 |            789.902 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3335.6289749430525
    time_step_min: 3054
  date: 2020-10-13_20-28-16
  done: false
  episode_len_mean: 789.6325547577933
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 260.61444491263626
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 256
  episodes_total: 11003
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4678051024675369
        entropy_coeff: 0.0005000000000000001
        kl: 0.005607184759962062
        model: {}
        policy_loss: -0.00997994407225633
        total_loss: 7.096481243769328
        vf_explained_var: 0.9879934191703796
        vf_loss: 7.106554945309957
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.833333333333336
    gpu_util_percent0: 0.334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14780012929120614
    mean_env_wait_ms: 1.2142803916492493
    mean_inference_ms: 4.388833237433604
    mean_raw_obs_processing_ms: 0.3837114803176425
  time_since_restore: 1436.91787648201
  time_this_iter_s: 26.494401693344116
  time_total_s: 1436.91787648201
  timers:
    learn_throughput: 8332.909
    learn_time_ms: 19416.028
    sample_throughput: 23603.768
    sample_time_ms: 6854.499
    update_time_ms: 27.952
  timestamp: 1602620896
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     54 |          1436.92 | 8736768 |  260.614 |              303.293 |              145.717 |            789.633 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3333.8342867357883
    time_step_min: 3054
  date: 2020-10-13_20-28-43
  done: false
  episode_len_mean: 789.420203281027
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 260.87382022795714
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 11216
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4456804369886716
        entropy_coeff: 0.0005000000000000001
        kl: 0.005640700381870071
        model: {}
        policy_loss: -0.009555026073940098
        total_loss: 6.3164375225702925
        vf_explained_var: 0.9872992038726807
        vf_loss: 6.326074282328288
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.809677419354838
    gpu_util_percent0: 0.32419354838709674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477713824322086
    mean_env_wait_ms: 1.2145751323465046
    mean_inference_ms: 4.386857781496819
    mean_raw_obs_processing_ms: 0.3836161417419929
  time_since_restore: 1463.3415586948395
  time_this_iter_s: 26.42368221282959
  time_total_s: 1463.3415586948395
  timers:
    learn_throughput: 8333.094
    learn_time_ms: 19415.599
    sample_throughput: 23646.226
    sample_time_ms: 6842.191
    update_time_ms: 27.72
  timestamp: 1602620923
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     55 |          1463.34 | 8898560 |  260.874 |              303.293 |              145.717 |             789.42 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3332.7957349312655
    time_step_min: 3054
  date: 2020-10-13_20-29-10
  done: false
  episode_len_mean: 789.2950070323488
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 261.0446820525934
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 160
  episodes_total: 11376
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.461775283018748
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052875694042692585
        model: {}
        policy_loss: -0.009517594526793497
        total_loss: 5.4199016491572065
        vf_explained_var: 0.9872874617576599
        vf_loss: 5.42951778570811
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.541935483870965
    gpu_util_percent0: 0.33548387096774196
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870956
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14774999642629777
    mean_env_wait_ms: 1.214787683552744
    mean_inference_ms: 4.3853829425794
    mean_raw_obs_processing_ms: 0.3835454794103799
  time_since_restore: 1489.8714349269867
  time_this_iter_s: 26.529876232147217
  time_total_s: 1489.8714349269867
  timers:
    learn_throughput: 8325.139
    learn_time_ms: 19434.15
    sample_throughput: 23621.003
    sample_time_ms: 6849.497
    update_time_ms: 28.078
  timestamp: 1602620950
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     56 |          1489.87 | 9060352 |  261.045 |              303.293 |              145.717 |            789.295 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3330.9388548233874
    time_step_min: 3054
  date: 2020-10-13_20-29-36
  done: false
  episode_len_mean: 789.062720771948
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 261.33558119316706
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 231
  episodes_total: 11607
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.44272469977537793
        entropy_coeff: 0.0005000000000000001
        kl: 0.005777890793979168
        model: {}
        policy_loss: -0.0090979779100356
        total_loss: 7.434427936871846
        vf_explained_var: 0.9864635467529297
        vf_loss: 7.443602800369263
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.736666666666668
    gpu_util_percent0: 0.289
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14772168031855645
    mean_env_wait_ms: 1.2150927335209047
    mean_inference_ms: 4.383290995730415
    mean_raw_obs_processing_ms: 0.38344912079263044
  time_since_restore: 1516.1415100097656
  time_this_iter_s: 26.27007508277893
  time_total_s: 1516.1415100097656
  timers:
    learn_throughput: 8322.984
    learn_time_ms: 19439.183
    sample_throughput: 23615.979
    sample_time_ms: 6850.955
    update_time_ms: 28.638
  timestamp: 1602620976
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     57 |          1516.14 | 9222144 |  261.336 |              303.293 |              145.717 |            789.063 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3328.9515992553734
    time_step_min: 3054
  date: 2020-10-13_20-30-03
  done: false
  episode_len_mean: 788.7513084585514
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 261.6369298250101
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 239
  episodes_total: 11846
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4169960568348567
        entropy_coeff: 0.0005000000000000001
        kl: 0.004996339984548588
        model: {}
        policy_loss: -0.00955103572535639
        total_loss: 5.380350391070048
        vf_explained_var: 0.9896233677864075
        vf_loss: 5.389984925587972
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.122580645161296
    gpu_util_percent0: 0.3580645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769024669299372
    mean_env_wait_ms: 1.2153970972271082
    mean_inference_ms: 4.381211088500097
    mean_raw_obs_processing_ms: 0.38334924876057924
  time_since_restore: 1542.6194114685059
  time_this_iter_s: 26.477901458740234
  time_total_s: 1542.6194114685059
  timers:
    learn_throughput: 8320.348
    learn_time_ms: 19445.341
    sample_throughput: 23637.701
    sample_time_ms: 6844.659
    update_time_ms: 30.557
  timestamp: 1602621003
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     58 |          1542.62 | 9383936 |  261.637 |              303.293 |              145.717 |            788.751 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3327.7068447412353
    time_step_min: 3054
  date: 2020-10-13_20-30-30
  done: false
  episode_len_mean: 788.5756162558295
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 261.83277898909137
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 162
  episodes_total: 12008
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4326689839363098
        entropy_coeff: 0.0005000000000000001
        kl: 0.005560785842438539
        model: {}
        policy_loss: -0.01230035779735772
        total_loss: 4.9922739664713545
        vf_explained_var: 0.9877254366874695
        vf_loss: 5.00472108523051
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.100000000000005
    gpu_util_percent0: 0.326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476711550465769
    mean_env_wait_ms: 1.2155968310268712
    mean_inference_ms: 4.379863239908209
    mean_raw_obs_processing_ms: 0.3832858393115314
  time_since_restore: 1568.9844992160797
  time_this_iter_s: 26.365087747573853
  time_total_s: 1568.9844992160797
  timers:
    learn_throughput: 8319.798
    learn_time_ms: 19446.626
    sample_throughput: 23692.182
    sample_time_ms: 6828.919
    update_time_ms: 30.284
  timestamp: 1602621030
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     59 |          1568.98 | 9545728 |  261.833 |              303.293 |              145.717 |            788.576 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3326.1090894189
    time_step_min: 3054
  date: 2020-10-13_20-30-56
  done: false
  episode_len_mean: 788.3061574944803
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 262.05879136445816
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 221
  episodes_total: 12229
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4189060578743617
        entropy_coeff: 0.0005000000000000001
        kl: 0.006021614070050418
        model: {}
        policy_loss: -0.010093243676237762
        total_loss: 6.481432318687439
        vf_explained_var: 0.9878926277160645
        vf_loss: 6.491659641265869
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.554838709677416
    gpu_util_percent0: 0.3506451612903227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147646978248163
    mean_env_wait_ms: 1.2158667157124454
    mean_inference_ms: 4.378032003143468
    mean_raw_obs_processing_ms: 0.383201517110549
  time_since_restore: 1595.2532081604004
  time_this_iter_s: 26.26870894432068
  time_total_s: 1595.2532081604004
  timers:
    learn_throughput: 8324.665
    learn_time_ms: 19435.257
    sample_throughput: 23666.483
    sample_time_ms: 6836.335
    update_time_ms: 30.553
  timestamp: 1602621056
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     60 |          1595.25 | 9707520 |  262.059 |              303.293 |              145.717 |            788.306 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3324.5120501285346
    time_step_min: 3054
  date: 2020-10-13_20-31-23
  done: false
  episode_len_mean: 788.0955434434113
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 262.3158411625067
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 247
  episodes_total: 12476
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3973720247546832
        entropy_coeff: 0.0005000000000000001
        kl: 0.005185622295054297
        model: {}
        policy_loss: -0.010166727685524771
        total_loss: 5.6593455870946245
        vf_explained_var: 0.9898659586906433
        vf_loss: 5.669646143913269
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.966666666666665
    gpu_util_percent0: 0.2653333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476172157272412
    mean_env_wait_ms: 1.2161582373726607
    mean_inference_ms: 4.376040176299009
    mean_raw_obs_processing_ms: 0.38310846398415077
  time_since_restore: 1621.7444047927856
  time_this_iter_s: 26.491196632385254
  time_total_s: 1621.7444047927856
  timers:
    learn_throughput: 8325.899
    learn_time_ms: 19432.377
    sample_throughput: 23670.878
    sample_time_ms: 6835.065
    update_time_ms: 29.812
  timestamp: 1602621083
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     61 |          1621.74 | 9869312 |  262.316 |              303.293 |              145.717 |            788.096 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3323.4093720266414
    time_step_min: 3054
  date: 2020-10-13_20-31-50
  done: false
  episode_len_mean: 787.9549050632911
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 262.4966836082342
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 12640
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4058798501888911
        entropy_coeff: 0.0005000000000000001
        kl: 0.005482674071875711
        model: {}
        policy_loss: -0.008212998790744072
        total_loss: 4.714298248291016
        vf_explained_var: 0.9888290762901306
        vf_loss: 4.7226459582646685
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73225806451613
    gpu_util_percent0: 0.3629032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475991299510336
    mean_env_wait_ms: 1.2163431801214153
    mean_inference_ms: 4.37478727878001
    mean_raw_obs_processing_ms: 0.3830492366142872
  time_since_restore: 1648.114149570465
  time_this_iter_s: 26.369744777679443
  time_total_s: 1648.114149570465
  timers:
    learn_throughput: 8324.65
    learn_time_ms: 19435.291
    sample_throughput: 23655.443
    sample_time_ms: 6839.525
    update_time_ms: 30.081
  timestamp: 1602621110
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     62 |          1648.11 | 10031104 |  262.497 |              303.293 |              145.717 |            787.955 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3322.009992193599
    time_step_min: 3054
  date: 2020-10-13_20-32-16
  done: false
  episode_len_mean: 787.7519083969465
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 262.70189273951536
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 12838
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.40383800864219666
        entropy_coeff: 0.0005000000000000001
        kl: 0.005184352203893165
        model: {}
        policy_loss: -0.011298387316249622
        total_loss: 6.758020639419556
        vf_explained_var: 0.9866301417350769
        vf_loss: 6.769456267356873
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.951612903225815
    gpu_util_percent0: 0.2729032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757823229400813
    mean_env_wait_ms: 1.2165662707447926
    mean_inference_ms: 4.373289988664813
    mean_raw_obs_processing_ms: 0.38297712203554934
  time_since_restore: 1674.8411116600037
  time_this_iter_s: 26.726962089538574
  time_total_s: 1674.8411116600037
  timers:
    learn_throughput: 8312.274
    learn_time_ms: 19464.228
    sample_throughput: 23621.592
    sample_time_ms: 6849.327
    update_time_ms: 31.09
  timestamp: 1602621136
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     63 |          1674.84 | 10192896 |  262.702 |              303.293 |              145.717 |            787.752 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3319.940114722753
    time_step_min: 3054
  date: 2020-10-13_20-32-43
  done: false
  episode_len_mean: 787.527283828131
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 263.0203569696815
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 265
  episodes_total: 13103
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.37833695362011593
        entropy_coeff: 0.0005000000000000001
        kl: 0.00497432976650695
        model: {}
        policy_loss: -0.00904030004555049
        total_loss: 6.6286492347717285
        vf_explained_var: 0.9883511066436768
        vf_loss: 6.6378166278203325
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.31666666666667
    gpu_util_percent0: 0.3113333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754992630839955
    mean_env_wait_ms: 1.216846061684579
    mean_inference_ms: 4.37129745216633
    mean_raw_obs_processing_ms: 0.3828863070861057
  time_since_restore: 1701.1317870616913
  time_this_iter_s: 26.290675401687622
  time_total_s: 1701.1317870616913
  timers:
    learn_throughput: 8318.12
    learn_time_ms: 19450.549
    sample_throughput: 23647.122
    sample_time_ms: 6841.932
    update_time_ms: 30.977
  timestamp: 1602621163
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     64 |          1701.13 | 10354688 |   263.02 |              303.293 |              145.717 |            787.527 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3318.8529145273333
    time_step_min: 3054
  date: 2020-10-13_20-33-10
  done: false
  episode_len_mean: 787.4322634116938
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 263.1863709427
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 13272
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.38818367818991345
        entropy_coeff: 0.0005000000000000001
        kl: 0.005403678476189573
        model: {}
        policy_loss: -0.008791962182537342
        total_loss: 4.96499502658844
        vf_explained_var: 0.9885714650154114
        vf_loss: 4.973947246869405
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73225806451613
    gpu_util_percent0: 0.28838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753273749607537
    mean_env_wait_ms: 1.2170178042131765
    mean_inference_ms: 4.370121829597021
    mean_raw_obs_processing_ms: 0.3828291311463199
  time_since_restore: 1727.7072086334229
  time_this_iter_s: 26.575421571731567
  time_total_s: 1727.7072086334229
  timers:
    learn_throughput: 8316.237
    learn_time_ms: 19454.953
    sample_throughput: 23616.411
    sample_time_ms: 6850.829
    update_time_ms: 31.338
  timestamp: 1602621190
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     65 |          1727.71 | 10516480 |  263.186 |              303.293 |              145.717 |            787.432 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3317.604422604423
    time_step_min: 3054
  date: 2020-10-13_20-33-37
  done: false
  episode_len_mean: 787.2763206776135
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 263.3700096289441
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 187
  episodes_total: 13459
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3901598701874415
        entropy_coeff: 0.0005000000000000001
        kl: 0.005447537211390833
        model: {}
        policy_loss: -0.00999179832676115
        total_loss: 5.5021640459696455
        vf_explained_var: 0.9887394309043884
        vf_loss: 5.512316902478536
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.08709677419355
    gpu_util_percent0: 0.2961290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14751435145559272
    mean_env_wait_ms: 1.2172084758630959
    mean_inference_ms: 4.368805257576698
    mean_raw_obs_processing_ms: 0.38276511020110504
  time_since_restore: 1754.0996279716492
  time_this_iter_s: 26.39241933822632
  time_total_s: 1754.0996279716492
  timers:
    learn_throughput: 8320.374
    learn_time_ms: 19445.281
    sample_throughput: 23633.159
    sample_time_ms: 6845.974
    update_time_ms: 30.861
  timestamp: 1602621217
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     66 |           1754.1 | 10678272 |   263.37 |              303.293 |              145.717 |            787.276 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3315.7638686131386
    time_step_min: 3054
  date: 2020-10-13_20-34-03
  done: false
  episode_len_mean: 787.0347465034965
  episode_reward_max: 303.29292929292905
  episode_reward_mean: 263.6431866744366
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 269
  episodes_total: 13728
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.36908528457085293
        entropy_coeff: 0.0005000000000000001
        kl: 0.005574712258142729
        model: {}
        policy_loss: -0.0077352875377982855
        total_loss: 7.273245175679524
        vf_explained_var: 0.9879145622253418
        vf_loss: 7.281130115191142
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280645161290323
    gpu_util_percent0: 0.3206451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748690491400818
    mean_env_wait_ms: 1.217464367649639
    mean_inference_ms: 4.366946617484597
    mean_raw_obs_processing_ms: 0.3826789835042501
  time_since_restore: 1780.6767251491547
  time_this_iter_s: 26.577097177505493
  time_total_s: 1780.6767251491547
  timers:
    learn_throughput: 8310.313
    learn_time_ms: 19468.822
    sample_throughput: 23609.918
    sample_time_ms: 6852.713
    update_time_ms: 29.832
  timestamp: 1602621243
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     67 |          1780.68 | 10840064 |  263.643 |              303.293 |              145.717 |            787.035 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3314.3908468468467
    time_step_min: 3050
  date: 2020-10-13_20-34-30
  done: false
  episode_len_mean: 786.9310220815652
  episode_reward_max: 303.89898989898995
  episode_reward_mean: 263.82819637066916
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 13903
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3753599176804225
        entropy_coeff: 0.0005000000000000001
        kl: 0.00516426598187536
        model: {}
        policy_loss: -0.011990850054038068
        total_loss: 5.293622771898906
        vf_explained_var: 0.9880210757255554
        vf_loss: 5.305769085884094
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.23225806451613
    gpu_util_percent0: 0.3109677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747092625264357
    mean_env_wait_ms: 1.2176236204412785
    mean_inference_ms: 4.365820685377929
    mean_raw_obs_processing_ms: 0.38262291082404376
  time_since_restore: 1807.2304542064667
  time_this_iter_s: 26.55372905731201
  time_total_s: 1807.2304542064667
  timers:
    learn_throughput: 8312.529
    learn_time_ms: 19463.631
    sample_throughput: 23571.49
    sample_time_ms: 6863.885
    update_time_ms: 29.66
  timestamp: 1602621270
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     68 |          1807.23 | 11001856 |  263.828 |              303.899 |              145.717 |            786.931 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3313.058383766465
    time_step_min: 3050
  date: 2020-10-13_20-34-57
  done: false
  episode_len_mean: 786.8681162509771
  episode_reward_max: 304.6565656565655
  episode_reward_mean: 264.01149848517144
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 14073
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3813230370481809
        entropy_coeff: 0.0005000000000000001
        kl: 0.005576765242343147
        model: {}
        policy_loss: -0.009249821014236659
        total_loss: 5.994030078252156
        vf_explained_var: 0.9869537353515625
        vf_loss: 6.003435492515564
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.480645161290326
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745381477233635
    mean_env_wait_ms: 1.2177747933067302
    mean_inference_ms: 4.364694256533708
    mean_raw_obs_processing_ms: 0.38256560868654826
  time_since_restore: 1833.7260727882385
  time_this_iter_s: 26.49561858177185
  time_total_s: 1833.7260727882385
  timers:
    learn_throughput: 8312.95
    learn_time_ms: 19462.646
    sample_throughput: 23561.309
    sample_time_ms: 6866.851
    update_time_ms: 31.448
  timestamp: 1602621297
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     69 |          1833.73 | 11163648 |  264.011 |              304.657 |              145.717 |            786.868 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3311.080061478273
    time_step_min: 3050
  date: 2020-10-13_20-35-24
  done: false
  episode_len_mean: 786.7468274996513
  episode_reward_max: 305.11111111111137
  episode_reward_mean: 264.31309468975064
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 269
  episodes_total: 14342
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3640394906202952
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049436708601812525
        model: {}
        policy_loss: -0.006588246421112369
        total_loss: 6.8674137989679975
        vf_explained_var: 0.9885039329528809
        vf_loss: 6.874153256416321
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.27
    gpu_util_percent0: 0.29066666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7966666666666673
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474301848333301
    mean_env_wait_ms: 1.2180114866970932
    mean_inference_ms: 4.363020898707267
    mean_raw_obs_processing_ms: 0.3824863080622429
  time_since_restore: 1860.0824537277222
  time_this_iter_s: 26.356380939483643
  time_total_s: 1860.0824537277222
  timers:
    learn_throughput: 8307.702
    learn_time_ms: 19474.94
    sample_throughput: 23579.426
    sample_time_ms: 6861.575
    update_time_ms: 32.041
  timestamp: 1602621324
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     70 |          1860.08 | 11325440 |  264.313 |              305.111 |              145.717 |            786.747 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3309.8613677099133
    time_step_min: 3050
  date: 2020-10-13_20-35-50
  done: false
  episode_len_mean: 786.6852208614283
  episode_reward_max: 305.11111111111137
  episode_reward_mean: 264.4968044279314
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 192
  episodes_total: 14534
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3534674321611722
        entropy_coeff: 0.0005000000000000001
        kl: 0.005419445573352277
        model: {}
        policy_loss: -0.010775467390582586
        total_loss: 5.534964203834534
        vf_explained_var: 0.9884769916534424
        vf_loss: 5.545899470647176
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.712903225806457
    gpu_util_percent0: 0.38322580645161286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.819354838709677
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14741372901563654
    mean_env_wait_ms: 1.2181608906918795
    mean_inference_ms: 4.361866055744134
    mean_raw_obs_processing_ms: 0.38242864397199383
  time_since_restore: 1886.4895641803741
  time_this_iter_s: 26.407110452651978
  time_total_s: 1886.4895641803741
  timers:
    learn_throughput: 8308.719
    learn_time_ms: 19472.556
    sample_throughput: 23597.115
    sample_time_ms: 6856.431
    update_time_ms: 29.889
  timestamp: 1602621350
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     71 |          1886.49 | 11487232 |  264.497 |              305.111 |              145.717 |            786.685 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3308.722922772817
    time_step_min: 3050
  date: 2020-10-13_20-36-17
  done: false
  episode_len_mean: 786.687869923124
  episode_reward_max: 305.26262626262655
  episode_reward_mean: 264.6774906009547
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 165
  episodes_total: 14699
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3651345248023669
        entropy_coeff: 0.0005000000000000001
        kl: 0.005380182294175029
        model: {}
        policy_loss: -0.011825502306843797
        total_loss: 4.975037892659505
        vf_explained_var: 0.9884076118469238
        vf_loss: 4.987029155095418
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.756666666666664
    gpu_util_percent0: 0.3236666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739859112174677
    mean_env_wait_ms: 1.2182856269757543
    mean_inference_ms: 4.360849311967718
    mean_raw_obs_processing_ms: 0.38237560903605905
  time_since_restore: 1912.7412078380585
  time_this_iter_s: 26.251643657684326
  time_total_s: 1912.7412078380585
  timers:
    learn_throughput: 8310.252
    learn_time_ms: 19468.965
    sample_throughput: 23622.384
    sample_time_ms: 6849.097
    update_time_ms: 28.176
  timestamp: 1602621377
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     72 |          1912.74 | 11649024 |  264.677 |              305.263 |              145.717 |            786.688 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3307.004625905068
    time_step_min: 3036
  date: 2020-10-13_20-36-44
  done: false
  episode_len_mean: 786.6783993576017
  episode_reward_max: 306.02020202020213
  episode_reward_mean: 264.92590452166195
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 245
  episodes_total: 14944
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3530029207468033
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052069373584042
        model: {}
        policy_loss: -0.011354393825361816
        total_loss: 7.1340252955754595
        vf_explained_var: 0.9878211617469788
        vf_loss: 7.145540157953898
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.645161290322587
    gpu_util_percent0: 0.35999999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737939958034765
    mean_env_wait_ms: 1.2184768233161252
    mean_inference_ms: 4.359457918483472
    mean_raw_obs_processing_ms: 0.38230889739792157
  time_since_restore: 1938.9836666584015
  time_this_iter_s: 26.242458820343018
  time_total_s: 1938.9836666584015
  timers:
    learn_throughput: 8328.236
    learn_time_ms: 19426.923
    sample_throughput: 23642.499
    sample_time_ms: 6843.27
    update_time_ms: 27.551
  timestamp: 1602621404
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     73 |          1938.98 | 11810816 |  264.926 |               306.02 |              145.717 |            786.678 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3305.694945490585
    time_step_min: 3036
  date: 2020-10-13_20-37-11
  done: false
  episode_len_mean: 786.695706654356
  episode_reward_max: 306.02020202020213
  episode_reward_mean: 265.13021662912837
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 219
  episodes_total: 15163
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3319677660862605
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056212386504436536
        model: {}
        policy_loss: -0.011522360475889096
        total_loss: 5.482268452644348
        vf_explained_var: 0.9896093010902405
        vf_loss: 5.493939399719238
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.377419354838707
    gpu_util_percent0: 0.3374193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736031393682927
    mean_env_wait_ms: 1.2186252934277861
    mean_inference_ms: 4.358183361044795
    mean_raw_obs_processing_ms: 0.3822437100651762
  time_since_restore: 1965.6576268672943
  time_this_iter_s: 26.673960208892822
  time_total_s: 1965.6576268672943
  timers:
    learn_throughput: 8315.25
    learn_time_ms: 19457.263
    sample_throughput: 23631.412
    sample_time_ms: 6846.48
    update_time_ms: 29.336
  timestamp: 1602621431
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     74 |          1965.66 | 11972608 |   265.13 |               306.02 |              145.717 |            786.696 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3304.437181330893
    time_step_min: 3036
  date: 2020-10-13_20-37-37
  done: false
  episode_len_mean: 786.724977162991
  episode_reward_max: 306.02020202020213
  episode_reward_mean: 265.3161004538402
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 163
  episodes_total: 15326
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3481028328339259
        entropy_coeff: 0.0005000000000000001
        kl: 0.005596096394583583
        model: {}
        policy_loss: -0.011133084670291282
        total_loss: 3.747858941555023
        vf_explained_var: 0.9911004900932312
        vf_loss: 3.7591486970583596
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45161290322581
    gpu_util_percent0: 0.3035483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734762817188435
    mean_env_wait_ms: 1.2187349808151393
    mean_inference_ms: 4.357297473972606
    mean_raw_obs_processing_ms: 0.38219839807821604
  time_since_restore: 1992.2777457237244
  time_this_iter_s: 26.620118856430054
  time_total_s: 1992.2777457237244
  timers:
    learn_throughput: 8315.417
    learn_time_ms: 19456.872
    sample_throughput: 23613.239
    sample_time_ms: 6851.749
    update_time_ms: 27.958
  timestamp: 1602621457
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     75 |          1992.28 | 12134400 |  265.316 |               306.02 |              145.717 |            786.725 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3303.129627241068
    time_step_min: 3036
  date: 2020-10-13_20-38-04
  done: false
  episode_len_mean: 786.7769409038239
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 265.5173070995782
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 208
  episodes_total: 15534
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.344746895134449
        entropy_coeff: 0.0005000000000000001
        kl: 0.005574784707278013
        model: {}
        policy_loss: -0.011940839467570186
        total_loss: 5.638649384180705
        vf_explained_var: 0.9892699122428894
        vf_loss: 5.650745153427124
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.322580645161292
    gpu_util_percent0: 0.32387096774193547
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733282663158412
    mean_env_wait_ms: 1.2188710639328126
    mean_inference_ms: 4.3561972666753865
    mean_raw_obs_processing_ms: 0.3821420179442154
  time_since_restore: 2018.8193707466125
  time_this_iter_s: 26.541625022888184
  time_total_s: 2018.8193707466125
  timers:
    learn_throughput: 8318.673
    learn_time_ms: 19449.256
    sample_throughput: 23561.131
    sample_time_ms: 6866.903
    update_time_ms: 26.254
  timestamp: 1602621484
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     76 |          2018.82 | 12296192 |  265.517 |              307.838 |              145.717 |            786.777 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3301.4182371977918
    time_step_min: 3036
  date: 2020-10-13_20-38-31
  done: false
  episode_len_mean: 786.8494330778489
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 265.7781539983351
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 253
  episodes_total: 15787
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3148241142431895
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051173852213347954
        model: {}
        policy_loss: -0.009240640594119517
        total_loss: 4.566282232602437
        vf_explained_var: 0.9920072555541992
        vf_loss: 4.575664361317952
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87333333333333
    gpu_util_percent0: 0.36300000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731167333034184
    mean_env_wait_ms: 1.2190189014561807
    mean_inference_ms: 4.354784958041309
    mean_raw_obs_processing_ms: 0.38206906654031075
  time_since_restore: 2044.9867024421692
  time_this_iter_s: 26.16733169555664
  time_total_s: 2044.9867024421692
  timers:
    learn_throughput: 8327.498
    learn_time_ms: 19428.645
    sample_throughput: 23630.199
    sample_time_ms: 6846.832
    update_time_ms: 25.73
  timestamp: 1602621511
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     77 |          2044.99 | 12457984 |  265.778 |              307.838 |              145.717 |            786.849 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3300.309812292046
    time_step_min: 3036
  date: 2020-10-13_20-38-58
  done: false
  episode_len_mean: 786.868270978254
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 265.9498234839464
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 170
  episodes_total: 15957
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3213893920183182
        entropy_coeff: 0.0005000000000000001
        kl: 0.004714472258153061
        model: {}
        policy_loss: -0.011277195008005947
        total_loss: 4.23752232392629
        vf_explained_var: 0.9905653595924377
        vf_loss: 4.248945474624634
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203225806451616
    gpu_util_percent0: 0.2799999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729896236461548
    mean_env_wait_ms: 1.2191176522010982
    mean_inference_ms: 4.353935632529216
    mean_raw_obs_processing_ms: 0.38202582963255494
  time_since_restore: 2071.392685174942
  time_this_iter_s: 26.405982732772827
  time_total_s: 2071.392685174942
  timers:
    learn_throughput: 8328.435
    learn_time_ms: 19426.458
    sample_throughput: 23680.346
    sample_time_ms: 6832.333
    update_time_ms: 26.87
  timestamp: 1602621538
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     78 |          2071.39 | 12619776 |   265.95 |              307.838 |              145.717 |            786.868 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3299.1434960864703
    time_step_min: 3036
  date: 2020-10-13_20-39-25
  done: false
  episode_len_mean: 786.9035718715119
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 266.1219387224596
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 16126
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3279392917950948
        entropy_coeff: 0.0005000000000000001
        kl: 0.004747464826020102
        model: {}
        policy_loss: -0.009505098908751583
        total_loss: 4.234664003054301
        vf_explained_var: 0.990907609462738
        vf_loss: 4.244325558344523
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72333333333334
    gpu_util_percent0: 0.3656666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728570340120958
    mean_env_wait_ms: 1.2192061460709902
    mean_inference_ms: 4.353043486763049
    mean_raw_obs_processing_ms: 0.3819774349278522
  time_since_restore: 2097.878945827484
  time_this_iter_s: 26.486260652542114
  time_total_s: 2097.878945827484
  timers:
    learn_throughput: 8321.982
    learn_time_ms: 19441.522
    sample_throughput: 23705.34
    sample_time_ms: 6825.129
    update_time_ms: 25.122
  timestamp: 1602621565
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     79 |          2097.88 | 12781568 |  266.122 |              307.838 |              145.717 |            786.904 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3297.2708282433423
    time_step_min: 3036
  date: 2020-10-13_20-39-51
  done: false
  episode_len_mean: 786.8728658536585
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 266.42263180586343
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 274
  episodes_total: 16400
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.30738384028275806
        entropy_coeff: 0.0005000000000000001
        kl: 0.004842223133891821
        model: {}
        policy_loss: -0.009525117831799434
        total_loss: 4.971691926320394
        vf_explained_var: 0.99152010679245
        vf_loss: 4.981367111206055
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.377419354838707
    gpu_util_percent0: 0.3009677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472663739582891
    mean_env_wait_ms: 1.2193486970641008
    mean_inference_ms: 4.351670456335643
    mean_raw_obs_processing_ms: 0.3819068715660553
  time_since_restore: 2124.540568113327
  time_this_iter_s: 26.661622285842896
  time_total_s: 2124.540568113327
  timers:
    learn_throughput: 8312.009
    learn_time_ms: 19464.85
    sample_throughput: 23688.732
    sample_time_ms: 6829.914
    update_time_ms: 26.219
  timestamp: 1602621591
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     80 |          2124.54 | 12943360 |  266.423 |              307.838 |              145.717 |            786.873 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3295.956101684681
    time_step_min: 3028
  date: 2020-10-13_20-40-18
  done: false
  episode_len_mean: 786.8584001446742
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 266.62450899981786
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 189
  episodes_total: 16589
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29351264983415604
        entropy_coeff: 0.0005000000000000001
        kl: 0.00474738422781229
        model: {}
        policy_loss: -0.00894037855323404
        total_loss: 3.6123553117116294
        vf_explained_var: 0.9922553896903992
        vf_loss: 3.6214405298233032
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.929032258064517
    gpu_util_percent0: 0.3451612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14725339069522897
    mean_env_wait_ms: 1.2194392443188398
    mean_inference_ms: 4.350778127545613
    mean_raw_obs_processing_ms: 0.38186061437534446
  time_since_restore: 2150.9880924224854
  time_this_iter_s: 26.447524309158325
  time_total_s: 2150.9880924224854
  timers:
    learn_throughput: 8310.153
    learn_time_ms: 19469.197
    sample_throughput: 23698.88
    sample_time_ms: 6826.989
    update_time_ms: 27.551
  timestamp: 1602621618
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     81 |          2150.99 | 13105152 |  266.625 |              307.838 |              145.717 |            786.858 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3294.880657698057
    time_step_min: 3028
  date: 2020-10-13_20-40-45
  done: false
  episode_len_mean: 786.8624127022026
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 266.79466303939523
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 16753
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.30822569131851196
        entropy_coeff: 0.0005000000000000001
        kl: 0.005055491502086322
        model: {}
        policy_loss: -0.010468179389135912
        total_loss: 4.045590460300446
        vf_explained_var: 0.9906083941459656
        vf_loss: 4.056211709976196
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.474193548387102
    gpu_util_percent0: 0.3183870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472416063364421
    mean_env_wait_ms: 1.2195115356319661
    mean_inference_ms: 4.34996829814985
    mean_raw_obs_processing_ms: 0.38181691519090727
  time_since_restore: 2177.4605991840363
  time_this_iter_s: 26.472506761550903
  time_total_s: 2177.4605991840363
  timers:
    learn_throughput: 8303.974
    learn_time_ms: 19483.684
    sample_throughput: 23684.893
    sample_time_ms: 6831.021
    update_time_ms: 29.664
  timestamp: 1602621645
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     82 |          2177.46 | 13266944 |  266.795 |              307.838 |              145.717 |            786.862 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3293.0890552248484
    time_step_min: 3024
  date: 2020-10-13_20-41-12
  done: false
  episode_len_mean: 786.8927331568108
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.04934606435046
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 242
  episodes_total: 16995
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.30028821031252545
        entropy_coeff: 0.0005000000000000001
        kl: 0.00516949799687912
        model: {}
        policy_loss: -0.011151680955663323
        total_loss: 5.393274426460266
        vf_explained_var: 0.9906125664710999
        vf_loss: 5.404575149218242
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.296774193548387
    gpu_util_percent0: 0.34967741935483865
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722627859061935
    mean_env_wait_ms: 1.2196176094269375
    mean_inference_ms: 4.348877119432012
    mean_raw_obs_processing_ms: 0.3817600796563337
  time_since_restore: 2204.423017978668
  time_this_iter_s: 26.962418794631958
  time_total_s: 2204.423017978668
  timers:
    learn_throughput: 8284.211
    learn_time_ms: 19530.165
    sample_throughput: 23606.263
    sample_time_ms: 6853.774
    update_time_ms: 30.859
  timestamp: 1602621672
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     83 |          2204.42 | 13428736 |  267.049 |              307.838 |              145.717 |            786.893 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3291.564107038976
    time_step_min: 3024
  date: 2020-10-13_20-41-39
  done: false
  episode_len_mean: 786.925078406319
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.26899204614386
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 17218
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.2772934337457021
        entropy_coeff: 0.0005000000000000001
        kl: 0.005035444589642187
        model: {}
        policy_loss: -0.010432839084387524
        total_loss: 4.142730514208476
        vf_explained_var: 0.9920956492424011
        vf_loss: 4.153300980726878
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35483870967742
    gpu_util_percent0: 0.33741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721068166060366
    mean_env_wait_ms: 1.2197027499121056
    mean_inference_ms: 4.347818128664357
    mean_raw_obs_processing_ms: 0.3817039338657728
  time_since_restore: 2230.879345178604
  time_this_iter_s: 26.456327199935913
  time_total_s: 2230.879345178604
  timers:
    learn_throughput: 8297.085
    learn_time_ms: 19499.861
    sample_throughput: 23577.564
    sample_time_ms: 6862.117
    update_time_ms: 31.62
  timestamp: 1602621699
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     84 |          2230.88 | 13590528 |  267.269 |              307.838 |              145.717 |            786.925 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3290.478303463378
    time_step_min: 3024
  date: 2020-10-13_20-42-06
  done: false
  episode_len_mean: 786.9530521834187
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.42807861132457
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 163
  episodes_total: 17381
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.29011810074249905
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049016874593993025
        model: {}
        policy_loss: -0.012001630871964153
        total_loss: 3.5467106898625693
        vf_explained_var: 0.9918622970581055
        vf_loss: 3.5588563283284507
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11935483870968
    gpu_util_percent0: 0.27548387096774196
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472000006821646
    mean_env_wait_ms: 1.2197642846303254
    mean_inference_ms: 4.347093261137266
    mean_raw_obs_processing_ms: 0.3816649681929667
  time_since_restore: 2257.4714159965515
  time_this_iter_s: 26.592070817947388
  time_total_s: 2257.4714159965515
  timers:
    learn_throughput: 8293.34
    learn_time_ms: 19508.667
    sample_throughput: 23621.979
    sample_time_ms: 6849.214
    update_time_ms: 32.638
  timestamp: 1602621726
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     85 |          2257.47 | 13752320 |  267.428 |              307.838 |              145.717 |            786.953 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3288.9958983707415
    time_step_min: 3024
  date: 2020-10-13_20-42-33
  done: false
  episode_len_mean: 786.9745193948356
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.64620209603714
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 201
  episodes_total: 17582
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2905880734324455
        entropy_coeff: 0.0005000000000000001
        kl: 0.005135899099210898
        model: {}
        policy_loss: -0.010168266812494645
        total_loss: 3.7239726185798645
        vf_explained_var: 0.9926049709320068
        vf_loss: 3.7342856923739114
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.163333333333338
    gpu_util_percent0: 0.342
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718756245604853
    mean_env_wait_ms: 1.2198370041343405
    mean_inference_ms: 4.346238644013822
    mean_raw_obs_processing_ms: 0.3816212578758651
  time_since_restore: 2283.8743727207184
  time_this_iter_s: 26.40295672416687
  time_total_s: 2283.8743727207184
  timers:
    learn_throughput: 8288.275
    learn_time_ms: 19520.588
    sample_throughput: 23687.195
    sample_time_ms: 6830.357
    update_time_ms: 32.843
  timestamp: 1602621753
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     86 |          2283.87 | 13914112 |  267.646 |              307.838 |              145.717 |            786.975 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3287.35663354107
    time_step_min: 3024
  date: 2020-10-13_20-43-00
  done: false
  episode_len_mean: 787.0316721789338
  episode_reward_max: 307.8383838383837
  episode_reward_mean: 267.89563837262693
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 257
  episodes_total: 17839
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2631889382998149
        entropy_coeff: 0.0005000000000000001
        kl: 0.004604808132474621
        model: {}
        policy_loss: -0.009357839425016815
        total_loss: 3.8989155888557434
        vf_explained_var: 0.9933080673217773
        vf_loss: 3.908404588699341
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.567741935483873
    gpu_util_percent0: 0.3851612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717017172430175
    mean_env_wait_ms: 1.2199143981205873
    mean_inference_ms: 4.345073453577278
    mean_raw_obs_processing_ms: 0.3815576878458921
  time_since_restore: 2310.1614413261414
  time_this_iter_s: 26.287068605422974
  time_total_s: 2310.1614413261414
  timers:
    learn_throughput: 8288.476
    learn_time_ms: 19520.114
    sample_throughput: 23654.051
    sample_time_ms: 6839.928
    update_time_ms: 34.804
  timestamp: 1602621780
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     87 |          2310.16 | 14075904 |  267.896 |              307.838 |              145.717 |            787.032 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3286.345159317133
    time_step_min: 3024
  date: 2020-10-13_20-43-27
  done: false
  episode_len_mean: 787.0565210149354
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.0581653523744
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 172
  episodes_total: 18011
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.26712100207805634
        entropy_coeff: 0.0005000000000000001
        kl: 0.004592248937115073
        model: {}
        policy_loss: -0.009840659738983959
        total_loss: 4.159258266290029
        vf_explained_var: 0.990882396697998
        vf_loss: 4.169232288996379
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.254838709677422
    gpu_util_percent0: 0.362258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716076669624642
    mean_env_wait_ms: 1.219966081175993
    mean_inference_ms: 4.344376674183132
    mean_raw_obs_processing_ms: 0.3815200865986084
  time_since_restore: 2336.5097620487213
  time_this_iter_s: 26.348320722579956
  time_total_s: 2336.5097620487213
  timers:
    learn_throughput: 8298.247
    learn_time_ms: 19497.13
    sample_throughput: 23616.111
    sample_time_ms: 6850.916
    update_time_ms: 39.644
  timestamp: 1602621807
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     88 |          2336.51 | 14237696 |  268.058 |              311.323 |              145.717 |            787.057 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3285.2285446733504
    time_step_min: 3024
  date: 2020-10-13_20-43-53
  done: false
  episode_len_mean: 787.0586844131559
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.2335965529233
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 171
  episodes_total: 18182
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.2779242917895317
        entropy_coeff: 0.0005000000000000001
        kl: 0.005256622641657789
        model: {}
        policy_loss: -0.009451207355596125
        total_loss: 3.4253863096237183
        vf_explained_var: 0.9924994111061096
        vf_loss: 3.434976279735565
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.600000000000005
    gpu_util_percent0: 0.31033333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715001561909394
    mean_env_wait_ms: 1.2200140259800833
    mean_inference_ms: 4.3436546266983305
    mean_raw_obs_processing_ms: 0.3814795108168083
  time_since_restore: 2362.9918756484985
  time_this_iter_s: 26.48211359977722
  time_total_s: 2362.9918756484985
  timers:
    learn_throughput: 8301.971
    learn_time_ms: 19488.385
    sample_throughput: 23595.796
    sample_time_ms: 6856.815
    update_time_ms: 41.803
  timestamp: 1602621833
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     89 |          2362.99 | 14399488 |  268.234 |              311.323 |              145.717 |            787.059 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3283.477835723598
    time_step_min: 3024
  date: 2020-10-13_20-44-20
  done: false
  episode_len_mean: 787.0879800390541
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.5053595183775
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 254
  episodes_total: 18436
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.26092210908730823
        entropy_coeff: 0.0005000000000000001
        kl: 0.005213543151815732
        model: {}
        policy_loss: -0.008479621440831883
        total_loss: 4.420211752255757
        vf_explained_var: 0.992413341999054
        vf_loss: 4.428821722666423
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.012903225806454
    gpu_util_percent0: 0.3632258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713487815881576
    mean_env_wait_ms: 1.220078771429444
    mean_inference_ms: 4.342630167497881
    mean_raw_obs_processing_ms: 0.3814250069186033
  time_since_restore: 2389.5023016929626
  time_this_iter_s: 26.51042604446411
  time_total_s: 2389.5023016929626
  timers:
    learn_throughput: 8303.98
    learn_time_ms: 19483.669
    sample_throughput: 23635.006
    sample_time_ms: 6845.439
    update_time_ms: 41.902
  timestamp: 1602621860
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     90 |           2389.5 | 14561280 |  268.505 |              311.323 |              145.717 |            787.088 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3282.0970237455676
    time_step_min: 3024
  date: 2020-10-13_20-44-47
  done: false
  episode_len_mean: 787.1156528269499
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.70999827694385
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 18642
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.24939925596117973
        entropy_coeff: 0.0005000000000000001
        kl: 0.004798348527401686
        model: {}
        policy_loss: -0.006871319356529663
        total_loss: 3.8024387756983438
        vf_explained_var: 0.9924138188362122
        vf_loss: 3.809434731801351
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.712903225806453
    gpu_util_percent0: 0.3190322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712371431617513
    mean_env_wait_ms: 1.2201232257406494
    mean_inference_ms: 4.34180548245658
    mean_raw_obs_processing_ms: 0.38138119093123324
  time_since_restore: 2415.8758351802826
  time_this_iter_s: 26.373533487319946
  time_total_s: 2415.8758351802826
  timers:
    learn_throughput: 8307.381
    learn_time_ms: 19475.693
    sample_throughput: 23644.524
    sample_time_ms: 6842.684
    update_time_ms: 42.674
  timestamp: 1602621887
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     91 |          2415.88 | 14723072 |   268.71 |              311.323 |              145.717 |            787.116 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3281.048516802471
    time_step_min: 3024
  date: 2020-10-13_20-45-14
  done: false
  episode_len_mean: 787.1419303376762
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 268.8674863498048
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 163
  episodes_total: 18805
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.26461029549439746
        entropy_coeff: 0.0005000000000000001
        kl: 0.004730994813144207
        model: {}
        policy_loss: -0.010463534825248644
        total_loss: 3.69461061557134
        vf_explained_var: 0.9918532371520996
        vf_loss: 3.7052063941955566
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.96451612903226
    gpu_util_percent0: 0.3070967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711390470743385
    mean_env_wait_ms: 1.220156529932269
    mean_inference_ms: 4.341153119903237
    mean_raw_obs_processing_ms: 0.3813449130520889
  time_since_restore: 2442.3626987934113
  time_this_iter_s: 26.486863613128662
  time_total_s: 2442.3626987934113
  timers:
    learn_throughput: 8301.513
    learn_time_ms: 19489.458
    sample_throughput: 23689.123
    sample_time_ms: 6829.801
    update_time_ms: 42.163
  timestamp: 1602621914
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     92 |          2442.36 | 14884864 |  268.867 |              311.323 |              145.717 |            787.142 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3279.4267452259455
    time_step_min: 3013
  date: 2020-10-13_20-45-41
  done: false
  episode_len_mean: 787.1345274990807
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.12081470268157
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 232
  episodes_total: 19037
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.26356350630521774
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048559077161674695
        model: {}
        policy_loss: -0.008704963935694346
        total_loss: 4.425322691599528
        vf_explained_var: 0.9918349385261536
        vf_loss: 4.434159358342488
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.215624999999996
    gpu_util_percent0: 0.2671875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710246553207773
    mean_env_wait_ms: 1.2202063841253714
    mean_inference_ms: 4.340334060185055
    mean_raw_obs_processing_ms: 0.3813033429487149
  time_since_restore: 2468.9860606193542
  time_this_iter_s: 26.623361825942993
  time_total_s: 2468.9860606193542
  timers:
    learn_throughput: 8312.374
    learn_time_ms: 19463.994
    sample_throughput: 23715.284
    sample_time_ms: 6822.267
    update_time_ms: 40.356
  timestamp: 1602621941
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     93 |          2468.99 | 15046656 |  269.121 |              311.323 |              145.717 |            787.135 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3277.8198451062945
    time_step_min: 3007
  date: 2020-10-13_20-46-08
  done: false
  episode_len_mean: 787.1460528364561
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.36334959078505
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 230
  episodes_total: 19267
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.24006237337986627
        entropy_coeff: 0.0005000000000000001
        kl: 0.00443189680421104
        model: {}
        policy_loss: -0.009926851313745525
        total_loss: 3.5811255971590676
        vf_explained_var: 0.9931641221046448
        vf_loss: 3.591172436873118
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.21935483870968
    gpu_util_percent0: 0.3541935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708838176998076
    mean_env_wait_ms: 1.2202361705112736
    mean_inference_ms: 4.339383214474495
    mean_raw_obs_processing_ms: 0.3812492534478031
  time_since_restore: 2495.618109703064
  time_this_iter_s: 26.632049083709717
  time_total_s: 2495.618109703064
  timers:
    learn_throughput: 8310.469
    learn_time_ms: 19468.456
    sample_throughput: 23696.517
    sample_time_ms: 6827.67
    update_time_ms: 46.282
  timestamp: 1602621968
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     94 |          2495.62 | 15208448 |  269.363 |              311.323 |              145.717 |            787.146 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3276.7372462125118
    time_step_min: 3007
  date: 2020-10-13_20-46-35
  done: false
  episode_len_mean: 787.1644540496038
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.53681302060426
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 19434
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2535593919456005
        entropy_coeff: 0.0005000000000000001
        kl: 0.004572930202508966
        model: {}
        policy_loss: -0.0108901071944274
        total_loss: 2.831205983956655
        vf_explained_var: 0.9933860301971436
        vf_loss: 2.8422229290008545
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.67666666666667
    gpu_util_percent0: 0.3046666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470802569147226
    mean_env_wait_ms: 1.2202642774696455
    mean_inference_ms: 4.338799767997016
    mean_raw_obs_processing_ms: 0.38121758621668245
  time_since_restore: 2522.1218461990356
  time_this_iter_s: 26.50373649597168
  time_total_s: 2522.1218461990356
  timers:
    learn_throughput: 8312.971
    learn_time_ms: 19462.596
    sample_throughput: 23710.637
    sample_time_ms: 6823.604
    update_time_ms: 46.525
  timestamp: 1602621995
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     95 |          2522.12 | 15370240 |  269.537 |              311.323 |              145.717 |            787.164 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3275.3305789339456
    time_step_min: 3007
  date: 2020-10-13_20-47-02
  done: false
  episode_len_mean: 787.1740437019304
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.74527684011707
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 19633
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.2574249133467674
        entropy_coeff: 0.0005000000000000001
        kl: 0.00437789751837651
        model: {}
        policy_loss: -0.008738268283195794
        total_loss: 3.3785081704457602
        vf_explained_var: 0.9932436347007751
        vf_loss: 3.3873751759529114
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.109677419354835
    gpu_util_percent0: 0.3296774193548388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707007836613004
    mean_env_wait_ms: 1.2202945172181838
    mean_inference_ms: 4.338107376528395
    mean_raw_obs_processing_ms: 0.38117957335433855
  time_since_restore: 2548.7008459568024
  time_this_iter_s: 26.578999757766724
  time_total_s: 2548.7008459568024
  timers:
    learn_throughput: 8310.271
    learn_time_ms: 19468.92
    sample_throughput: 23676.091
    sample_time_ms: 6833.561
    update_time_ms: 46.818
  timestamp: 1602622022
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     96 |           2548.7 | 15532032 |  269.745 |              311.323 |              145.717 |            787.174 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3273.708595598973
    time_step_min: 3007
  date: 2020-10-13_20-47-29
  done: false
  episode_len_mean: 787.2295972243174
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 269.99387295797
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 254
  episodes_total: 19887
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2361191933353742
        entropy_coeff: 0.0005000000000000001
        kl: 0.004461579645673434
        model: {}
        policy_loss: -0.007947136803219715
        total_loss: 3.9202739000320435
        vf_explained_var: 0.9932805895805359
        vf_loss: 3.928339143594106
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.658064516129034
    gpu_util_percent0: 0.29096774193548397
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705600981919187
    mean_env_wait_ms: 1.2203179276964684
    mean_inference_ms: 4.337140608146401
    mean_raw_obs_processing_ms: 0.3811275138447844
  time_since_restore: 2575.1039113998413
  time_this_iter_s: 26.40306544303894
  time_total_s: 2575.1039113998413
  timers:
    learn_throughput: 8303.974
    learn_time_ms: 19483.682
    sample_throughput: 23686.721
    sample_time_ms: 6830.494
    update_time_ms: 44.998
  timestamp: 1602622049
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     97 |           2575.1 | 15693824 |  269.994 |              311.323 |              145.717 |             787.23 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3272.6159604731247
    time_step_min: 3007
  date: 2020-10-13_20-47-56
  done: false
  episode_len_mean: 787.2852728631946
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.1595597137584
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 178
  episodes_total: 20065
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.23126362388332686
        entropy_coeff: 0.0005000000000000001
        kl: 0.004464010902059575
        model: {}
        policy_loss: -0.009416341490577906
        total_loss: 2.99352898200353
        vf_explained_var: 0.9935998320579529
        vf_loss: 3.0030609170595803
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.684375000000003
    gpu_util_percent0: 0.3275
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470484049002455
    mean_env_wait_ms: 1.2203376872081009
    mean_inference_ms: 4.336557578356591
    mean_raw_obs_processing_ms: 0.38109649892546055
  time_since_restore: 2602.001926422119
  time_this_iter_s: 26.898015022277832
  time_total_s: 2602.001926422119
  timers:
    learn_throughput: 8279.275
    learn_time_ms: 19541.809
    sample_throughput: 23677.213
    sample_time_ms: 6833.237
    update_time_ms: 38.951
  timestamp: 1602622076
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     98 |             2602 | 15855616 |   270.16 |              311.323 |              145.717 |            787.285 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3271.5971094832707
    time_step_min: 3007
  date: 2020-10-13_20-48-24
  done: false
  episode_len_mean: 787.2972024515619
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.32549546473024
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 20232
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.24772328014175096
        entropy_coeff: 0.0005000000000000001
        kl: 0.004681173246353865
        model: {}
        policy_loss: -0.009907095324403295
        total_loss: 3.0358158548672995
        vf_explained_var: 0.993319571018219
        vf_loss: 3.0458468397458396
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05161290322581
    gpu_util_percent0: 0.24838709677419352
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147040361986247
    mean_env_wait_ms: 1.2203517357771985
    mean_inference_ms: 4.335980943887995
    mean_raw_obs_processing_ms: 0.3810647633669926
  time_since_restore: 2629.095222711563
  time_this_iter_s: 27.09329628944397
  time_total_s: 2629.095222711563
  timers:
    learn_throughput: 8264.991
    learn_time_ms: 19575.581
    sample_throughput: 23592.167
    sample_time_ms: 6857.869
    update_time_ms: 38.75
  timestamp: 1602622104
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |     99 |           2629.1 | 16017408 |  270.325 |              311.323 |              145.717 |            787.297 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3270.012954634337
    time_step_min: 3007
  date: 2020-10-13_20-48-51
  done: false
  episode_len_mean: 787.3193223979691
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.5659376423875
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 252
  episodes_total: 20484
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.23757333805163702
        entropy_coeff: 0.0005000000000000001
        kl: 0.004610497465667625
        model: {}
        policy_loss: -0.008927385647742389
        total_loss: 4.04001663128535
        vf_explained_var: 0.99312424659729
        vf_loss: 4.049062748750051
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.877419354838707
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702761827400887
    mean_env_wait_ms: 1.2203694098319497
    mean_inference_ms: 4.335150900973375
    mean_raw_obs_processing_ms: 0.3810206986491963
  time_since_restore: 2655.5792875289917
  time_this_iter_s: 26.48406481742859
  time_total_s: 2655.5792875289917
  timers:
    learn_throughput: 8272.777
    learn_time_ms: 19557.157
    sample_throughput: 23541.161
    sample_time_ms: 6872.728
    update_time_ms: 38.786
  timestamp: 1602622131
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    100 |          2655.58 | 16179200 |  270.566 |              311.323 |              145.717 |            787.319 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3268.748113024966
    time_step_min: 3007
  date: 2020-10-13_20-49-18
  done: false
  episode_len_mean: 787.356928875145
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.7558924185808
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 212
  episodes_total: 20696
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.21814849972724915
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042378978493313
        model: {}
        policy_loss: -0.008028034179005772
        total_loss: 3.258223215738932
        vf_explained_var: 0.9937694668769836
        vf_loss: 3.266360282897949
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.803225806451614
    gpu_util_percent0: 0.34645161290322574
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701842286680541
    mean_env_wait_ms: 1.2203799230781702
    mean_inference_ms: 4.334453155351575
    mean_raw_obs_processing_ms: 0.38098250588148463
  time_since_restore: 2682.4335236549377
  time_this_iter_s: 26.854236125946045
  time_total_s: 2682.4335236549377
  timers:
    learn_throughput: 8261.274
    learn_time_ms: 19584.389
    sample_throughput: 23465.286
    sample_time_ms: 6894.951
    update_time_ms: 38.363
  timestamp: 1602622158
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    101 |          2682.43 | 16340992 |  270.756 |              311.323 |              145.717 |            787.357 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3267.7349848768545
    time_step_min: 3007
  date: 2020-10-13_20-49-45
  done: false
  episode_len_mean: 787.3735915999424
  episode_reward_max: 311.3232323232324
  episode_reward_mean: 270.90694885761286
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 161
  episodes_total: 20857
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.23141959185401598
        entropy_coeff: 0.0005000000000000001
        kl: 0.004781549524826308
        model: {}
        policy_loss: -0.008303154177156102
        total_loss: 2.709630091985067
        vf_explained_var: 0.9937422275543213
        vf_loss: 2.718048930168152
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.996875000000003
    gpu_util_percent0: 0.345625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701078015243318
    mean_env_wait_ms: 1.2203849725544642
    mean_inference_ms: 4.333931375645324
    mean_raw_obs_processing_ms: 0.38095327032448306
  time_since_restore: 2709.07426571846
  time_this_iter_s: 26.64074206352234
  time_total_s: 2709.07426571846
  timers:
    learn_throughput: 8261.897
    learn_time_ms: 19582.911
    sample_throughput: 23405.683
    sample_time_ms: 6912.509
    update_time_ms: 36.962
  timestamp: 1602622185
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    102 |          2709.07 | 16502784 |  270.907 |              311.323 |              145.717 |            787.374 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3266.3458778807317
    time_step_min: 2985
  date: 2020-10-13_20-50-12
  done: false
  episode_len_mean: 787.3928249418688
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.10886926494567
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 216
  episodes_total: 21073
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.23476263011495271
        entropy_coeff: 0.0005000000000000001
        kl: 0.004458297664920489
        model: {}
        policy_loss: -0.010898580202289546
        total_loss: 3.2126270731290183
        vf_explained_var: 0.9940955638885498
        vf_loss: 3.2236429850260415
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77741935483871
    gpu_util_percent0: 0.32516129032258073
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470013098143505
    mean_env_wait_ms: 1.2203946354893673
    mean_inference_ms: 4.3332925708691725
    mean_raw_obs_processing_ms: 0.38091928170522754
  time_since_restore: 2735.9474782943726
  time_this_iter_s: 26.873212575912476
  time_total_s: 2735.9474782943726
  timers:
    learn_throughput: 8252.951
    learn_time_ms: 19604.14
    sample_throughput: 23401.351
    sample_time_ms: 6913.789
    update_time_ms: 38.698
  timestamp: 1602622212
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    103 |          2735.95 | 16664576 |  271.109 |              313.747 |              145.717 |            787.393 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3264.953454511296
    time_step_min: 2985
  date: 2020-10-13_20-50-39
  done: false
  episode_len_mean: 787.4012852385197
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.31441247694346
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 246
  episodes_total: 21319
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.2095079111556212
        entropy_coeff: 0.0005000000000000001
        kl: 0.004064000347473969
        model: {}
        policy_loss: -0.007974418074203035
        total_loss: 3.6233737667401633
        vf_explained_var: 0.993736982345581
        vf_loss: 3.6314529180526733
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.558064516129033
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698954019828567
    mean_env_wait_ms: 1.2203907590664955
    mean_inference_ms: 4.33246348957698
    mean_raw_obs_processing_ms: 0.38087241228722724
  time_since_restore: 2762.612716436386
  time_this_iter_s: 26.66523814201355
  time_total_s: 2762.612716436386
  timers:
    learn_throughput: 8249.398
    learn_time_ms: 19612.584
    sample_throughput: 23400.999
    sample_time_ms: 6913.893
    update_time_ms: 32.259
  timestamp: 1602622239
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    104 |          2762.61 | 16826368 |  271.314 |              313.747 |              145.717 |            787.401 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3264.003261882572
    time_step_min: 2985
  date: 2020-10-13_20-51-06
  done: false
  episode_len_mean: 787.4226079672376
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.4605676083244
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 169
  episodes_total: 21488
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.21215776354074478
        entropy_coeff: 0.0005000000000000001
        kl: 0.004357238727000852
        model: {}
        policy_loss: -0.009719787897968976
        total_loss: 2.438874820868174
        vf_explained_var: 0.9945716857910156
        vf_loss: 2.4487006862958274
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.06451612903226
    gpu_util_percent0: 0.33387096774193553
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698272342714183
    mean_env_wait_ms: 1.2203931089292182
    mean_inference_ms: 4.3319734822260765
    mean_raw_obs_processing_ms: 0.38084543303654
  time_since_restore: 2789.1018702983856
  time_this_iter_s: 26.48915386199951
  time_total_s: 2789.1018702983856
  timers:
    learn_throughput: 8250.97
    learn_time_ms: 19608.846
    sample_throughput: 23396.963
    sample_time_ms: 6915.085
    update_time_ms: 31.501
  timestamp: 1602622266
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    105 |           2789.1 | 16988160 |  271.461 |              313.747 |              145.717 |            787.423 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3262.879203621582
    time_step_min: 2985
  date: 2020-10-13_20-51-33
  done: false
  episode_len_mean: 787.444916036169
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.61888818056923
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 188
  episodes_total: 21676
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.22695259129007658
        entropy_coeff: 0.0005000000000000001
        kl: 0.004886048574311038
        model: {}
        policy_loss: -0.010865791712906988
        total_loss: 2.7513832449913025
        vf_explained_var: 0.9945626854896545
        vf_loss: 2.762362480163574
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58064516129033
    gpu_util_percent0: 0.2954838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697507473744337
    mean_env_wait_ms: 1.2203911226519537
    mean_inference_ms: 4.331432025683727
    mean_raw_obs_processing_ms: 0.38081517771651663
  time_since_restore: 2815.633809328079
  time_this_iter_s: 26.531939029693604
  time_total_s: 2815.633809328079
  timers:
    learn_throughput: 8249.743
    learn_time_ms: 19611.763
    sample_throughput: 23429.381
    sample_time_ms: 6905.517
    update_time_ms: 33.098
  timestamp: 1602622293
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    106 |          2815.63 | 17149952 |  271.619 |              313.747 |              145.717 |            787.445 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3261.406272826881
    time_step_min: 2985
  date: 2020-10-13_20-52-00
  done: false
  episode_len_mean: 787.4860477840598
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 271.8472279792268
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 256
  episodes_total: 21932
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.20683047796289125
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040669710336563485
        model: {}
        policy_loss: -0.009566397726302966
        total_loss: 2.7036399642626443
        vf_explained_var: 0.9954362511634827
        vf_loss: 2.7133097449938455
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.309677419354838
    gpu_util_percent0: 0.26
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696216612004329
    mean_env_wait_ms: 1.2203834741882693
    mean_inference_ms: 4.330635395537218
    mean_raw_obs_processing_ms: 0.3807696870636331
  time_since_restore: 2842.1357333660126
  time_this_iter_s: 26.50192403793335
  time_total_s: 2842.1357333660126
  timers:
    learn_throughput: 8246.201
    learn_time_ms: 19620.186
    sample_throughput: 23430.926
    sample_time_ms: 6905.062
    update_time_ms: 34.658
  timestamp: 1602622320
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    107 |          2842.14 | 17311744 |  271.847 |              313.747 |              145.717 |            787.486 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3260.364175456068
    time_step_min: 2985
  date: 2020-10-13_20-52-28
  done: false
  episode_len_mean: 787.5276911252769
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.0044648300446
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 187
  episodes_total: 22119
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.19546745344996452
        entropy_coeff: 0.0005000000000000001
        kl: 0.00398314530805995
        model: {}
        policy_loss: -0.007873045280575752
        total_loss: 2.0725629329681396
        vf_explained_var: 0.9957442879676819
        vf_loss: 2.080533673365911
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.271875
    gpu_util_percent0: 0.3734375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469560729846152
    mean_env_wait_ms: 1.2203758821143915
    mean_inference_ms: 4.330111110952227
    mean_raw_obs_processing_ms: 0.380742136461915
  time_since_restore: 2869.015793323517
  time_this_iter_s: 26.880059957504272
  time_total_s: 2869.015793323517
  timers:
    learn_throughput: 8246.713
    learn_time_ms: 19618.968
    sample_throughput: 23463.627
    sample_time_ms: 6895.439
    update_time_ms: 41.541
  timestamp: 1602622348
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    108 |          2869.02 | 17473536 |  272.004 |              313.747 |              145.717 |            787.528 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3259.476432262413
    time_step_min: 2985
  date: 2020-10-13_20-52-55
  done: false
  episode_len_mean: 787.5573307005341
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.14390142958996
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 164
  episodes_total: 22283
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.21093723302086195
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045021023058022065
        model: {}
        policy_loss: -0.01046574228287985
        total_loss: 2.077916463216146
        vf_explained_var: 0.995455265045166
        vf_loss: 2.08848774433136
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56129032258065
    gpu_util_percent0: 0.2670967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694932516514567
    mean_env_wait_ms: 1.2203671330526975
    mean_inference_ms: 4.329642784369876
    mean_raw_obs_processing_ms: 0.38071558014185003
  time_since_restore: 2895.57298207283
  time_this_iter_s: 26.557188749313354
  time_total_s: 2895.57298207283
  timers:
    learn_throughput: 8259.012
    learn_time_ms: 19589.752
    sample_throughput: 23562.826
    sample_time_ms: 6866.409
    update_time_ms: 45.172
  timestamp: 1602622375
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    109 |          2895.57 | 17635328 |  272.144 |              313.747 |              145.717 |            787.557 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3258.254001422728
    time_step_min: 2985
  date: 2020-10-13_20-53-22
  done: false
  episode_len_mean: 787.6022202486679
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.33629142221497
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 237
  episodes_total: 22520
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.20596953481435776
        entropy_coeff: 0.0005000000000000001
        kl: 0.004496099737783273
        model: {}
        policy_loss: -0.009117586785578169
        total_loss: 2.7136199871699014
        vf_explained_var: 0.9953751564025879
        vf_loss: 2.722840507825216
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53548387096774
    gpu_util_percent0: 0.31322580645161285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469395799254594
    mean_env_wait_ms: 1.2203569675586565
    mean_inference_ms: 4.329010870984133
    mean_raw_obs_processing_ms: 0.38068166523195396
  time_since_restore: 2922.219883441925
  time_this_iter_s: 26.64690136909485
  time_total_s: 2922.219883441925
  timers:
    learn_throughput: 8249.507
    learn_time_ms: 19612.324
    sample_throughput: 23603.984
    sample_time_ms: 6854.436
    update_time_ms: 43.129
  timestamp: 1602622402
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    110 |          2922.22 | 17797120 |  272.336 |              313.747 |              145.717 |            787.602 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3257.0302443319392
    time_step_min: 2985
  date: 2020-10-13_20-53-48
  done: false
  episode_len_mean: 787.6562458778525
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.52058375604076
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 223
  episodes_total: 22743
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.18245642135540643
        entropy_coeff: 0.0005000000000000001
        kl: 0.004177523893304169
        model: {}
        policy_loss: -0.006182894188289841
        total_loss: 2.2897426883379617
        vf_explained_var: 0.9958382248878479
        vf_loss: 2.2960168520609536
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56129032258065
    gpu_util_percent0: 0.31774193548387103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693087083068845
    mean_env_wait_ms: 1.2203349705874016
    mean_inference_ms: 4.328357497963806
    mean_raw_obs_processing_ms: 0.3806433579056866
  time_since_restore: 2948.535243034363
  time_this_iter_s: 26.315359592437744
  time_total_s: 2948.535243034363
  timers:
    learn_throughput: 8263.109
    learn_time_ms: 19580.039
    sample_throughput: 23681.887
    sample_time_ms: 6831.888
    update_time_ms: 42.828
  timestamp: 1602622428
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    111 |          2948.54 | 17958912 |  272.521 |              313.747 |              145.717 |            787.656 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3256.166368046148
    time_step_min: 2985
  date: 2020-10-13_20-54-15
  done: false
  episode_len_mean: 787.6954737898826
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.6533463481216
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 168
  episodes_total: 22911
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.19364663834373155
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046233854567011195
        model: {}
        policy_loss: -0.009124417009237126
        total_loss: 2.153838813304901
        vf_explained_var: 0.995291531085968
        vf_loss: 2.1630599896113076
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.27333333333333
    gpu_util_percent0: 0.2796666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692448685277573
    mean_env_wait_ms: 1.2203207196406893
    mean_inference_ms: 4.327913735834742
    mean_raw_obs_processing_ms: 0.3806184910474049
  time_since_restore: 2974.8587725162506
  time_this_iter_s: 26.323529481887817
  time_total_s: 2974.8587725162506
  timers:
    learn_throughput: 8271.497
    learn_time_ms: 19560.184
    sample_throughput: 23728.356
    sample_time_ms: 6818.509
    update_time_ms: 43.962
  timestamp: 1602622455
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    112 |          2974.86 | 18120704 |  272.653 |              313.747 |              145.717 |            787.695 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3255.1842994541203
    time_step_min: 2985
  date: 2020-10-13_20-54-42
  done: false
  episode_len_mean: 787.7565988749459
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 272.80740551337686
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 199
  episodes_total: 23110
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.20393426095445952
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041561375837773085
        model: {}
        policy_loss: -0.007814861898320183
        total_loss: 2.656420866648356
        vf_explained_var: 0.9950793385505676
        vf_loss: 2.664337714513143
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7483870967742
    gpu_util_percent0: 0.3319354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469177346691992
    mean_env_wait_ms: 1.2203044034487283
    mean_inference_ms: 4.327432242799083
    mean_raw_obs_processing_ms: 0.38059243072686794
  time_since_restore: 3001.406304359436
  time_this_iter_s: 26.547531843185425
  time_total_s: 3001.406304359436
  timers:
    learn_throughput: 8275.865
    learn_time_ms: 19549.86
    sample_throughput: 23801.765
    sample_time_ms: 6797.479
    update_time_ms: 41.634
  timestamp: 1602622482
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    113 |          3001.41 | 18282496 |  272.807 |              313.747 |              145.717 |            787.757 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3253.889388874604
    time_step_min: 2985
  date: 2020-10-13_20-55-10
  done: false
  episode_len_mean: 787.8023285677596
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.0050492079427
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 252
  episodes_total: 23362
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.18517948811252913
        entropy_coeff: 0.0005000000000000001
        kl: 0.005169444329415758
        model: {}
        policy_loss: -0.007178351321878533
        total_loss: 2.651616891225179
        vf_explained_var: 0.99568110704422
        vf_loss: 2.6588878631591797
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.150000000000002
    gpu_util_percent0: 0.30624999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690654941695294
    mean_env_wait_ms: 1.2202741866257325
    mean_inference_ms: 4.326706050759536
    mean_raw_obs_processing_ms: 0.38055022763998714
  time_since_restore: 3028.3238644599915
  time_this_iter_s: 26.91756010055542
  time_total_s: 3028.3238644599915
  timers:
    learn_throughput: 8274.104
    learn_time_ms: 19554.02
    sample_throughput: 23725.151
    sample_time_ms: 6819.43
    update_time_ms: 41.939
  timestamp: 1602622510
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    114 |          3028.32 | 18444288 |  273.005 |              313.747 |              145.717 |            787.802 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3253.006294390337
    time_step_min: 2985
  date: 2020-10-13_20-55-37
  done: false
  episode_len_mean: 787.8456310267194
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.14702652882835
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 179
  episodes_total: 23541
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.17693759376804033
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520641989074647
        model: {}
        policy_loss: -0.008549871447030455
        total_loss: 2.3380528887112937
        vf_explained_var: 0.9951727986335754
        vf_loss: 2.346691310405731
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96129032258066
    gpu_util_percent0: 0.3167741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469014486864729
    mean_env_wait_ms: 1.220252719826418
    mean_inference_ms: 4.326269218992414
    mean_raw_obs_processing_ms: 0.380525975481179
  time_since_restore: 3054.8211698532104
  time_this_iter_s: 26.497305393218994
  time_total_s: 3054.8211698532104
  timers:
    learn_throughput: 8280.022
    learn_time_ms: 19540.044
    sample_throughput: 23675.775
    sample_time_ms: 6833.652
    update_time_ms: 41.822
  timestamp: 1602622537
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    115 |          3054.82 | 18606080 |  273.147 |              313.747 |              145.717 |            787.846 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3252.040369916811
    time_step_min: 2985
  date: 2020-10-13_20-56-04
  done: false
  episode_len_mean: 787.8978447003248
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.2932296519541
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 168
  episodes_total: 23709
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.19318794459104538
        entropy_coeff: 0.0005000000000000001
        kl: 0.005103734554722905
        model: {}
        policy_loss: -0.009572173541528173
        total_loss: 2.062616149584452
        vf_explained_var: 0.9956663250923157
        vf_loss: 2.0722849369049072
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.525806451612905
    gpu_util_percent0: 0.2570967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468953046743778
    mean_env_wait_ms: 1.220230631915759
    mean_inference_ms: 4.32584074901924
    mean_raw_obs_processing_ms: 0.3805011783436598
  time_since_restore: 3081.473395586014
  time_this_iter_s: 26.652225732803345
  time_total_s: 3081.473395586014
  timers:
    learn_throughput: 8278.744
    learn_time_ms: 19543.06
    sample_throughput: 23645.868
    sample_time_ms: 6842.295
    update_time_ms: 41.513
  timestamp: 1602622564
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    116 |          3081.47 | 18767872 |  273.293 |              313.747 |              145.717 |            787.898 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3250.701211867948
    time_step_min: 2985
  date: 2020-10-13_20-56-31
  done: false
  episode_len_mean: 787.9596794390183
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.4973843114338
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 249
  episodes_total: 23958
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.1883237063884735
        entropy_coeff: 0.0005000000000000001
        kl: 0.005285117503566046
        model: {}
        policy_loss: -0.008873280491873933
        total_loss: 2.5894548892974854
        vf_explained_var: 0.995692253112793
        vf_loss: 2.5984223087628684
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.958064516129035
    gpu_util_percent0: 0.32580645161290317
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688585214922711
    mean_env_wait_ms: 1.2201969317221688
    mean_inference_ms: 4.325219582754748
    mean_raw_obs_processing_ms: 0.38046632620056947
  time_since_restore: 3108.0213103294373
  time_this_iter_s: 26.547914743423462
  time_total_s: 3108.0213103294373
  timers:
    learn_throughput: 8280.872
    learn_time_ms: 19538.04
    sample_throughput: 23616.361
    sample_time_ms: 6850.844
    update_time_ms: 41.558
  timestamp: 1602622591
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    117 |          3108.02 | 18929664 |  273.497 |              313.747 |              145.717 |             787.96 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3249.568108376833
    time_step_min: 2985
  date: 2020-10-13_20-56-58
  done: false
  episode_len_mean: 788.0351320036415
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.6633533046261
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 208
  episodes_total: 24166
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.16562138870358467
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037574616338436804
        model: {}
        policy_loss: -0.009835591190494597
        total_loss: 2.84849351644516
        vf_explained_var: 0.9947153925895691
        vf_loss: 2.858411947886149
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.658064516129027
    gpu_util_percent0: 0.34225806451612906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687934810591208
    mean_env_wait_ms: 1.2201628684801922
    mean_inference_ms: 4.324706947517528
    mean_raw_obs_processing_ms: 0.3804364814264353
  time_since_restore: 3134.4858515262604
  time_this_iter_s: 26.46454119682312
  time_total_s: 3134.4858515262604
  timers:
    learn_throughput: 8297.539
    learn_time_ms: 19498.794
    sample_throughput: 23600.732
    sample_time_ms: 6855.381
    update_time_ms: 35.445
  timestamp: 1602622618
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    118 |          3134.49 | 19091456 |  273.663 |              313.747 |              145.717 |            788.035 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3248.696634298881
    time_step_min: 2985
  date: 2020-10-13_20-57-25
  done: false
  episode_len_mean: 788.0884432023672
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.7988661064034
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 166
  episodes_total: 24332
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.1790087049206098
        entropy_coeff: 0.0005000000000000001
        kl: 0.004484872003862013
        model: {}
        policy_loss: -0.007627869034574057
        total_loss: 1.894111563762029
        vf_explained_var: 0.9958425164222717
        vf_loss: 1.901828944683075
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.375
    gpu_util_percent0: 0.2725
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687406529150163
    mean_env_wait_ms: 1.2201369793972348
    mean_inference_ms: 4.3243264532322625
    mean_raw_obs_processing_ms: 0.3804151606575083
  time_since_restore: 3161.28804063797
  time_this_iter_s: 26.802189111709595
  time_total_s: 3161.28804063797
  timers:
    learn_throughput: 8284.814
    learn_time_ms: 19528.742
    sample_throughput: 23602.363
    sample_time_ms: 6854.907
    update_time_ms: 31.406
  timestamp: 1602622645
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    119 |          3161.29 | 19253248 |  273.799 |              313.747 |              145.717 |            788.088 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3247.578212974296
    time_step_min: 2985
  date: 2020-10-13_20-57-52
  done: false
  episode_len_mean: 788.1217295623115
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 273.9706412894121
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 206
  episodes_total: 24538
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.18523199607928595
        entropy_coeff: 0.0005000000000000001
        kl: 0.004502383215973775
        model: {}
        policy_loss: -0.010654812766006216
        total_loss: 2.2246050238609314
        vf_explained_var: 0.9959731698036194
        vf_loss: 2.235352416833242
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.554838709677423
    gpu_util_percent0: 0.34419354838709676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686750633158166
    mean_env_wait_ms: 1.2201033262164247
    mean_inference_ms: 4.323876078070124
    mean_raw_obs_processing_ms: 0.3803892701828424
  time_since_restore: 3187.7404868602753
  time_this_iter_s: 26.452446222305298
  time_total_s: 3187.7404868602753
  timers:
    learn_throughput: 8291.167
    learn_time_ms: 19513.78
    sample_throughput: 23606.108
    sample_time_ms: 6853.819
    update_time_ms: 34.484
  timestamp: 1602622672
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    120 |          3187.74 | 19415040 |  273.971 |              313.747 |              145.717 |            788.122 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3246.2695880452343
    time_step_min: 2985
  date: 2020-10-13_20-58-19
  done: false
  episode_len_mean: 788.1631837986122
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 274.17512546800907
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 250
  episodes_total: 24788
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.16276827827095985
        entropy_coeff: 0.0005000000000000001
        kl: 0.00412072289812689
        model: {}
        policy_loss: -0.009454146483525014
        total_loss: 2.279143989086151
        vf_explained_var: 0.9961078763008118
        vf_loss: 2.288679520289103
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.648387096774197
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685845293201336
    mean_env_wait_ms: 1.2200586302710332
    mean_inference_ms: 4.323246848784285
    mean_raw_obs_processing_ms: 0.38035335872854686
  time_since_restore: 3214.339683532715
  time_this_iter_s: 26.599196672439575
  time_total_s: 3214.339683532715
  timers:
    learn_throughput: 8285.167
    learn_time_ms: 19527.91
    sample_throughput: 23560.555
    sample_time_ms: 6867.071
    update_time_ms: 35.023
  timestamp: 1602622699
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    121 |          3214.34 | 19576832 |  274.175 |              313.747 |              145.717 |            788.163 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3245.258913174253
    time_step_min: 2985
  date: 2020-10-13_20-58-46
  done: false
  episode_len_mean: 788.1857549172776
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 274.3315148844531
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 175
  episodes_total: 24963
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.1596402848760287
        entropy_coeff: 0.0005000000000000001
        kl: 0.005416504534271856
        model: {}
        policy_loss: -0.00870276420027949
        total_loss: 1.4710561831792195
        vf_explained_var: 0.9967753887176514
        vf_loss: 1.479838788509369
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.47096774193549
    gpu_util_percent0: 0.3125806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685350193279675
    mean_env_wait_ms: 1.2200239312473167
    mean_inference_ms: 4.32285755648231
    mean_raw_obs_processing_ms: 0.3803311636349505
  time_since_restore: 3240.828667640686
  time_this_iter_s: 26.48898410797119
  time_total_s: 3240.828667640686
  timers:
    learn_throughput: 8280.494
    learn_time_ms: 19538.93
    sample_throughput: 23541.614
    sample_time_ms: 6872.596
    update_time_ms: 33.641
  timestamp: 1602622726
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    122 |          3240.83 | 19738624 |  274.332 |              313.747 |              145.717 |            788.186 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3244.2511844567425
    time_step_min: 2985
  date: 2020-10-13_20-59-13
  done: false
  episode_len_mean: 788.2116126466494
  episode_reward_max: 313.7474747474749
  episode_reward_mean: 274.487660458231
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 182
  episodes_total: 25145
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.180524201442798
        entropy_coeff: 0.0005000000000000001
        kl: 0.004025873844511807
        model: {}
        policy_loss: -0.008737184204316387
        total_loss: 2.357350011666616
        vf_explained_var: 0.9952574372291565
        vf_loss: 2.366177499294281
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.19677419354839
    gpu_util_percent0: 0.3274193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684767538674842
    mean_env_wait_ms: 1.219989609765543
    mean_inference_ms: 4.322458175249807
    mean_raw_obs_processing_ms: 0.38030783570687005
  time_since_restore: 3267.1854569911957
  time_this_iter_s: 26.356789350509644
  time_total_s: 3267.1854569911957
  timers:
    learn_throughput: 8289.957
    learn_time_ms: 19516.627
    sample_throughput: 23537.775
    sample_time_ms: 6873.717
    update_time_ms: 35.15
  timestamp: 1602622753
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    123 |          3267.19 | 19900416 |  274.488 |              313.747 |              145.717 |            788.212 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3242.902815013405
    time_step_min: 2983
  date: 2020-10-13_20-59-40
  done: false
  episode_len_mean: 788.2408238815375
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 274.69141915373
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 247
  episodes_total: 25392
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.16817189256350198
        entropy_coeff: 0.0005000000000000001
        kl: 0.003952971640198181
        model: {}
        policy_loss: -0.008649292712410292
        total_loss: 2.81756599744161
        vf_explained_var: 0.9952892661094666
        vf_loss: 2.8262993494669595
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4483870967742
    gpu_util_percent0: 0.33903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683878673422243
    mean_env_wait_ms: 1.2199376566885576
    mean_inference_ms: 4.321889115244964
    mean_raw_obs_processing_ms: 0.38027427332643093
  time_since_restore: 3293.516553401947
  time_this_iter_s: 26.331096410751343
  time_total_s: 3293.516553401947
  timers:
    learn_throughput: 8301.933
    learn_time_ms: 19488.473
    sample_throughput: 23650.27
    sample_time_ms: 6841.021
    update_time_ms: 34.751
  timestamp: 1602622780
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    124 |          3293.52 | 20062208 |  274.691 |              314.051 |              145.717 |            788.241 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3241.7715749941317
    time_step_min: 2983
  date: 2020-10-13_21-00-07
  done: false
  episode_len_mean: 788.261899179367
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 274.85554845050734
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 25590
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.15114022915561995
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037504432257264853
        model: {}
        policy_loss: -0.008024289476452395
        total_loss: 2.2723763585090637
        vf_explained_var: 0.9954743385314941
        vf_loss: 2.2804762721061707
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.870967741935488
    gpu_util_percent0: 0.26161290322580644
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468338380181296
    mean_env_wait_ms: 1.2198953734890194
    mean_inference_ms: 4.321465369185532
    mean_raw_obs_processing_ms: 0.3802497320179387
  time_since_restore: 3320.3232929706573
  time_this_iter_s: 26.806739568710327
  time_total_s: 3320.3232929706573
  timers:
    learn_throughput: 8283.796
    learn_time_ms: 19531.142
    sample_throughput: 23696.624
    sample_time_ms: 6827.639
    update_time_ms: 35.626
  timestamp: 1602622807
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    125 |          3320.32 | 20224000 |  274.856 |              314.051 |              145.717 |            788.262 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3240.8137510202496
    time_step_min: 2983
  date: 2020-10-13_21-00-34
  done: false
  episode_len_mean: 788.2614046666926
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.0015674860182
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 167
  episodes_total: 25757
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.16752837101618448
        entropy_coeff: 0.0005000000000000001
        kl: 0.004407510122594734
        model: {}
        policy_loss: -0.007346496883352908
        total_loss: 2.0575188398361206
        vf_explained_var: 0.9953958988189697
        vf_loss: 2.0649491449197135
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.745161290322578
    gpu_util_percent0: 0.3380645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682887527181707
    mean_env_wait_ms: 1.2198587764337527
    mean_inference_ms: 4.32111541116199
    mean_raw_obs_processing_ms: 0.38022903106971456
  time_since_restore: 3346.7443165779114
  time_this_iter_s: 26.42102360725403
  time_total_s: 3346.7443165779114
  timers:
    learn_throughput: 8292.029
    learn_time_ms: 19511.751
    sample_throughput: 23711.539
    sample_time_ms: 6823.344
    update_time_ms: 34.466
  timestamp: 1602622834
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    126 |          3346.74 | 20385792 |  275.002 |              314.051 |              145.717 |            788.261 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3239.4418461301384
    time_step_min: 2983
  date: 2020-10-13_21-01-01
  done: false
  episode_len_mean: 788.2647296517222
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.2016761807024
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 228
  episodes_total: 25985
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.176772673924764
        entropy_coeff: 0.0005000000000000001
        kl: 0.005184893767970304
        model: {}
        policy_loss: -0.006322067997340734
        total_loss: 2.315654675165812
        vf_explained_var: 0.9958818554878235
        vf_loss: 2.3220651348431907
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34375
    gpu_util_percent0: 0.2678125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682193715495045
    mean_env_wait_ms: 1.2198086609034458
    mean_inference_ms: 4.320661575771033
    mean_raw_obs_processing_ms: 0.38020397304396
  time_since_restore: 3373.6436746120453
  time_this_iter_s: 26.89935803413391
  time_total_s: 3373.6436746120453
  timers:
    learn_throughput: 8281.518
    learn_time_ms: 19536.516
    sample_throughput: 23679.449
    sample_time_ms: 6832.591
    update_time_ms: 34.638
  timestamp: 1602622861
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    127 |          3373.64 | 20547584 |  275.202 |              314.051 |              145.717 |            788.265 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3238.1139704478637
    time_step_min: 2983
  date: 2020-10-13_21-01-29
  done: false
  episode_len_mean: 788.2727411419199
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.39961921360896
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 234
  episodes_total: 26219
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.15511388828357062
        entropy_coeff: 0.0005000000000000001
        kl: 0.004329611896537244
        model: {}
        policy_loss: -0.008789342808692405
        total_loss: 2.1119957665602365
        vf_explained_var: 0.9960920214653015
        vf_loss: 2.120862672726313
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.732258064516135
    gpu_util_percent0: 0.32258064516129037
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468154800015373
    mean_env_wait_ms: 1.2197525908924527
    mean_inference_ms: 4.32014321710402
    mean_raw_obs_processing_ms: 0.3801717713348785
  time_since_restore: 3400.5280776023865
  time_this_iter_s: 26.884402990341187
  time_total_s: 3400.5280776023865
  timers:
    learn_throughput: 8269.292
    learn_time_ms: 19565.399
    sample_throughput: 23634.947
    sample_time_ms: 6845.456
    update_time_ms: 33.866
  timestamp: 1602622889
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    128 |          3400.53 | 20709376 |    275.4 |              314.051 |              145.717 |            788.273 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3237.162765109838
    time_step_min: 2983
  date: 2020-10-13_21-01-56
  done: false
  episode_len_mean: 788.2795906765208
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.54598859544836
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 166
  episodes_total: 26385
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.15931311622262
        entropy_coeff: 0.0005000000000000001
        kl: 0.004361555601159732
        model: {}
        policy_loss: -0.010182505958558371
        total_loss: 1.4885950684547424
        vf_explained_var: 0.9966419339179993
        vf_loss: 1.4988572200139363
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.206249999999997
    gpu_util_percent0: 0.2565625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468109872291486
    mean_env_wait_ms: 1.2197131850907208
    mean_inference_ms: 4.319815060686481
    mean_raw_obs_processing_ms: 0.38015271400864314
  time_since_restore: 3427.4589552879333
  time_this_iter_s: 26.930877685546875
  time_total_s: 3427.4589552879333
  timers:
    learn_throughput: 8276.667
    learn_time_ms: 19547.964
    sample_throughput: 23533.638
    sample_time_ms: 6874.925
    update_time_ms: 33.686
  timestamp: 1602622916
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    129 |          3427.46 | 20871168 |  275.546 |              314.051 |              145.717 |             788.28 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3236.0583694219545
    time_step_min: 2983
  date: 2020-10-13_21-02-23
  done: false
  episode_len_mean: 788.281570928789
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.7120305868753
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 198
  episodes_total: 26583
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17179154232144356
        entropy_coeff: 0.0005000000000000001
        kl: 0.004558554695298274
        model: {}
        policy_loss: -0.006725228300638264
        total_loss: 1.883975436290105
        vf_explained_var: 0.9963571429252625
        vf_loss: 1.8907865385214488
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.661290322580648
    gpu_util_percent0: 0.2916129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680498068225178
    mean_env_wait_ms: 1.2196661844907204
    mean_inference_ms: 4.319443507024117
    mean_raw_obs_processing_ms: 0.3801299627989566
  time_since_restore: 3453.9699413776398
  time_this_iter_s: 26.51098608970642
  time_total_s: 3453.9699413776398
  timers:
    learn_throughput: 8278.063
    learn_time_ms: 19544.669
    sample_throughput: 23509.543
    sample_time_ms: 6881.971
    update_time_ms: 32.636
  timestamp: 1602622943
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    130 |          3453.97 | 21032960 |  275.712 |              314.051 |              145.717 |            788.282 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3234.639588228712
    time_step_min: 2983
  date: 2020-10-13_21-02-51
  done: false
  episode_len_mean: 788.2970304407764
  episode_reward_max: 314.0505050505048
  episode_reward_mean: 275.9265797811942
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 256
  episodes_total: 26839
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.15825197969873747
        entropy_coeff: 0.0005000000000000001
        kl: 0.003911467889944713
        model: {}
        policy_loss: -0.008784900870523416
        total_loss: 2.381384531656901
        vf_explained_var: 0.9959767460823059
        vf_loss: 2.3902485966682434
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.625
    gpu_util_percent0: 0.3109375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679727171189283
    mean_env_wait_ms: 1.21959982074346
    mean_inference_ms: 4.318900307623364
    mean_raw_obs_processing_ms: 0.3800973042089801
  time_since_restore: 3480.684380531311
  time_this_iter_s: 26.714439153671265
  time_total_s: 3480.684380531311
  timers:
    learn_throughput: 8271.44
    learn_time_ms: 19560.317
    sample_throughput: 23557.394
    sample_time_ms: 6867.992
    update_time_ms: 32.906
  timestamp: 1602622971
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    131 |          3480.68 | 21194752 |  275.927 |              314.051 |              145.717 |            788.297 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3233.6164079001
    time_step_min: 2978
  date: 2020-10-13_21-03-18
  done: false
  episode_len_mean: 788.3138626688876
  episode_reward_max: 314.80808080808055
  episode_reward_mean: 276.0745264976247
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 176
  episodes_total: 27015
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.14602292825778326
        entropy_coeff: 0.0005000000000000001
        kl: 0.004378903075121343
        model: {}
        policy_loss: -0.008366142290469725
        total_loss: 1.7862057387828827
        vf_explained_var: 0.9961981773376465
        vf_loss: 1.7946448524792988
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.61290322580646
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467936173249132
    mean_env_wait_ms: 1.2195549508650718
    mean_inference_ms: 4.318574487052504
    mean_raw_obs_processing_ms: 0.38007841487522825
  time_since_restore: 3507.241579055786
  time_this_iter_s: 26.557198524475098
  time_total_s: 3507.241579055786
  timers:
    learn_throughput: 8271.427
    learn_time_ms: 19560.349
    sample_throughput: 23543.092
    sample_time_ms: 6872.164
    update_time_ms: 34.672
  timestamp: 1602622998
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    132 |          3507.24 | 21356544 |  276.075 |              314.808 |              145.717 |            788.314 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3232.6280739213666
    time_step_min: 2977
  date: 2020-10-13_21-03-45
  done: false
  episode_len_mean: 788.342453662842
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.21974266049716
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 177
  episodes_total: 27192
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.16214740027983984
        entropy_coeff: 0.0005000000000000001
        kl: 0.004044433900465568
        model: {}
        policy_loss: -0.007778106645370523
        total_loss: 2.16769078373909
        vf_explained_var: 0.9955258369445801
        vf_loss: 2.1755499641100564
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.312903225806455
    gpu_util_percent0: 0.347741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678887182102102
    mean_env_wait_ms: 1.2195083106261428
    mean_inference_ms: 4.318252065308748
    mean_raw_obs_processing_ms: 0.3800598825286317
  time_since_restore: 3533.6406185626984
  time_this_iter_s: 26.39903950691223
  time_total_s: 3533.6406185626984
  timers:
    learn_throughput: 8270.595
    learn_time_ms: 19562.316
    sample_throughput: 23538.753
    sample_time_ms: 6873.431
    update_time_ms: 33.959
  timestamp: 1602623025
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    133 |          3533.64 | 21518336 |   276.22 |               314.96 |              145.717 |            788.342 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3231.324647835924
    time_step_min: 2977
  date: 2020-10-13_21-04-12
  done: false
  episode_len_mean: 788.3818082391542
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.4198750170313
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 238
  episodes_total: 27430
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.1610215405623118
        entropy_coeff: 0.0005000000000000001
        kl: 0.004177411707739036
        model: {}
        policy_loss: -0.009487569427922912
        total_loss: 2.4388925631841025
        vf_explained_var: 0.9958627223968506
        vf_loss: 2.4484606782595315
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.438709677419357
    gpu_util_percent0: 0.3380645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678223125253595
    mean_env_wait_ms: 1.2194416140225532
    mean_inference_ms: 4.317793765736734
    mean_raw_obs_processing_ms: 0.38003207870758104
  time_since_restore: 3560.0709562301636
  time_this_iter_s: 26.43033766746521
  time_total_s: 3560.0709562301636
  timers:
    learn_throughput: 8267.503
    learn_time_ms: 19569.632
    sample_throughput: 23527.974
    sample_time_ms: 6876.58
    update_time_ms: 33.426
  timestamp: 1602623052
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    134 |          3560.07 | 21680128 |   276.42 |               314.96 |              145.717 |            788.382 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3230.0831794314686
    time_step_min: 2977
  date: 2020-10-13_21-04-39
  done: false
  episode_len_mean: 788.4385196975726
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.60860787449786
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 213
  episodes_total: 27643
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.1374009164671103
        entropy_coeff: 0.0005000000000000001
        kl: 0.003439911912816266
        model: {}
        policy_loss: -0.009646366641391069
        total_loss: 1.6807946264743805
        vf_explained_var: 0.9967214465141296
        vf_loss: 1.6905097166697185
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.122580645161293
    gpu_util_percent0: 0.26677419354838716
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677691148354674
    mean_env_wait_ms: 1.2193822267545447
    mean_inference_ms: 4.317387472332006
    mean_raw_obs_processing_ms: 0.38000641994309997
  time_since_restore: 3587.0321118831635
  time_this_iter_s: 26.961155652999878
  time_total_s: 3587.0321118831635
  timers:
    learn_throughput: 8265.065
    learn_time_ms: 19575.407
    sample_throughput: 23496.118
    sample_time_ms: 6885.903
    update_time_ms: 33.317
  timestamp: 1602623079
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | RUNNING  | 172.17.0.4:427 |    135 |          3587.03 | 21841920 |  276.609 |               314.96 |              145.717 |            788.439 |
+-------------------------+----------+----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_3e643_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3229.0721382289416
    time_step_min: 2977
  date: 2020-10-13_21-05-06
  done: true
  episode_len_mean: 788.4840333716916
  episode_reward_max: 314.9595959595955
  episode_reward_mean: 276.75267599760537
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 165
  episodes_total: 27808
  experiment_id: 4a31c3766adc46d09631443b6a308332
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.14746239284674326
        entropy_coeff: 0.0005000000000000001
        kl: 0.004442893531328688
        model: {}
        policy_loss: -0.008263095058888817
        total_loss: 1.513249506553014
        vf_explained_var: 0.9966320991516113
        vf_loss: 1.521586388349533
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.418750000000003
    gpu_util_percent0: 0.30093749999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677299358731621
    mean_env_wait_ms: 1.2193358241498375
    mean_inference_ms: 4.317099878818037
    mean_raw_obs_processing_ms: 0.3799894557477176
  time_since_restore: 3613.7087557315826
  time_this_iter_s: 26.67664384841919
  time_total_s: 3613.7087557315826
  timers:
    learn_throughput: 8254.89
    learn_time_ms: 19599.533
    sample_throughput: 23493.735
    sample_time_ms: 6886.602
    update_time_ms: 32.822
  timestamp: 1602623106
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: 3e643_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | TERMINATED |       |    136 |          3613.71 | 22003712 |  276.753 |               314.96 |              145.717 |            788.484 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_3e643_00000 | TERMINATED |       |    136 |          3613.71 | 22003712 |  276.753 |               314.96 |              145.717 |            788.484 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


