2020-10-13 22:06:55,917	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_68fe0_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=35442)[0m 2020-10-13 22:06:58,759	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=35416)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35416)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35451)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35451)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35401)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35401)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35353)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35353)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35413)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35413)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35454)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35454)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35395)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35395)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35421)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35421)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35439)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35439)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35430)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35430)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35426)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35426)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35408)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35408)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35432)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35432)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35448)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35448)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35390)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35390)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35429)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35429)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35433)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35433)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35444)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35444)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35428)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35428)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35417)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35417)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35441)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35441)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35397)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35397)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35331)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35331)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35378)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35378)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35388)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35388)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35435)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35435)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35407)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35407)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35394)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35394)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35332)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35332)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35338)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35338)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35351)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35351)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35337)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35337)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35345)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35345)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35320)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35320)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35340)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35340)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35396)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35396)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35406)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35406)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35330)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35330)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35324)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35324)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35356)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35356)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35393)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35393)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35424)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35424)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35349)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35349)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35319)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35319)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35321)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35321)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35402)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35402)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35409)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35409)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35379)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35379)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35415)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35415)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35326)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35326)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35376)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35376)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35328)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35328)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35325)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35325)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35335)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35335)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35398)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35398)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35336)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35336)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35333)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35333)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35334)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35334)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35385)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35385)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35318)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35318)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35440)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35440)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35404)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35404)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35350)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35350)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35414)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35414)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35347)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35347)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35377)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35377)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35389)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35389)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35322)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35322)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35383)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35383)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35446)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35446)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35399)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35399)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35352)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35352)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35381)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35381)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35437)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35437)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35342)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35342)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35323)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35323)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35450)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35450)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35373)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35373)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=35346)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35346)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3669
    time_step_mean: 3363.603448275862
    time_step_min: 3125
  date: 2020-10-13_22-07-32
  done: false
  episode_len_mean: 881.5316455696203
  episode_reward_max: 283.8080808080806
  episode_reward_mean: 244.05069684183616
  episode_reward_min: 164.4141414141411
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1715679466724396
        entropy_coeff: 0.0005000000000000001
        kl: 0.003930501930881292
        model: {}
        policy_loss: -0.00778718293683293
        total_loss: 516.1756820678711
        vf_explained_var: 0.4896630346775055
        vf_loss: 516.1832809448242
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.76666666666667
    gpu_util_percent0: 0.3533333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.515151515151515
    vram_util_percent0: 0.08750757824224535
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16976918876815186
    mean_env_wait_ms: 1.1465780541424986
    mean_inference_ms: 5.63812442531038
    mean_raw_obs_processing_ms: 0.4493811131259768
  time_since_restore: 28.207874536514282
  time_this_iter_s: 28.207874536514282
  time_total_s: 28.207874536514282
  timers:
    learn_throughput: 8414.28
    learn_time_ms: 19228.265
    sample_throughput: 18186.634
    sample_time_ms: 8896.204
    update_time_ms: 49.048
  timestamp: 1602626852
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 27.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      1 |          28.2079 | 161792 |  244.051 |              283.808 |              164.414 |            881.532 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3869
    time_step_mean: 3377.0
    time_step_min: 3125
  date: 2020-10-13_22-07-58
  done: false
  episode_len_mean: 881.126582278481
  episode_reward_max: 283.8080808080806
  episode_reward_mean: 242.05942334739814
  episode_reward_min: 164.4141414141411
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1408922374248505
        entropy_coeff: 0.0005000000000000001
        kl: 0.006987970671616495
        model: {}
        policy_loss: -0.009066562740675485
        total_loss: 136.38060887654623
        vf_explained_var: 0.7857485413551331
        vf_loss: 136.38954798380533
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.956666666666667
    gpu_util_percent0: 0.33833333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.749999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16595565289549327
    mean_env_wait_ms: 1.1425383244281968
    mean_inference_ms: 5.4564414080863965
    mean_raw_obs_processing_ms: 0.4410081718619282
  time_since_restore: 54.589292764663696
  time_this_iter_s: 26.381418228149414
  time_total_s: 54.589292764663696
  timers:
    learn_throughput: 8552.935
    learn_time_ms: 18916.547
    sample_throughput: 19489.04
    sample_time_ms: 8301.692
    update_time_ms: 35.23
  timestamp: 1602626878
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      2 |          54.5893 | 323584 |  242.059 |              283.808 |              164.414 |            881.127 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3871
    time_step_mean: 3384.8194444444443
    time_step_min: 3104
  date: 2020-10-13_22-08-25
  done: false
  episode_len_mean: 875.0569620253165
  episode_reward_max: 284.86868686868706
  episode_reward_mean: 241.46439074287184
  episode_reward_min: 164.4141414141411
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1264353493849437
        entropy_coeff: 0.0005000000000000001
        kl: 0.009802788573627671
        model: {}
        policy_loss: -0.012675396748818457
        total_loss: 60.41511758168539
        vf_explained_var: 0.8837491869926453
        vf_loss: 60.427374521891274
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.286666666666672
    gpu_util_percent0: 0.3413333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16314250558557347
    mean_env_wait_ms: 1.141521075634034
    mean_inference_ms: 5.303253556635692
    mean_raw_obs_processing_ms: 0.4339791997539912
  time_since_restore: 80.94859886169434
  time_this_iter_s: 26.35930609703064
  time_total_s: 80.94859886169434
  timers:
    learn_throughput: 8543.464
    learn_time_ms: 18937.518
    sample_throughput: 20296.816
    sample_time_ms: 7971.299
    update_time_ms: 30.629
  timestamp: 1602626905
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      3 |          80.9486 | 485376 |  241.464 |              284.869 |              164.414 |            875.057 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3387.8050847457625
    time_step_min: 3104
  date: 2020-10-13_22-08-51
  done: false
  episode_len_mean: 871.5268987341772
  episode_reward_max: 284.86868686868706
  episode_reward_mean: 241.93859480884805
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1075291832288106
        entropy_coeff: 0.0005000000000000001
        kl: 0.00853245899391671
        model: {}
        policy_loss: -0.011526559355843347
        total_loss: 44.10186163584391
        vf_explained_var: 0.9141210913658142
        vf_loss: 44.1130895614624
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.18666666666667
    gpu_util_percent0: 0.3306666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1610959184870948
    mean_env_wait_ms: 1.1414400351193834
    mean_inference_ms: 5.185402143184151
    mean_raw_obs_processing_ms: 0.42824197962480604
  time_since_restore: 106.9048445224762
  time_this_iter_s: 25.95624566078186
  time_total_s: 106.9048445224762
  timers:
    learn_throughput: 8565.184
    learn_time_ms: 18889.494
    sample_throughput: 20838.713
    sample_time_ms: 7764.011
    update_time_ms: 28.244
  timestamp: 1602626931
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      4 |          106.905 | 647168 |  241.939 |              284.869 |              160.172 |            871.527 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3381.8594164456235
    time_step_min: 3104
  date: 2020-10-13_22-09-17
  done: false
  episode_len_mean: 866.8391959798995
  episode_reward_max: 284.86868686868706
  episode_reward_mean: 242.8055301761333
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 164
  episodes_total: 796
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0599133570988972
        entropy_coeff: 0.0005000000000000001
        kl: 0.00805330699464927
        model: {}
        policy_loss: -0.009734194648141662
        total_loss: 32.486494382222496
        vf_explained_var: 0.9489476084709167
        vf_loss: 32.49595355987549
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.106666666666666
    gpu_util_percent0: 0.337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7566666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15948420664714852
    mean_env_wait_ms: 1.1425155625334713
    mean_inference_ms: 5.091876840040507
    mean_raw_obs_processing_ms: 0.4233150960295667
  time_since_restore: 133.19106793403625
  time_this_iter_s: 26.28622341156006
  time_total_s: 133.19106793403625
  timers:
    learn_throughput: 8544.922
    learn_time_ms: 18934.287
    sample_throughput: 21226.027
    sample_time_ms: 7622.34
    update_time_ms: 30.982
  timestamp: 1602626957
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      5 |          133.191 | 808960 |  242.806 |              284.869 |              160.172 |            866.839 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3367.4323308270677
    time_step_min: 3078
  date: 2020-10-13_22-09-43
  done: false
  episode_len_mean: 856.883363471971
  episode_reward_max: 288.8080808080812
  episode_reward_mean: 245.21240433265757
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 310
  episodes_total: 1106
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.071306178967158
        entropy_coeff: 0.0005000000000000001
        kl: 0.008238797464097539
        model: {}
        policy_loss: -0.011969462172904363
        total_loss: 28.96843910217285
        vf_explained_var: 0.9578704833984375
        vf_loss: 28.980120023091633
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.21
    gpu_util_percent0: 0.3309999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7566666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15741924387931205
    mean_env_wait_ms: 1.1450000561584883
    mean_inference_ms: 4.967885430958054
    mean_raw_obs_processing_ms: 0.4170096861030615
  time_since_restore: 159.13366746902466
  time_this_iter_s: 25.942599534988403
  time_total_s: 159.13366746902466
  timers:
    learn_throughput: 8550.831
    learn_time_ms: 18921.202
    sample_throughput: 21521.893
    sample_time_ms: 7517.554
    update_time_ms: 32.355
  timestamp: 1602626983
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      6 |          159.134 | 970752 |  245.212 |              288.808 |              160.172 |            856.883 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3362.1800327332244
    time_step_min: 3078
  date: 2020-10-13_22-10-09
  done: false
  episode_len_mean: 852.179588607595
  episode_reward_max: 293.65656565656565
  episode_reward_mean: 246.19576300984542
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0482013523578644
        entropy_coeff: 0.0005000000000000001
        kl: 0.007311464326145749
        model: {}
        policy_loss: -0.011051583365770057
        total_loss: 19.482070922851562
        vf_explained_var: 0.9613587260246277
        vf_loss: 19.492915948232014
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.776666666666667
    gpu_util_percent0: 0.30833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15664677374587196
    mean_env_wait_ms: 1.146156001850862
    mean_inference_ms: 4.921524509615941
    mean_raw_obs_processing_ms: 0.41452041681854546
  time_since_restore: 185.3673665523529
  time_this_iter_s: 26.233699083328247
  time_total_s: 185.3673665523529
  timers:
    learn_throughput: 8540.371
    learn_time_ms: 18944.375
    sample_throughput: 21706.646
    sample_time_ms: 7453.57
    update_time_ms: 31.95
  timestamp: 1602627009
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      7 |          185.367 | 1132544 |  246.196 |              293.657 |              160.172 |             852.18 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3353.78115942029
    time_step_min: 3064
  date: 2020-10-13_22-10-36
  done: false
  episode_len_mean: 847.381153305204
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 247.39319353876326
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0248040159543355
        entropy_coeff: 0.0005000000000000001
        kl: 0.007192811734663944
        model: {}
        policy_loss: -0.012482399101524303
        total_loss: 16.573991378148396
        vf_explained_var: 0.9641326069831848
        vf_loss: 16.586267232894897
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15
    gpu_util_percent0: 0.299
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15596246643203368
    mean_env_wait_ms: 1.147334905089992
    mean_inference_ms: 4.880459387525693
    mean_raw_obs_processing_ms: 0.41218094493991725
  time_since_restore: 211.4631495475769
  time_this_iter_s: 26.095782995224
  time_total_s: 211.4631495475769
  timers:
    learn_throughput: 8535.888
    learn_time_ms: 18954.326
    sample_throughput: 21879.232
    sample_time_ms: 7394.775
    update_time_ms: 32.866
  timestamp: 1602627036
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      8 |          211.463 | 1294336 |  247.393 |              296.687 |              160.172 |            847.381 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3344.8988764044943
    time_step_min: 3064
  date: 2020-10-13_22-11-01
  done: false
  episode_len_mean: 840.897201946472
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 248.74430435744316
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 1644
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9738969107468923
        entropy_coeff: 0.0005000000000000001
        kl: 0.006379361497238278
        model: {}
        policy_loss: -0.012259047235905504
        total_loss: 22.226102352142334
        vf_explained_var: 0.9665055871009827
        vf_loss: 22.23821036020915
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.416666666666668
    gpu_util_percent0: 0.3686666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15514653723159755
    mean_env_wait_ms: 1.1495807259016915
    mean_inference_ms: 4.831508854421997
    mean_raw_obs_processing_ms: 0.4093647657097717
  time_since_restore: 237.1747977733612
  time_this_iter_s: 25.7116482257843
  time_total_s: 237.1747977733612
  timers:
    learn_throughput: 8541.365
    learn_time_ms: 18942.171
    sample_throughput: 22083.452
    sample_time_ms: 7326.391
    update_time_ms: 32.815
  timestamp: 1602627061
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |      9 |          237.175 | 1456128 |  248.744 |              296.687 |              160.172 |            840.897 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3333.3327939590076
    time_step_min: 3036
  date: 2020-10-13_22-11-27
  done: false
  episode_len_mean: 834.3728902953586
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 250.30573136427577
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 252
  episodes_total: 1896
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.968529686331749
        entropy_coeff: 0.0005000000000000001
        kl: 0.006252618157304823
        model: {}
        policy_loss: -0.010011641182548678
        total_loss: 15.227359533309937
        vf_explained_var: 0.97138911485672
        vf_loss: 15.2372305393219
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.155172413793107
    gpu_util_percent0: 0.3410344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15439936830392179
    mean_env_wait_ms: 1.1518208111829038
    mean_inference_ms: 4.785845469140192
    mean_raw_obs_processing_ms: 0.4066985893033469
  time_since_restore: 262.90268301963806
  time_this_iter_s: 25.727885246276855
  time_total_s: 262.90268301963806
  timers:
    learn_throughput: 8545.994
    learn_time_ms: 18931.911
    sample_throughput: 22243.538
    sample_time_ms: 7273.663
    update_time_ms: 33.444
  timestamp: 1602627087
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     10 |          262.903 | 1617920 |  250.306 |              296.687 |              160.172 |            834.373 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3328.4209741550694
    time_step_min: 3036
  date: 2020-10-13_22-11-53
  done: false
  episode_len_mean: 830.8437195715677
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 251.11142092787668
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9550741414229075
        entropy_coeff: 0.0005000000000000001
        kl: 0.006006665799456338
        model: {}
        policy_loss: -0.012216550302885784
        total_loss: 14.387996912002563
        vf_explained_var: 0.9684069752693176
        vf_loss: 14.400090297063192
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.570000000000004
    gpu_util_percent0: 0.3616666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1539976857889633
    mean_env_wait_ms: 1.153174528543198
    mean_inference_ms: 4.761309749130478
    mean_raw_obs_processing_ms: 0.40524095548558
  time_since_restore: 288.8352880477905
  time_this_iter_s: 25.932605028152466
  time_total_s: 288.8352880477905
  timers:
    learn_throughput: 8552.401
    learn_time_ms: 18917.729
    sample_throughput: 22919.47
    sample_time_ms: 7059.151
    update_time_ms: 32.774
  timestamp: 1602627113
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     11 |          288.835 | 1779712 |  251.111 |              296.687 |              160.172 |            830.844 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3323.5875862068965
    time_step_min: 3036
  date: 2020-10-13_22-12-19
  done: false
  episode_len_mean: 827.5119530897609
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 251.97233498722005
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 163
  episodes_total: 2217
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9188966850439707
        entropy_coeff: 0.0005000000000000001
        kl: 0.00623135210480541
        model: {}
        policy_loss: -0.012058262596838176
        total_loss: 13.930777152379354
        vf_explained_var: 0.9721924662590027
        vf_loss: 13.942671616872152
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.820689655172416
    gpu_util_percent0: 0.3417241379310344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15361937074471768
    mean_env_wait_ms: 1.1546824050937157
    mean_inference_ms: 4.73838624981782
    mean_raw_obs_processing_ms: 0.40384850449065063
  time_since_restore: 314.6864523887634
  time_this_iter_s: 25.8511643409729
  time_total_s: 314.6864523887634
  timers:
    learn_throughput: 8541.872
    learn_time_ms: 18941.047
    sample_throughput: 23173.463
    sample_time_ms: 6981.779
    update_time_ms: 33.374
  timestamp: 1602627139
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     12 |          314.686 | 1941504 |  251.972 |              296.687 |              160.172 |            827.512 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3314.2779565567175
    time_step_min: 3036
  date: 2020-10-13_22-12-45
  done: false
  episode_len_mean: 821.9600474683544
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 253.37343370412998
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 311
  episodes_total: 2528
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8911013106505076
        entropy_coeff: 0.0005000000000000001
        kl: 0.00614805705845356
        model: {}
        policy_loss: -0.01046816335777597
        total_loss: 14.937302589416504
        vf_explained_var: 0.9774847030639648
        vf_loss: 14.947601795196533
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.593333333333337
    gpu_util_percent0: 0.3393333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15299834973426818
    mean_env_wait_ms: 1.1574570951073122
    mean_inference_ms: 4.700268070592721
    mean_raw_obs_processing_ms: 0.4015874002013379
  time_since_restore: 340.3926787376404
  time_this_iter_s: 25.706226348876953
  time_total_s: 340.3926787376404
  timers:
    learn_throughput: 8547.29
    learn_time_ms: 18929.041
    sample_throughput: 23361.668
    sample_time_ms: 6925.533
    update_time_ms: 35.187
  timestamp: 1602627165
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     13 |          340.393 | 2103296 |  253.373 |              296.687 |              160.172 |             821.96 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3309.3282904689863
    time_step_min: 3034
  date: 2020-10-13_22-13-11
  done: false
  episode_len_mean: 819.2516753536858
  episode_reward_max: 299.5656565656566
  episode_reward_mean: 254.2009709906211
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 2686
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8883989155292511
        entropy_coeff: 0.0005000000000000001
        kl: 0.005655435457204779
        model: {}
        policy_loss: -0.011879076599143445
        total_loss: 9.973519086837769
        vf_explained_var: 0.9757456183433533
        vf_loss: 9.98527709643046
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.53448275862069
    gpu_util_percent0: 0.2968965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1527258869736946
    mean_env_wait_ms: 1.1587525458885821
    mean_inference_ms: 4.683546483990477
    mean_raw_obs_processing_ms: 0.40058114005667417
  time_since_restore: 366.2178213596344
  time_this_iter_s: 25.82514262199402
  time_total_s: 366.2178213596344
  timers:
    learn_throughput: 8539.315
    learn_time_ms: 18946.718
    sample_throughput: 23477.219
    sample_time_ms: 6891.446
    update_time_ms: 37.031
  timestamp: 1602627191
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     14 |          366.218 | 2265088 |  254.201 |              299.566 |              160.172 |            819.252 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3304.058110516934
    time_step_min: 3034
  date: 2020-10-13_22-13-37
  done: false
  episode_len_mean: 816.7463997190025
  episode_reward_max: 299.5656565656566
  episode_reward_mean: 255.00758906238366
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 2847
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8732159187396368
        entropy_coeff: 0.0005000000000000001
        kl: 0.006058271353443463
        model: {}
        policy_loss: -0.012418695783708245
        total_loss: 9.868411302566528
        vf_explained_var: 0.9765015244483948
        vf_loss: 9.88066061337789
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.460000000000004
    gpu_util_percent0: 0.3353333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1524683760741996
    mean_env_wait_ms: 1.1601094006047445
    mean_inference_ms: 4.667901061572472
    mean_raw_obs_processing_ms: 0.39962012735960273
  time_since_restore: 391.9581050872803
  time_this_iter_s: 25.740283727645874
  time_total_s: 391.9581050872803
  timers:
    learn_throughput: 8549.532
    learn_time_ms: 18924.076
    sample_throughput: 23580.284
    sample_time_ms: 6861.325
    update_time_ms: 36.944
  timestamp: 1602627217
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     15 |          391.958 | 2426880 |  255.008 |              299.566 |              160.172 |            816.746 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3294.7485567671583
    time_step_min: 3016
  date: 2020-10-13_22-14-03
  done: false
  episode_len_mean: 812.3917721518987
  episode_reward_max: 299.5656565656566
  episode_reward_mean: 256.1996228103824
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 313
  episodes_total: 3160
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8344785422086716
        entropy_coeff: 0.0005000000000000001
        kl: 0.00587861891835928
        model: {}
        policy_loss: -0.010444189373326177
        total_loss: 13.899945179621378
        vf_explained_var: 0.9787079691886902
        vf_loss: 13.910218795140585
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.151724137931037
    gpu_util_percent0: 0.29103448275862065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520251449635531
    mean_env_wait_ms: 1.1626915326380205
    mean_inference_ms: 4.640495208939668
    mean_raw_obs_processing_ms: 0.397975979048053
  time_since_restore: 417.7132771015167
  time_this_iter_s: 25.75517201423645
  time_total_s: 417.7132771015167
  timers:
    learn_throughput: 8549.389
    learn_time_ms: 18924.394
    sample_throughput: 23643.258
    sample_time_ms: 6843.05
    update_time_ms: 35.749
  timestamp: 1602627243
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     16 |          417.713 | 2588672 |    256.2 |              299.566 |              160.172 |            812.392 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3290.7713675213677
    time_step_min: 3010
  date: 2020-10-13_22-14-28
  done: false
  episode_len_mean: 810.3351416515974
  episode_reward_max: 299.5656565656566
  episode_reward_mean: 256.7730377920252
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 3318
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8384240468343099
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053978989987323684
        model: {}
        policy_loss: -0.011896416496407861
        total_loss: 9.386300961176554
        vf_explained_var: 0.9776299595832825
        vf_loss: 9.398076931635538
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.99310344827586
    gpu_util_percent0: 0.3631034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15182617573105509
    mean_env_wait_ms: 1.163872000049845
    mean_inference_ms: 4.628276091108134
    mean_raw_obs_processing_ms: 0.3972292297013976
  time_since_restore: 443.1823751926422
  time_this_iter_s: 25.46909809112549
  time_total_s: 443.1823751926422
  timers:
    learn_throughput: 8567.786
    learn_time_ms: 18883.757
    sample_throughput: 23770.794
    sample_time_ms: 6806.336
    update_time_ms: 35.687
  timestamp: 1602627268
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     17 |          443.182 | 2750464 |  256.773 |              299.566 |              160.172 |            810.335 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3287.6860634274076
    time_step_min: 2984
  date: 2020-10-13_22-14-54
  done: false
  episode_len_mean: 808.4596148318483
  episode_reward_max: 303.05050505050514
  episode_reward_mean: 257.21201088203105
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 3479
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8263530830542246
        entropy_coeff: 0.0005000000000000001
        kl: 0.00601558949953566
        model: {}
        policy_loss: -0.013046822976320982
        total_loss: 10.768564303716024
        vf_explained_var: 0.9754762649536133
        vf_loss: 10.78142261505127
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.80333333333333
    gpu_util_percent0: 0.35066666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15163589112131481
    mean_env_wait_ms: 1.1650821968352814
    mean_inference_ms: 4.6166996109756
    mean_raw_obs_processing_ms: 0.3965113370340443
  time_since_restore: 468.9861180782318
  time_this_iter_s: 25.8037428855896
  time_total_s: 468.9861180782318
  timers:
    learn_throughput: 8574.562
    learn_time_ms: 18868.835
    sample_throughput: 23826.868
    sample_time_ms: 6790.318
    update_time_ms: 36.305
  timestamp: 1602627294
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     18 |          468.986 | 2912256 |  257.212 |              303.051 |              160.172 |             808.46 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3281.0016
    time_step_min: 2954
  date: 2020-10-13_22-15-20
  done: false
  episode_len_mean: 805.3974156118144
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 258.3012322593019
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 313
  episodes_total: 3792
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7947459469238917
        entropy_coeff: 0.0005000000000000001
        kl: 0.005477127657892804
        model: {}
        policy_loss: -0.012167421659493508
        total_loss: 14.687130689620972
        vf_explained_var: 0.977111279964447
        vf_loss: 14.699147860209147
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993333333333332
    gpu_util_percent0: 0.38699999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15129875464548076
    mean_env_wait_ms: 1.1673190760769674
    mean_inference_ms: 4.596019647133525
    mean_raw_obs_processing_ms: 0.39524732643594496
  time_since_restore: 494.93105602264404
  time_this_iter_s: 25.94493794441223
  time_total_s: 494.93105602264404
  timers:
    learn_throughput: 8565.475
    learn_time_ms: 18888.853
    sample_throughput: 23824.326
    sample_time_ms: 6791.042
    update_time_ms: 38.328
  timestamp: 1602627320
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     19 |          494.931 | 3074048 |  258.301 |              307.596 |              160.172 |            805.397 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3277.4357727737975
    time_step_min: 2954
  date: 2020-10-13_22-15-46
  done: false
  episode_len_mean: 804.0949367088608
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 258.89404168264934
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 3950
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8061949710051218
        entropy_coeff: 0.0005000000000000001
        kl: 0.006130845169536769
        model: {}
        policy_loss: -0.010873976105358452
        total_loss: 8.992526610692343
        vf_explained_var: 0.9773077964782715
        vf_loss: 9.003190676371256
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.17586206896552
    gpu_util_percent0: 0.34103448275862064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1511453376188894
    mean_env_wait_ms: 1.1683399801580854
    mean_inference_ms: 4.586623744839183
    mean_raw_obs_processing_ms: 0.3946672296570274
  time_since_restore: 520.6555016040802
  time_this_iter_s: 25.724445581436157
  time_total_s: 520.6555016040802
  timers:
    learn_throughput: 8566.747
    learn_time_ms: 18886.05
    sample_throughput: 23829.906
    sample_time_ms: 6789.452
    update_time_ms: 39.114
  timestamp: 1602627346
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     20 |          520.656 | 3235840 |  258.894 |              307.596 |              160.172 |            804.095 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3274.6622418879056
    time_step_min: 2954
  date: 2020-10-13_22-16-12
  done: false
  episode_len_mean: 803.0717761557178
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 259.3341812283419
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 4110
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.804583822687467
        entropy_coeff: 0.0005000000000000001
        kl: 0.006675932789221406
        model: {}
        policy_loss: -0.011738290796832493
        total_loss: 10.735313495000204
        vf_explained_var: 0.9747953414916992
        vf_loss: 10.746786197026571
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.600000000000005
    gpu_util_percent0: 0.32766666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15099934855699582
    mean_env_wait_ms: 1.1693431829082483
    mean_inference_ms: 4.5777002304534395
    mean_raw_obs_processing_ms: 0.3941069163490701
  time_since_restore: 546.4973020553589
  time_this_iter_s: 25.841800451278687
  time_total_s: 546.4973020553589
  timers:
    learn_throughput: 8569.221
    learn_time_ms: 18880.596
    sample_throughput: 23839.962
    sample_time_ms: 6786.588
    update_time_ms: 37.517
  timestamp: 1602627372
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     21 |          546.497 | 3397632 |  259.334 |              307.596 |              160.172 |            803.072 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3269.2985825331502
    time_step_min: 2954
  date: 2020-10-13_22-16-38
  done: false
  episode_len_mean: 801.3052536231884
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 260.13030440272297
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 306
  episodes_total: 4416
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7650169382492701
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053621438176681595
        model: {}
        policy_loss: -0.01004139548361612
        total_loss: 15.226029952367147
        vf_explained_var: 0.9767293930053711
        vf_loss: 15.235918045043945
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.393103448275863
    gpu_util_percent0: 0.3631034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15074127639689588
    mean_env_wait_ms: 1.171145474057675
    mean_inference_ms: 4.561777121190126
    mean_raw_obs_processing_ms: 0.3931199451450627
  time_since_restore: 572.1333301067352
  time_this_iter_s: 25.636028051376343
  time_total_s: 572.1333301067352
  timers:
    learn_throughput: 8575.231
    learn_time_ms: 18867.364
    sample_throughput: 23870.864
    sample_time_ms: 6777.802
    update_time_ms: 36.667
  timestamp: 1602627398
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     22 |          572.133 | 3559424 |   260.13 |              307.596 |              160.172 |            801.305 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3266.2654185022025
    time_step_min: 2954
  date: 2020-10-13_22-17-04
  done: false
  episode_len_mean: 800.4751200349192
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 260.6454417593659
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 166
  episodes_total: 4582
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.758562500278155
        entropy_coeff: 0.0005000000000000001
        kl: 0.005646925768814981
        model: {}
        policy_loss: -0.011008052000155052
        total_loss: 7.616831421852112
        vf_explained_var: 0.9816978573799133
        vf_loss: 7.6276540358861284
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.883333333333333
    gpu_util_percent0: 0.348
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1506142679426653
    mean_env_wait_ms: 1.1720595651266184
    mean_inference_ms: 4.553980359182816
    mean_raw_obs_processing_ms: 0.39264034780469026
  time_since_restore: 597.9176061153412
  time_this_iter_s: 25.784276008605957
  time_total_s: 597.9176061153412
  timers:
    learn_throughput: 8572.928
    learn_time_ms: 18872.433
    sample_throughput: 23862.858
    sample_time_ms: 6780.076
    update_time_ms: 36.309
  timestamp: 1602627424
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     23 |          597.918 | 3721216 |  260.645 |              307.596 |              160.172 |            800.475 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3264.125744680851
    time_step_min: 2954
  date: 2020-10-13_22-17-30
  done: false
  episode_len_mean: 799.7191058625052
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 261.01878975329004
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 4742
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7632499734560648
        entropy_coeff: 0.0005000000000000001
        kl: 0.006014090691072245
        model: {}
        policy_loss: -0.012040382775012404
        total_loss: 8.783557653427124
        vf_explained_var: 0.9784457087516785
        vf_loss: 8.79537852605184
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.220689655172414
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15049763901164226
    mean_env_wait_ms: 1.1728901491738213
    mean_inference_ms: 4.546837631934812
    mean_raw_obs_processing_ms: 0.3921922892876783
  time_since_restore: 623.6390388011932
  time_this_iter_s: 25.72143268585205
  time_total_s: 623.6390388011932
  timers:
    learn_throughput: 8580.073
    learn_time_ms: 18856.716
    sample_throughput: 23845.704
    sample_time_ms: 6784.954
    update_time_ms: 35.862
  timestamp: 1602627450
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     24 |          623.639 | 3883008 |  261.019 |              307.596 |              160.172 |            799.719 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3260.5959879638917
    time_step_min: 2954
  date: 2020-10-13_22-17-56
  done: false
  episode_len_mean: 798.5134274915456
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 261.55970687580003
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 285
  episodes_total: 5027
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7313970873753229
        entropy_coeff: 0.0005000000000000001
        kl: 0.00564456715558966
        model: {}
        policy_loss: -0.011325549858156592
        total_loss: 12.305595318476358
        vf_explained_var: 0.9806763529777527
        vf_loss: 12.31672191619873
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.350000000000005
    gpu_util_percent0: 0.26999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15030486334362336
    mean_env_wait_ms: 1.1742959892948812
    mean_inference_ms: 4.534858833723727
    mean_raw_obs_processing_ms: 0.3914471594506022
  time_since_restore: 649.5827038288116
  time_this_iter_s: 25.943665027618408
  time_total_s: 649.5827038288116
  timers:
    learn_throughput: 8578.506
    learn_time_ms: 18860.161
    sample_throughput: 23792.242
    sample_time_ms: 6800.2
    update_time_ms: 36.234
  timestamp: 1602627476
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     25 |          649.583 | 4044800 |   261.56 |              307.596 |              160.172 |            798.513 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3258.44682907966
    time_step_min: 2954
  date: 2020-10-13_22-18-22
  done: false
  episode_len_mean: 797.7278481012659
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 261.8983622182702
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 5214
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7117179979880651
        entropy_coeff: 0.0005000000000000001
        kl: 0.00563451717607677
        model: {}
        policy_loss: -0.010599843536814054
        total_loss: 8.839935143788656
        vf_explained_var: 0.9808418154716492
        vf_loss: 8.850327412287394
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.006896551724143
    gpu_util_percent0: 0.3855172413793103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1501847542202213
    mean_env_wait_ms: 1.1751613908974357
    mean_inference_ms: 4.527557750649374
    mean_raw_obs_processing_ms: 0.39100242947680314
  time_since_restore: 675.204098701477
  time_this_iter_s: 25.621394872665405
  time_total_s: 675.204098701477
  timers:
    learn_throughput: 8584.583
    learn_time_ms: 18846.809
    sample_throughput: 23802.543
    sample_time_ms: 6797.257
    update_time_ms: 37.305
  timestamp: 1602627502
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     26 |          675.204 | 4206592 |  261.898 |              307.596 |              160.172 |            797.728 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3256.272693173293
    time_step_min: 2954
  date: 2020-10-13_22-18-47
  done: false
  episode_len_mean: 797.1222553033123
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 262.24409709299925
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 5374
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7274518807729086
        entropy_coeff: 0.0005000000000000001
        kl: 0.005952913508129616
        model: {}
        policy_loss: -0.013128781732424008
        total_loss: 8.445004940032959
        vf_explained_var: 0.9791472554206848
        vf_loss: 8.457902193069458
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65
    gpu_util_percent0: 0.3266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15008815418845367
    mean_env_wait_ms: 1.1758612890385478
    mean_inference_ms: 4.521688035774238
    mean_raw_obs_processing_ms: 0.39063605680455227
  time_since_restore: 700.8945958614349
  time_this_iter_s: 25.690497159957886
  time_total_s: 700.8945958614349
  timers:
    learn_throughput: 8579.89
    learn_time_ms: 18857.118
    sample_throughput: 23770.052
    sample_time_ms: 6806.548
    update_time_ms: 39.261
  timestamp: 1602627527
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     27 |          700.895 | 4368384 |  262.244 |              307.596 |              160.172 |            797.122 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3253.5230658768623
    time_step_min: 2954
  date: 2020-10-13_22-19-13
  done: false
  episode_len_mean: 796.3996080527347
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 262.647706352677
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 239
  episodes_total: 5613
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6916379481554031
        entropy_coeff: 0.0005000000000000001
        kl: 0.006040201522409916
        model: {}
        policy_loss: -0.011756399088577988
        total_loss: 11.367794275283813
        vf_explained_var: 0.980556309223175
        vf_loss: 11.379292249679565
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.51724137931035
    gpu_util_percent0: 0.3127586206896552
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1499509952649189
    mean_env_wait_ms: 1.176874353681553
    mean_inference_ms: 4.5133030617396255
    mean_raw_obs_processing_ms: 0.3901106235462089
  time_since_restore: 726.5925605297089
  time_this_iter_s: 25.697964668273926
  time_total_s: 726.5925605297089
  timers:
    learn_throughput: 8585.35
    learn_time_ms: 18845.125
    sample_throughput: 23765.284
    sample_time_ms: 6807.913
    update_time_ms: 38.247
  timestamp: 1602627553
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     28 |          726.593 | 4530176 |  262.648 |              312.444 |              160.172 |              796.4 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3249.9229841488627
    time_step_min: 2954
  date: 2020-10-13_22-19-39
  done: false
  episode_len_mean: 795.7624016421485
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 263.1489665730172
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 233
  episodes_total: 5846
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6752575387557348
        entropy_coeff: 0.0005000000000000001
        kl: 0.00550704119571795
        model: {}
        policy_loss: -0.012052623749089738
        total_loss: 7.815642714500427
        vf_explained_var: 0.9837474822998047
        vf_loss: 7.827482144037883
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.666666666666668
    gpu_util_percent0: 0.301
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14982595838647048
    mean_env_wait_ms: 1.1777719488858687
    mean_inference_ms: 4.50563804866926
    mean_raw_obs_processing_ms: 0.3896415411541419
  time_since_restore: 752.255651473999
  time_this_iter_s: 25.66309094429016
  time_total_s: 752.255651473999
  timers:
    learn_throughput: 8598.872
    learn_time_ms: 18815.492
    sample_throughput: 23780.513
    sample_time_ms: 6803.554
    update_time_ms: 35.493
  timestamp: 1602627579
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     29 |          752.256 | 4691968 |  263.149 |              312.444 |              160.172 |            795.762 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3247.832606507883
    time_step_min: 2954
  date: 2020-10-13_22-20-05
  done: false
  episode_len_mean: 795.2989673550966
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 263.47953721088305
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 6004
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6948543091615041
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058716343094905215
        model: {}
        policy_loss: -0.00948784469316403
        total_loss: 7.720956802368164
        vf_explained_var: 0.9807594418525696
        vf_loss: 7.730204939842224
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48666666666667
    gpu_util_percent0: 0.3546666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14974569854889128
    mean_env_wait_ms: 1.1783516710580755
    mean_inference_ms: 4.5007383530224665
    mean_raw_obs_processing_ms: 0.38933627785074015
  time_since_restore: 778.3812921047211
  time_this_iter_s: 26.125640630722046
  time_total_s: 778.3812921047211
  timers:
    learn_throughput: 8589.366
    learn_time_ms: 18836.315
    sample_throughput: 23731.442
    sample_time_ms: 6817.622
    update_time_ms: 39.962
  timestamp: 1602627605
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     30 |          778.381 | 4853760 |   263.48 |              312.444 |              160.172 |            795.299 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3245.0175182481753
    time_step_min: 2954
  date: 2020-10-13_22-20-31
  done: false
  episode_len_mean: 794.780248106976
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 263.85682831212074
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 203
  episodes_total: 6207
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6673331558704376
        entropy_coeff: 0.0005000000000000001
        kl: 0.005985880736261606
        model: {}
        policy_loss: -0.009181171145883127
        total_loss: 9.948039134343466
        vf_explained_var: 0.9806312918663025
        vf_loss: 9.956955432891846
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.644827586206898
    gpu_util_percent0: 0.3544827586206896
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496470675771457
    mean_env_wait_ms: 1.179098851391625
    mean_inference_ms: 4.494746186196463
    mean_raw_obs_processing_ms: 0.3889685268627788
  time_since_restore: 804.1203317642212
  time_this_iter_s: 25.739039659500122
  time_total_s: 804.1203317642212
  timers:
    learn_throughput: 8598.562
    learn_time_ms: 18816.17
    sample_throughput: 23700.166
    sample_time_ms: 6826.619
    update_time_ms: 39.863
  timestamp: 1602627631
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     31 |           804.12 | 5015552 |  263.857 |              312.444 |              160.172 |             794.78 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3241.925097125097
    time_step_min: 2954
  date: 2020-10-13_22-20-57
  done: false
  episode_len_mean: 794.0824455766559
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 264.24290613405947
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 270
  episodes_total: 6477
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6367034415404002
        entropy_coeff: 0.0005000000000000001
        kl: 0.004866847263959547
        model: {}
        policy_loss: -0.009333334203499058
        total_loss: 9.950478156407675
        vf_explained_var: 0.9824070930480957
        vf_loss: 9.959642966588339
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.476666666666667
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14952402487722083
    mean_env_wait_ms: 1.1799802344497174
    mean_inference_ms: 4.487193447709951
    mean_raw_obs_processing_ms: 0.3884945082530839
  time_since_restore: 829.6713492870331
  time_this_iter_s: 25.55101752281189
  time_total_s: 829.6713492870331
  timers:
    learn_throughput: 8603.21
    learn_time_ms: 18806.003
    sample_throughput: 23728.257
    sample_time_ms: 6818.537
    update_time_ms: 40.317
  timestamp: 1602627657
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     32 |          829.671 | 5177344 |  264.243 |              312.444 |              160.172 |            794.082 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3240.3413709432816
    time_step_min: 2954
  date: 2020-10-13_22-21-23
  done: false
  episode_len_mean: 793.6538577456299
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 264.50458015964347
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 159
  episodes_total: 6636
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6493706206480662
        entropy_coeff: 0.0005000000000000001
        kl: 0.006435106275603175
        model: {}
        policy_loss: -0.011277024847610543
        total_loss: 7.387680570284526
        vf_explained_var: 0.9817662239074707
        vf_loss: 7.39896035194397
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.17931034482759
    gpu_util_percent0: 0.33586206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14945538375959086
    mean_env_wait_ms: 1.1804853996852707
    mean_inference_ms: 4.483018039264795
    mean_raw_obs_processing_ms: 0.38823517537863694
  time_since_restore: 855.4364101886749
  time_this_iter_s: 25.765060901641846
  time_total_s: 855.4364101886749
  timers:
    learn_throughput: 8607.292
    learn_time_ms: 18797.085
    sample_throughput: 23708.484
    sample_time_ms: 6824.224
    update_time_ms: 41.013
  timestamp: 1602627683
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     33 |          855.436 | 5339136 |  264.505 |              312.444 |              160.172 |            793.654 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3238.330138602182
    time_step_min: 2954
  date: 2020-10-13_22-21-49
  done: false
  episode_len_mean: 793.1553341148887
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 264.828099281206
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 6824
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6318176537752151
        entropy_coeff: 0.0005000000000000001
        kl: 0.006097383797168732
        model: {}
        policy_loss: -0.012870894696485871
        total_loss: 8.550239086151123
        vf_explained_var: 0.9823779463768005
        vf_loss: 8.563121239344278
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.556666666666665
    gpu_util_percent0: 0.35033333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14937975169374468
    mean_env_wait_ms: 1.1810835277346796
    mean_inference_ms: 4.47832264598641
    mean_raw_obs_processing_ms: 0.38795017323038056
  time_since_restore: 881.1282134056091
  time_this_iter_s: 25.691803216934204
  time_total_s: 881.1282134056091
  timers:
    learn_throughput: 8605.315
    learn_time_ms: 18801.405
    sample_throughput: 23733.51
    sample_time_ms: 6817.028
    update_time_ms: 40.254
  timestamp: 1602627709
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     34 |          881.128 | 5500928 |  264.828 |              312.444 |              160.172 |            793.155 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3235.360600056609
    time_step_min: 2954
  date: 2020-10-13_22-22-15
  done: false
  episode_len_mean: 792.5229319077097
  episode_reward_max: 313.3535353535354
  episode_reward_mean: 265.3049643878288
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 284
  episodes_total: 7108
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6010112563769022
        entropy_coeff: 0.0005000000000000001
        kl: 0.005382241448387504
        model: {}
        policy_loss: -0.008189600625579866
        total_loss: 10.26532506942749
        vf_explained_var: 0.9825909733772278
        vf_loss: 10.273545980453491
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.068965517241384
    gpu_util_percent0: 0.3224137931034482
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14926669244148807
    mean_env_wait_ms: 1.1819073028137008
    mean_inference_ms: 4.471436308915966
    mean_raw_obs_processing_ms: 0.38752238506173853
  time_since_restore: 906.7535715103149
  time_this_iter_s: 25.62535810470581
  time_total_s: 906.7535715103149
  timers:
    learn_throughput: 8617.22
    learn_time_ms: 18775.429
    sample_throughput: 23754.255
    sample_time_ms: 6811.074
    update_time_ms: 39.956
  timestamp: 1602627735
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     35 |          906.754 | 5662720 |  265.305 |              313.354 |              160.172 |            792.523 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3233.2241904234706
    time_step_min: 2901
  date: 2020-10-13_22-22-40
  done: false
  episode_len_mean: 792.2623830489819
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 265.6072294213462
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 7268
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6170352200667063
        entropy_coeff: 0.0005000000000000001
        kl: 0.0059466949896886945
        model: {}
        policy_loss: -0.01173960545565933
        total_loss: 6.957229097684224
        vf_explained_var: 0.9826402068138123
        vf_loss: 6.9689797560373945
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62758620689655
    gpu_util_percent0: 0.38103448275862073
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14920671684220246
    mean_env_wait_ms: 1.1823484425645518
    mean_inference_ms: 4.467803257760412
    mean_raw_obs_processing_ms: 0.387299684665395
  time_since_restore: 932.3488187789917
  time_this_iter_s: 25.595247268676758
  time_total_s: 932.3488187789917
  timers:
    learn_throughput: 8618.345
    learn_time_ms: 18772.977
    sample_throughput: 23750.264
    sample_time_ms: 6812.219
    update_time_ms: 38.576
  timestamp: 1602627760
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     36 |          932.349 | 5824512 |  265.607 |              315.626 |              160.172 |            792.262 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3231.484586262845
    time_step_min: 2901
  date: 2020-10-13_22-23-06
  done: false
  episode_len_mean: 791.9973111051357
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 265.87070761391817
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 170
  episodes_total: 7438
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6063422014315923
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055696020523707075
        model: {}
        policy_loss: -0.01051254443397435
        total_loss: 8.678027391433716
        vf_explained_var: 0.9808769226074219
        vf_loss: 8.68856438000997
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.569999999999997
    gpu_util_percent0: 0.30233333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14914672734932152
    mean_env_wait_ms: 1.1828194177597178
    mean_inference_ms: 4.4641030218807085
    mean_raw_obs_processing_ms: 0.38707678243290006
  time_since_restore: 958.1735606193542
  time_this_iter_s: 25.82474184036255
  time_total_s: 958.1735606193542
  timers:
    learn_throughput: 8619.637
    learn_time_ms: 18770.164
    sample_throughput: 23696.381
    sample_time_ms: 6827.709
    update_time_ms: 37.312
  timestamp: 1602627786
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     37 |          958.174 | 5986304 |  265.871 |              315.626 |              160.172 |            791.997 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3228.3207179087008
    time_step_min: 2901
  date: 2020-10-13_22-23-32
  done: false
  episode_len_mean: 791.5625404216789
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 266.34716979652956
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 293
  episodes_total: 7731
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5738042294979095
        entropy_coeff: 0.0005000000000000001
        kl: 0.00562783897233506
        model: {}
        policy_loss: -0.010044375330229135
        total_loss: 9.108115196228027
        vf_explained_var: 0.9847708344459534
        vf_loss: 9.118165254592896
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.817241379310342
    gpu_util_percent0: 0.31551724137931036
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14904693249560177
    mean_env_wait_ms: 1.1835600349914241
    mean_inference_ms: 4.457947492650937
    mean_raw_obs_processing_ms: 0.3866980164653683
  time_since_restore: 983.7509834766388
  time_this_iter_s: 25.577422857284546
  time_total_s: 983.7509834766388
  timers:
    learn_throughput: 8622.293
    learn_time_ms: 18764.382
    sample_throughput: 23724.611
    sample_time_ms: 6819.585
    update_time_ms: 37.952
  timestamp: 1602627812
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     38 |          983.751 | 6148096 |  266.347 |              315.626 |              160.172 |            791.563 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3226.70692288114
    time_step_min: 2901
  date: 2020-10-13_22-23-58
  done: false
  episode_len_mean: 791.2292405063291
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 266.6031517708733
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 169
  episodes_total: 7900
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5792370488246282
        entropy_coeff: 0.0005000000000000001
        kl: 0.00606264709495008
        model: {}
        policy_loss: -0.01067247850005515
        total_loss: 6.964995781580607
        vf_explained_var: 0.9832236170768738
        vf_loss: 6.975654721260071
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.517241379310345
    gpu_util_percent0: 0.3262068965517241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14899109979119424
    mean_env_wait_ms: 1.18397041527144
    mean_inference_ms: 4.454563981744343
    mean_raw_obs_processing_ms: 0.38649369453133187
  time_since_restore: 1009.1879482269287
  time_this_iter_s: 25.436964750289917
  time_total_s: 1009.1879482269287
  timers:
    learn_throughput: 8629.257
    learn_time_ms: 18749.239
    sample_throughput: 23727.982
    sample_time_ms: 6818.616
    update_time_ms: 37.801
  timestamp: 1602627838
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     39 |          1009.19 | 6309888 |  266.603 |              315.626 |              160.172 |            791.229 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3225.08203465902
    time_step_min: 2901
  date: 2020-10-13_22-24-24
  done: false
  episode_len_mean: 790.9283145231303
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 266.82990891176433
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 163
  episodes_total: 8063
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5844850142796835
        entropy_coeff: 0.0005000000000000001
        kl: 0.005840145400725305
        model: {}
        policy_loss: -0.011256719739321852
        total_loss: 7.129114309946696
        vf_explained_var: 0.9831411838531494
        vf_loss: 7.140371203422546
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92758620689655
    gpu_util_percent0: 0.31034482758620685
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14894025553221815
    mean_env_wait_ms: 1.1843635900608933
    mean_inference_ms: 4.45143905432116
    mean_raw_obs_processing_ms: 0.3863058066543537
  time_since_restore: 1034.6490454673767
  time_this_iter_s: 25.461097240447998
  time_total_s: 1034.6490454673767
  timers:
    learn_throughput: 8649.599
    learn_time_ms: 18705.145
    sample_throughput: 23778.921
    sample_time_ms: 6804.009
    update_time_ms: 30.812
  timestamp: 1602627864
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     40 |          1034.65 | 6471680 |   266.83 |              315.626 |              160.172 |            790.928 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3222.5124503789248
    time_step_min: 2901
  date: 2020-10-13_22-24-49
  done: false
  episode_len_mean: 790.3646918013166
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 267.24296828246565
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 292
  episodes_total: 8355
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.547782431046168
        entropy_coeff: 0.0005000000000000001
        kl: 0.005563245193722348
        model: {}
        policy_loss: -0.00940501414394627
        total_loss: 8.741782426834106
        vf_explained_var: 0.9853256344795227
        vf_loss: 8.751183350880941
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.59
    gpu_util_percent0: 0.3213333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14885365167200865
    mean_env_wait_ms: 1.1850191031896948
    mean_inference_ms: 4.446059523612591
    mean_raw_obs_processing_ms: 0.38597764687164876
  time_since_restore: 1060.3688325881958
  time_this_iter_s: 25.719787120819092
  time_total_s: 1060.3688325881958
  timers:
    learn_throughput: 8653.847
    learn_time_ms: 18695.963
    sample_throughput: 23758.092
    sample_time_ms: 6809.974
    update_time_ms: 30.665
  timestamp: 1602627889
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     41 |          1060.37 | 6633472 |  267.243 |              315.626 |              160.172 |            790.365 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3220.718492343934
    time_step_min: 2901
  date: 2020-10-13_22-25-15
  done: false
  episode_len_mean: 790.01031411158
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 267.49992186279104
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 177
  episodes_total: 8532
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5557845085859299
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057312653710444765
        model: {}
        policy_loss: -0.011130651265072325
        total_loss: 6.433091362317403
        vf_explained_var: 0.9845104217529297
        vf_loss: 6.444213231404622
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.70344827586207
    gpu_util_percent0: 0.33000000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14880232857770342
    mean_env_wait_ms: 1.1854080471588482
    mean_inference_ms: 4.4429318711890575
    mean_raw_obs_processing_ms: 0.38579110828573004
  time_since_restore: 1085.8495090007782
  time_this_iter_s: 25.480676412582397
  time_total_s: 1085.8495090007782
  timers:
    learn_throughput: 8654.815
    learn_time_ms: 18693.871
    sample_throughput: 23753.685
    sample_time_ms: 6811.238
    update_time_ms: 32.255
  timestamp: 1602627915
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     42 |          1085.85 | 6795264 |    267.5 |              315.626 |              160.172 |             790.01 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3218.8590084363805
    time_step_min: 2901
  date: 2020-10-13_22-25-41
  done: false
  episode_len_mean: 789.5771132834963
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 267.7375073332521
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 163
  episodes_total: 8695
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.557673399647077
        entropy_coeff: 0.0005000000000000001
        kl: 0.005991844770809014
        model: {}
        policy_loss: -0.01311191441588259
        total_loss: 7.294785380363464
        vf_explained_var: 0.9819095134735107
        vf_loss: 7.307876666386922
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.920689655172406
    gpu_util_percent0: 0.37965517241379315
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14875750419102288
    mean_env_wait_ms: 1.185765928586197
    mean_inference_ms: 4.440172201002963
    mean_raw_obs_processing_ms: 0.38562717986518724
  time_since_restore: 1111.4485335350037
  time_this_iter_s: 25.599024534225464
  time_total_s: 1111.4485335350037
  timers:
    learn_throughput: 8661.907
    learn_time_ms: 18678.565
    sample_throughput: 23760.136
    sample_time_ms: 6809.389
    update_time_ms: 31.894
  timestamp: 1602627941
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     43 |          1111.45 | 6957056 |  267.738 |              315.626 |              160.172 |            789.577 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3216.1969120608637
    time_step_min: 2901
  date: 2020-10-13_22-26-07
  done: false
  episode_len_mean: 788.793986636971
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 268.1476907156195
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 285
  episodes_total: 8980
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5189951856931051
        entropy_coeff: 0.0005000000000000001
        kl: 0.005256618373095989
        model: {}
        policy_loss: -0.009501781566844633
        total_loss: 8.741390466690063
        vf_explained_var: 0.9847350120544434
        vf_loss: 8.750888903935751
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77666666666666
    gpu_util_percent0: 0.33166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14868368411842112
    mean_env_wait_ms: 1.1863537206766153
    mean_inference_ms: 4.435516973658828
    mean_raw_obs_processing_ms: 0.38534472861491953
  time_since_restore: 1137.2697670459747
  time_this_iter_s: 25.82123351097107
  time_total_s: 1137.2697670459747
  timers:
    learn_throughput: 8662.124
    learn_time_ms: 18678.099
    sample_throughput: 23723.949
    sample_time_ms: 6819.775
    update_time_ms: 31.432
  timestamp: 1602627967
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     44 |          1137.27 | 7118848 |  268.148 |              315.626 |              160.172 |            788.794 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3214.5473580355183
    time_step_min: 2901
  date: 2020-10-13_22-26-33
  done: false
  episode_len_mean: 788.2790266259275
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 268.37831942295065
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 9164
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5165195763111115
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055745857922981186
        model: {}
        policy_loss: -0.011673807865008712
        total_loss: 6.475711345672607
        vf_explained_var: 0.9840334057807922
        vf_loss: 6.487364610036214
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.483333333333334
    gpu_util_percent0: 0.3813333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486357356037029
    mean_env_wait_ms: 1.1867325481057356
    mean_inference_ms: 4.432620934071281
    mean_raw_obs_processing_ms: 0.3851757540515304
  time_since_restore: 1163.1125314235687
  time_this_iter_s: 25.842764377593994
  time_total_s: 1163.1125314235687
  timers:
    learn_throughput: 8650.937
    learn_time_ms: 18702.251
    sample_throughput: 23740.747
    sample_time_ms: 6814.95
    update_time_ms: 32.537
  timestamp: 1602627993
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     45 |          1163.11 | 7280640 |  268.378 |              315.626 |              160.172 |            788.279 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3212.951883745963
    time_step_min: 2901
  date: 2020-10-13_22-26-59
  done: false
  episode_len_mean: 787.7806472353193
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 268.5902087744137
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 168
  episodes_total: 9332
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5223224957784017
        entropy_coeff: 0.0005000000000000001
        kl: 0.0062071040738373995
        model: {}
        policy_loss: -0.011719808033376466
        total_loss: 7.1881502866744995
        vf_explained_var: 0.982536792755127
        vf_loss: 7.199820796648662
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.80689655172414
    gpu_util_percent0: 0.3651724137931034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485966899262682
    mean_env_wait_ms: 1.1870782376368003
    mean_inference_ms: 4.4301154637177165
    mean_raw_obs_processing_ms: 0.38503029401580335
  time_since_restore: 1188.6203300952911
  time_this_iter_s: 25.507798671722412
  time_total_s: 1188.6203300952911
  timers:
    learn_throughput: 8658.845
    learn_time_ms: 18685.172
    sample_throughput: 23715.73
    sample_time_ms: 6822.139
    update_time_ms: 32.765
  timestamp: 1602628019
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     46 |          1188.62 | 7442432 |   268.59 |              315.626 |              160.172 |            787.781 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3210.696378248617
    time_step_min: 2901
  date: 2020-10-13_22-27-25
  done: false
  episode_len_mean: 786.9504312584434
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 268.94608665896214
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 291
  episodes_total: 9623
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.48436538875102997
        entropy_coeff: 0.0005000000000000001
        kl: 0.005025261780247092
        model: {}
        policy_loss: -0.009326218181134513
        total_loss: 10.158210357030233
        vf_explained_var: 0.9820401072502136
        vf_loss: 10.167527596155802
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.233333333333338
    gpu_util_percent0: 0.3933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14852788956333068
    mean_env_wait_ms: 1.1876482873051926
    mean_inference_ms: 4.425856525883062
    mean_raw_obs_processing_ms: 0.38477777863318796
  time_since_restore: 1214.6016199588776
  time_this_iter_s: 25.981289863586426
  time_total_s: 1214.6016199588776
  timers:
    learn_throughput: 8647.532
    learn_time_ms: 18709.616
    sample_throughput: 23771.11
    sample_time_ms: 6806.245
    update_time_ms: 31.459
  timestamp: 1602628045
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     47 |           1214.6 | 7604224 |  268.946 |              315.626 |              160.172 |             786.95 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3209.3080787369286
    time_step_min: 2901
  date: 2020-10-13_22-27-51
  done: false
  episode_len_mean: 786.5277664352797
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 269.15540872176234
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 9796
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4940425877769788
        entropy_coeff: 0.0005000000000000001
        kl: 0.005495002182821433
        model: {}
        policy_loss: -0.011561811668798327
        total_loss: 6.041600624720256
        vf_explained_var: 0.984626293182373
        vf_loss: 6.053134838740031
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.789655172413795
    gpu_util_percent0: 0.31034482758620696
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484881679602756
    mean_env_wait_ms: 1.1879777819359478
    mean_inference_ms: 4.42343091927126
    mean_raw_obs_processing_ms: 0.38463316141759213
  time_since_restore: 1240.303415775299
  time_this_iter_s: 25.70179581642151
  time_total_s: 1240.303415775299
  timers:
    learn_throughput: 8646.327
    learn_time_ms: 18712.223
    sample_throughput: 23740.117
    sample_time_ms: 6815.131
    update_time_ms: 31.714
  timestamp: 1602628071
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     48 |           1240.3 | 7766016 |  269.155 |              315.626 |              160.172 |            786.528 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3208.2380904421393
    time_step_min: 2901
  date: 2020-10-13_22-28-17
  done: false
  episode_len_mean: 786.0723096981245
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 269.3472048739324
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 9971
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49546317259470624
        entropy_coeff: 0.0005000000000000001
        kl: 0.005329665184641878
        model: {}
        policy_loss: -0.009244550912020108
        total_loss: 7.911429484685262
        vf_explained_var: 0.9816546440124512
        vf_loss: 7.920655131340027
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.749999999999996
    gpu_util_percent0: 0.31466666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484512934681375
    mean_env_wait_ms: 1.18831542055033
    mean_inference_ms: 4.4210929997683905
    mean_raw_obs_processing_ms: 0.3844987970061558
  time_since_restore: 1266.2443153858185
  time_this_iter_s: 25.94089961051941
  time_total_s: 1266.2443153858185
  timers:
    learn_throughput: 8630.859
    learn_time_ms: 18745.759
    sample_throughput: 23685.158
    sample_time_ms: 6830.944
    update_time_ms: 31.936
  timestamp: 1602628097
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     49 |          1266.24 | 7927808 |  269.347 |              315.626 |              160.172 |            786.072 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3205.9704472061844
    time_step_min: 2901
  date: 2020-10-13_22-28-43
  done: false
  episode_len_mean: 785.3676054965402
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 269.68567558441845
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 290
  episodes_total: 10261
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.46066005776325863
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050635455797115965
        model: {}
        policy_loss: -0.008463503210805357
        total_loss: 9.269286394119263
        vf_explained_var: 0.9833102822303772
        vf_loss: 9.277727127075195
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.16206896551724
    gpu_util_percent0: 0.35413793103448266
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14839135102590215
    mean_env_wait_ms: 1.1888476556002254
    mean_inference_ms: 4.417286303526641
    mean_raw_obs_processing_ms: 0.38427405227961375
  time_since_restore: 1291.9337096214294
  time_this_iter_s: 25.689394235610962
  time_total_s: 1291.9337096214294
  timers:
    learn_throughput: 8623.046
    learn_time_ms: 18762.743
    sample_throughput: 23673.57
    sample_time_ms: 6834.288
    update_time_ms: 33.224
  timestamp: 1602628123
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     50 |          1291.93 | 8089600 |  269.686 |              315.626 |              160.172 |            785.368 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3204.7633352590024
    time_step_min: 2901
  date: 2020-10-13_22-29-09
  done: false
  episode_len_mean: 784.9677790563867
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 269.8372301844684
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 167
  episodes_total: 10428
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4764942228794098
        entropy_coeff: 0.0005000000000000001
        kl: 0.005219100043177605
        model: {}
        policy_loss: -0.010618934946251102
        total_loss: 6.132206002871196
        vf_explained_var: 0.9842545390129089
        vf_loss: 6.142802357673645
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.59
    gpu_util_percent0: 0.3306666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14835665906678816
    mean_env_wait_ms: 1.1891473476150338
    mean_inference_ms: 4.415175386190194
    mean_raw_obs_processing_ms: 0.38414896043829877
  time_since_restore: 1317.4977974891663
  time_this_iter_s: 25.564087867736816
  time_total_s: 1317.4977974891663
  timers:
    learn_throughput: 8630.806
    learn_time_ms: 18745.873
    sample_throughput: 23681.64
    sample_time_ms: 6831.959
    update_time_ms: 36.071
  timestamp: 1602628149
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     51 |           1317.5 | 8251392 |  269.837 |              315.626 |              160.172 |            784.968 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3203.441251536062
    time_step_min: 2901
  date: 2020-10-13_22-29-35
  done: false
  episode_len_mean: 784.4849825816779
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 270.05279896222373
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 193
  episodes_total: 10621
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47358305503924686
        entropy_coeff: 0.0005000000000000001
        kl: 0.005458969506435096
        model: {}
        policy_loss: -0.010881804259649167
        total_loss: 8.382658878962198
        vf_explained_var: 0.9813825488090515
        vf_loss: 8.39350430170695
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.075862068965513
    gpu_util_percent0: 0.35793103448275865
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14832130115192207
    mean_env_wait_ms: 1.1895010858154513
    mean_inference_ms: 4.412892646625827
    mean_raw_obs_processing_ms: 0.38401832059678276
  time_since_restore: 1343.1014170646667
  time_this_iter_s: 25.60361957550049
  time_total_s: 1343.1014170646667
  timers:
    learn_throughput: 8628.533
    learn_time_ms: 18750.811
    sample_throughput: 23658.608
    sample_time_ms: 6838.61
    update_time_ms: 35.532
  timestamp: 1602628175
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     52 |           1343.1 | 8413184 |  270.053 |              315.626 |              160.172 |            784.485 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3201.2385295743507
    time_step_min: 2901
  date: 2020-10-13_22-30-00
  done: false
  episode_len_mean: 783.8873898678414
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 270.3710257864994
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 275
  episodes_total: 10896
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4343193396925926
        entropy_coeff: 0.0005000000000000001
        kl: 0.005061795896229644
        model: {}
        policy_loss: -0.009902053971018177
        total_loss: 7.9938474496205645
        vf_explained_var: 0.9849822521209717
        vf_loss: 8.003713607788086
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.444827586206895
    gpu_util_percent0: 0.33655172413793105
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482687084483149
    mean_env_wait_ms: 1.1899737604917437
    mean_inference_ms: 4.409593036953572
    mean_raw_obs_processing_ms: 0.38382667386969604
  time_since_restore: 1368.7347424030304
  time_this_iter_s: 25.633325338363647
  time_total_s: 1368.7347424030304
  timers:
    learn_throughput: 8626.816
    learn_time_ms: 18754.544
    sample_throughput: 23660.501
    sample_time_ms: 6838.063
    update_time_ms: 34.697
  timestamp: 1602628200
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     53 |          1368.73 | 8574976 |  270.371 |              315.626 |              160.172 |            783.887 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3199.685333091305
    time_step_min: 2901
  date: 2020-10-13_22-30-27
  done: false
  episode_len_mean: 783.5109403254972
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 270.5860549436499
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 164
  episodes_total: 11060
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4581292246778806
        entropy_coeff: 0.0005000000000000001
        kl: 0.005291610917386909
        model: {}
        policy_loss: -0.010529855353524908
        total_loss: 5.566484133402507
        vf_explained_var: 0.9845022559165955
        vf_loss: 5.57697860399882
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.51
    gpu_util_percent0: 0.339
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14823840125730497
    mean_env_wait_ms: 1.1902520294127201
    mean_inference_ms: 4.40772011627783
    mean_raw_obs_processing_ms: 0.383716894573061
  time_since_restore: 1394.8497684001923
  time_this_iter_s: 26.115025997161865
  time_total_s: 1394.8497684001923
  timers:
    learn_throughput: 8620.213
    learn_time_ms: 18768.911
    sample_throughput: 23603.6
    sample_time_ms: 6854.548
    update_time_ms: 34.463
  timestamp: 1602628227
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     54 |          1394.85 | 8736768 |  270.586 |              315.626 |              160.172 |            783.511 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3197.8887800534285
    time_step_min: 2901
  date: 2020-10-13_22-30-53
  done: false
  episode_len_mean: 783.0655606813343
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 270.8659017427648
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 212
  episodes_total: 11272
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4526110664010048
        entropy_coeff: 0.0005000000000000001
        kl: 0.005053112360959251
        model: {}
        policy_loss: -0.010565120882044235
        total_loss: 7.863551656405131
        vf_explained_var: 0.9828733801841736
        vf_loss: 7.874090313911438
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.513333333333335
    gpu_util_percent0: 0.35133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14820320242368407
    mean_env_wait_ms: 1.1906164861414459
    mean_inference_ms: 4.405463681882036
    mean_raw_obs_processing_ms: 0.3835884753906721
  time_since_restore: 1420.4180028438568
  time_this_iter_s: 25.56823444366455
  time_total_s: 1420.4180028438568
  timers:
    learn_throughput: 8638.111
    learn_time_ms: 18730.021
    sample_throughput: 23557.662
    sample_time_ms: 6867.914
    update_time_ms: 31.231
  timestamp: 1602628253
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     55 |          1420.42 | 8898560 |  270.866 |              319.566 |              160.172 |            783.066 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3195.6956483899044
    time_step_min: 2901
  date: 2020-10-13_22-31-18
  done: false
  episode_len_mean: 782.5561047519944
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 271.1800050452496
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 260
  episodes_total: 11532
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4222170164187749
        entropy_coeff: 0.0005000000000000001
        kl: 0.004911028741238018
        model: {}
        policy_loss: -0.01075010719553878
        total_loss: 6.202483693758647
        vf_explained_var: 0.9874430298805237
        vf_loss: 6.213199337323506
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.999999999999996
    gpu_util_percent0: 0.31448275862068964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14815746617974662
    mean_env_wait_ms: 1.1910293472636049
    mean_inference_ms: 4.402601210757482
    mean_raw_obs_processing_ms: 0.3834228575465193
  time_since_restore: 1445.8881208896637
  time_this_iter_s: 25.470118045806885
  time_total_s: 1445.8881208896637
  timers:
    learn_throughput: 8635.69
    learn_time_ms: 18735.273
    sample_throughput: 23590.218
    sample_time_ms: 6858.436
    update_time_ms: 30.609
  timestamp: 1602628278
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     56 |          1445.89 | 9060352 |   271.18 |              319.566 |              160.172 |            782.556 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3194.1242060085838
    time_step_min: 2901
  date: 2020-10-13_22-31-44
  done: false
  episode_len_mean: 782.2710400273692
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 271.4071643565314
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 11692
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4464958707491557
        entropy_coeff: 0.0005000000000000001
        kl: 0.005758141516707838
        model: {}
        policy_loss: -0.010978214733768255
        total_loss: 5.500918626785278
        vf_explained_var: 0.9844598770141602
        vf_loss: 5.511976043383281
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.8
    gpu_util_percent0: 0.3443333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14813101604007847
    mean_env_wait_ms: 1.1912812833156528
    mean_inference_ms: 4.40094962342812
    mean_raw_obs_processing_ms: 0.3833280653872694
  time_since_restore: 1471.7090277671814
  time_this_iter_s: 25.8209068775177
  time_total_s: 1471.7090277671814
  timers:
    learn_throughput: 8639.752
    learn_time_ms: 18726.464
    sample_throughput: 23589.957
    sample_time_ms: 6858.512
    update_time_ms: 30.472
  timestamp: 1602628304
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     57 |          1471.71 | 9222144 |  271.407 |              319.566 |              160.172 |            782.271 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3192.0986615034935
    time_step_min: 2901
  date: 2020-10-13_22-32-10
  done: false
  episode_len_mean: 781.8736683164165
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 271.7133689042086
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 11921
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.43150653193394345
        entropy_coeff: 0.0005000000000000001
        kl: 0.005714221857488155
        model: {}
        policy_loss: -0.01010762486839667
        total_loss: 6.938943028450012
        vf_explained_var: 0.9855101704597473
        vf_loss: 6.949123382568359
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.07241379310345
    gpu_util_percent0: 0.3296551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14809457381968
    mean_env_wait_ms: 1.1916449477475333
    mean_inference_ms: 4.398704918381481
    mean_raw_obs_processing_ms: 0.38319974509014354
  time_since_restore: 1497.2285888195038
  time_this_iter_s: 25.519561052322388
  time_total_s: 1497.2285888195038
  timers:
    learn_throughput: 8646.158
    learn_time_ms: 18712.588
    sample_throughput: 23607.241
    sample_time_ms: 6853.491
    update_time_ms: 29.581
  timestamp: 1602628330
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     58 |          1497.23 | 9383936 |  271.713 |              319.566 |              160.172 |            781.874 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3189.911153275037
    time_step_min: 2901
  date: 2020-10-13_22-32-36
  done: false
  episode_len_mean: 781.5081387701414
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 272.04074450522984
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 243
  episodes_total: 12164
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.404120405515035
        entropy_coeff: 0.0005000000000000001
        kl: 0.004897760188517471
        model: {}
        policy_loss: -0.010030022653154447
        total_loss: 6.890608310699463
        vf_explained_var: 0.9857112765312195
        vf_loss: 6.900717814763387
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.360000000000007
    gpu_util_percent0: 0.31566666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14805701518452974
    mean_env_wait_ms: 1.1919918954094022
    mean_inference_ms: 4.396275469227423
    mean_raw_obs_processing_ms: 0.3830623598061063
  time_since_restore: 1523.0995438098907
  time_this_iter_s: 25.870954990386963
  time_total_s: 1523.0995438098907
  timers:
    learn_throughput: 8645.486
    learn_time_ms: 18714.043
    sample_throughput: 23645.064
    sample_time_ms: 6842.528
    update_time_ms: 31.339
  timestamp: 1602628356
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     59 |           1523.1 | 9545728 |  272.041 |              319.566 |              160.172 |            781.508 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3188.435072864935
    time_step_min: 2901
  date: 2020-10-13_22-33-02
  done: false
  episode_len_mean: 781.2807302231238
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 272.24465343086035
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 12325
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.42455152918895084
        entropy_coeff: 0.0005000000000000001
        kl: 0.005471882022296389
        model: {}
        policy_loss: -0.012106145654494563
        total_loss: 5.095531066258748
        vf_explained_var: 0.9861391186714172
        vf_loss: 5.107781012852986
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.576666666666668
    gpu_util_percent0: 0.3433333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14803268981765383
    mean_env_wait_ms: 1.192224191850634
    mean_inference_ms: 4.394761992894576
    mean_raw_obs_processing_ms: 0.3829767577763261
  time_since_restore: 1548.882732629776
  time_this_iter_s: 25.783188819885254
  time_total_s: 1548.882732629776
  timers:
    learn_throughput: 8641.138
    learn_time_ms: 18723.46
    sample_throughput: 23645.064
    sample_time_ms: 6842.528
    update_time_ms: 30.928
  timestamp: 1602628382
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     60 |          1548.88 | 9707520 |  272.245 |              319.566 |              160.172 |            781.281 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3186.366214057508
    time_step_min: 2901
  date: 2020-10-13_22-33-28
  done: false
  episode_len_mean: 780.9582072918325
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 272.5457721619958
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 237
  episodes_total: 12562
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4055274575948715
        entropy_coeff: 0.0005000000000000001
        kl: 0.005624759243801236
        model: {}
        policy_loss: -0.0117449215540546
        total_loss: 7.4614401658376055
        vf_explained_var: 0.9850519299507141
        vf_loss: 7.473317543665568
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.389655172413796
    gpu_util_percent0: 0.3351724137931035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14799943641635635
    mean_env_wait_ms: 1.1925630283385578
    mean_inference_ms: 4.392647567865899
    mean_raw_obs_processing_ms: 0.38285818218315204
  time_since_restore: 1574.5240664482117
  time_this_iter_s: 25.64133381843567
  time_total_s: 1574.5240664482117
  timers:
    learn_throughput: 8639.081
    learn_time_ms: 18727.919
    sample_throughput: 23627.545
    sample_time_ms: 6847.601
    update_time_ms: 27.821
  timestamp: 1602628408
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     61 |          1574.52 | 9869312 |  272.546 |              319.566 |              160.172 |            780.958 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3184.6000784006274
    time_step_min: 2901
  date: 2020-10-13_22-33-55
  done: false
  episode_len_mean: 780.6356958662186
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 272.80721886363835
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 235
  episodes_total: 12797
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.381344939271609
        entropy_coeff: 0.0005000000000000001
        kl: 0.005135919394282003
        model: {}
        policy_loss: -0.008930078712486042
        total_loss: 6.595044096310933
        vf_explained_var: 0.9862293601036072
        vf_loss: 6.604100624720256
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.700000000000003
    gpu_util_percent0: 0.3203333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14796539003669723
    mean_env_wait_ms: 1.1928685849004004
    mean_inference_ms: 4.390489795937524
    mean_raw_obs_processing_ms: 0.38273642691023163
  time_since_restore: 1600.4055650234222
  time_this_iter_s: 25.88149857521057
  time_total_s: 1600.4055650234222
  timers:
    learn_throughput: 8635.09
    learn_time_ms: 18736.575
    sample_throughput: 23567.187
    sample_time_ms: 6865.138
    update_time_ms: 28.501
  timestamp: 1602628435
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     62 |          1600.41 | 10031104 |  272.807 |              319.566 |              160.172 |            780.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3183.2258614014713
    time_step_min: 2897
  date: 2020-10-13_22-34-20
  done: false
  episode_len_mean: 780.4298062823184
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 273.02102525603334
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 12957
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4042324995001157
        entropy_coeff: 0.0005000000000000001
        kl: 0.005389541115922232
        model: {}
        policy_loss: -0.010069002693247361
        total_loss: 5.4410390456517534
        vf_explained_var: 0.9849755764007568
        vf_loss: 5.451243003209432
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.548275862068966
    gpu_util_percent0: 0.36241379310344823
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14794352100673444
    mean_env_wait_ms: 1.193078377162944
    mean_inference_ms: 4.389125184046083
    mean_raw_obs_processing_ms: 0.3826596199099184
  time_since_restore: 1626.0042417049408
  time_this_iter_s: 25.598676681518555
  time_total_s: 1626.0042417049408
  timers:
    learn_throughput: 8637.491
    learn_time_ms: 18731.366
    sample_throughput: 23561.365
    sample_time_ms: 6866.835
    update_time_ms: 27.8
  timestamp: 1602628460
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     63 |             1626 | 10192896 |  273.021 |              319.566 |              160.172 |             780.43 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3181.1480777997267
    time_step_min: 2896
  date: 2020-10-13_22-34-46
  done: false
  episode_len_mean: 780.1008027870342
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 273.3253184679268
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 247
  episodes_total: 13204
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.38297328104575473
        entropy_coeff: 0.0005000000000000001
        kl: 0.005209332370820145
        model: {}
        policy_loss: -0.010129818848023811
        total_loss: 8.08438503742218
        vf_explained_var: 0.9840841889381409
        vf_loss: 8.094641208648682
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.303333333333335
    gpu_util_percent0: 0.3703333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14791190752565575
    mean_env_wait_ms: 1.1933907261956027
    mean_inference_ms: 4.38708208533616
    mean_raw_obs_processing_ms: 0.3825443160953038
  time_since_restore: 1651.5972971916199
  time_this_iter_s: 25.593055486679077
  time_total_s: 1651.5972971916199
  timers:
    learn_throughput: 8655.473
    learn_time_ms: 18692.45
    sample_throughput: 23615.414
    sample_time_ms: 6851.118
    update_time_ms: 29.911
  timestamp: 1602628486
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     64 |           1651.6 | 10354688 |  273.325 |              319.566 |              160.172 |            780.101 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3179.368464296385
    time_step_min: 2896
  date: 2020-10-13_22-35-12
  done: false
  episode_len_mean: 779.8517498138496
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 273.5901156012846
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 13430
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.36295940975348157
        entropy_coeff: 0.0005000000000000001
        kl: 0.004922469689821203
        model: {}
        policy_loss: -0.008573521762931099
        total_loss: 5.949928323427836
        vf_explained_var: 0.9866483211517334
        vf_loss: 5.958621978759766
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03103448275862
    gpu_util_percent0: 0.36379310344827587
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478815091348918
    mean_env_wait_ms: 1.193668201219636
    mean_inference_ms: 4.385206483595507
    mean_raw_obs_processing_ms: 0.38244043716285875
  time_since_restore: 1677.2865271568298
  time_this_iter_s: 25.68922996520996
  time_total_s: 1677.2865271568298
  timers:
    learn_throughput: 8641.974
    learn_time_ms: 18721.648
    sample_throughput: 23678.324
    sample_time_ms: 6832.916
    update_time_ms: 30.458
  timestamp: 1602628512
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     65 |          1677.29 | 10516480 |   273.59 |              319.566 |              160.172 |            779.852 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3178.149184321252
    time_step_min: 2884
  date: 2020-10-13_22-35-38
  done: false
  episode_len_mean: 779.7122672750019
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 273.7857811316491
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 159
  episodes_total: 13589
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.38605013489723206
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642061548617979
        model: {}
        policy_loss: -0.010416361369910495
        total_loss: 5.3233656485875445
        vf_explained_var: 0.9852609038352966
        vf_loss: 5.333939750989278
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72333333333334
    gpu_util_percent0: 0.3526666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14786147204452654
    mean_env_wait_ms: 1.1938566807442024
    mean_inference_ms: 4.383953501054907
    mean_raw_obs_processing_ms: 0.38237034596026365
  time_since_restore: 1702.7519979476929
  time_this_iter_s: 25.465470790863037
  time_total_s: 1702.7519979476929
  timers:
    learn_throughput: 8652.106
    learn_time_ms: 18699.726
    sample_throughput: 23640.215
    sample_time_ms: 6843.931
    update_time_ms: 39.86
  timestamp: 1602628538
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     66 |          1702.75 | 10678272 |  273.786 |              319.566 |              160.172 |            779.712 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3176.05530187722
    time_step_min: 2884
  date: 2020-10-13_22-36-04
  done: false
  episode_len_mean: 779.4473589132162
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 274.07706153229674
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 250
  episodes_total: 13839
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.36056224753459293
        entropy_coeff: 0.0005000000000000001
        kl: 0.005020897253416479
        model: {}
        policy_loss: -0.010234104430613419
        total_loss: 7.156545162200928
        vf_explained_var: 0.9859125018119812
        vf_loss: 7.166928052902222
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.8448275862069
    gpu_util_percent0: 0.2879310344827586
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14783148440245542
    mean_env_wait_ms: 1.1941444514439252
    mean_inference_ms: 4.382049919808845
    mean_raw_obs_processing_ms: 0.38226316029425134
  time_since_restore: 1728.5900855064392
  time_this_iter_s: 25.838087558746338
  time_total_s: 1728.5900855064392
  timers:
    learn_throughput: 8655.279
    learn_time_ms: 18692.87
    sample_throughput: 23614.532
    sample_time_ms: 6851.374
    update_time_ms: 40.171
  timestamp: 1602628564
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     67 |          1728.59 | 10840064 |  274.077 |              319.566 |              160.172 |            779.447 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3174.5090947999142
    time_step_min: 2884
  date: 2020-10-13_22-36-30
  done: false
  episode_len_mean: 779.2545338169405
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 274.31829280645155
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 14061
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.341914323469003
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052836076356470585
        model: {}
        policy_loss: -0.010361551064609861
        total_loss: 5.40642003218333
        vf_explained_var: 0.9880139827728271
        vf_loss: 5.416919390360515
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97666666666667
    gpu_util_percent0: 0.36966666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478044394751574
    mean_env_wait_ms: 1.1943860557609283
    mean_inference_ms: 4.380352829497263
    mean_raw_obs_processing_ms: 0.38216906283886504
  time_since_restore: 1754.2102887630463
  time_this_iter_s: 25.620203256607056
  time_total_s: 1754.2102887630463
  timers:
    learn_throughput: 8655.256
    learn_time_ms: 18692.919
    sample_throughput: 23577.406
    sample_time_ms: 6862.163
    update_time_ms: 39.286
  timestamp: 1602628590
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     68 |          1754.21 | 11001856 |  274.318 |              319.566 |              160.172 |            779.255 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3173.3434414668545
    time_step_min: 2884
  date: 2020-10-13_22-36-56
  done: false
  episode_len_mean: 779.1016031500492
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 274.4913904904764
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 14222
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.36352455367644626
        entropy_coeff: 0.0005000000000000001
        kl: 0.005572410494399567
        model: {}
        policy_loss: -0.011343164544086903
        total_loss: 5.250432054201762
        vf_explained_var: 0.9859452247619629
        vf_loss: 5.2619220813115435
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.255172413793108
    gpu_util_percent0: 0.30586206896551726
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14778562297392225
    mean_env_wait_ms: 1.194559333374219
    mean_inference_ms: 4.379198016665349
    mean_raw_obs_processing_ms: 0.3821048427037907
  time_since_restore: 1779.9232196807861
  time_this_iter_s: 25.712930917739868
  time_total_s: 1779.9232196807861
  timers:
    learn_throughput: 8673.941
    learn_time_ms: 18652.652
    sample_throughput: 23491.958
    sample_time_ms: 6887.123
    update_time_ms: 37.293
  timestamp: 1602628616
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     69 |          1779.92 | 11163648 |  274.491 |              319.566 |              160.172 |            779.102 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3171.347124047124
    time_step_min: 2868
  date: 2020-10-13_22-37-22
  done: false
  episode_len_mean: 778.9200525152017
  episode_reward_max: 320.6262626262624
  episode_reward_mean: 274.8061104410607
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 250
  episodes_total: 14472
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3488816171884537
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051874959996591015
        model: {}
        policy_loss: -0.0095658558808888
        total_loss: 6.619034767150879
        vf_explained_var: 0.9869111180305481
        vf_loss: 6.628742774327596
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.926666666666673
    gpu_util_percent0: 0.3223333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8033333333333337
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14775867937991422
    mean_env_wait_ms: 1.1948152383592527
    mean_inference_ms: 4.377463311721201
    mean_raw_obs_processing_ms: 0.3820051358941898
  time_since_restore: 1805.7472875118256
  time_this_iter_s: 25.82406783103943
  time_total_s: 1805.7472875118256
  timers:
    learn_throughput: 8678.433
    learn_time_ms: 18642.997
    sample_throughput: 23445.726
    sample_time_ms: 6900.703
    update_time_ms: 36.236
  timestamp: 1602628642
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     70 |          1805.75 | 11325440 |  274.806 |              320.626 |              160.172 |             778.92 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3169.531704320524
    time_step_min: 2868
  date: 2020-10-13_22-37-49
  done: false
  episode_len_mean: 778.7327298713673
  episode_reward_max: 320.6262626262624
  episode_reward_mean: 275.083167480976
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 221
  episodes_total: 14693
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.32677702854077023
        entropy_coeff: 0.0005000000000000001
        kl: 0.004798869020305574
        model: {}
        policy_loss: -0.011433972329541575
        total_loss: 5.6798553466796875
        vf_explained_var: 0.9868795275688171
        vf_loss: 5.691422780354817
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.206666666666667
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.860000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477328378921447
    mean_env_wait_ms: 1.1950312869512172
    mean_inference_ms: 4.375906243614533
    mean_raw_obs_processing_ms: 0.3819195895875354
  time_since_restore: 1831.740287065506
  time_this_iter_s: 25.99299955368042
  time_total_s: 1831.740287065506
  timers:
    learn_throughput: 8667.707
    learn_time_ms: 18666.067
    sample_throughput: 23412.086
    sample_time_ms: 6910.619
    update_time_ms: 37.61
  timestamp: 1602628669
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     71 |          1831.74 | 11487232 |  275.083 |              320.626 |              160.172 |            778.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3168.1851201728327
    time_step_min: 2868
  date: 2020-10-13_22-38-15
  done: false
  episode_len_mean: 778.602800592433
  episode_reward_max: 322.7474747474748
  episode_reward_mean: 275.28919394565014
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 14854
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3494248017668724
        entropy_coeff: 0.0005000000000000001
        kl: 0.005758242992063363
        model: {}
        policy_loss: -0.007309712387116936
        total_loss: 5.1422200202941895
        vf_explained_var: 0.9858047366142273
        vf_loss: 5.149686296780904
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.40666666666667
    gpu_util_percent0: 0.3423333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477153303402649
    mean_env_wait_ms: 1.1951858383998584
    mean_inference_ms: 4.374849438228313
    mean_raw_obs_processing_ms: 0.381860303770635
  time_since_restore: 1857.4789805412292
  time_this_iter_s: 25.738693475723267
  time_total_s: 1857.4789805412292
  timers:
    learn_throughput: 8670.897
    learn_time_ms: 18659.2
    sample_throughput: 23438.819
    sample_time_ms: 6902.737
    update_time_ms: 37.404
  timestamp: 1602628695
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     72 |          1857.48 | 11649024 |  275.289 |              322.747 |              160.172 |            778.603 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3166.175416473087
    time_step_min: 2849
  date: 2020-10-13_22-38-41
  done: false
  episode_len_mean: 778.3403269574426
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 275.5920265598603
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 255
  episodes_total: 15109
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3287614757815997
        entropy_coeff: 0.0005000000000000001
        kl: 0.005143803466732304
        model: {}
        policy_loss: -0.00944469571307612
        total_loss: 7.402278820673625
        vf_explained_var: 0.9853112101554871
        vf_loss: 7.411871870358785
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000002
    gpu_util_percent0: 0.339
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8666666666666676
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476904027219774
    mean_env_wait_ms: 1.1954180753469696
    mean_inference_ms: 4.3732428413342745
    mean_raw_obs_processing_ms: 0.38176705977706615
  time_since_restore: 1883.5611305236816
  time_this_iter_s: 26.082149982452393
  time_total_s: 1883.5611305236816
  timers:
    learn_throughput: 8661.894
    learn_time_ms: 18678.595
    sample_throughput: 23346.49
    sample_time_ms: 6930.035
    update_time_ms: 38.11
  timestamp: 1602628721
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     73 |          1883.56 | 11810816 |  275.592 |              323.505 |              160.172 |             778.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3164.4845569951576
    time_step_min: 2849
  date: 2020-10-13_22-39-07
  done: false
  episode_len_mean: 778.1622291829809
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 275.854682296734
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 215
  episodes_total: 15324
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3131432856122653
        entropy_coeff: 0.0005000000000000001
        kl: 0.004759159792835514
        model: {}
        policy_loss: -0.008013654531775197
        total_loss: 4.949917833010356
        vf_explained_var: 0.9882403016090393
        vf_loss: 4.9580732981363935
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.776666666666667
    gpu_util_percent0: 0.38099999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14766636882195724
    mean_env_wait_ms: 1.1956052643388182
    mean_inference_ms: 4.371826321861683
    mean_raw_obs_processing_ms: 0.3816891785554466
  time_since_restore: 1909.4318265914917
  time_this_iter_s: 25.87069606781006
  time_total_s: 1909.4318265914917
  timers:
    learn_throughput: 8661.176
    learn_time_ms: 18680.142
    sample_throughput: 23255.426
    sample_time_ms: 6957.172
    update_time_ms: 36.321
  timestamp: 1602628747
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     74 |          1909.43 | 11972608 |  275.855 |              323.505 |              160.172 |            778.162 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3163.1884789644014
    time_step_min: 2849
  date: 2020-10-13_22-39-34
  done: false
  episode_len_mean: 778.0116834495224
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 276.05051874281156
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 168
  episodes_total: 15492
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.33238303661346436
        entropy_coeff: 0.0005000000000000001
        kl: 0.005614192263844113
        model: {}
        policy_loss: -0.00885884469607845
        total_loss: 5.7727369864781695
        vf_explained_var: 0.9844608902931213
        vf_loss: 5.7817533413569135
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.860000000000003
    gpu_util_percent0: 0.33066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14764886897589466
    mean_env_wait_ms: 1.1957491071841513
    mean_inference_ms: 4.370799906988837
    mean_raw_obs_processing_ms: 0.3816319976395584
  time_since_restore: 1935.540241241455
  time_this_iter_s: 26.10841464996338
  time_total_s: 1935.540241241455
  timers:
    learn_throughput: 8668.945
    learn_time_ms: 18663.401
    sample_throughput: 23068.064
    sample_time_ms: 7013.679
    update_time_ms: 37.522
  timestamp: 1602628774
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     75 |          1935.54 | 12134400 |  276.051 |              323.505 |              160.172 |            778.012 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3161.177204711875
    time_step_min: 2849
  date: 2020-10-13_22-40-00
  done: false
  episode_len_mean: 777.753857877691
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 276.3437730322851
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 255
  episodes_total: 15747
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3125377769271533
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045004094718024135
        model: {}
        policy_loss: -0.012710183019711016
        total_loss: 7.016487161318461
        vf_explained_var: 0.9863986372947693
        vf_loss: 7.029346426328023
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.70666666666667
    gpu_util_percent0: 0.27666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8666666666666676
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762582248086256
    mean_env_wait_ms: 1.1959630946791249
    mean_inference_ms: 4.369314384998959
    mean_raw_obs_processing_ms: 0.38154608520511274
  time_since_restore: 1961.3920600414276
  time_this_iter_s: 25.851818799972534
  time_total_s: 1961.3920600414276
  timers:
    learn_throughput: 8646.145
    learn_time_ms: 18712.618
    sample_throughput: 23082.438
    sample_time_ms: 7009.312
    update_time_ms: 30.195
  timestamp: 1602628800
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     76 |          1961.39 | 12296192 |  276.344 |              323.505 |              160.172 |            777.754 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3159.457458841272
    time_step_min: 2849
  date: 2020-10-13_22-40-26
  done: false
  episode_len_mean: 777.5397342692404
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 276.59342991205614
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 209
  episodes_total: 15956
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.30025260895490646
        entropy_coeff: 0.0005000000000000001
        kl: 0.004504078921551506
        model: {}
        policy_loss: -0.008543379177960256
        total_loss: 4.541686534881592
        vf_explained_var: 0.9885548949241638
        vf_loss: 4.5503761768341064
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.783333333333328
    gpu_util_percent0: 0.3543333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476044341330981
    mean_env_wait_ms: 1.196126804652042
    mean_inference_ms: 4.368029962735273
    mean_raw_obs_processing_ms: 0.3814756008397249
  time_since_restore: 1987.2571988105774
  time_this_iter_s: 25.86513876914978
  time_total_s: 1987.2571988105774
  timers:
    learn_throughput: 8653.907
    learn_time_ms: 18695.833
    sample_throughput: 23030.319
    sample_time_ms: 7025.174
    update_time_ms: 31.689
  timestamp: 1602628826
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     77 |          1987.26 | 12457984 |  276.593 |              323.505 |              160.172 |             777.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3158.1062480571964
    time_step_min: 2849
  date: 2020-10-13_22-40-52
  done: false
  episode_len_mean: 777.3722949091585
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 276.8043734924742
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 171
  episodes_total: 16127
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.31814805666605633
        entropy_coeff: 0.0005000000000000001
        kl: 0.004912271319578092
        model: {}
        policy_loss: -0.011369439637443671
        total_loss: 4.840783596038818
        vf_explained_var: 0.9866332411766052
        vf_loss: 4.852310061454773
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92068965517241
    gpu_util_percent0: 0.39517241379310347
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14758830070611545
    mean_env_wait_ms: 1.196260944274624
    mean_inference_ms: 4.367062117280729
    mean_raw_obs_processing_ms: 0.381421750444954
  time_since_restore: 2012.6525571346283
  time_this_iter_s: 25.395358324050903
  time_total_s: 2012.6525571346283
  timers:
    learn_throughput: 8666.54
    learn_time_ms: 18668.582
    sample_throughput: 23015.3
    sample_time_ms: 7029.758
    update_time_ms: 31.285
  timestamp: 1602628852
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     78 |          2012.65 | 12619776 |  276.804 |              323.505 |              160.172 |            777.372 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3156.116139685646
    time_step_min: 2849
  date: 2020-10-13_22-41-18
  done: false
  episode_len_mean: 777.1172451656195
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.0987037458092
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 266
  episodes_total: 16393
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.29372237126032513
        entropy_coeff: 0.0005000000000000001
        kl: 0.004794335303207238
        model: {}
        policy_loss: -0.010221628273332803
        total_loss: 7.2412718534469604
        vf_explained_var: 0.9855754375457764
        vf_loss: 7.251639485359192
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.02
    gpu_util_percent0: 0.2916666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756528138036873
    mean_env_wait_ms: 1.1964615189735772
    mean_inference_ms: 4.365597286057763
    mean_raw_obs_processing_ms: 0.38133614705228364
  time_since_restore: 2038.5937538146973
  time_this_iter_s: 25.94119668006897
  time_total_s: 2038.5937538146973
  timers:
    learn_throughput: 8654.518
    learn_time_ms: 18694.513
    sample_throughput: 23033.613
    sample_time_ms: 7024.169
    update_time_ms: 33.239
  timestamp: 1602628878
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     79 |          2038.59 | 12781568 |  277.099 |              325.475 |              160.172 |            777.117 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3154.7213392155677
    time_step_min: 2849
  date: 2020-10-13_22-41-44
  done: false
  episode_len_mean: 776.934715775514
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.30936040737714
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 16589
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2889004598061244
        entropy_coeff: 0.0005000000000000001
        kl: 0.005035359490041931
        model: {}
        policy_loss: -0.0105992041232336
        total_loss: 4.523562908172607
        vf_explained_var: 0.9883689880371094
        vf_loss: 4.534306089083354
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.224137931034488
    gpu_util_percent0: 0.33931034482758615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475468756276705
    mean_env_wait_ms: 1.1966089329070522
    mean_inference_ms: 4.364499022690287
    mean_raw_obs_processing_ms: 0.3812767563772311
  time_since_restore: 2064.0981595516205
  time_this_iter_s: 25.504405736923218
  time_total_s: 2064.0981595516205
  timers:
    learn_throughput: 8670.83
    learn_time_ms: 18659.344
    sample_throughput: 23025.802
    sample_time_ms: 7026.552
    update_time_ms: 33.544
  timestamp: 1602628904
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     80 |           2064.1 | 12943360 |  277.309 |              325.475 |              160.172 |            776.935 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3153.5677033492825
    time_step_min: 2849
  date: 2020-10-13_22-42-10
  done: false
  episode_len_mean: 776.7821858966711
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.4740839971123
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 16762
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3076997523506482
        entropy_coeff: 0.0005000000000000001
        kl: 0.005693516267153124
        model: {}
        policy_loss: -0.00956670832723224
        total_loss: 5.133278687795003
        vf_explained_var: 0.9868350028991699
        vf_loss: 5.142998774846395
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48666666666667
    gpu_util_percent0: 0.29133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753201172640704
    mean_env_wait_ms: 1.1967358234496752
    mean_inference_ms: 4.36358392872113
    mean_raw_obs_processing_ms: 0.3812262782338945
  time_since_restore: 2089.936994791031
  time_this_iter_s: 25.8388352394104
  time_total_s: 2089.936994791031
  timers:
    learn_throughput: 8672.458
    learn_time_ms: 18655.841
    sample_throughput: 23062.608
    sample_time_ms: 7015.338
    update_time_ms: 32.204
  timestamp: 1602628930
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     81 |          2089.94 | 13105152 |  277.474 |              325.475 |              160.172 |            776.782 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3151.7855125338356
    time_step_min: 2849
  date: 2020-10-13_22-42-36
  done: false
  episode_len_mean: 776.522070908664
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.74066207982617
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 274
  episodes_total: 17036
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2834418366352717
        entropy_coeff: 0.0005000000000000001
        kl: 0.005275121851203342
        model: {}
        policy_loss: -0.009265318689964866
        total_loss: 6.91647748152415
        vf_explained_var: 0.986660897731781
        vf_loss: 6.925883889198303
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.593548387096778
    gpu_util_percent0: 0.31967741935483873
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475088142243286
    mean_env_wait_ms: 1.1969223162229887
    mean_inference_ms: 4.3621495696804615
    mean_raw_obs_processing_ms: 0.3811421305323358
  time_since_restore: 2115.8834981918335
  time_this_iter_s: 25.946503400802612
  time_total_s: 2115.8834981918335
  timers:
    learn_throughput: 8666.65
    learn_time_ms: 18668.343
    sample_throughput: 23039.989
    sample_time_ms: 7022.226
    update_time_ms: 32.455
  timestamp: 1602628956
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     82 |          2115.88 | 13266944 |  277.741 |              325.475 |              160.172 |            776.522 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3150.613154831199
    time_step_min: 2849
  date: 2020-10-13_22-43-03
  done: false
  episode_len_mean: 776.3493787016606
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.91905760660836
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 17222
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2752121885617574
        entropy_coeff: 0.0005000000000000001
        kl: 0.004554063236961762
        model: {}
        policy_loss: -0.010158129036426544
        total_loss: 4.648556550343831
        vf_explained_var: 0.9879704117774963
        vf_loss: 4.658851822217305
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793333333333337
    gpu_util_percent0: 0.3173333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749307001534728
    mean_env_wait_ms: 1.1970528474717805
    mean_inference_ms: 4.361193426970197
    mean_raw_obs_processing_ms: 0.38109003657012924
  time_since_restore: 2141.811731338501
  time_this_iter_s: 25.92823314666748
  time_total_s: 2141.811731338501
  timers:
    learn_throughput: 8681.498
    learn_time_ms: 18636.415
    sample_throughput: 23015.362
    sample_time_ms: 7029.739
    update_time_ms: 31.932
  timestamp: 1602628983
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     83 |          2141.81 | 13428736 |  277.919 |              325.475 |              160.172 |            776.349 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3149.4643659618596
    time_step_min: 2849
  date: 2020-10-13_22-43-29
  done: false
  episode_len_mean: 776.1916776826255
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.0987662706727
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 177
  episodes_total: 17399
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.29051440705855686
        entropy_coeff: 0.0005000000000000001
        kl: 0.005475612551284333
        model: {}
        policy_loss: -0.00846125721000135
        total_loss: 5.494869629542033
        vf_explained_var: 0.9860510230064392
        vf_loss: 5.5034758647282915
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.437931034482766
    gpu_util_percent0: 0.3237931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747898175927998
    mean_env_wait_ms: 1.1971730003066312
    mean_inference_ms: 4.360321227393505
    mean_raw_obs_processing_ms: 0.38104287809343673
  time_since_restore: 2167.4648299217224
  time_this_iter_s: 25.653098583221436
  time_total_s: 2167.4648299217224
  timers:
    learn_throughput: 8681.295
    learn_time_ms: 18636.851
    sample_throughput: 23096.901
    sample_time_ms: 7004.922
    update_time_ms: 33.276
  timestamp: 1602629009
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     84 |          2167.46 | 13590528 |  278.099 |              325.475 |              160.172 |            776.192 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3147.7114664851993
    time_step_min: 2849
  date: 2020-10-13_22-43-55
  done: false
  episode_len_mean: 775.9970015840688
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.3649358486426
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 277
  episodes_total: 17676
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.26342381288607913
        entropy_coeff: 0.0005000000000000001
        kl: 0.004692555676835279
        model: {}
        policy_loss: -0.007998384205469241
        total_loss: 6.075648824373881
        vf_explained_var: 0.9882777333259583
        vf_loss: 6.083778540293376
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.873333333333335
    gpu_util_percent0: 0.359
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745754789104046
    mean_env_wait_ms: 1.1973464036698231
    mean_inference_ms: 4.358973071922257
    mean_raw_obs_processing_ms: 0.3809639499510888
  time_since_restore: 2193.3371753692627
  time_this_iter_s: 25.872345447540283
  time_total_s: 2193.3371753692627
  timers:
    learn_throughput: 8672.066
    learn_time_ms: 18656.684
    sample_throughput: 23247.059
    sample_time_ms: 6959.676
    update_time_ms: 33.732
  timestamp: 1602629035
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     85 |          2193.34 | 13752320 |  278.365 |              325.475 |              160.172 |            775.997 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3146.632382663373
    time_step_min: 2849
  date: 2020-10-13_22-44-21
  done: false
  episode_len_mean: 775.8654643217207
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.53880917384896
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 178
  episodes_total: 17854
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.2600918958584468
        entropy_coeff: 0.0005000000000000001
        kl: 0.004823713796213269
        model: {}
        policy_loss: -0.010017351460798333
        total_loss: 4.0900519490242
        vf_explained_var: 0.9890416264533997
        vf_loss: 4.100199202696483
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.809999999999995
    gpu_util_percent0: 0.334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14744296784943245
    mean_env_wait_ms: 1.1974587300251474
    mean_inference_ms: 4.358108222018768
    mean_raw_obs_processing_ms: 0.3809168356329636
  time_since_restore: 2219.0317401885986
  time_this_iter_s: 25.694564819335938
  time_total_s: 2219.0317401885986
  timers:
    learn_throughput: 8686.862
    learn_time_ms: 18624.906
    sample_throughput: 23189.715
    sample_time_ms: 6976.886
    update_time_ms: 32.256
  timestamp: 1602629061
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     86 |          2219.03 | 13914112 |  278.539 |              325.475 |              160.172 |            775.865 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3145.5827406090243
    time_step_min: 2849
  date: 2020-10-13_22-44-47
  done: false
  episode_len_mean: 775.7283512584544
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.70278122168577
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 18038
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.27556434522072476
        entropy_coeff: 0.0005000000000000001
        kl: 0.005423782975412905
        model: {}
        policy_loss: -0.010063629000796936
        total_loss: 4.869331796964009
        vf_explained_var: 0.9880831837654114
        vf_loss: 4.879533012708028
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.026666666666664
    gpu_util_percent0: 0.3189999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14742948938493347
    mean_env_wait_ms: 1.1975730527629194
    mean_inference_ms: 4.357262562460395
    mean_raw_obs_processing_ms: 0.38087100641322835
  time_since_restore: 2244.8985047340393
  time_this_iter_s: 25.866764545440674
  time_total_s: 2244.8985047340393
  timers:
    learn_throughput: 8683.279
    learn_time_ms: 18632.593
    sample_throughput: 23212.417
    sample_time_ms: 6970.063
    update_time_ms: 31.923
  timestamp: 1602629087
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     87 |           2244.9 | 14075904 |  278.703 |              325.475 |              160.172 |            775.728 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3143.9228200777275
    time_step_min: 2849
  date: 2020-10-13_22-45-13
  done: false
  episode_len_mean: 775.5347059144776
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.94616637678183
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 273
  episodes_total: 18311
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2466991270581881
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045042647592102485
        model: {}
        policy_loss: -0.008488403292479537
        total_loss: 5.143421570460002
        vf_explained_var: 0.9899156093597412
        vf_loss: 5.152033408482869
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.939999999999998
    gpu_util_percent0: 0.3983333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740939613263257
    mean_env_wait_ms: 1.1977318576551743
    mean_inference_ms: 4.356019610724402
    mean_raw_obs_processing_ms: 0.3807989906359757
  time_since_restore: 2270.6149463653564
  time_this_iter_s: 25.71644163131714
  time_total_s: 2270.6149463653564
  timers:
    learn_throughput: 8666.791
    learn_time_ms: 18668.041
    sample_throughput: 23233.435
    sample_time_ms: 6963.757
    update_time_ms: 33.187
  timestamp: 1602629113
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     88 |          2270.61 | 14237696 |  278.946 |              325.475 |              160.172 |            775.535 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3142.8778464541315
    time_step_min: 2849
  date: 2020-10-13_22-45-40
  done: false
  episode_len_mean: 775.4229146381045
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.1155944383793
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 18486
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.25028316924969357
        entropy_coeff: 0.0005000000000000001
        kl: 0.004874534555710852
        model: {}
        policy_loss: -0.008682492460745076
        total_loss: 3.7969141801198325
        vf_explained_var: 0.9895575642585754
        vf_loss: 3.8057217399279275
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.753333333333334
    gpu_util_percent0: 0.33466666666666656
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739593568774303
    mean_env_wait_ms: 1.1978309209397888
    mean_inference_ms: 4.355218803830076
    mean_raw_obs_processing_ms: 0.38075526873809934
  time_since_restore: 2296.44136428833
  time_this_iter_s: 25.826417922973633
  time_total_s: 2296.44136428833
  timers:
    learn_throughput: 8676.065
    learn_time_ms: 18648.085
    sample_throughput: 23199.886
    sample_time_ms: 6973.827
    update_time_ms: 31.115
  timestamp: 1602629140
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     89 |          2296.44 | 14399488 |  279.116 |              325.475 |              160.172 |            775.423 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3141.683357484034
    time_step_min: 2849
  date: 2020-10-13_22-46-06
  done: false
  episode_len_mean: 775.3229451137885
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.29906291834004
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 189
  episodes_total: 18675
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.26849807302157086
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054943005088716745
        model: {}
        policy_loss: -0.009653241674338156
        total_loss: 4.333018501599629
        vf_explained_var: 0.9894297122955322
        vf_loss: 4.342805941899617
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33666666666667
    gpu_util_percent0: 0.30166666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738333996485928
    mean_env_wait_ms: 1.1979386961305767
    mean_inference_ms: 4.354411479795898
    mean_raw_obs_processing_ms: 0.3807121411609274
  time_since_restore: 2322.2736518383026
  time_this_iter_s: 25.832287549972534
  time_total_s: 2322.2736518383026
  timers:
    learn_throughput: 8666.277
    learn_time_ms: 18669.146
    sample_throughput: 23193.738
    sample_time_ms: 6975.676
    update_time_ms: 30.925
  timestamp: 1602629166
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     90 |          2322.27 | 14561280 |  279.299 |              325.475 |              160.172 |            775.323 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3139.9357278882776
    time_step_min: 2849
  date: 2020-10-13_22-46-32
  done: false
  episode_len_mean: 775.1970336746543
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.54248864662674
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 271
  episodes_total: 18946
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.23660230884949365
        entropy_coeff: 0.0005000000000000001
        kl: 0.004229308241823067
        model: {}
        policy_loss: -0.010361460653560547
        total_loss: 6.4848151206970215
        vf_explained_var: 0.9871156811714172
        vf_loss: 6.49529488881429
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.50689655172414
    gpu_util_percent0: 0.28586206896551725
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736411281629774
    mean_env_wait_ms: 1.1980791336380798
    mean_inference_ms: 4.353247501552173
    mean_raw_obs_processing_ms: 0.3806450775899515
  time_since_restore: 2347.9842705726624
  time_this_iter_s: 25.71061873435974
  time_total_s: 2347.9842705726624
  timers:
    learn_throughput: 8672.847
    learn_time_ms: 18655.004
    sample_throughput: 23191.978
    sample_time_ms: 6976.205
    update_time_ms: 30.767
  timestamp: 1602629192
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     91 |          2347.98 | 14723072 |  279.542 |              325.475 |              160.172 |            775.197 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3138.8942705876184
    time_step_min: 2849
  date: 2020-10-13_22-46-58
  done: false
  episode_len_mean: 775.1213452586433
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.7028039693974
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 19119
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.24192707613110542
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053346438411002355
        model: {}
        policy_loss: -0.007106361968908459
        total_loss: 4.053497612476349
        vf_explained_var: 0.9891877770423889
        vf_loss: 4.060725073019664
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26
    gpu_util_percent0: 0.2923333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735164805234507
    mean_env_wait_ms: 1.1981691594588855
    mean_inference_ms: 4.352510449464393
    mean_raw_obs_processing_ms: 0.38060373556161514
  time_since_restore: 2373.8236980438232
  time_this_iter_s: 25.83942747116089
  time_total_s: 2373.8236980438232
  timers:
    learn_throughput: 8678.078
    learn_time_ms: 18643.759
    sample_throughput: 23188.353
    sample_time_ms: 6977.296
    update_time_ms: 30.091
  timestamp: 1602629218
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     92 |          2373.82 | 14884864 |  279.703 |              325.475 |              160.172 |            775.121 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3137.7503503399594
    time_step_min: 2849
  date: 2020-10-13_22-47-24
  done: false
  episode_len_mean: 775.0277590760785
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.8723278148935
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 19309
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.25334326302011806
        entropy_coeff: 0.0005000000000000001
        kl: 0.005550891626626253
        model: {}
        policy_loss: -0.012007400044240057
        total_loss: 4.474414626757304
        vf_explained_var: 0.9896135926246643
        vf_loss: 4.486548860867818
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.130000000000003
    gpu_util_percent0: 0.36766666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733960923797537
    mean_env_wait_ms: 1.198264609064976
    mean_inference_ms: 4.35173196535056
    mean_raw_obs_processing_ms: 0.38056186149784677
  time_since_restore: 2399.4458203315735
  time_this_iter_s: 25.622122287750244
  time_total_s: 2399.4458203315735
  timers:
    learn_throughput: 8682.211
    learn_time_ms: 18634.885
    sample_throughput: 23233.913
    sample_time_ms: 6963.614
    update_time_ms: 29.975
  timestamp: 1602629244
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     93 |          2399.45 | 15046656 |  279.872 |              325.475 |              160.172 |            775.028 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3136.0240057327123
    time_step_min: 2839
  date: 2020-10-13_22-47-50
  done: false
  episode_len_mean: 774.8940191020992
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.14354846281907
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 270
  episodes_total: 19579
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2210515650610129
        entropy_coeff: 0.0005000000000000001
        kl: 0.004453400227551659
        model: {}
        policy_loss: -0.00836365584594508
        total_loss: 4.5613594849904375
        vf_explained_var: 0.9907524585723877
        vf_loss: 4.569833596547444
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.256666666666664
    gpu_util_percent0: 0.3333333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732203556037804
    mean_env_wait_ms: 1.1983914993370832
    mean_inference_ms: 4.350663864569337
    mean_raw_obs_processing_ms: 0.38050079844081325
  time_since_restore: 2425.359663963318
  time_this_iter_s: 25.913843631744385
  time_total_s: 2425.359663963318
  timers:
    learn_throughput: 8674.609
    learn_time_ms: 18651.215
    sample_throughput: 23228.095
    sample_time_ms: 6965.358
    update_time_ms: 28.846
  timestamp: 1602629270
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     94 |          2425.36 | 15208448 |  280.144 |              325.475 |              160.172 |            774.894 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3134.9877720838194
    time_step_min: 2839
  date: 2020-10-13_22-48-17
  done: false
  episode_len_mean: 774.8060351374614
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.3083040418872
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 172
  episodes_total: 19751
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.2316630259156227
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040806956045950455
        model: {}
        policy_loss: -0.008456611190922558
        total_loss: 4.0203032692273455
        vf_explained_var: 0.9889214634895325
        vf_loss: 4.028875728448232
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.47666666666667
    gpu_util_percent0: 0.35800000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731029309120053
    mean_env_wait_ms: 1.1984721965373442
    mean_inference_ms: 4.34997944483081
    mean_raw_obs_processing_ms: 0.3804620066933659
  time_since_restore: 2451.5320630073547
  time_this_iter_s: 26.172399044036865
  time_total_s: 2451.5320630073547
  timers:
    learn_throughput: 8662.741
    learn_time_ms: 18676.767
    sample_throughput: 23213.272
    sample_time_ms: 6969.806
    update_time_ms: 28.612
  timestamp: 1602629297
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     95 |          2451.53 | 15370240 |  280.308 |              325.475 |              160.172 |            774.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3133.665226564855
    time_step_min: 2839
  date: 2020-10-13_22-48-43
  done: false
  episode_len_mean: 774.693503108081
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.4952325541357
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 19948
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.24208191906412443
        entropy_coeff: 0.0005000000000000001
        kl: 0.00469647313002497
        model: {}
        policy_loss: -0.008889193115464877
        total_loss: 4.746521472930908
        vf_explained_var: 0.988551139831543
        vf_loss: 4.755531708399455
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.743333333333336
    gpu_util_percent0: 0.3473333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729823615445145
    mean_env_wait_ms: 1.1985609030027906
    mean_inference_ms: 4.349230042445871
    mean_raw_obs_processing_ms: 0.38042002862585395
  time_since_restore: 2477.590708732605
  time_this_iter_s: 26.058645725250244
  time_total_s: 2477.590708732605
  timers:
    learn_throughput: 8652.748
    learn_time_ms: 18698.338
    sample_throughput: 23173.929
    sample_time_ms: 6981.639
    update_time_ms: 30.278
  timestamp: 1602629323
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     96 |          2477.59 | 15532032 |  280.495 |              325.475 |              160.172 |            774.694 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3131.9620265714852
    time_step_min: 2839
  date: 2020-10-13_22-49-10
  done: false
  episode_len_mean: 774.5555060848917
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.76618215398264
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 266
  episodes_total: 20214
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.21615375702579817
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043829032219946384
        model: {}
        policy_loss: -0.009730958918225951
        total_loss: 4.789487361907959
        vf_explained_var: 0.9898057579994202
        vf_loss: 4.799326459566752
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.80666666666667
    gpu_util_percent0: 0.328
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728219871540968
    mean_env_wait_ms: 1.1986728470435746
    mean_inference_ms: 4.348238291266019
    mean_raw_obs_processing_ms: 0.3803650655317498
  time_since_restore: 2503.491042613983
  time_this_iter_s: 25.900333881378174
  time_total_s: 2503.491042613983
  timers:
    learn_throughput: 8656.307
    learn_time_ms: 18690.65
    sample_throughput: 23145.268
    sample_time_ms: 6990.284
    update_time_ms: 30.182
  timestamp: 1602629350
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     97 |          2503.49 | 15693824 |  280.766 |              325.475 |              160.172 |            774.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3130.9395309965093
    time_step_min: 2839
  date: 2020-10-13_22-49-36
  done: false
  episode_len_mean: 774.5000245302458
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.92261772907403
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 169
  episodes_total: 20383
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.22247247149546942
        entropy_coeff: 0.0005000000000000001
        kl: 0.004486261362520357
        model: {}
        policy_loss: -0.01045249072679629
        total_loss: 3.540562887986501
        vf_explained_var: 0.9903184771537781
        vf_loss: 3.5511265794436135
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7
    gpu_util_percent0: 0.3466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727140730655133
    mean_env_wait_ms: 1.198744534969073
    mean_inference_ms: 4.3476127078041396
    mean_raw_obs_processing_ms: 0.38032932736194625
  time_since_restore: 2529.178485393524
  time_this_iter_s: 25.687442779541016
  time_total_s: 2529.178485393524
  timers:
    learn_throughput: 8664.872
    learn_time_ms: 18672.175
    sample_throughput: 23092.251
    sample_time_ms: 7006.333
    update_time_ms: 29.063
  timestamp: 1602629376
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     98 |          2529.18 | 15855616 |  280.923 |              325.475 |              160.172 |              774.5 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3129.715092227576
    time_step_min: 2839
  date: 2020-10-13_22-50-02
  done: false
  episode_len_mean: 774.4038564281898
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 281.0948711948275
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 206
  episodes_total: 20589
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.23181903610626856
        entropy_coeff: 0.0005000000000000001
        kl: 0.004677652070919673
        model: {}
        policy_loss: -0.010011315243900754
        total_loss: 5.311827023824056
        vf_explained_var: 0.9880104064941406
        vf_loss: 5.321954131126404
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.330000000000002
    gpu_util_percent0: 0.32133333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726018292604118
    mean_env_wait_ms: 1.198831823270113
    mean_inference_ms: 4.346903184260678
    mean_raw_obs_processing_ms: 0.38029047078379713
  time_since_restore: 2554.8685886859894
  time_this_iter_s: 25.69010329246521
  time_total_s: 2554.8685886859894
  timers:
    learn_throughput: 8668.148
    learn_time_ms: 18665.117
    sample_throughput: 23118.207
    sample_time_ms: 6998.467
    update_time_ms: 29.343
  timestamp: 1602629402
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |     99 |          2554.87 | 16017408 |  281.095 |              325.475 |              160.172 |            774.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3128.1833301288216
    time_step_min: 2839
  date: 2020-10-13_22-50-28
  done: false
  episode_len_mean: 774.2745850522882
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 281.3305253436214
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 257
  episodes_total: 20846
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.2023782953619957
        entropy_coeff: 0.0005000000000000001
        kl: 0.00405684191112717
        model: {}
        policy_loss: -0.009388180328339027
        total_loss: 4.744685212771098
        vf_explained_var: 0.9900166392326355
        vf_loss: 4.754174590110779
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.826666666666668
    gpu_util_percent0: 0.2886666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472444874090069
    mean_env_wait_ms: 1.1989230121286407
    mean_inference_ms: 4.34596376063068
    mean_raw_obs_processing_ms: 0.3802365416718899
  time_since_restore: 2580.8070747852325
  time_this_iter_s: 25.938486099243164
  time_total_s: 2580.8070747852325
  timers:
    learn_throughput: 8668.098
    learn_time_ms: 18665.225
    sample_throughput: 23061.661
    sample_time_ms: 7015.627
    update_time_ms: 30.866
  timestamp: 1602629428
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    100 |          2580.81 | 16179200 |  281.331 |              325.475 |              160.172 |            774.275 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3127.2248128546225
    time_step_min: 2839
  date: 2020-10-13_22-50-55
  done: false
  episode_len_mean: 774.1690221270521
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 281.4788498835608
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 169
  episodes_total: 21015
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.21184253816803297
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042800351123635965
        model: {}
        policy_loss: -0.010074328553552428
        total_loss: 3.7910202145576477
        vf_explained_var: 0.9894771575927734
        vf_loss: 3.8012004693349204
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.683333333333334
    gpu_util_percent0: 0.3426666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14723456323499035
    mean_env_wait_ms: 1.1989879909437142
    mean_inference_ms: 4.345379160958071
    mean_raw_obs_processing_ms: 0.38020395789048983
  time_since_restore: 2606.573978662491
  time_this_iter_s: 25.7669038772583
  time_total_s: 2606.573978662491
  timers:
    learn_throughput: 8671.578
    learn_time_ms: 18657.734
    sample_throughput: 23023.381
    sample_time_ms: 7027.291
    update_time_ms: 31.199
  timestamp: 1602629455
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    101 |          2606.57 | 16340992 |  281.479 |              325.475 |              160.172 |            774.169 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3125.8344186485465
    time_step_min: 2839
  date: 2020-10-13_22-51-21
  done: false
  episode_len_mean: 774.0408307431478
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 281.6766720611027
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 219
  episodes_total: 21234
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.22323922937115034
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050403246423229575
        model: {}
        policy_loss: -0.009667658693312356
        total_loss: 4.959194461504619
        vf_explained_var: 0.9885984063148499
        vf_loss: 4.968973716100057
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95333333333333
    gpu_util_percent0: 0.3516666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472235413032436
    mean_env_wait_ms: 1.1990732886309763
    mean_inference_ms: 4.344673221734088
    mean_raw_obs_processing_ms: 0.3801658051973224
  time_since_restore: 2632.61780500412
  time_this_iter_s: 26.04382634162903
  time_total_s: 2632.61780500412
  timers:
    learn_throughput: 8664.836
    learn_time_ms: 18672.251
    sample_throughput: 23007.823
    sample_time_ms: 7032.043
    update_time_ms: 30.209
  timestamp: 1602629481
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    102 |          2632.62 | 16502784 |  281.677 |              325.475 |              160.172 |            774.041 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3124.2488339552237
    time_step_min: 2831
  date: 2020-10-13_22-51-47
  done: false
  episode_len_mean: 773.8888371659995
  episode_reward_max: 326.2323232323229
  episode_reward_mean: 281.9130975521907
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 248
  episodes_total: 21482
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.1936775098244349
        entropy_coeff: 0.0005000000000000001
        kl: 0.004403773307179411
        model: {}
        policy_loss: -0.008988071019606044
        total_loss: 3.9723556439081826
        vf_explained_var: 0.9909936785697937
        vf_loss: 3.9814405838648477
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.673333333333336
    gpu_util_percent0: 0.3646666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472087209802785
    mean_env_wait_ms: 1.1991514449830551
    mean_inference_ms: 4.343805543500956
    mean_raw_obs_processing_ms: 0.3801159647969502
  time_since_restore: 2658.375720977783
  time_this_iter_s: 25.75791597366333
  time_total_s: 2658.375720977783
  timers:
    learn_throughput: 8656.308
    learn_time_ms: 18690.648
    sample_throughput: 23032.783
    sample_time_ms: 7024.422
    update_time_ms: 31.087
  timestamp: 1602629507
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    103 |          2658.38 | 16664576 |  281.913 |              326.232 |              160.172 |            773.889 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3123.276695209442
    time_step_min: 2831
  date: 2020-10-13_22-52-13
  done: false
  episode_len_mean: 773.7868988774426
  episode_reward_max: 326.2323232323229
  episode_reward_mean: 282.0585207178731
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 165
  episodes_total: 21647
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.20866921047369638
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041684470876740915
        model: {}
        policy_loss: -0.008656489400891587
        total_loss: 3.79058971007665
        vf_explained_var: 0.989311158657074
        vf_loss: 3.799350599447886
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.589999999999996
    gpu_util_percent0: 0.34266666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719975162449095
    mean_env_wait_ms: 1.1992085445414489
    mean_inference_ms: 4.343269689696047
    mean_raw_obs_processing_ms: 0.380085470756219
  time_since_restore: 2684.0566334724426
  time_this_iter_s: 25.680912494659424
  time_total_s: 2684.0566334724426
  timers:
    learn_throughput: 8665.883
    learn_time_ms: 18669.996
    sample_throughput: 23040.277
    sample_time_ms: 7022.138
    update_time_ms: 30.406
  timestamp: 1602629533
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    104 |          2684.06 | 16826368 |  282.059 |              326.232 |              160.172 |            773.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3121.967527709078
    time_step_min: 2828
  date: 2020-10-13_22-52-40
  done: false
  episode_len_mean: 773.6560614371914
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 282.25733611485117
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 21876
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.21525794391830763
        entropy_coeff: 0.0005000000000000001
        kl: 0.004697754746302962
        model: {}
        policy_loss: -0.008619171579198337
        total_loss: 4.619357625643413
        vf_explained_var: 0.9897849559783936
        vf_loss: 4.628084301948547
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333337
    gpu_util_percent0: 0.3333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471886433071216
    mean_env_wait_ms: 1.1992857075925387
    mean_inference_ms: 4.342560379451658
    mean_raw_obs_processing_ms: 0.3800456427340297
  time_since_restore: 2709.7929170131683
  time_this_iter_s: 25.736283540725708
  time_total_s: 2709.7929170131683
  timers:
    learn_throughput: 8683.343
    learn_time_ms: 18632.454
    sample_throughput: 23057.797
    sample_time_ms: 7016.802
    update_time_ms: 28.439
  timestamp: 1602629560
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    105 |          2709.79 | 16988160 |  282.257 |              326.687 |              160.172 |            773.656 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3120.7082861414397
    time_step_min: 2828
  date: 2020-10-13_22-53-06
  done: false
  episode_len_mean: 773.5319466425503
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 282.4568451871188
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 239
  episodes_total: 22115
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.18876768772800764
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037186899959730604
        model: {}
        policy_loss: -0.008131141478467422
        total_loss: 4.283570170402527
        vf_explained_var: 0.9903204441070557
        vf_loss: 4.291795651117961
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.713333333333335
    gpu_util_percent0: 0.35766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717527319800422
    mean_env_wait_ms: 1.1993563755042662
    mean_inference_ms: 4.341774136637545
    mean_raw_obs_processing_ms: 0.3800017307966552
  time_since_restore: 2735.559765100479
  time_this_iter_s: 25.76684808731079
  time_total_s: 2735.559765100479
  timers:
    learn_throughput: 8691.021
    learn_time_ms: 18615.995
    sample_throughput: 23099.455
    sample_time_ms: 7004.148
    update_time_ms: 27.79
  timestamp: 1602629586
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    106 |          2735.56 | 17149952 |  282.457 |              326.687 |              160.172 |            773.532 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3119.7741108763094
    time_step_min: 2828
  date: 2020-10-13_22-53-32
  done: false
  episode_len_mean: 773.4342323744559
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 282.6014672597718
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 168
  episodes_total: 22283
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.20253701011339822
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041220731412371
        model: {}
        policy_loss: -0.009113873481207216
        total_loss: 3.457788904507955
        vf_explained_var: 0.9901456236839294
        vf_loss: 3.4670040011405945
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280000000000005
    gpu_util_percent0: 0.32233333333333325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716669904801627
    mean_env_wait_ms: 1.1994077745156964
    mean_inference_ms: 4.341256551628704
    mean_raw_obs_processing_ms: 0.37997323310893577
  time_since_restore: 2761.2681958675385
  time_this_iter_s: 25.708430767059326
  time_total_s: 2761.2681958675385
  timers:
    learn_throughput: 8695.137
    learn_time_ms: 18607.183
    sample_throughput: 23127.563
    sample_time_ms: 6995.635
    update_time_ms: 26.414
  timestamp: 1602629612
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    107 |          2761.27 | 17311744 |  282.601 |              326.687 |              160.172 |            773.434 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3118.335631365921
    time_step_min: 2828
  date: 2020-10-13_22-53-59
  done: false
  episode_len_mean: 773.290166481687
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 282.8173028845615
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 242
  episodes_total: 22525
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.2062460370361805
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043828646109128995
        model: {}
        policy_loss: -0.006959196558455005
        total_loss: 4.403973579406738
        vf_explained_var: 0.9902663230895996
        vf_loss: 4.411036094029744
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.856666666666666
    gpu_util_percent0: 0.31466666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715591203748762
    mean_env_wait_ms: 1.1994782837298532
    mean_inference_ms: 4.3405627020092306
    mean_raw_obs_processing_ms: 0.37993219040233056
  time_since_restore: 2787.484569311142
  time_this_iter_s: 26.216373443603516
  time_total_s: 2787.484569311142
  timers:
    learn_throughput: 8677.235
    learn_time_ms: 18645.571
    sample_throughput: 23085.17
    sample_time_ms: 7008.482
    update_time_ms: 26.499
  timestamp: 1602629639
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    108 |          2787.48 | 17473536 |  282.817 |              326.687 |              160.172 |             773.29 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3116.9465386647876
    time_step_min: 2828
  date: 2020-10-13_22-54-25
  done: false
  episode_len_mean: 773.1590769230769
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 283.0131224331224
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 225
  episodes_total: 22750
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.18024898320436478
        entropy_coeff: 0.0005000000000000001
        kl: 0.003887702633316318
        model: {}
        policy_loss: -0.010001509741414338
        total_loss: 3.7521414359410605
        vf_explained_var: 0.9911050200462341
        vf_loss: 3.762233078479767
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.753333333333334
    gpu_util_percent0: 0.36233333333333345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714334044798816
    mean_env_wait_ms: 1.1995405919955338
    mean_inference_ms: 4.339843804259165
    mean_raw_obs_processing_ms: 0.379893222094935
  time_since_restore: 2813.20662689209
  time_this_iter_s: 25.722057580947876
  time_total_s: 2813.20662689209
  timers:
    learn_throughput: 8671.922
    learn_time_ms: 18656.995
    sample_throughput: 23115.406
    sample_time_ms: 6999.315
    update_time_ms: 26.171
  timestamp: 1602629665
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    109 |          2813.21 | 17635328 |  283.013 |              326.687 |              160.172 |            773.159 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3115.949989071038
    time_step_min: 2828
  date: 2020-10-13_22-54-51
  done: false
  episode_len_mean: 773.0685080944277
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 283.1586423205745
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 167
  episodes_total: 22917
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.2003182793656985
        entropy_coeff: 0.0005000000000000001
        kl: 0.004819937632419169
        model: {}
        policy_loss: -0.010830288670452623
        total_loss: 3.2065762281417847
        vf_explained_var: 0.9909068942070007
        vf_loss: 3.2175065875053406
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.053333333333335
    gpu_util_percent0: 0.3390000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471354899731512
    mean_env_wait_ms: 1.1995883439725195
    mean_inference_ms: 4.339364916755889
    mean_raw_obs_processing_ms: 0.3798683020647133
  time_since_restore: 2838.9245071411133
  time_this_iter_s: 25.717880249023438
  time_total_s: 2838.9245071411133
  timers:
    learn_throughput: 8673.742
    learn_time_ms: 18653.079
    sample_throughput: 23178.297
    sample_time_ms: 6980.323
    update_time_ms: 25.719
  timestamp: 1602629691
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    110 |          2838.92 | 17797120 |  283.159 |              326.687 |              160.172 |            773.069 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3114.579964546673
    time_step_min: 2828
  date: 2020-10-13_22-55-18
  done: false
  episode_len_mean: 772.9293081869579
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 283.3710533325138
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 254
  episodes_total: 23171
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.19308664898077646
        entropy_coeff: 0.0005000000000000001
        kl: 0.004499280165570478
        model: {}
        policy_loss: -0.006127382007737954
        total_loss: 4.901983181635539
        vf_explained_var: 0.9898405075073242
        vf_loss: 4.908207217852275
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.800000000000004
    gpu_util_percent0: 0.376
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712424577376754
    mean_env_wait_ms: 1.1996510796576625
    mean_inference_ms: 4.338659604641355
    mean_raw_obs_processing_ms: 0.3798258997153834
  time_since_restore: 2865.0300261974335
  time_this_iter_s: 26.10551905632019
  time_total_s: 2865.0300261974335
  timers:
    learn_throughput: 8659.012
    learn_time_ms: 18684.812
    sample_throughput: 23174.225
    sample_time_ms: 6981.549
    update_time_ms: 26.14
  timestamp: 1602629718
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    111 |          2865.03 | 17958912 |  283.371 |              326.687 |              160.172 |            772.929 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3113.332176520994
    time_step_min: 2809
  date: 2020-10-13_22-55-44
  done: false
  episode_len_mean: 772.8234539389274
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 283.55343659847125
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 211
  episodes_total: 23382
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.17463851099212965
        entropy_coeff: 0.0005000000000000001
        kl: 0.004277046498221655
        model: {}
        policy_loss: -0.007814210703751693
        total_loss: 2.9094037612279258
        vf_explained_var: 0.9926970601081848
        vf_loss: 2.917305290699005
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.338709677419352
    gpu_util_percent0: 0.38161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711334181973218
    mean_env_wait_ms: 1.1997049097404275
    mean_inference_ms: 4.338025428184228
    mean_raw_obs_processing_ms: 0.37979247121299414
  time_since_restore: 2891.0334339141846
  time_this_iter_s: 26.0034077167511
  time_total_s: 2891.0334339141846
  timers:
    learn_throughput: 8658.63
    learn_time_ms: 18685.635
    sample_throughput: 23228.533
    sample_time_ms: 6965.227
    update_time_ms: 34.63
  timestamp: 1602629744
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    112 |          2891.03 | 18120704 |  283.553 |              329.566 |              160.172 |            772.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3112.4548509208457
    time_step_min: 2809
  date: 2020-10-13_22-56-11
  done: false
  episode_len_mean: 772.739693457309
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 283.68228457032427
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 171
  episodes_total: 23553
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.19276007761557898
        entropy_coeff: 0.0005000000000000001
        kl: 0.004376984162566562
        model: {}
        policy_loss: -0.01104148793577527
        total_loss: 3.180401782194773
        vf_explained_var: 0.9914665222167969
        vf_loss: 3.191539704799652
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35333333333334
    gpu_util_percent0: 0.36233333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710556625684185
    mean_env_wait_ms: 1.1997476276794767
    mean_inference_ms: 4.337560291083198
    mean_raw_obs_processing_ms: 0.3797681804621581
  time_since_restore: 2916.981414794922
  time_this_iter_s: 25.947980880737305
  time_total_s: 2916.981414794922
  timers:
    learn_throughput: 8649.043
    learn_time_ms: 18706.348
    sample_throughput: 23237.643
    sample_time_ms: 6962.496
    update_time_ms: 34.984
  timestamp: 1602629771
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    113 |          2916.98 | 18282496 |  283.682 |              329.566 |              160.172 |             772.74 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3110.940731081479
    time_step_min: 2809
  date: 2020-10-13_22-56-37
  done: false
  episode_len_mean: 772.6221709006928
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 283.916644929242
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 262
  episodes_total: 23815
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.18339999889334044
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043749943530807895
        model: {}
        policy_loss: -0.006947610915328066
        total_loss: 4.638766368230184
        vf_explained_var: 0.9903349876403809
        vf_loss: 4.645805676778157
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.456666666666663
    gpu_util_percent0: 0.3413333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709411354655894
    mean_env_wait_ms: 1.1998036424166998
    mean_inference_ms: 4.33685648503931
    mean_raw_obs_processing_ms: 0.3797266608681013
  time_since_restore: 2942.7720906734467
  time_this_iter_s: 25.79067587852478
  time_total_s: 2942.7720906734467
  timers:
    learn_throughput: 8647.527
    learn_time_ms: 18709.626
    sample_throughput: 23193.213
    sample_time_ms: 6975.834
    update_time_ms: 36.233
  timestamp: 1602629797
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    114 |          2942.77 | 18444288 |  283.917 |              329.566 |              160.172 |            772.622 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3109.8491990655766
    time_step_min: 2809
  date: 2020-10-13_22-57-03
  done: false
  episode_len_mean: 772.5273173981844
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.0827185825104
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 199
  episodes_total: 24014
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.16781583055853844
        entropy_coeff: 0.0005000000000000001
        kl: 0.004889018988857667
        model: {}
        policy_loss: -0.009682772739324719
        total_loss: 3.5239917437235513
        vf_explained_var: 0.9909854531288147
        vf_loss: 3.5337583820025125
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.936666666666664
    gpu_util_percent0: 0.357
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708492145928934
    mean_env_wait_ms: 1.199849963948159
    mean_inference_ms: 4.336299593150252
    mean_raw_obs_processing_ms: 0.3796976619545535
  time_since_restore: 2968.450058221817
  time_this_iter_s: 25.67796754837036
  time_total_s: 2968.450058221817
  timers:
    learn_throughput: 8648.73
    learn_time_ms: 18707.024
    sample_throughput: 23209.741
    sample_time_ms: 6970.866
    update_time_ms: 36.982
  timestamp: 1602629823
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    115 |          2968.45 | 18606080 |  284.083 |              329.566 |              160.172 |            772.527 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3108.83231871454
    time_step_min: 2809
  date: 2020-10-13_22-57-30
  done: false
  episode_len_mean: 772.4584728595643
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.24044613316596
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 24189
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.18804708371559778
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044516730510319276
        model: {}
        policy_loss: -0.01162540833077704
        total_loss: 3.059577524662018
        vf_explained_var: 0.9915698170661926
        vf_loss: 3.0712968508402505
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73333333333334
    gpu_util_percent0: 0.3373333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707729167614178
    mean_env_wait_ms: 1.1998868935151095
    mean_inference_ms: 4.335842071657368
    mean_raw_obs_processing_ms: 0.3796739567819688
  time_since_restore: 2994.231659889221
  time_this_iter_s: 25.781601667404175
  time_total_s: 2994.231659889221
  timers:
    learn_throughput: 8648.75
    learn_time_ms: 18706.981
    sample_throughput: 23241.185
    sample_time_ms: 6961.435
    update_time_ms: 37.539
  timestamp: 1602629850
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    116 |          2994.23 | 18767872 |   284.24 |              329.566 |              160.172 |            772.458 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3107.3032363785333
    time_step_min: 2809
  date: 2020-10-13_22-57-56
  done: false
  episode_len_mean: 772.3368231637494
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.46565854851474
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 263
  episodes_total: 24452
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.17432812228798866
        entropy_coeff: 0.0005000000000000001
        kl: 0.004144462815020233
        model: {}
        policy_loss: -0.008397253603713276
        total_loss: 4.37483274936676
        vf_explained_var: 0.9908452033996582
        vf_loss: 4.383317192395528
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.76206896551724
    gpu_util_percent0: 0.35034482758620694
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706621775932494
    mean_env_wait_ms: 1.1999368522492884
    mean_inference_ms: 4.335179590168196
    mean_raw_obs_processing_ms: 0.3796348315739014
  time_since_restore: 3019.8920419216156
  time_this_iter_s: 25.66038203239441
  time_total_s: 3019.8920419216156
  timers:
    learn_throughput: 8646.388
    learn_time_ms: 18712.091
    sample_throughput: 23279.822
    sample_time_ms: 6949.881
    update_time_ms: 37.704
  timestamp: 1602629876
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    117 |          3019.89 | 18929664 |  284.466 |              329.566 |              160.172 |            772.337 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3106.191343222922
    time_step_min: 2809
  date: 2020-10-13_22-58-22
  done: false
  episode_len_mean: 772.2565423783827
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.6371509143449
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 195
  episodes_total: 24647
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.16233477865656218
        entropy_coeff: 0.0005000000000000001
        kl: 0.004291001319264372
        model: {}
        policy_loss: -0.008012217675665548
        total_loss: 3.577251593271891
        vf_explained_var: 0.9904074668884277
        vf_loss: 3.5853449503580728
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.056666666666665
    gpu_util_percent0: 0.33333333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705775803811705
    mean_env_wait_ms: 1.1999757403018296
    mean_inference_ms: 4.334659195541571
    mean_raw_obs_processing_ms: 0.3796074898152118
  time_since_restore: 3045.497102022171
  time_this_iter_s: 25.60506010055542
  time_total_s: 3045.497102022171
  timers:
    learn_throughput: 8664.339
    learn_time_ms: 18673.323
    sample_throughput: 23362.921
    sample_time_ms: 6925.162
    update_time_ms: 39.686
  timestamp: 1602629902
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    118 |           3045.5 | 19091456 |  284.637 |              329.566 |              160.172 |            772.257 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3105.152325628303
    time_step_min: 2809
  date: 2020-10-13_22-58-48
  done: false
  episode_len_mean: 772.1902057911482
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.8015542644031
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 24831
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.17706493909160295
        entropy_coeff: 0.0005000000000000001
        kl: 0.004648963765551646
        model: {}
        policy_loss: -0.007830245094131291
        total_loss: 3.3011738061904907
        vf_explained_var: 0.9911198616027832
        vf_loss: 3.3090926011403403
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.296666666666663
    gpu_util_percent0: 0.313
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470511408221224
    mean_env_wait_ms: 1.2000103474345505
    mean_inference_ms: 4.334219027622396
    mean_raw_obs_processing_ms: 0.37958495032099265
  time_since_restore: 3071.460847377777
  time_this_iter_s: 25.96374535560608
  time_total_s: 3071.460847377777
  timers:
    learn_throughput: 8653.838
    learn_time_ms: 18695.981
    sample_throughput: 23362.502
    sample_time_ms: 6925.286
    update_time_ms: 40.016
  timestamp: 1602629928
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    119 |          3071.46 | 19253248 |  284.802 |              329.566 |              160.172 |             772.19 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3103.6541578506126
    time_step_min: 2809
  date: 2020-10-13_22-59-15
  done: false
  episode_len_mean: 772.1144633533936
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.0285111688404
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 260
  episodes_total: 25091
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.16731526081760725
        entropy_coeff: 0.0005000000000000001
        kl: 0.004477322955305378
        model: {}
        policy_loss: -0.009297993674408644
        total_loss: 3.5320072372754416
        vf_explained_var: 0.9924858212471008
        vf_loss: 3.541388968626658
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.506666666666668
    gpu_util_percent0: 0.3286666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147039700032794
    mean_env_wait_ms: 1.2000504232596916
    mean_inference_ms: 4.333576901814967
    mean_raw_obs_processing_ms: 0.37954702624831466
  time_since_restore: 3097.33744096756
  time_this_iter_s: 25.876593589782715
  time_total_s: 3097.33744096756
  timers:
    learn_throughput: 8650.646
    learn_time_ms: 18702.88
    sample_throughput: 23338.958
    sample_time_ms: 6932.272
    update_time_ms: 40.847
  timestamp: 1602629955
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    120 |          3097.34 | 19415040 |  285.029 |              329.566 |              160.172 |            772.114 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3102.5843569221015
    time_step_min: 2809
  date: 2020-10-13_22-59-41
  done: false
  episode_len_mean: 772.0536392405063
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.1919451636619
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 189
  episodes_total: 25280
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.15328088526924452
        entropy_coeff: 0.0005000000000000001
        kl: 0.003679691484042754
        model: {}
        policy_loss: -0.010515831701923162
        total_loss: 2.6866751313209534
        vf_explained_var: 0.9926435947418213
        vf_loss: 2.697267552216848
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.68
    gpu_util_percent0: 0.3016666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703174635431882
    mean_env_wait_ms: 1.2000828044668304
    mean_inference_ms: 4.333098661982802
    mean_raw_obs_processing_ms: 0.3795216504483916
  time_since_restore: 3123.307863473892
  time_this_iter_s: 25.970422506332397
  time_total_s: 3123.307863473892
  timers:
    learn_throughput: 8662.612
    learn_time_ms: 18677.045
    sample_throughput: 23299.454
    sample_time_ms: 6944.026
    update_time_ms: 40.146
  timestamp: 1602629981
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    121 |          3123.31 | 19576832 |  285.192 |              329.566 |              160.172 |            772.054 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3101.5473352999015
    time_step_min: 2809
  date: 2020-10-13_23-00-08
  done: false
  episode_len_mean: 771.9876310519496
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.3478869267537
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 25467
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.17071770379940668
        entropy_coeff: 0.0005000000000000001
        kl: 0.003886608600926896
        model: {}
        policy_loss: -0.00895706876569117
        total_loss: 3.2321362495422363
        vf_explained_var: 0.9915964603424072
        vf_loss: 3.2411786715189614
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.736666666666665
    gpu_util_percent0: 0.297
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470257981042948
    mean_env_wait_ms: 1.2001127528289581
    mean_inference_ms: 4.332670462113646
    mean_raw_obs_processing_ms: 0.37949944936257235
  time_since_restore: 3149.263114452362
  time_this_iter_s: 25.95525097846985
  time_total_s: 3149.263114452362
  timers:
    learn_throughput: 8664.024
    learn_time_ms: 18674.002
    sample_throughput: 23276.76
    sample_time_ms: 6950.796
    update_time_ms: 31.268
  timestamp: 1602630008
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    122 |          3149.26 | 19738624 |  285.348 |              329.566 |              160.172 |            771.988 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3100.0367947669665
    time_step_min: 2809
  date: 2020-10-13_23-00-34
  done: false
  episode_len_mean: 771.9261807580175
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.57558677150513
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 258
  episodes_total: 25725
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.15733924259742102
        entropy_coeff: 0.0005000000000000001
        kl: 0.004441594860206048
        model: {}
        policy_loss: -0.008154638237707937
        total_loss: 3.6197384198506675
        vf_explained_var: 0.9923160672187805
        vf_loss: 3.6279717286427817
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.733333333333338
    gpu_util_percent0: 0.3193333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470141675480668
    mean_env_wait_ms: 1.200144152578641
    mean_inference_ms: 4.332053689748924
    mean_raw_obs_processing_ms: 0.37946306565159177
  time_since_restore: 3174.877126932144
  time_this_iter_s: 25.614012479782104
  time_total_s: 3174.877126932144
  timers:
    learn_throughput: 8676.985
    learn_time_ms: 18646.108
    sample_throughput: 23295.697
    sample_time_ms: 6945.145
    update_time_ms: 30.002
  timestamp: 1602630034
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    123 |          3174.88 | 19900416 |  285.576 |              329.566 |              160.172 |            771.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3098.8659012717926
    time_step_min: 2809
  date: 2020-10-13_23-01-00
  done: false
  episode_len_mean: 771.8986530817027
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.7483998255099
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 25911
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.1461001473168532
        entropy_coeff: 0.0005000000000000001
        kl: 0.004736314256054659
        model: {}
        policy_loss: -0.009588659126166021
        total_loss: 2.083389868338903
        vf_explained_var: 0.9941903948783875
        vf_loss: 2.093051532904307
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.69666666666667
    gpu_util_percent0: 0.3436666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700708429537324
    mean_env_wait_ms: 1.2001692963057002
    mean_inference_ms: 4.33161379259016
    mean_raw_obs_processing_ms: 0.3794395602785334
  time_since_restore: 3200.6540780067444
  time_this_iter_s: 25.77695107460022
  time_total_s: 3200.6540780067444
  timers:
    learn_throughput: 8675.089
    learn_time_ms: 18650.183
    sample_throughput: 23315.329
    sample_time_ms: 6939.297
    update_time_ms: 28.854
  timestamp: 1602630060
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    124 |          3200.65 | 20062208 |  285.748 |              329.566 |              160.172 |            771.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3097.7743235463445
    time_step_min: 2809
  date: 2020-10-13_23-01-26
  done: false
  episode_len_mean: 771.8722841705943
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.9107955053466
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 26097
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.1647727092107137
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038835582866643867
        model: {}
        policy_loss: -0.0076420325397824245
        total_loss: 2.931694527467092
        vf_explained_var: 0.99238520860672
        vf_loss: 2.939418931802114
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000002
    gpu_util_percent0: 0.3463333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700124042426538
    mean_env_wait_ms: 1.2001930734551252
    mean_inference_ms: 4.331220093563697
    mean_raw_obs_processing_ms: 0.3794189316853891
  time_since_restore: 3226.4501461982727
  time_this_iter_s: 25.79606819152832
  time_total_s: 3226.4501461982727
  timers:
    learn_throughput: 8679.493
    learn_time_ms: 18640.72
    sample_throughput: 23246.219
    sample_time_ms: 6959.928
    update_time_ms: 28.059
  timestamp: 1602630086
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    125 |          3226.45 | 20224000 |  285.911 |              329.566 |              160.172 |            771.872 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3096.1824157452793
    time_step_min: 2809
  date: 2020-10-13_23-01-53
  done: false
  episode_len_mean: 771.8131709722696
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.14718253434535
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 264
  episodes_total: 26361
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.1526032475133737
        entropy_coeff: 0.0005000000000000001
        kl: 0.004989463758344452
        model: {}
        policy_loss: -0.007013317488599569
        total_loss: 3.4168810645739236
        vf_explained_var: 0.9925634264945984
        vf_loss: 3.423970659573873
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.40666666666667
    gpu_util_percent0: 0.38333333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699046101754118
    mean_env_wait_ms: 1.2002183294701918
    mean_inference_ms: 4.3306269084775995
    mean_raw_obs_processing_ms: 0.3793837423877636
  time_since_restore: 3252.4017374515533
  time_this_iter_s: 25.95159125328064
  time_total_s: 3252.4017374515533
  timers:
    learn_throughput: 8672.183
    learn_time_ms: 18656.432
    sample_throughput: 23214.414
    sample_time_ms: 6969.463
    update_time_ms: 28.158
  timestamp: 1602630113
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    126 |           3252.4 | 20385792 |  286.147 |              329.566 |              160.172 |            771.813 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3095.1322214256065
    time_step_min: 2809
  date: 2020-10-13_23-02-19
  done: false
  episode_len_mean: 771.7946727950872
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.3014677536773
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 182
  episodes_total: 26543
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.14293036858240762
        entropy_coeff: 0.0005000000000000001
        kl: 0.003728000020297865
        model: {}
        policy_loss: -0.008170336106559262
        total_loss: 2.9872645139694214
        vf_explained_var: 0.9919014573097229
        vf_loss: 2.9955062667528787
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34
    gpu_util_percent0: 0.373
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698369240941134
    mean_env_wait_ms: 1.2002371648956052
    mean_inference_ms: 4.330222611066428
    mean_raw_obs_processing_ms: 0.3793619331311876
  time_since_restore: 3278.5696742534637
  time_this_iter_s: 26.1679368019104
  time_total_s: 3278.5696742534637
  timers:
    learn_throughput: 8661.275
    learn_time_ms: 18679.929
    sample_throughput: 23132.187
    sample_time_ms: 6994.237
    update_time_ms: 30.013
  timestamp: 1602630139
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    127 |          3278.57 | 20547584 |  286.301 |              329.566 |              160.172 |            771.795 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3094.0028848675584
    time_step_min: 2809
  date: 2020-10-13_23-02-46
  done: false
  episode_len_mean: 771.7503086073392
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.4713083024159
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 26733
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.15876951068639755
        entropy_coeff: 0.0005000000000000001
        kl: 0.004037241781285654
        model: {}
        policy_loss: -0.00954190470414081
        total_loss: 2.4400351444880166
        vf_explained_var: 0.9935087561607361
        vf_loss: 2.4496564467748008
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20322580645162
    gpu_util_percent0: 0.31806451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697847180967635
    mean_env_wait_ms: 1.2002571626632048
    mean_inference_ms: 4.3298530850518855
    mean_raw_obs_processing_ms: 0.37934306336162626
  time_since_restore: 3304.617014169693
  time_this_iter_s: 26.047339916229248
  time_total_s: 3304.617014169693
  timers:
    learn_throughput: 8646.701
    learn_time_ms: 18711.414
    sample_throughput: 23111.444
    sample_time_ms: 7000.514
    update_time_ms: 27.935
  timestamp: 1602630166
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    128 |          3304.62 | 20709376 |  286.471 |              329.566 |              160.172 |             771.75 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3092.5000927678207
    time_step_min: 2809
  date: 2020-10-13_23-03-12
  done: false
  episode_len_mean: 771.7054573746805
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.6977252799193
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 258
  episodes_total: 26991
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.14370925227801004
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035917371860705316
        model: {}
        policy_loss: -0.008402867129613393
        total_loss: 3.3414038022359214
        vf_explained_var: 0.9927160739898682
        vf_loss: 3.349878489971161
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.166666666666668
    gpu_util_percent0: 0.3323333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696809307296885
    mean_env_wait_ms: 1.2002733640703107
    mean_inference_ms: 4.329292186642835
    mean_raw_obs_processing_ms: 0.3793088923940135
  time_since_restore: 3330.6263251304626
  time_this_iter_s: 26.009310960769653
  time_total_s: 3330.6263251304626
  timers:
    learn_throughput: 8648.348
    learn_time_ms: 18707.851
    sample_throughput: 23087.291
    sample_time_ms: 7007.838
    update_time_ms: 28.118
  timestamp: 1602630192
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    129 |          3330.63 | 20871168 |  286.698 |              329.566 |              160.172 |            771.705 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3091.4036044668856
    time_step_min: 2809
  date: 2020-10-13_23-03-39
  done: false
  episode_len_mean: 771.6939098436062
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.8660942451191
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 27175
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.13174298902352652
        entropy_coeff: 0.0005000000000000001
        kl: 0.004347070857572059
        model: {}
        policy_loss: -0.008877743481813619
        total_loss: 2.0630916754404702
        vf_explained_var: 0.9942089915275574
        vf_loss: 2.072035312652588
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.846666666666668
    gpu_util_percent0: 0.372
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696143503838877
    mean_env_wait_ms: 1.2002879165947773
    mean_inference_ms: 4.328902978379641
    mean_raw_obs_processing_ms: 0.3792879367134783
  time_since_restore: 3356.207436323166
  time_this_iter_s: 25.581111192703247
  time_total_s: 3356.207436323166
  timers:
    learn_throughput: 8654.835
    learn_time_ms: 18693.829
    sample_throughput: 23135.387
    sample_time_ms: 6993.27
    update_time_ms: 26.213
  timestamp: 1602630219
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    130 |          3356.21 | 21032960 |  286.866 |              329.566 |              160.172 |            771.694 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3090.298817759233
    time_step_min: 2798
  date: 2020-10-13_23-04-05
  done: false
  episode_len_mean: 771.6669590322698
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.03212588554106
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 27363
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.1517644263803959
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039003227720968425
        model: {}
        policy_loss: -0.009901351989052879
        total_loss: 2.3748079538345337
        vf_explained_var: 0.9937583804130554
        vf_loss: 2.3847851554552713
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.646666666666658
    gpu_util_percent0: 0.2693333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695644800144564
    mean_env_wait_ms: 1.2003019412613425
    mean_inference_ms: 4.328548515652384
    mean_raw_obs_processing_ms: 0.3792688693167941
  time_since_restore: 3382.2961666584015
  time_this_iter_s: 26.088730335235596
  time_total_s: 3382.2961666584015
  timers:
    learn_throughput: 8644.274
    learn_time_ms: 18716.668
    sample_throughput: 23180.063
    sample_time_ms: 6979.791
    update_time_ms: 27.793
  timestamp: 1602630245
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    131 |           3382.3 | 21194752 |  287.032 |              331.232 |              160.172 |            771.667 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3088.800899234925
    time_step_min: 2798
  date: 2020-10-13_23-04-31
  done: false
  episode_len_mean: 771.6471887332102
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.2579591944206
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 258
  episodes_total: 27621
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.13860419020056725
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040165691170841455
        model: {}
        policy_loss: -0.011186293248708049
        total_loss: 2.730085849761963
        vf_explained_var: 0.9940776824951172
        vf_loss: 2.741341511408488
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.646666666666665
    gpu_util_percent0: 0.3576666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694648704460536
    mean_env_wait_ms: 1.2003127703951963
    mean_inference_ms: 4.32802872743887
    mean_raw_obs_processing_ms: 0.3792373548035904
  time_since_restore: 3407.48680973053
  time_this_iter_s: 25.190643072128296
  time_total_s: 3407.48680973053
  timers:
    learn_throughput: 8676.019
    learn_time_ms: 18648.185
    sample_throughput: 23204.988
    sample_time_ms: 6972.294
    update_time_ms: 28.16
  timestamp: 1602630271
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    132 |          3407.49 | 21356544 |  287.258 |              331.232 |              160.172 |            771.647 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3087.696380334954
    time_step_min: 2798
  date: 2020-10-13_23-04-57
  done: false
  episode_len_mean: 771.638256554105
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.42040972896507
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 27807
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.12539237551391125
        entropy_coeff: 0.0005000000000000001
        kl: 0.003125751914922148
        model: {}
        policy_loss: -0.00843535780829067
        total_loss: 2.545394460360209
        vf_explained_var: 0.9930445551872253
        vf_loss: 2.553892433643341
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.813793103448276
    gpu_util_percent0: 0.3262068965517241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469401575995169
    mean_env_wait_ms: 1.200320844947038
    mean_inference_ms: 4.32764759400037
    mean_raw_obs_processing_ms: 0.37921678209806775
  time_since_restore: 3433.0267293453217
  time_this_iter_s: 25.53991961479187
  time_total_s: 3433.0267293453217
  timers:
    learn_throughput: 8677.767
    learn_time_ms: 18644.428
    sample_throughput: 23216.426
    sample_time_ms: 6968.859
    update_time_ms: 27.722
  timestamp: 1602630297
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    133 |          3433.03 | 21518336 |   287.42 |              331.232 |              160.172 |            771.638 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3086.6044148688775
    time_step_min: 2798
  date: 2020-10-13_23-05-23
  done: false
  episode_len_mean: 771.6435894687958
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.5825417393309
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 27993
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.143315768490235
        entropy_coeff: 0.0005000000000000001
        kl: 0.003707238104349623
        model: {}
        policy_loss: -0.009574485285459863
        total_loss: 2.5298949480056763
        vf_explained_var: 0.9933602213859558
        vf_loss: 2.539541184902191
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.593333333333337
    gpu_util_percent0: 0.3506666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693555030328323
    mean_env_wait_ms: 1.200331362406916
    mean_inference_ms: 4.327312130144846
    mean_raw_obs_processing_ms: 0.37919938174624185
  time_since_restore: 3458.6401920318604
  time_this_iter_s: 25.613462686538696
  time_total_s: 3458.6401920318604
  timers:
    learn_throughput: 8679.114
    learn_time_ms: 18641.534
    sample_throughput: 23263.246
    sample_time_ms: 6954.833
    update_time_ms: 27.764
  timestamp: 1602630323
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    134 |          3458.64 | 21680128 |  287.583 |              331.232 |              160.172 |            771.644 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3085.170932293513
    time_step_min: 2798
  date: 2020-10-13_23-05-49
  done: false
  episode_len_mean: 771.6557765821889
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.80680548941206
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 259
  episodes_total: 28252
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.13739688570300737
        entropy_coeff: 0.0005000000000000001
        kl: 0.004209300541939835
        model: {}
        policy_loss: -0.00774512122249386
        total_loss: 3.0571309129397073
        vf_explained_var: 0.9934402108192444
        vf_loss: 3.064944644769033
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33666666666667
    gpu_util_percent0: 0.32566666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692604664765951
    mean_env_wait_ms: 1.2003333603615605
    mean_inference_ms: 4.326819660582067
    mean_raw_obs_processing_ms: 0.3791690754424299
  time_since_restore: 3484.2988390922546
  time_this_iter_s: 25.658647060394287
  time_total_s: 3484.2988390922546
  timers:
    learn_throughput: 8679.044
    learn_time_ms: 18641.684
    sample_throughput: 23313.113
    sample_time_ms: 6939.957
    update_time_ms: 28.429
  timestamp: 1602630349
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    135 |           3484.3 | 21841920 |  287.807 |              331.232 |              160.172 |            771.656 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3084.0746205585097
    time_step_min: 2798
  date: 2020-10-13_23-06-16
  done: false
  episode_len_mean: 771.6721755335982
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.97063713544605
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 28439
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.12631800646583238
        entropy_coeff: 0.0005000000000000001
        kl: 0.005201198354673882
        model: {}
        policy_loss: -0.009248104861399042
        total_loss: 1.9554169476032257
        vf_explained_var: 0.994600772857666
        vf_loss: 1.964728186527888
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45666666666667
    gpu_util_percent0: 0.32366666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469197432882847
    mean_env_wait_ms: 1.200336667825534
    mean_inference_ms: 4.32645220702707
    mean_raw_obs_processing_ms: 0.37914882781772685
  time_since_restore: 3510.127515077591
  time_this_iter_s: 25.828675985336304
  time_total_s: 3510.127515077591
  timers:
    learn_throughput: 8680.69
    learn_time_ms: 18638.15
    sample_throughput: 23340.425
    sample_time_ms: 6931.836
    update_time_ms: 26.763
  timestamp: 1602630376
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    136 |          3510.13 | 22003712 |  287.971 |              331.232 |              160.172 |            771.672 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3083.043254593176
    time_step_min: 2798
  date: 2020-10-13_23-06-42
  done: false
  episode_len_mean: 771.7036726421358
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.1272765393742
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 178
  episodes_total: 28617
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.14297903329133987
        entropy_coeff: 0.0005000000000000001
        kl: 0.00392979724953572
        model: {}
        policy_loss: -0.00978231765596623
        total_loss: 2.057941049337387
        vf_explained_var: 0.9943475127220154
        vf_loss: 2.067794839541117
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.983333333333334
    gpu_util_percent0: 0.3436666666666668
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691513074969953
    mean_env_wait_ms: 1.2003382398459017
    mean_inference_ms: 4.3261412421020955
    mean_raw_obs_processing_ms: 0.37913202788701716
  time_since_restore: 3535.8615260124207
  time_this_iter_s: 25.734010934829712
  time_total_s: 3535.8615260124207
  timers:
    learn_throughput: 8690.663
    learn_time_ms: 18616.762
    sample_throughput: 23438.627
    sample_time_ms: 6902.793
    update_time_ms: 24.652
  timestamp: 1602630402
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    137 |          3535.86 | 22165504 |  288.127 |              331.232 |              160.172 |            771.704 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3081.633099764118
    time_step_min: 2798
  date: 2020-10-13_23-07-09
  done: false
  episode_len_mean: 771.7493591963977
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.3412493483501
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 253
  episodes_total: 28870
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.14113491649429002
        entropy_coeff: 0.0005000000000000001
        kl: 0.004532522909964125
        model: {}
        policy_loss: -0.010601744269176075
        total_loss: 2.8867114981015525
        vf_explained_var: 0.9938721656799316
        vf_loss: 2.897383769353231
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.530000000000005
    gpu_util_percent0: 0.40199999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469071024293356
    mean_env_wait_ms: 1.2003358221851617
    mean_inference_ms: 4.325690296960567
    mean_raw_obs_processing_ms: 0.37910488135770004
  time_since_restore: 3561.6772062778473
  time_this_iter_s: 25.815680265426636
  time_total_s: 3561.6772062778473
  timers:
    learn_throughput: 8695.328
    learn_time_ms: 18606.773
    sample_throughput: 23490.867
    sample_time_ms: 6887.443
    update_time_ms: 32.55
  timestamp: 1602630429
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    138 |          3561.68 | 22327296 |  288.341 |              331.232 |              160.172 |            771.749 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3080.5477347114556
    time_step_min: 2798
  date: 2020-10-13_23-07-35
  done: false
  episode_len_mean: 771.7780988750129
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.5081676502876
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 29067
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.12842008223136267
        entropy_coeff: 0.0005000000000000001
        kl: 0.0077807044532770915
        model: {}
        policy_loss: -0.009057981437460208
        total_loss: 2.194779713948568
        vf_explained_var: 0.9943049550056458
        vf_loss: 2.2039019068082175
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.240000000000006
    gpu_util_percent0: 0.3433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690043040172546
    mean_env_wait_ms: 1.2003337650631014
    mean_inference_ms: 4.325323461139351
    mean_raw_obs_processing_ms: 0.3790842820631682
  time_since_restore: 3587.7017447948456
  time_this_iter_s: 26.02453851699829
  time_total_s: 3587.7017447948456
  timers:
    learn_throughput: 8690.883
    learn_time_ms: 18616.291
    sample_throughput: 23528.26
    sample_time_ms: 6876.497
    update_time_ms: 34.017
  timestamp: 1602630455
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: 68fe0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | RUNNING  | 172.17.0.4:35442 |    139 |           3587.7 | 22489088 |  288.508 |              331.232 |              160.172 |            771.778 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_68fe0_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3079.5207534246574
    time_step_min: 2798
  date: 2020-10-13_23-08-02
  done: true
  episode_len_mean: 771.8019971274194
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.66396990906253
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 29242
  experiment_id: 3a3356390f4143dc9f271ffca9d66852
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.13905109713474909
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038545686402358115
        model: {}
        policy_loss: -0.008588418323294414
        total_loss: 2.069778402646383
        vf_explained_var: 0.9941542148590088
        vf_loss: 2.078436324993769
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73333333333333
    gpu_util_percent0: 0.32533333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 35442
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689529483444913
    mean_env_wait_ms: 1.2003289070926162
    mean_inference_ms: 4.32501866127833
    mean_raw_obs_processing_ms: 0.37906737048993516
  time_since_restore: 3613.667244911194
  time_this_iter_s: 25.965500116348267
  time_total_s: 3613.667244911194
  timers:
    learn_throughput: 8679.019
    learn_time_ms: 18641.738
    sample_throughput: 23487.71
    sample_time_ms: 6888.368
    update_time_ms: 33.928
  timestamp: 1602630482
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: 68fe0_00000
  
2020-10-13 23:08:02,785	WARNING util.py:136 -- The `process_trial` operation took 0.5062046051025391 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | TERMINATED |       |    140 |          3613.67 | 22650880 |  288.664 |              331.232 |              160.172 |            771.802 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 27.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_68fe0_00000 | TERMINATED |       |    140 |          3613.67 | 22650880 |  288.664 |              331.232 |              160.172 |            771.802 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


