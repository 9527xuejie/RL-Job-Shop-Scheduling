2020-10-11 01:29:07,148	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8266[39m[22m
== Status ==
Memory usage on this node: 32.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_288fe_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=54948)[0m 2020-10-11 01:29:10,031	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=54988)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54988)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54940)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54940)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54873)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54873)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54882)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54882)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54959)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54959)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54937)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54937)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54932)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54932)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54961)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54961)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54947)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54947)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54996)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54996)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54957)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54957)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54945)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54945)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=55009)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=55009)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54986)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54986)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=55012)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=55012)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54963)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54963)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54949)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54949)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54995)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54995)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54979)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54979)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54968)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54968)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54896)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54896)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54901)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54901)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=55003)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=55003)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54877)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54877)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54954)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54954)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54875)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54875)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54922)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54922)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54869)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54869)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54971)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54971)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54977)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54977)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54966)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54966)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54938)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54938)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54887)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54887)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54939)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54939)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54871)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54871)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54936)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54936)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54981)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54981)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54885)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54885)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54920)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54920)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54943)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54943)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=55010)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=55010)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54953)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54953)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54906)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54906)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54886)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54886)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54894)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54894)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54969)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54969)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=55018)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=55018)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54975)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54975)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54872)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54872)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54884)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54884)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54868)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54868)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54951)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54951)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54883)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54883)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54962)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54962)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54879)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54879)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54904)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54904)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54911)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54911)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54973)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54973)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54952)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54952)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54878)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54878)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54870)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54870)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54934)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54934)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54982)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54982)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54874)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54874)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54900)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54900)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54944)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54944)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54880)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54880)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54891)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54891)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54958)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54958)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54942)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54942)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54903)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54903)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54956)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54956)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54927)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54927)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54929)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54929)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=55016)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=55016)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54908)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54908)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54965)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54965)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54888)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54888)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=54897)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=54897)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-11_01-29-48
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.1781494532312666
        entropy_coeff: 0.0
        kl: 0.011431540828198195
        model: {}
        policy_loss: -0.01341040903103671
        total_loss: 9.507874420710973
        vf_explained_var: 0.7608073353767395
        vf_loss: 9.518998350415911
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.4974358974359
    gpu_util_percent0: 0.3025641025641026
    gpu_util_percent1: 0.0002564102564102564
    gpu_util_percent2: 0.0002564102564102564
    ram_util_percent: 6.276923076923078
    vram_util_percent0: 0.19015902649881009
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17221812077163362
    mean_env_wait_ms: 1.2009141208875986
    mean_inference_ms: 5.9990717636725535
    mean_raw_obs_processing_ms: 0.461939861466024
  time_since_restore: 32.56083583831787
  time_this_iter_s: 32.56083583831787
  time_total_s: 32.56083583831787
  timers:
    learn_throughput: 7004.243
    learn_time_ms: 23099.143
    sample_throughput: 17226.901
    sample_time_ms: 9391.823
    update_time_ms: 27.912
  timestamp: 1602379788
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      1 |          32.5608 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3611.2638888888887
    time_step_min: 3311
  date: 2020-10-11_01-30-19
  done: false
  episode_len_mean: 878.5569620253165
  episode_reward_max: 271.47474747474746
  episode_reward_mean: 217.23663853727126
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.14065716947828
        entropy_coeff: 0.0
        kl: 0.011749034680958306
        model: {}
        policy_loss: -0.012738524324959144
        total_loss: 8.230057614190239
        vf_explained_var: 0.8967045545578003
        vf_loss: 8.240446090698242
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.862162162162164
    gpu_util_percent0: 0.357027027027027
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.464864864864865
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16741770823786783
    mean_env_wait_ms: 1.1999588189610535
    mean_inference_ms: 5.6914319441521215
    mean_raw_obs_processing_ms: 0.44895511807879357
  time_since_restore: 63.48217296600342
  time_this_iter_s: 30.921337127685547
  time_total_s: 63.48217296600342
  timers:
    learn_throughput: 7063.44
    learn_time_ms: 22905.552
    sample_throughput: 18475.275
    sample_time_ms: 8757.217
    update_time_ms: 33.68
  timestamp: 1602379819
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      2 |          63.4822 | 323584 |  217.237 |              271.475 |              145.717 |            878.557 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3612.9058295964123
    time_step_min: 3210
  date: 2020-10-11_01-30-49
  done: false
  episode_len_mean: 866.3649789029536
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 218.51125175808704
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.116195218903678
        entropy_coeff: 0.0
        kl: 0.012675430226538862
        model: {}
        policy_loss: -0.016366921065907394
        total_loss: 7.88534368787493
        vf_explained_var: 0.9417376518249512
        vf_loss: 7.899175541741507
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.245945945945948
    gpu_util_percent0: 0.40189189189189184
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.481081081081081
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16430757546551422
    mean_env_wait_ms: 1.2016672250674656
    mean_inference_ms: 5.4751554274911305
    mean_raw_obs_processing_ms: 0.43938531242101647
  time_since_restore: 94.00425434112549
  time_this_iter_s: 30.52208137512207
  time_total_s: 94.00425434112549
  timers:
    learn_throughput: 7064.519
    learn_time_ms: 22902.053
    sample_throughput: 19369.608
    sample_time_ms: 8352.88
    update_time_ms: 33.086
  timestamp: 1602379849
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      3 |          94.0043 | 485376 |  218.511 |              279.657 |              145.717 |            866.365 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4100
    time_step_mean: 3616.7168874172185
    time_step_min: 3210
  date: 2020-10-11_01-31-19
  done: false
  episode_len_mean: 857.3623417721519
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 218.78847014448263
  episode_reward_min: 144.80808080808097
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.0791470493589128
        entropy_coeff: 0.0
        kl: 0.01226102833503059
        model: {}
        policy_loss: -0.01725318482411759
        total_loss: 8.04550211770194
        vf_explained_var: 0.9615011215209961
        vf_loss: 8.06030283655439
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.858333333333334
    gpu_util_percent0: 0.3647222222222222
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.480555555555556
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16219245652458286
    mean_env_wait_ms: 1.2047293918336754
    mean_inference_ms: 5.323046989167906
    mean_raw_obs_processing_ms: 0.432298907700675
  time_since_restore: 123.85995602607727
  time_this_iter_s: 29.855701684951782
  time_total_s: 123.85995602607727
  timers:
    learn_throughput: 7091.694
    learn_time_ms: 22814.294
    sample_throughput: 20085.275
    sample_time_ms: 8055.254
    update_time_ms: 47.013
  timestamp: 1602379879
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      4 |           123.86 | 647168 |  218.788 |              279.657 |              144.808 |            857.362 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4100
    time_step_mean: 3610.6992316136116
    time_step_min: 3210
  date: 2020-10-11_01-31-49
  done: false
  episode_len_mean: 844.863684771033
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 219.42291928873382
  episode_reward_min: 144.80808080808097
  episodes_this_iter: 307
  episodes_total: 939
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.0480106217520577
        entropy_coeff: 0.0
        kl: 0.01192767299445612
        model: {}
        policy_loss: -0.0159328430059499
        total_loss: 10.980154173714775
        vf_explained_var: 0.9769945740699768
        vf_loss: 10.99370173045567
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.70277777777778
    gpu_util_percent0: 0.3811111111111112
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4833333333333325
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1596100248096779
    mean_env_wait_ms: 1.2115224466895005
    mean_inference_ms: 5.137985428476765
    mean_raw_obs_processing_ms: 0.42400573435427735
  time_since_restore: 153.9596917629242
  time_this_iter_s: 30.099735736846924
  time_total_s: 153.9596917629242
  timers:
    learn_throughput: 7109.104
    learn_time_ms: 22758.425
    sample_throughput: 20404.821
    sample_time_ms: 7929.106
    update_time_ms: 55.222
  timestamp: 1602379909
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      5 |           153.96 | 808960 |  219.423 |              279.657 |              144.808 |            844.864 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4100
    time_step_mean: 3603.7411873840447
    time_step_min: 3210
  date: 2020-10-11_01-32-19
  done: false
  episode_len_mean: 840.8905967450271
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 220.36002886002873
  episode_reward_min: 144.80808080808097
  episodes_this_iter: 167
  episodes_total: 1106
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.0219160403524126
        entropy_coeff: 0.0
        kl: 0.013516566782657589
        model: {}
        policy_loss: -0.016949521843343973
        total_loss: 5.318371261869158
        vf_explained_var: 0.9853286743164062
        vf_loss: 5.332617316927228
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.374285714285712
    gpu_util_percent0: 0.3685714285714286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.491428571428571
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15869279757402055
    mean_env_wait_ms: 1.2141916275056026
    mean_inference_ms: 5.069749679248956
    mean_raw_obs_processing_ms: 0.4209450031743705
  time_since_restore: 183.77970433235168
  time_this_iter_s: 29.82001256942749
  time_total_s: 183.77970433235168
  timers:
    learn_throughput: 7126.583
    learn_time_ms: 22702.606
    sample_throughput: 20667.381
    sample_time_ms: 7828.374
    update_time_ms: 49.383
  timestamp: 1602379939
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      6 |           183.78 | 970752 |   220.36 |              279.657 |              144.808 |            840.891 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4100
    time_step_mean: 3600.760517799353
    time_step_min: 3210
  date: 2020-10-11_01-32-50
  done: false
  episode_len_mean: 838.9066455696203
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 220.6512434471294
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.9845942642007556
        entropy_coeff: 0.0
        kl: 0.012624194180326802
        model: {}
        policy_loss: -0.018214210368958966
        total_loss: 4.955698524202619
        vf_explained_var: 0.9882784485816956
        vf_loss: 4.971387965338571
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716216216216214
    gpu_util_percent0: 0.3481081081081081
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4837837837837835
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15795799091533028
    mean_env_wait_ms: 1.2160033492096314
    mean_inference_ms: 5.01516095885512
    mean_raw_obs_processing_ms: 0.41844081812046513
  time_since_restore: 213.85779309272766
  time_this_iter_s: 30.078088760375977
  time_total_s: 213.85779309272766
  timers:
    learn_throughput: 7129.397
    learn_time_ms: 22693.644
    sample_throughput: 20864.223
    sample_time_ms: 7754.518
    update_time_ms: 45.113
  timestamp: 1602379970
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      7 |          213.858 | 1132544 |  220.651 |              279.657 |              132.838 |            838.907 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4100
    time_step_mean: 3596.345050215208
    time_step_min: 3210
  date: 2020-10-11_01-33-20
  done: false
  episode_len_mean: 837.760900140647
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 221.3778928525763
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.9421724166188922
        entropy_coeff: 0.0
        kl: 0.011364509484597616
        model: {}
        policy_loss: -0.01768286805599928
        total_loss: 4.553801230021885
        vf_explained_var: 0.9901912808418274
        vf_loss: 4.5692110402243475
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.111428571428572
    gpu_util_percent0: 0.32685714285714285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.488571428571428
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.157327460068182
    mean_env_wait_ms: 1.217517073094542
    mean_inference_ms: 4.968392189349639
    mean_raw_obs_processing_ms: 0.4162613208625761
  time_since_restore: 243.7252004146576
  time_this_iter_s: 29.86740732192993
  time_total_s: 243.7252004146576
  timers:
    learn_throughput: 7131.053
    learn_time_ms: 22688.374
    sample_throughput: 21071.749
    sample_time_ms: 7678.148
    update_time_ms: 42.336
  timestamp: 1602380000
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      8 |          243.725 | 1294336 |  221.378 |              279.657 |              132.838 |            837.761 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3583.748050389922
    time_step_min: 3210
  date: 2020-10-11_01-33-50
  done: false
  episode_len_mean: 835.5958702064896
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 223.28658264056486
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 273
  episodes_total: 1695
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.8857077402727944
        entropy_coeff: 0.0
        kl: 0.01054007579971637
        model: {}
        policy_loss: -0.014801818411797285
        total_loss: 5.526667322431292
        vf_explained_var: 0.9923786520957947
        vf_loss: 5.53936106818063
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.680555555555557
    gpu_util_percent0: 0.32444444444444437
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.480555555555555
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1564368120719112
    mean_env_wait_ms: 1.2197697736331257
    mean_inference_ms: 4.903092840047876
    mean_raw_obs_processing_ms: 0.41318888296532835
  time_since_restore: 273.64607548713684
  time_this_iter_s: 29.920875072479248
  time_total_s: 273.64607548713684
  timers:
    learn_throughput: 7129.363
    learn_time_ms: 22693.754
    sample_throughput: 21246.798
    sample_time_ms: 7614.888
    update_time_ms: 39.868
  timestamp: 1602380030
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |      9 |          273.646 | 1456128 |  223.287 |              279.657 |              132.838 |            835.596 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3575.1049250535334
    time_step_min: 3210
  date: 2020-10-11_01-34-19
  done: false
  episode_len_mean: 834.6629746835443
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 224.29400012786084
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 201
  episodes_total: 1896
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.8550363779067993
        entropy_coeff: 0.0
        kl: 0.010378314248685325
        model: {}
        policy_loss: -0.014523112564347684
        total_loss: 3.7496881314686368
        vf_explained_var: 0.9930557608604431
        vf_loss: 3.7621355056762695
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.338888888888892
    gpu_util_percent0: 0.35611111111111104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4944444444444445
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15593366383508903
    mean_env_wait_ms: 1.221139731490717
    mean_inference_ms: 4.8650326098510135
    mean_raw_obs_processing_ms: 0.4114245792446973
  time_since_restore: 303.5224995613098
  time_this_iter_s: 29.876424074172974
  time_total_s: 303.5224995613098
  timers:
    learn_throughput: 7133.879
    learn_time_ms: 22679.387
    sample_throughput: 21371.65
    sample_time_ms: 7570.403
    update_time_ms: 39.462
  timestamp: 1602380059
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     10 |          303.522 | 1617920 |  224.294 |              279.657 |              132.838 |            834.663 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3569.836130306022
    time_step_min: 3210
  date: 2020-10-11_01-34-49
  done: false
  episode_len_mean: 834.1548198636806
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 224.9890285523196
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.8195758036204747
        entropy_coeff: 0.0
        kl: 0.010642150416970253
        model: {}
        policy_loss: -0.015840005861329182
        total_loss: 3.158179930278233
        vf_explained_var: 0.9939475059509277
        vf_loss: 3.1718914679118564
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.180000000000003
    gpu_util_percent0: 0.3637142857142857
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.491428571428571
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.155580858423928
    mean_env_wait_ms: 1.222069248579385
    mean_inference_ms: 4.838798870869561
    mean_raw_obs_processing_ms: 0.4102025729070071
  time_since_restore: 333.40801429748535
  time_this_iter_s: 29.885514736175537
  time_total_s: 333.40801429748535
  timers:
    learn_throughput: 7150.028
    learn_time_ms: 22628.163
    sample_throughput: 22001.111
    sample_time_ms: 7353.81
    update_time_ms: 38.825
  timestamp: 1602380089
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     11 |          333.408 | 1779712 |  224.989 |              279.657 |              132.838 |            834.155 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3564.019688644689
    time_step_min: 3210
  date: 2020-10-11_01-35-20
  done: false
  episode_len_mean: 833.9773960216999
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 225.83710979597046
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.7897078905786786
        entropy_coeff: 0.0
        kl: 0.010068052647901433
        model: {}
        policy_loss: -0.0172844803892076
        total_loss: 2.847164119992937
        vf_explained_var: 0.994350016117096
        vf_loss: 2.8624349662235806
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98611111111111
    gpu_util_percent0: 0.39444444444444443
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.491666666666666
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15526674160563847
    mean_env_wait_ms: 1.222889310065686
    mean_inference_ms: 4.815156785893617
    mean_raw_obs_processing_ms: 0.4090972286993646
  time_since_restore: 363.51795172691345
  time_this_iter_s: 30.1099374294281
  time_total_s: 363.51795172691345
  timers:
    learn_throughput: 7153.029
    learn_time_ms: 22618.671
    sample_throughput: 22216.833
    sample_time_ms: 7282.406
    update_time_ms: 37.733
  timestamp: 1602380120
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     12 |          363.518 | 1941504 |  225.837 |              279.657 |              132.838 |            833.977 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3558.977253580455
    time_step_min: 3210
  date: 2020-10-11_01-35-50
  done: false
  episode_len_mean: 834.255620316403
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 226.54672032565452
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 190
  episodes_total: 2402
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.7421867719718388
        entropy_coeff: 0.0
        kl: 0.009045395667531661
        model: {}
        policy_loss: -0.013870293257891067
        total_loss: 3.169439264706203
        vf_explained_var: 0.995323657989502
        vf_loss: 3.181500485965184
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.202702702702698
    gpu_util_percent0: 0.3618918918918919
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.481081081081081
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1549276465477162
    mean_env_wait_ms: 1.2236474669552948
    mean_inference_ms: 4.789253140958404
    mean_raw_obs_processing_ms: 0.4078472451892863
  time_since_restore: 393.7315056324005
  time_this_iter_s: 30.21355390548706
  time_total_s: 393.7315056324005
  timers:
    learn_throughput: 7149.977
    learn_time_ms: 22628.324
    sample_throughput: 22363.19
    sample_time_ms: 7234.746
    update_time_ms: 38.209
  timestamp: 1602380150
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     13 |          393.732 | 2103296 |  226.547 |              279.657 |              132.838 |            834.256 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3551.8144523899136
    time_step_min: 3210
  date: 2020-10-11_01-36-20
  done: false
  episode_len_mean: 835.0983240223463
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 227.5738953783646
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 283
  episodes_total: 2685
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.7150351788316455
        entropy_coeff: 0.0
        kl: 0.00817285611161164
        model: {}
        policy_loss: -0.012300753267481923
        total_loss: 3.4924605914524625
        vf_explained_var: 0.9946344494819641
        vf_loss: 3.503126723425729
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.858333333333334
    gpu_util_percent0: 0.4394444444444445
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4750000000000005
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15450317273504716
    mean_env_wait_ms: 1.224581259441053
    mean_inference_ms: 4.757286688399202
    mean_raw_obs_processing_ms: 0.40638102099361545
  time_since_restore: 423.76468777656555
  time_this_iter_s: 30.03318214416504
  time_total_s: 423.76468777656555
  timers:
    learn_throughput: 7149.911
    learn_time_ms: 22628.535
    sample_throughput: 22313.28
    sample_time_ms: 7250.929
    update_time_ms: 38.697
  timestamp: 1602380180
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     14 |          423.765 | 2265088 |  227.574 |              279.657 |              132.838 |            835.098 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3549.2457386363635
    time_step_min: 3210
  date: 2020-10-11_01-36-50
  done: false
  episode_len_mean: 834.993670886076
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 228.0268720964923
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 159
  episodes_total: 2844
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.6855689287185669
        entropy_coeff: 0.0
        kl: 0.009044077513473374
        model: {}
        policy_loss: -0.014783558485630368
        total_loss: 2.4313596997942244
        vf_explained_var: 0.9955347180366516
        vf_loss: 2.4443344388689314
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.9
    gpu_util_percent0: 0.3705555555555556
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4944444444444445
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15429054869922207
    mean_env_wait_ms: 1.2250372112883354
    mean_inference_ms: 4.741339176766831
    mean_raw_obs_processing_ms: 0.4056391520613233
  time_since_restore: 453.95661520957947
  time_this_iter_s: 30.191927433013916
  time_total_s: 453.95661520957947
  timers:
    learn_throughput: 7145.581
    learn_time_ms: 22642.245
    sample_throughput: 22308.691
    sample_time_ms: 7252.42
    update_time_ms: 31.993
  timestamp: 1602380210
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     15 |          453.957 | 2426880 |  228.027 |              279.657 |              132.838 |            834.994 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3547.3708809683926
    time_step_min: 3210
  date: 2020-10-11_01-37-20
  done: false
  episode_len_mean: 834.6672218520986
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 228.515003465703
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 158
  episodes_total: 3002
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.6787012645176479
        entropy_coeff: 0.0
        kl: 0.007596756830545408
        model: {}
        policy_loss: -0.011960656347842555
        total_loss: 2.7067537137440274
        vf_explained_var: 0.9944841265678406
        vf_loss: 2.7171949999673024
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.6
    gpu_util_percent0: 0.4454285714285714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.494285714285715
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1540930925104462
    mean_env_wait_ms: 1.225469058364248
    mean_inference_ms: 4.726559965351387
    mean_raw_obs_processing_ms: 0.40494467178679755
  time_since_restore: 483.66047954559326
  time_this_iter_s: 29.703864336013794
  time_total_s: 483.66047954559326
  timers:
    learn_throughput: 7147.464
    learn_time_ms: 22636.279
    sample_throughput: 22329.883
    sample_time_ms: 7245.537
    update_time_ms: 32.061
  timestamp: 1602380240
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     16 |           483.66 | 2588672 |  228.515 |              279.657 |              132.838 |            834.667 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3544.436085431941
    time_step_min: 3210
  date: 2020-10-11_01-37-50
  done: false
  episode_len_mean: 834.3848341232227
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 228.9065058164584
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 163
  episodes_total: 3165
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.6565082754407611
        entropy_coeff: 0.0
        kl: 0.007748972957155534
        model: {}
        policy_loss: -0.01254352806634935
        total_loss: 2.5723507404327393
        vf_explained_var: 0.9954479932785034
        vf_loss: 2.583344442503793
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15833333333333
    gpu_util_percent0: 0.34444444444444433
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.491666666666666
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15390664083105507
    mean_env_wait_ms: 1.225893489736546
    mean_inference_ms: 4.7125158228347965
    mean_raw_obs_processing_ms: 0.4042844389105405
  time_since_restore: 513.5386888980865
  time_this_iter_s: 29.878209352493286
  time_total_s: 513.5386888980865
  timers:
    learn_throughput: 7149.483
    learn_time_ms: 22629.889
    sample_throughput: 22358.997
    sample_time_ms: 7236.103
    update_time_ms: 32.422
  timestamp: 1602380270
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     17 |          513.539 | 2750464 |  228.907 |              279.657 |              132.838 |            834.385 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3538.667539267016
    time_step_min: 3210
  date: 2020-10-11_01-38-20
  done: false
  episode_len_mean: 833.5790536641662
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 229.90827490135047
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 301
  episodes_total: 3466
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.6123675235680172
        entropy_coeff: 0.0
        kl: 0.008196754647152764
        model: {}
        policy_loss: -0.013380040389685226
        total_loss: 2.9476159811019897
        vf_explained_var: 0.9957508444786072
        vf_loss: 2.959356665611267
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.419444444444437
    gpu_util_percent0: 0.35472222222222227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.480555555555556
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1535962388306347
    mean_env_wait_ms: 1.2265976094395035
    mean_inference_ms: 4.689114876211699
    mean_raw_obs_processing_ms: 0.4031930558050461
  time_since_restore: 543.489030122757
  time_this_iter_s: 29.95034122467041
  time_total_s: 543.489030122757
  timers:
    learn_throughput: 7151.358
    learn_time_ms: 22623.956
    sample_throughput: 22340.551
    sample_time_ms: 7242.077
    update_time_ms: 32.293
  timestamp: 1602380300
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     18 |          543.489 | 2912256 |  229.908 |              279.657 |              132.838 |            833.579 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3534.7190793122572
    time_step_min: 3210
  date: 2020-10-11_01-38-51
  done: false
  episode_len_mean: 832.9416620803522
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 230.46392377267443
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 168
  episodes_total: 3634
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.5940072451319013
        entropy_coeff: 0.0
        kl: 0.00883752912549036
        model: {}
        policy_loss: -0.014115128840785474
        total_loss: 1.9954565508025033
        vf_explained_var: 0.9961031675338745
        vf_loss: 2.0078041638646806
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.980555555555558
    gpu_util_percent0: 0.31833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.497222222222223
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1534383005551949
    mean_env_wait_ms: 1.2270025413830323
    mean_inference_ms: 4.677601455150651
    mean_raw_obs_processing_ms: 0.40266358018369414
  time_since_restore: 573.632818698883
  time_this_iter_s: 30.1437885761261
  time_total_s: 573.632818698883
  timers:
    learn_throughput: 7156.413
    learn_time_ms: 22607.975
    sample_throughput: 22225.545
    sample_time_ms: 7279.552
    update_time_ms: 32.121
  timestamp: 1602380331
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | RUNNING  | 172.17.0.4:54948 |     19 |          573.633 | 3074048 |  230.464 |              279.657 |              132.838 |            832.942 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_288fe_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3531.3674282678003
    time_step_min: 3210
  date: 2020-10-11_01-39-21
  done: true
  episode_len_mean: 832.3241033755274
  episode_reward_max: 279.65656565656525
  episode_reward_mean: 230.99299162511184
  episode_reward_min: 132.83838383838375
  episodes_this_iter: 158
  episodes_total: 3792
  experiment_id: cc97e3bb953240da97ab3edabe739443
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.5915451858724866
        entropy_coeff: 0.0
        kl: 0.008201912644186191
        model: {}
        policy_loss: -0.013318307178061721
        total_loss: 1.7551110301698958
        vf_explained_var: 0.996185839176178
        vf_loss: 1.7667889254433768
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.602777777777778
    gpu_util_percent0: 0.35833333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.491666666666666
    vram_util_percent0: 0.20465726467694328
    vram_util_percent1: 0.0009075233687267447
    vram_util_percent2: 0.0009075233687267447
  pid: 54948
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15330026375860287
    mean_env_wait_ms: 1.2273716099348704
    mean_inference_ms: 4.667371227138014
    mean_raw_obs_processing_ms: 0.4021938508868008
  time_since_restore: 603.5742137432098
  time_this_iter_s: 29.941395044326782
  time_total_s: 603.5742137432098
  timers:
    learn_throughput: 7156.403
    learn_time_ms: 22608.007
    sample_throughput: 22190.357
    sample_time_ms: 7291.095
    update_time_ms: 32.84
  timestamp: 1602380361
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 288fe_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | TERMINATED |       |     20 |          603.574 | 3235840 |  230.993 |              279.657 |              132.838 |            832.324 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


[2m[33m(pid=raylet)[0m E1011 01:39:21.452195 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 01:39:21.452316 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 01:39:21.452370 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 01:39:21.452422 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 01:39:21.452483 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 01:39:21.452571 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 01:39:21.452610 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 01:39:21.452702 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 01:39:21.453155 54827 54827 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/537.16 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_288fe_00000 | TERMINATED |       |     20 |          603.574 | 3235840 |  230.993 |              279.657 |              132.838 |            832.324 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


