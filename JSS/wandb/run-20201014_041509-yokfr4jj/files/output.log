2020-10-14 04:15:12,931	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_dbd8f_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=66685)[0m 2020-10-14 04:15:15,760	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=66563)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66563)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66641)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66641)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66567)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66567)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66644)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66644)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66605)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66605)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66694)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66694)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66568)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66568)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66676)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66676)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66692)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66692)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66681)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66681)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66558)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66558)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66606)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66606)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66636)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66636)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66678)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66678)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66565)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66565)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66695)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66695)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66690)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66690)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66614)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66614)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66616)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66616)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66645)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66645)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66562)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66562)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66682)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66682)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66573)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66573)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66611)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66611)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66594)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66594)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66647)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66647)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66675)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66675)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66643)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66643)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66569)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66569)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66693)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66693)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66624)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66624)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66557)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66557)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66646)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66646)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66610)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66610)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66566)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66566)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66635)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66635)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66584)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66584)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66596)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66596)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66670)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66670)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66581)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66581)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66680)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66680)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66561)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66561)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66648)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66648)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66587)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66587)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66613)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66613)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66560)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66560)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66666)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66666)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66570)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66570)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66663)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66663)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66634)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66634)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66556)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66556)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66588)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66588)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66582)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66582)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66572)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66572)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66673)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66673)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66555)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66555)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66661)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66661)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66580)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66580)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66631)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66631)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66576)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66576)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66687)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66687)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66627)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66627)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66664)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66664)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66658)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66658)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66578)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66578)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66689)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66689)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66590)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66590)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66618)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66618)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66621)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66621)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66571)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66571)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66649)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66649)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66608)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66608)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66592)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66592)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66628)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66628)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66642)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66642)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66629)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66629)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66697)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66697)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66655)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66655)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=66554)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=66554)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3522.9508196721313
    time_step_min: 3235
  date: 2020-10-14_04-15-49
  done: false
  episode_len_mean: 892.2341772151899
  episode_reward_max: 254.5757575757571
  episode_reward_mean: 208.69735327963153
  episode_reward_min: 139.27272727272737
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1810161769390106
        entropy_coeff: 0.0005000000000000001
        kl: 0.003497340988057355
        model: {}
        policy_loss: -0.007581961224786937
        total_loss: 425.68299611409503
        vf_explained_var: 0.5529740452766418
        vf_loss: 425.6904652913411
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.170588235294115
    gpu_util_percent0: 0.2994117647058823
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.547058823529412
    vram_util_percent0: 0.08636872262844136
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17095153587178782
    mean_env_wait_ms: 1.168813726054772
    mean_inference_ms: 5.898888129516677
    mean_raw_obs_processing_ms: 0.4592139870342409
  time_since_restore: 28.538620948791504
  time_this_iter_s: 28.538620948791504
  time_total_s: 28.538620948791504
  timers:
    learn_throughput: 8464.868
    learn_time_ms: 19113.352
    sample_throughput: 17363.382
    sample_time_ms: 9318.0
    update_time_ms: 69.885
  timestamp: 1602648949
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      1 |          28.5386 | 161792 |  208.697 |              254.576 |              139.273 |            892.234 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3532.1642857142856
    time_step_min: 3205
  date: 2020-10-14_04-16-15
  done: false
  episode_len_mean: 889.6677215189874
  episode_reward_max: 259.1212121212121
  episode_reward_mean: 208.27157652474088
  episode_reward_min: 139.27272727272737
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1515157421429951
        entropy_coeff: 0.0005000000000000001
        kl: 0.008129845761383573
        model: {}
        policy_loss: -0.010888179541022206
        total_loss: 106.39584986368816
        vf_explained_var: 0.8193144202232361
        vf_loss: 106.40650304158528
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.493333333333332
    gpu_util_percent0: 0.39599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7466666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16556673064910027
    mean_env_wait_ms: 1.1657583449612512
    mean_inference_ms: 5.5745783280283066
    mean_raw_obs_processing_ms: 0.4430597165782889
  time_since_restore: 54.51020693778992
  time_this_iter_s: 25.971585988998413
  time_total_s: 54.51020693778992
  timers:
    learn_throughput: 8581.87
    learn_time_ms: 18852.768
    sample_throughput: 19547.879
    sample_time_ms: 8276.703
    update_time_ms: 46.014
  timestamp: 1602648975
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      2 |          54.5102 | 323584 |  208.272 |              259.121 |              139.273 |            889.668 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3539.1141552511417
    time_step_min: 3205
  date: 2020-10-14_04-16-41
  done: false
  episode_len_mean: 883.3628691983122
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 208.68073136427546
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1436680853366852
        entropy_coeff: 0.0005000000000000001
        kl: 0.008007710915990174
        model: {}
        policy_loss: -0.011135648053217059
        total_loss: 57.12984085083008
        vf_explained_var: 0.8894199728965759
        vf_loss: 57.1407470703125
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.223333333333333
    gpu_util_percent0: 0.4413333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16223110188347464
    mean_env_wait_ms: 1.165996932740463
    mean_inference_ms: 5.361080852021955
    mean_raw_obs_processing_ms: 0.43267874981785936
  time_since_restore: 80.49748945236206
  time_this_iter_s: 25.987282514572144
  time_total_s: 80.49748945236206
  timers:
    learn_throughput: 8568.116
    learn_time_ms: 18883.031
    sample_throughput: 20654.012
    sample_time_ms: 7833.442
    update_time_ms: 47.134
  timestamp: 1602649001
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      3 |          80.4975 | 485376 |  208.681 |                  267 |              136.242 |            883.363 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3544.412751677852
    time_step_min: 3205
  date: 2020-10-14_04-17-07
  done: false
  episode_len_mean: 879.0142405063291
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 207.51759685462198
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1258440613746643
        entropy_coeff: 0.0005000000000000001
        kl: 0.008104618444728354
        model: {}
        policy_loss: -0.012452503734190637
        total_loss: 42.44776312510172
        vf_explained_var: 0.9244468212127686
        vf_loss: 42.45996824900309
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.262068965517237
    gpu_util_percent0: 0.4351724137931034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1599632652833398
    mean_env_wait_ms: 1.167221722032426
    mean_inference_ms: 5.212318557907806
    mean_raw_obs_processing_ms: 0.4250724101267688
  time_since_restore: 105.96794867515564
  time_this_iter_s: 25.47045922279358
  time_total_s: 105.96794867515564
  timers:
    learn_throughput: 8611.014
    learn_time_ms: 18788.961
    sample_throughput: 21301.837
    sample_time_ms: 7595.213
    update_time_ms: 44.258
  timestamp: 1602649027
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      4 |          105.968 | 647168 |  207.518 |                  267 |              136.242 |            879.014 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3537.816976127321
    time_step_min: 3205
  date: 2020-10-14_04-17-33
  done: false
  episode_len_mean: 873.3012658227848
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 208.89221327195992
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0834401945273082
        entropy_coeff: 0.0005000000000000001
        kl: 0.008102510396080712
        model: {}
        policy_loss: -0.0116378908775611
        total_loss: 34.47308222452799
        vf_explained_var: 0.941558837890625
        vf_loss: 34.48445192972819
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.07
    gpu_util_percent0: 0.376
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15831939693525557
    mean_env_wait_ms: 1.1692547379290748
    mean_inference_ms: 5.1024577613721425
    mean_raw_obs_processing_ms: 0.4193847401493736
  time_since_restore: 131.95236229896545
  time_this_iter_s: 25.984413623809814
  time_total_s: 131.95236229896545
  timers:
    learn_throughput: 8586.744
    learn_time_ms: 18842.067
    sample_throughput: 21733.951
    sample_time_ms: 7444.206
    update_time_ms: 41.324
  timestamp: 1602649053
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      5 |          131.952 | 808960 |  208.892 |                  267 |              136.242 |            873.301 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3512.602432179607
    time_step_min: 3204
  date: 2020-10-14_04-17-58
  done: false
  episode_len_mean: 862.558371040724
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 212.5103523927052
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 315
  episodes_total: 1105
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0828134020169575
        entropy_coeff: 0.0005000000000000001
        kl: 0.007772813783958554
        model: {}
        policy_loss: -0.010905310687424693
        total_loss: 29.839585304260254
        vf_explained_var: 0.9594511389732361
        vf_loss: 29.85025469462077
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.386206896551723
    gpu_util_percent0: 0.3741379310344827
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15612864435712862
    mean_env_wait_ms: 1.1733697236033562
    mean_inference_ms: 4.956586631915873
    mean_raw_obs_processing_ms: 0.41217253637357876
  time_since_restore: 157.42168951034546
  time_this_iter_s: 25.469327211380005
  time_total_s: 157.42168951034546
  timers:
    learn_throughput: 8608.033
    learn_time_ms: 18795.468
    sample_throughput: 22048.354
    sample_time_ms: 7338.054
    update_time_ms: 41.35
  timestamp: 1602649078
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      6 |          157.422 | 970752 |   212.51 |                  267 |              136.242 |            862.558 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3501.2597719869705
    time_step_min: 3175
  date: 2020-10-14_04-18-24
  done: false
  episode_len_mean: 857.7254746835443
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 214.3693421557344
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 159
  episodes_total: 1264
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0574177900950115
        entropy_coeff: 0.0005000000000000001
        kl: 0.00769848582179596
        model: {}
        policy_loss: -0.01350904762512073
        total_loss: 17.78907855351766
        vf_explained_var: 0.9648939967155457
        vf_loss: 17.802346070607502
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.526666666666674
    gpu_util_percent0: 0.3896666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1553435546842076
    mean_env_wait_ms: 1.1750662012162918
    mean_inference_ms: 4.904304443838389
    mean_raw_obs_processing_ms: 0.4095899112129358
  time_since_restore: 183.23892331123352
  time_this_iter_s: 25.81723380088806
  time_total_s: 183.23892331123352
  timers:
    learn_throughput: 8602.864
    learn_time_ms: 18806.761
    sample_throughput: 22291.019
    sample_time_ms: 7258.17
    update_time_ms: 51.873
  timestamp: 1602649104
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      7 |          183.239 | 1132544 |  214.369 |                  267 |              136.242 |            857.725 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3493.8376623376626
    time_step_min: 3158
  date: 2020-10-14_04-18-50
  done: false
  episode_len_mean: 853.4549929676512
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 215.6040787623065
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0409260789553325
        entropy_coeff: 0.0005000000000000001
        kl: 0.00779755685168008
        model: {}
        policy_loss: -0.011598864497500472
        total_loss: 19.022697925567627
        vf_explained_var: 0.9629490971565247
        vf_loss: 19.03403838475545
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.65517241379311
    gpu_util_percent0: 0.3620689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15468057258773296
    mean_env_wait_ms: 1.1766222789965723
    mean_inference_ms: 4.8597905986382965
    mean_raw_obs_processing_ms: 0.4073188318824942
  time_since_restore: 208.82342410087585
  time_this_iter_s: 25.584500789642334
  time_total_s: 208.82342410087585
  timers:
    learn_throughput: 8611.996
    learn_time_ms: 18786.818
    sample_throughput: 22443.225
    sample_time_ms: 7208.946
    update_time_ms: 47.891
  timestamp: 1602649130
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      8 |          208.823 | 1294336 |  215.604 |                  267 |              136.242 |            853.455 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3486.4576051779936
    time_step_min: 3158
  date: 2020-10-14_04-19-16
  done: false
  episode_len_mean: 850.1170145477546
  episode_reward_max: 267.00000000000045
  episode_reward_mean: 216.83727215226253
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 159
  episodes_total: 1581
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0032605528831482
        entropy_coeff: 0.0005000000000000001
        kl: 0.007478718568260471
        model: {}
        policy_loss: -0.013388285306670392
        total_loss: 17.289390722910564
        vf_explained_var: 0.9693426489830017
        vf_loss: 17.302532354990642
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.127586206896552
    gpu_util_percent0: 0.3275862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15409932200364063
    mean_env_wait_ms: 1.1781019013068719
    mean_inference_ms: 4.820979240040372
    mean_raw_obs_processing_ms: 0.4052962399989313
  time_since_restore: 234.33526229858398
  time_this_iter_s: 25.51183819770813
  time_total_s: 234.33526229858398
  timers:
    learn_throughput: 8616.727
    learn_time_ms: 18776.502
    sample_throughput: 22617.773
    sample_time_ms: 7153.312
    update_time_ms: 47.236
  timestamp: 1602649156
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |      9 |          234.335 | 1456128 |  216.837 |                  267 |              136.242 |            850.117 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3468.207212055974
    time_step_min: 3158
  date: 2020-10-14_04-19-42
  done: false
  episode_len_mean: 843.5290390707497
  episode_reward_max: 268.96969696969654
  episode_reward_mean: 219.8268695401746
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 313
  episodes_total: 1894
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9728382378816605
        entropy_coeff: 0.0005000000000000001
        kl: 0.0068471186483899755
        model: {}
        policy_loss: -0.010848217488576969
        total_loss: 18.585219701131184
        vf_explained_var: 0.9735828042030334
        vf_loss: 18.595869382222492
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.840000000000007
    gpu_util_percent0: 0.3123333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1531800256770803
    mean_env_wait_ms: 1.1809668759840048
    mean_inference_ms: 4.759371230204509
    mean_raw_obs_processing_ms: 0.40221088496797086
  time_since_restore: 260.3743929862976
  time_this_iter_s: 26.039130687713623
  time_total_s: 260.3743929862976
  timers:
    learn_throughput: 8600.298
    learn_time_ms: 18812.371
    sample_throughput: 22722.557
    sample_time_ms: 7120.325
    update_time_ms: 45.351
  timestamp: 1602649182
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     10 |          260.374 | 1617920 |  219.827 |               268.97 |              136.242 |            843.529 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3459.940039643211
    time_step_min: 3149
  date: 2020-10-14_04-20-07
  done: false
  episode_len_mean: 840.819376825706
  episode_reward_max: 268.96969696969654
  episode_reward_mean: 221.11539936856386
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 160
  episodes_total: 2054
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.952570786078771
        entropy_coeff: 0.0005000000000000001
        kl: 0.006939189663777749
        model: {}
        policy_loss: -0.01326275585355082
        total_loss: 12.24478809038798
        vf_explained_var: 0.9761103987693787
        vf_loss: 12.257833242416382
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.27241379310345
    gpu_util_percent0: 0.3648275862068965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15279481663140063
    mean_env_wait_ms: 1.182209843927421
    mean_inference_ms: 4.733656963246705
    mean_raw_obs_processing_ms: 0.4009141017220568
  time_since_restore: 285.88407135009766
  time_this_iter_s: 25.50967836380005
  time_total_s: 285.88407135009766
  timers:
    learn_throughput: 8621.674
    learn_time_ms: 18765.729
    sample_throughput: 23567.862
    sample_time_ms: 6864.942
    update_time_ms: 42.433
  timestamp: 1602649207
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     11 |          285.884 | 1779712 |  221.115 |               268.97 |              136.242 |            840.819 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3451.996783088235
    time_step_min: 3149
  date: 2020-10-14_04-20-33
  done: false
  episode_len_mean: 838.5524412296564
  episode_reward_max: 276.5454545454551
  episode_reward_mean: 222.440161104718
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.93861157198747
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065783633617684245
        model: {}
        policy_loss: -0.01096414635533923
        total_loss: 11.370628595352173
        vf_explained_var: 0.9759250283241272
        vf_loss: 11.381404479344686
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94333333333333
    gpu_util_percent0: 0.36500000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15244999762497916
    mean_env_wait_ms: 1.1833051039465339
    mean_inference_ms: 4.7107676492065504
    mean_raw_obs_processing_ms: 0.3997242489091511
  time_since_restore: 311.62115120887756
  time_this_iter_s: 25.737079858779907
  time_total_s: 311.62115120887756
  timers:
    learn_throughput: 8612.361
    learn_time_ms: 18786.021
    sample_throughput: 23696.191
    sample_time_ms: 6827.764
    update_time_ms: 42.391
  timestamp: 1602649233
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     12 |          311.621 | 1941504 |   222.44 |              276.545 |              136.242 |            838.552 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3441.9974768713205
    time_step_min: 3121
  date: 2020-10-14_04-20-59
  done: false
  episode_len_mean: 835.330985915493
  episode_reward_max: 276.5454545454551
  episode_reward_mean: 223.75008159473768
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 202
  episodes_total: 2414
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8984403163194656
        entropy_coeff: 0.0005000000000000001
        kl: 0.006221253269662459
        model: {}
        policy_loss: -0.009880342788771182
        total_loss: 16.48670530319214
        vf_explained_var: 0.974186360836029
        vf_loss: 16.49641251564026
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84333333333333
    gpu_util_percent0: 0.3526666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15206296937366148
    mean_env_wait_ms: 1.1847892907662063
    mean_inference_ms: 4.684545127250306
    mean_raw_obs_processing_ms: 0.398369443862946
  time_since_restore: 337.45509147644043
  time_this_iter_s: 25.833940267562866
  time_total_s: 337.45509147644043
  timers:
    learn_throughput: 8611.202
    learn_time_ms: 18788.551
    sample_throughput: 23758.278
    sample_time_ms: 6809.921
    update_time_ms: 41.648
  timestamp: 1602649259
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     13 |          337.455 | 2103296 |   223.75 |              276.545 |              136.242 |            835.331 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3429.613061532654
    time_step_min: 3121
  date: 2020-10-14_04-21-25
  done: false
  episode_len_mean: 831.2167597765363
  episode_reward_max: 276.5454545454553
  episode_reward_mean: 225.59765250268038
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 271
  episodes_total: 2685
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8792577832937241
        entropy_coeff: 0.0005000000000000001
        kl: 0.006186536280438304
        model: {}
        policy_loss: -0.009713826865966743
        total_loss: 12.887195348739624
        vf_explained_var: 0.9778575897216797
        vf_loss: 12.896730581919352
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.524137931034485
    gpu_util_percent0: 0.31241379310344825
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15160048297239279
    mean_env_wait_ms: 1.1865770107961637
    mean_inference_ms: 4.6545318216897416
    mean_raw_obs_processing_ms: 0.3968315246205982
  time_since_restore: 363.03388261795044
  time_this_iter_s: 25.57879114151001
  time_total_s: 363.03388261795044
  timers:
    learn_throughput: 8604.292
    learn_time_ms: 18803.639
    sample_throughput: 23776.213
    sample_time_ms: 6804.784
    update_time_ms: 41.753
  timestamp: 1602649285
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     14 |          363.034 | 2265088 |  225.598 |              276.545 |              136.242 |            831.217 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3422.2681623931626
    time_step_min: 3121
  date: 2020-10-14_04-21-50
  done: false
  episode_len_mean: 829.1111111111111
  episode_reward_max: 276.5454545454553
  episode_reward_mean: 226.5676170992626
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 159
  episodes_total: 2844
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8780574252208074
        entropy_coeff: 0.0005000000000000001
        kl: 0.006002183809566001
        model: {}
        policy_loss: -0.010840655624633655
        total_loss: 10.460712591807047
        vf_explained_var: 0.9777989387512207
        vf_loss: 10.471392234166464
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43448275862069
    gpu_util_percent0: 0.3289655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15136686720772935
    mean_env_wait_ms: 1.187561942239848
    mean_inference_ms: 4.638979870000984
    mean_raw_obs_processing_ms: 0.39602700283589737
  time_since_restore: 388.60838556289673
  time_this_iter_s: 25.57450294494629
  time_total_s: 388.60838556289673
  timers:
    learn_throughput: 8619.912
    learn_time_ms: 18769.565
    sample_throughput: 23796.596
    sample_time_ms: 6798.956
    update_time_ms: 40.933
  timestamp: 1602649310
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     15 |          388.608 | 2426880 |  226.568 |              276.545 |              136.242 |            829.111 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3415.3331087908387
    time_step_min: 3112
  date: 2020-10-14_04-22-16
  done: false
  episode_len_mean: 826.9144758735441
  episode_reward_max: 279.42424242424227
  episode_reward_mean: 227.6205818585186
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 3005
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8447853823502859
        entropy_coeff: 0.0005000000000000001
        kl: 0.0059578693471848965
        model: {}
        policy_loss: -0.009374744794816555
        total_loss: 11.64163851737976
        vf_explained_var: 0.9763595461845398
        vf_loss: 11.650839964548746
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970000000000006
    gpu_util_percent0: 0.34133333333333327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15115018766980515
    mean_env_wait_ms: 1.1885433592222465
    mean_inference_ms: 4.624444665818862
    mean_raw_obs_processing_ms: 0.3952612970799783
  time_since_restore: 414.36487460136414
  time_this_iter_s: 25.756489038467407
  time_total_s: 414.36487460136414
  timers:
    learn_throughput: 8608.911
    learn_time_ms: 18793.55
    sample_throughput: 23778.109
    sample_time_ms: 6804.242
    update_time_ms: 39.358
  timestamp: 1602649336
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     16 |          414.365 | 2588672 |  227.621 |              279.424 |              136.242 |            826.914 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3404.028981086028
    time_step_min: 3093
  date: 2020-10-14_04-22-42
  done: false
  episode_len_mean: 823.415509957755
  episode_reward_max: 279.42424242424227
  episode_reward_mean: 229.41505276055662
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 309
  episodes_total: 3314
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8260619839032491
        entropy_coeff: 0.0005000000000000001
        kl: 0.006421650837485989
        model: {}
        policy_loss: -0.009145767389175793
        total_loss: 13.133175134658813
        vf_explained_var: 0.9803077578544617
        vf_loss: 13.142091671625773
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.81666666666667
    gpu_util_percent0: 0.31366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1507789071566395
    mean_env_wait_ms: 1.1903568594552503
    mean_inference_ms: 4.599648708598892
    mean_raw_obs_processing_ms: 0.3939778837895492
  time_since_restore: 440.27728176116943
  time_this_iter_s: 25.912407159805298
  time_total_s: 440.27728176116943
  timers:
    learn_throughput: 8602.319
    learn_time_ms: 18807.952
    sample_throughput: 23774.688
    sample_time_ms: 6805.221
    update_time_ms: 32.282
  timestamp: 1602649362
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     17 |          440.277 | 2750464 |  229.415 |              279.424 |              136.242 |            823.416 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3398.445930232558
    time_step_min: 3088
  date: 2020-10-14_04-23-08
  done: false
  episode_len_mean: 821.7310126582279
  episode_reward_max: 279.42424242424227
  episode_reward_mean: 230.2455887993862
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 162
  episodes_total: 3476
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7999657342831293
        entropy_coeff: 0.0005000000000000001
        kl: 0.005766421129616599
        model: {}
        policy_loss: -0.012520057265646756
        total_loss: 8.275206168492636
        vf_explained_var: 0.9822821617126465
        vf_loss: 8.28754965464274
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.465517241379306
    gpu_util_percent0: 0.29793103448275865
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15060649934784207
    mean_env_wait_ms: 1.1912169640289625
    mean_inference_ms: 4.5881158962458155
    mean_raw_obs_processing_ms: 0.39338192687345
  time_since_restore: 465.7614939212799
  time_this_iter_s: 25.484212160110474
  time_total_s: 465.7614939212799
  timers:
    learn_throughput: 8603.638
    learn_time_ms: 18805.068
    sample_throughput: 23803.812
    sample_time_ms: 6796.894
    update_time_ms: 32.733
  timestamp: 1602649388
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     18 |          465.761 | 2912256 |  230.246 |              279.424 |              136.242 |            821.731 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3392.4371873262926
    time_step_min: 3088
  date: 2020-10-14_04-23-34
  done: false
  episode_len_mean: 820.1821684094662
  episode_reward_max: 279.42424242424227
  episode_reward_mean: 231.0936608795717
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 3634
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7937028606732687
        entropy_coeff: 0.0005000000000000001
        kl: 0.005935679965962966
        model: {}
        policy_loss: -0.012428551142268892
        total_loss: 9.381741841634115
        vf_explained_var: 0.978752613067627
        vf_loss: 9.393973429997763
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.040000000000006
    gpu_util_percent0: 0.36099999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15044915283180216
    mean_env_wait_ms: 1.1920186280611351
    mean_inference_ms: 4.577577778903684
    mean_raw_obs_processing_ms: 0.39282903782315065
  time_since_restore: 491.641530752182
  time_this_iter_s: 25.8800368309021
  time_total_s: 491.641530752182
  timers:
    learn_throughput: 8590.834
    learn_time_ms: 18833.095
    sample_throughput: 23767.445
    sample_time_ms: 6807.295
    update_time_ms: 31.632
  timestamp: 1602649414
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     19 |          491.642 | 3074048 |  231.094 |              279.424 |              136.242 |            820.182 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3384.686385946784
    time_step_min: 3076
  date: 2020-10-14_04-24-00
  done: false
  episode_len_mean: 817.9669823393908
  episode_reward_max: 280.181818181819
  episode_reward_mean: 232.309545415765
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 273
  episodes_total: 3907
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7688640058040619
        entropy_coeff: 0.0005000000000000001
        kl: 0.005751285973625879
        model: {}
        policy_loss: -0.008068479248322546
        total_loss: 13.53387713432312
        vf_explained_var: 0.9797406196594238
        vf_loss: 13.541754881540934
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.565517241379318
    gpu_util_percent0: 0.36896551724137927
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15019954099501828
    mean_env_wait_ms: 1.1933457636787608
    mean_inference_ms: 4.560948529953053
    mean_raw_obs_processing_ms: 0.3919612051679977
  time_since_restore: 517.2342035770416
  time_this_iter_s: 25.59267282485962
  time_total_s: 517.2342035770416
  timers:
    learn_throughput: 8612.767
    learn_time_ms: 18785.137
    sample_throughput: 23759.925
    sample_time_ms: 6809.449
    update_time_ms: 32.001
  timestamp: 1602649440
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     20 |          517.234 | 3235840 |   232.31 |              280.182 |              136.242 |            817.967 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3378.114931237721
    time_step_min: 3076
  date: 2020-10-14_04-24-26
  done: false
  episode_len_mean: 816.805988315482
  episode_reward_max: 282.0000000000004
  episode_reward_mean: 233.28526009855122
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 201
  episodes_total: 4108
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7478803247213364
        entropy_coeff: 0.0005000000000000001
        kl: 0.005493286298587918
        model: {}
        policy_loss: -0.009481565718791293
        total_loss: 8.676706790924072
        vf_explained_var: 0.9827273488044739
        vf_loss: 8.686012983322144
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.823333333333334
    gpu_util_percent0: 0.32366666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15003492737193316
    mean_env_wait_ms: 1.1942534149228878
    mean_inference_ms: 4.549846612449616
    mean_raw_obs_processing_ms: 0.39141363101079946
  time_since_restore: 543.0849173069
  time_this_iter_s: 25.8507137298584
  time_total_s: 543.0849173069
  timers:
    learn_throughput: 8600.911
    learn_time_ms: 18811.03
    sample_throughput: 23733.908
    sample_time_ms: 6816.914
    update_time_ms: 32.375
  timestamp: 1602649466
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     21 |          543.085 | 3397632 |  233.285 |                  282 |              136.242 |            816.806 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3372.9333333333334
    time_step_min: 3076
  date: 2020-10-14_04-24-51
  done: false
  episode_len_mean: 815.9709329582747
  episode_reward_max: 282.0000000000004
  episode_reward_mean: 234.00482319680629
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 4266
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7441265483697256
        entropy_coeff: 0.0005000000000000001
        kl: 0.006011400294179718
        model: {}
        policy_loss: -0.012443608846903468
        total_loss: 8.12285848458608
        vf_explained_var: 0.981548547744751
        vf_loss: 8.135072708129883
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.55172413793104
    gpu_util_percent0: 0.3444827586206896
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14991307841844934
    mean_env_wait_ms: 1.1948966235658538
    mean_inference_ms: 4.541711240634092
    mean_raw_obs_processing_ms: 0.390998428057138
  time_since_restore: 568.7182126045227
  time_this_iter_s: 25.63329529762268
  time_total_s: 568.7182126045227
  timers:
    learn_throughput: 8602.38
    learn_time_ms: 18807.819
    sample_throughput: 23761.796
    sample_time_ms: 6808.913
    update_time_ms: 32.721
  timestamp: 1602649491
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     22 |          568.718 | 3559424 |  234.005 |                  282 |              136.242 |            815.971 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3366.973940629957
    time_step_min: 3076
  date: 2020-10-14_04-25-17
  done: false
  episode_len_mean: 815.1571139581929
  episode_reward_max: 282.0000000000004
  episode_reward_mean: 234.8332890605311
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 183
  episodes_total: 4449
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7222094535827637
        entropy_coeff: 0.0005000000000000001
        kl: 0.005956807329008977
        model: {}
        policy_loss: -0.009062373486813158
        total_loss: 8.264012813568115
        vf_explained_var: 0.9844412207603455
        vf_loss: 8.27284061908722
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.570000000000004
    gpu_util_percent0: 0.3766666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14978130914815882
    mean_env_wait_ms: 1.1956278938919191
    mean_inference_ms: 4.532905639570085
    mean_raw_obs_processing_ms: 0.39053907083899275
  time_since_restore: 594.6625230312347
  time_this_iter_s: 25.944310426712036
  time_total_s: 594.6625230312347
  timers:
    learn_throughput: 8596.6
    learn_time_ms: 18820.463
    sample_throughput: 23770.191
    sample_time_ms: 6806.508
    update_time_ms: 32.668
  timestamp: 1602649517
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     23 |          594.663 | 3721216 |  234.833 |                  282 |              136.242 |            815.157 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3358.6475218038713
    time_step_min: 3076
  date: 2020-10-14_04-25-43
  done: false
  episode_len_mean: 814.0994300189993
  episode_reward_max: 282.4545454545455
  episode_reward_mean: 236.12900378068204
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 288
  episodes_total: 4737
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6992855221033096
        entropy_coeff: 0.0005000000000000001
        kl: 0.005327076263104876
        model: {}
        policy_loss: -0.00848967404696547
        total_loss: 9.076302925745646
        vf_explained_var: 0.9848020672798157
        vf_loss: 9.084609985351562
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.113793103448277
    gpu_util_percent0: 0.3562068965517241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14959078568214065
    mean_env_wait_ms: 1.196605893108773
    mean_inference_ms: 4.519946224858827
    mean_raw_obs_processing_ms: 0.38989116867707135
  time_since_restore: 620.345579624176
  time_this_iter_s: 25.683056592941284
  time_total_s: 620.345579624176
  timers:
    learn_throughput: 8592.644
    learn_time_ms: 18829.129
    sample_throughput: 23762.918
    sample_time_ms: 6808.591
    update_time_ms: 31.275
  timestamp: 1602649543
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     24 |          620.346 | 3883008 |  236.129 |              282.455 |              136.242 |            814.099 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3354.6355409296584
    time_step_min: 3076
  date: 2020-10-14_04-26-09
  done: false
  episode_len_mean: 813.6945692119232
  episode_reward_max: 282.90909090909133
  episode_reward_mean: 236.7266973532796
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 4898
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6953709373871485
        entropy_coeff: 0.0005000000000000001
        kl: 0.005563315353356302
        model: {}
        policy_loss: -0.011534025679187229
        total_loss: 7.18784765402476
        vf_explained_var: 0.9845226407051086
        vf_loss: 7.199173092842102
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78666666666667
    gpu_util_percent0: 0.26833333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14949264687860497
    mean_env_wait_ms: 1.1971151808196612
    mean_inference_ms: 4.513317291624422
    mean_raw_obs_processing_ms: 0.3895579786933225
  time_since_restore: 646.0394768714905
  time_this_iter_s: 25.693897247314453
  time_total_s: 646.0394768714905
  timers:
    learn_throughput: 8589.976
    learn_time_ms: 18834.977
    sample_throughput: 23751.908
    sample_time_ms: 6811.747
    update_time_ms: 33.405
  timestamp: 1602649569
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     25 |          646.039 | 4044800 |  236.727 |              282.909 |              136.242 |            813.695 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3350.9976109894487
    time_step_min: 3026
  date: 2020-10-14_04-26-35
  done: false
  episode_len_mean: 813.2152599327931
  episode_reward_max: 286.24242424242414
  episode_reward_mean: 237.2804482859829
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 5059
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6938041498263677
        entropy_coeff: 0.0005000000000000001
        kl: 0.00575402588583529
        model: {}
        policy_loss: -0.010851105515030213
        total_loss: 7.853219707806905
        vf_explained_var: 0.9833803176879883
        vf_loss: 7.863842288653056
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.065517241379315
    gpu_util_percent0: 0.3975862068965518
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14940021925818736
    mean_env_wait_ms: 1.1975815964452092
    mean_inference_ms: 4.506982092900674
    mean_raw_obs_processing_ms: 0.38923956954082417
  time_since_restore: 671.5306475162506
  time_this_iter_s: 25.491170644760132
  time_total_s: 671.5306475162506
  timers:
    learn_throughput: 8599.444
    learn_time_ms: 18814.24
    sample_throughput: 23776.449
    sample_time_ms: 6804.717
    update_time_ms: 34.909
  timestamp: 1602649595
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     26 |          671.531 | 4206592 |   237.28 |              286.242 |              136.242 |            813.215 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3344.712121212121
    time_step_min: 3026
  date: 2020-10-14_04-27-01
  done: false
  episode_len_mean: 812.4174191121143
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 238.21776455297896
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 257
  episodes_total: 5316
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6754096349080404
        entropy_coeff: 0.0005000000000000001
        kl: 0.005268798054506381
        model: {}
        policy_loss: -0.01019210034670929
        total_loss: 9.916938702265421
        vf_explained_var: 0.9842239022254944
        vf_loss: 9.926941951115927
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.766666666666666
    gpu_util_percent0: 0.34700000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14926026472772014
    mean_env_wait_ms: 1.1982965029793797
    mean_inference_ms: 4.4976533822382425
    mean_raw_obs_processing_ms: 0.3887707509318437
  time_since_restore: 697.1292181015015
  time_this_iter_s: 25.598570585250854
  time_total_s: 697.1292181015015
  timers:
    learn_throughput: 8612.415
    learn_time_ms: 18785.904
    sample_throughput: 23781.87
    sample_time_ms: 6803.166
    update_time_ms: 32.661
  timestamp: 1602649621
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     27 |          697.129 | 4368384 |  238.218 |              289.424 |              136.242 |            812.417 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3339.8709501274116
    time_step_min: 3026
  date: 2020-10-14_04-27-26
  done: false
  episode_len_mean: 811.7710669077758
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 238.8827880979779
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 5530
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6553066670894623
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051280740881338716
        model: {}
        policy_loss: -0.009533356826674813
        total_loss: 7.269570549329122
        vf_explained_var: 0.9859780669212341
        vf_loss: 7.278918703397115
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.282758620689656
    gpu_util_percent0: 0.3275862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14915200530198738
    mean_env_wait_ms: 1.1988069756001265
    mean_inference_ms: 4.490241531067799
    mean_raw_obs_processing_ms: 0.3884070404855471
  time_since_restore: 722.8491690158844
  time_this_iter_s: 25.719950914382935
  time_total_s: 722.8491690158844
  timers:
    learn_throughput: 8607.862
    learn_time_ms: 18795.84
    sample_throughput: 23743.803
    sample_time_ms: 6814.073
    update_time_ms: 34.224
  timestamp: 1602649646
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     28 |          722.849 | 4530176 |  238.883 |              289.424 |              136.242 |            811.771 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3336.404812455768
    time_step_min: 3026
  date: 2020-10-14_04-27-52
  done: false
  episode_len_mean: 811.354606188467
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 239.4166773217406
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 158
  episodes_total: 5688
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6610687722762426
        entropy_coeff: 0.0005000000000000001
        kl: 0.005322185306188961
        model: {}
        policy_loss: -0.011243697178239623
        total_loss: 6.071420470873515
        vf_explained_var: 0.9862317442893982
        vf_loss: 6.082462549209595
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5896551724138
    gpu_util_percent0: 0.3806896551724137
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1490765221879278
    mean_env_wait_ms: 1.1991579650150583
    mean_inference_ms: 4.4851392157560745
    mean_raw_obs_processing_ms: 0.3881527849433829
  time_since_restore: 748.3333804607391
  time_this_iter_s: 25.484211444854736
  time_total_s: 748.3333804607391
  timers:
    learn_throughput: 8622.996
    learn_time_ms: 18762.852
    sample_throughput: 23771.939
    sample_time_ms: 6806.008
    update_time_ms: 34.973
  timestamp: 1602649672
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     29 |          748.333 | 4691968 |  239.417 |              289.424 |              136.242 |            811.355 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3332.803390991608
    time_step_min: 3026
  date: 2020-10-14_04-28-18
  done: false
  episode_len_mean: 810.8731914893617
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 239.9534493874919
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 187
  episodes_total: 5875
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6483458032210668
        entropy_coeff: 0.0005000000000000001
        kl: 0.006111707344340782
        model: {}
        policy_loss: -0.0106555049715098
        total_loss: 8.00598931312561
        vf_explained_var: 0.9853588938713074
        vf_loss: 8.016357819239298
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.40333333333334
    gpu_util_percent0: 0.32266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14899246205087147
    mean_env_wait_ms: 1.199567791072205
    mean_inference_ms: 4.479429695073161
    mean_raw_obs_processing_ms: 0.3878695186656806
  time_since_restore: 774.1694629192352
  time_this_iter_s: 25.836082458496094
  time_total_s: 774.1694629192352
  timers:
    learn_throughput: 8607.005
    learn_time_ms: 18797.712
    sample_throughput: 23811.305
    sample_time_ms: 6794.756
    update_time_ms: 34.628
  timestamp: 1602649698
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     30 |          774.169 | 4853760 |  239.953 |              289.424 |              136.242 |            810.873 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3327.303694017653
    time_step_min: 3026
  date: 2020-10-14_04-28-44
  done: false
  episode_len_mean: 810.2309067273318
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 240.72891245900666
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 279
  episodes_total: 6154
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6224197347958883
        entropy_coeff: 0.0005000000000000001
        kl: 0.005031730552824835
        model: {}
        policy_loss: -0.008331117608274022
        total_loss: 9.511263291041056
        vf_explained_var: 0.9848304390907288
        vf_loss: 9.519402503967285
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.862068965517242
    gpu_util_percent0: 0.386551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14887193190890077
    mean_env_wait_ms: 1.200097587128897
    mean_inference_ms: 4.471318888235075
    mean_raw_obs_processing_ms: 0.387471838185048
  time_since_restore: 799.8665311336517
  time_this_iter_s: 25.697068214416504
  time_total_s: 799.8665311336517
  timers:
    learn_throughput: 8607.93
    learn_time_ms: 18795.691
    sample_throughput: 23853.686
    sample_time_ms: 6782.683
    update_time_ms: 32.284
  timestamp: 1602649724
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     31 |          799.867 | 5015552 |  240.729 |              289.424 |              136.242 |            810.231 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3324.074952259707
    time_step_min: 3011
  date: 2020-10-14_04-29-10
  done: false
  episode_len_mean: 809.8982594936709
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 241.21406309934784
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 166
  episodes_total: 6320
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6152809311946233
        entropy_coeff: 0.0005000000000000001
        kl: 0.005592694894100229
        model: {}
        policy_loss: -0.010222488936657706
        total_loss: 6.031432429949443
        vf_explained_var: 0.9868378639221191
        vf_loss: 6.041403333346049
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.740000000000006
    gpu_util_percent0: 0.30066666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14880575120386166
    mean_env_wait_ms: 1.2003917376845414
    mean_inference_ms: 4.466811399479319
    mean_raw_obs_processing_ms: 0.38725257830881954
  time_since_restore: 825.4988515377045
  time_this_iter_s: 25.632320404052734
  time_total_s: 825.4988515377045
  timers:
    learn_throughput: 8611.682
    learn_time_ms: 18787.503
    sample_throughput: 23836.487
    sample_time_ms: 6787.577
    update_time_ms: 34.509
  timestamp: 1602649750
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     32 |          825.499 | 5177344 |  241.214 |              289.424 |              136.242 |            809.898 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3321.071849782744
    time_step_min: 3011
  date: 2020-10-14_04-29-36
  done: false
  episode_len_mean: 809.7006172839506
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 241.67134306023192
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 160
  episodes_total: 6480
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6128020832935969
        entropy_coeff: 0.0005000000000000001
        kl: 0.005626710519815485
        model: {}
        policy_loss: -0.011386892847137156
        total_loss: 6.6085009177525835
        vf_explained_var: 0.9855687022209167
        vf_loss: 6.6196315685908
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.91333333333334
    gpu_util_percent0: 0.3573333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14874403565420322
    mean_env_wait_ms: 1.2006486927312854
    mean_inference_ms: 4.462626513561133
    mean_raw_obs_processing_ms: 0.3870489660468986
  time_since_restore: 851.2353620529175
  time_this_iter_s: 25.736510515213013
  time_total_s: 851.2353620529175
  timers:
    learn_throughput: 8624.278
    learn_time_ms: 18760.063
    sample_throughput: 23819.316
    sample_time_ms: 6792.47
    update_time_ms: 35.347
  timestamp: 1602649776
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     33 |          851.235 | 5339136 |  241.671 |              289.424 |              136.242 |            809.701 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3316.9723053892217
    time_step_min: 3011
  date: 2020-10-14_04-30-02
  done: false
  episode_len_mean: 809.5023823704586
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 242.33120815059468
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 236
  episodes_total: 6716
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5948016991217931
        entropy_coeff: 0.0005000000000000001
        kl: 0.005257600375140707
        model: {}
        policy_loss: -0.010794992907904088
        total_loss: 8.705323378245035
        vf_explained_var: 0.9856746792793274
        vf_loss: 8.715890169143677
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43448275862069
    gpu_util_percent0: 0.3324137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14865744532532
    mean_env_wait_ms: 1.2010276390099246
    mean_inference_ms: 4.456835515849531
    mean_raw_obs_processing_ms: 0.38675933232811305
  time_since_restore: 877.0943551063538
  time_this_iter_s: 25.85899305343628
  time_total_s: 877.0943551063538
  timers:
    learn_throughput: 8616.415
    learn_time_ms: 18777.184
    sample_throughput: 23828.014
    sample_time_ms: 6789.991
    update_time_ms: 37.216
  timestamp: 1602649802
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     34 |          877.094 | 5500928 |  242.331 |              289.424 |              136.242 |            809.502 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3312.533198322002
    time_step_min: 3011
  date: 2020-10-14_04-30-28
  done: false
  episode_len_mean: 809.4299899266082
  episode_reward_max: 289.4242424242428
  episode_reward_mean: 242.97450254451257
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 233
  episodes_total: 6949
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5731659332911173
        entropy_coeff: 0.0005000000000000001
        kl: 0.004918006753238539
        model: {}
        policy_loss: -0.013124067646761736
        total_loss: 6.96964172522227
        vf_explained_var: 0.9871130585670471
        vf_loss: 6.982560594876607
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.939999999999998
    gpu_util_percent0: 0.3943333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14857871226185415
    mean_env_wait_ms: 1.201303731830431
    mean_inference_ms: 4.451331107214727
    mean_raw_obs_processing_ms: 0.3865012471492786
  time_since_restore: 903.0785732269287
  time_this_iter_s: 25.98421812057495
  time_total_s: 903.0785732269287
  timers:
    learn_throughput: 8602.245
    learn_time_ms: 18808.114
    sample_throughput: 23831.627
    sample_time_ms: 6788.962
    update_time_ms: 35.422
  timestamp: 1602649828
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     35 |          903.079 | 5662720 |  242.975 |              289.424 |              136.242 |             809.43 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3309.539016115352
    time_step_min: 2960
  date: 2020-10-14_04-30-54
  done: false
  episode_len_mean: 809.256258790436
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 243.4278012189404
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 7110
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5802024255196253
        entropy_coeff: 0.0005000000000000001
        kl: 0.005665121600031853
        model: {}
        policy_loss: -0.010481108650613654
        total_loss: 6.6643790404001875
        vf_explained_var: 0.9848917126655579
        vf_loss: 6.674866875012715
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.846666666666668
    gpu_util_percent0: 0.3436666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485264627413504
    mean_env_wait_ms: 1.2014974103647045
    mean_inference_ms: 4.447737423948273
    mean_raw_obs_processing_ms: 0.3863254944789424
  time_since_restore: 928.8208458423615
  time_this_iter_s: 25.74227261543274
  time_total_s: 928.8208458423615
  timers:
    learn_throughput: 8593.394
    learn_time_ms: 18827.486
    sample_throughput: 23817.58
    sample_time_ms: 6792.966
    update_time_ms: 35.335
  timestamp: 1602649854
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     36 |          928.821 | 5824512 |  243.428 |              296.242 |              136.242 |            809.256 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3306.5756656090493
    time_step_min: 2960
  date: 2020-10-14_04-31-20
  done: false
  episode_len_mean: 809.1177762525738
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 243.86341798215506
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 175
  episodes_total: 7285
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5707181990146637
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055595429148525
        model: {}
        policy_loss: -0.010776971925224643
        total_loss: 6.928751269976298
        vf_explained_var: 0.9861412048339844
        vf_loss: 6.939535578091939
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.243333333333336
    gpu_util_percent0: 0.33266666666666656
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484726364201677
    mean_env_wait_ms: 1.2016824030498592
    mean_inference_ms: 4.443978575148465
    mean_raw_obs_processing_ms: 0.38614768159462004
  time_since_restore: 954.789972782135
  time_this_iter_s: 25.96912693977356
  time_total_s: 954.789972782135
  timers:
    learn_throughput: 8581.137
    learn_time_ms: 18854.377
    sample_throughput: 23790.435
    sample_time_ms: 6800.716
    update_time_ms: 37.197
  timestamp: 1602649880
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     37 |           954.79 | 5986304 |  243.863 |              296.242 |              136.242 |            809.118 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3302.1002663115846
    time_step_min: 2960
  date: 2020-10-14_04-31-46
  done: false
  episode_len_mean: 808.9057778955738
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 244.52353243540622
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 261
  episodes_total: 7546
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5507474193970362
        entropy_coeff: 0.0005000000000000001
        kl: 0.004939862565758328
        model: {}
        policy_loss: -0.008943449628228942
        total_loss: 9.609710693359375
        vf_explained_var: 0.9844183921813965
        vf_loss: 9.618682702382406
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25172413793104
    gpu_util_percent0: 0.3741379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14839329820423297
    mean_env_wait_ms: 1.2019554512380846
    mean_inference_ms: 4.4387028014659435
    mean_raw_obs_processing_ms: 0.3858844269381952
  time_since_restore: 980.3691415786743
  time_this_iter_s: 25.579168796539307
  time_total_s: 980.3691415786743
  timers:
    learn_throughput: 8579.331
    learn_time_ms: 18858.347
    sample_throughput: 23851.148
    sample_time_ms: 6783.405
    update_time_ms: 35.625
  timestamp: 1602649906
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     38 |          980.369 | 6148096 |  244.524 |              296.242 |              136.242 |            808.906 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3298.669218790553
    time_step_min: 2960
  date: 2020-10-14_04-32-12
  done: false
  episode_len_mean: 808.7885559287006
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 245.01595782156357
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 196
  episodes_total: 7742
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.538197527329127
        entropy_coeff: 0.0005000000000000001
        kl: 0.006036433197247486
        model: {}
        policy_loss: -0.010254086198983714
        total_loss: 5.736980835596721
        vf_explained_var: 0.9881876111030579
        vf_loss: 5.747352917989095
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.723333333333336
    gpu_util_percent0: 0.2903333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14833983540913473
    mean_env_wait_ms: 1.2021152261994856
    mean_inference_ms: 4.4348546258684785
    mean_raw_obs_processing_ms: 0.3857039831608781
  time_since_restore: 1006.2761740684509
  time_this_iter_s: 25.90703248977661
  time_total_s: 1006.2761740684509
  timers:
    learn_throughput: 8566.48
    learn_time_ms: 18886.638
    sample_throughput: 23805.414
    sample_time_ms: 6796.437
    update_time_ms: 35.663
  timestamp: 1602649932
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     39 |          1006.28 | 6309888 |  245.016 |              296.242 |              136.242 |            808.789 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3296.1144164759726
    time_step_min: 2960
  date: 2020-10-14_04-32-38
  done: false
  episode_len_mean: 808.5966843837003
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 245.392236717977
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 160
  episodes_total: 7902
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5471722483634949
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055558883274594946
        model: {}
        policy_loss: -0.009315188857726753
        total_loss: 6.58670171101888
        vf_explained_var: 0.9855363965034485
        vf_loss: 6.59615163008372
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46
    gpu_util_percent0: 0.3656666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14829670804260078
    mean_env_wait_ms: 1.202239722682376
    mean_inference_ms: 4.431862378506805
    mean_raw_obs_processing_ms: 0.38555961735861816
  time_since_restore: 1031.9239892959595
  time_this_iter_s: 25.647815227508545
  time_total_s: 1031.9239892959595
  timers:
    learn_throughput: 8580.679
    learn_time_ms: 18855.383
    sample_throughput: 23798.005
    sample_time_ms: 6798.553
    update_time_ms: 36.905
  timestamp: 1602649958
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     40 |          1031.92 | 6471680 |  245.392 |              296.242 |              136.242 |            808.597 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3292.6551510648837
    time_step_min: 2960
  date: 2020-10-14_04-33-03
  done: false
  episode_len_mean: 808.4330621301775
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 245.87276985834674
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 8112
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.534255251288414
        entropy_coeff: 0.0005000000000000001
        kl: 0.005973616226886709
        model: {}
        policy_loss: -0.01130775130392673
        total_loss: 6.658358136812846
        vf_explained_var: 0.9877612590789795
        vf_loss: 6.669783552487691
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25172413793104
    gpu_util_percent0: 0.4282758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14824191439528353
    mean_env_wait_ms: 1.202408361279009
    mean_inference_ms: 4.428141951458312
    mean_raw_obs_processing_ms: 0.385378978598013
  time_since_restore: 1057.5744557380676
  time_this_iter_s: 25.650466442108154
  time_total_s: 1057.5744557380676
  timers:
    learn_throughput: 8590.938
    learn_time_ms: 18832.869
    sample_throughput: 23745.671
    sample_time_ms: 6813.537
    update_time_ms: 38.614
  timestamp: 1602649983
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     41 |          1057.57 | 6633472 |  245.873 |              296.242 |              136.242 |            808.433 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3289.2574174174174
    time_step_min: 2960
  date: 2020-10-14_04-33-30
  done: false
  episode_len_mean: 808.2537973926563
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 246.4274644543751
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 249
  episodes_total: 8361
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.511997178196907
        entropy_coeff: 0.0005000000000000001
        kl: 0.005024631430084507
        model: {}
        policy_loss: -0.010512914877229681
        total_loss: 6.375741799672444
        vf_explained_var: 0.9888591766357422
        vf_loss: 6.3863853216171265
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.576666666666668
    gpu_util_percent0: 0.272
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14818046719360897
    mean_env_wait_ms: 1.202562429175877
    mean_inference_ms: 4.423873461614131
    mean_raw_obs_processing_ms: 0.38517206598988163
  time_since_restore: 1083.6954381465912
  time_this_iter_s: 26.12098240852356
  time_total_s: 1083.6954381465912
  timers:
    learn_throughput: 8573.75
    learn_time_ms: 18870.622
    sample_throughput: 23708.868
    sample_time_ms: 6824.113
    update_time_ms: 37.786
  timestamp: 1602650010
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     42 |           1083.7 | 6795264 |  246.427 |              296.242 |              136.242 |            808.254 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3287.04343220339
    time_step_min: 2960
  date: 2020-10-14_04-33-56
  done: false
  episode_len_mean: 808.2243319268636
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 246.76638750372925
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 171
  episodes_total: 8532
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5099739829699198
        entropy_coeff: 0.0005000000000000001
        kl: 0.005351927014999092
        model: {}
        policy_loss: -0.011824537985376082
        total_loss: 5.416117231051127
        vf_explained_var: 0.9883833527565002
        vf_loss: 5.4280628363291425
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72666666666667
    gpu_util_percent0: 0.26033333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14813973591611085
    mean_env_wait_ms: 1.2026640958478494
    mean_inference_ms: 4.421070885730757
    mean_raw_obs_processing_ms: 0.38504267323140406
  time_since_restore: 1109.6357214450836
  time_this_iter_s: 25.94028329849243
  time_total_s: 1109.6357214450836
  timers:
    learn_throughput: 8571.195
    learn_time_ms: 18876.248
    sample_throughput: 23657.0
    sample_time_ms: 6839.075
    update_time_ms: 36.71
  timestamp: 1602650036
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     43 |          1109.64 | 6957056 |  246.766 |              296.242 |              136.242 |            808.224 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3284.6597807270628
    time_step_min: 2960
  date: 2020-10-14_04-34-22
  done: false
  episode_len_mean: 808.2108952993909
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 247.1329697387621
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 169
  episodes_total: 8701
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5150950103998184
        entropy_coeff: 0.0005000000000000001
        kl: 0.005764365817109744
        model: {}
        policy_loss: -0.011448757897596806
        total_loss: 5.494906306266785
        vf_explained_var: 0.9883672595024109
        vf_loss: 5.506468613942464
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.666666666666668
    gpu_util_percent0: 0.29800000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14810204951954017
    mean_env_wait_ms: 1.2027453324260977
    mean_inference_ms: 4.418418174771012
    mean_raw_obs_processing_ms: 0.38491860867450345
  time_since_restore: 1135.6197011470795
  time_this_iter_s: 25.98397970199585
  time_total_s: 1135.6197011470795
  timers:
    learn_throughput: 8567.533
    learn_time_ms: 18884.316
    sample_throughput: 23640.794
    sample_time_ms: 6843.763
    update_time_ms: 35.067
  timestamp: 1602650062
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     44 |          1135.62 | 7118848 |  247.133 |              296.242 |              136.242 |            808.211 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3281.3542837868226
    time_step_min: 2960
  date: 2020-10-14_04-34-48
  done: false
  episode_len_mean: 808.1973124300112
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 247.62341782890488
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 229
  episodes_total: 8930
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4971722587943077
        entropy_coeff: 0.0005000000000000001
        kl: 0.005169067066162825
        model: {}
        policy_loss: -0.008737684285733849
        total_loss: 6.912025690078735
        vf_explained_var: 0.9879312515258789
        vf_loss: 6.920882701873779
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.600000000000005
    gpu_util_percent0: 0.3573333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14805159985077296
    mean_env_wait_ms: 1.202872003870985
    mean_inference_ms: 4.414976487354489
    mean_raw_obs_processing_ms: 0.38474668429138525
  time_since_restore: 1161.3081152439117
  time_this_iter_s: 25.688414096832275
  time_total_s: 1161.3081152439117
  timers:
    learn_throughput: 8582.053
    learn_time_ms: 18852.366
    sample_throughput: 23639.099
    sample_time_ms: 6844.254
    update_time_ms: 35.248
  timestamp: 1602650088
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     45 |          1161.31 | 7280640 |  247.623 |              296.242 |              136.242 |            808.197 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3278.586082191781
    time_step_min: 2960
  date: 2020-10-14_04-35-14
  done: false
  episode_len_mean: 808.1566422879598
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 248.04835055058825
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 231
  episodes_total: 9161
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.48112762719392776
        entropy_coeff: 0.0005000000000000001
        kl: 0.005684606071251134
        model: {}
        policy_loss: -0.010469781302769357
        total_loss: 5.727801442146301
        vf_explained_var: 0.9894949793815613
        vf_loss: 5.738369425137837
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.686666666666667
    gpu_util_percent0: 0.32933333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14800250294638118
    mean_env_wait_ms: 1.2029366356424835
    mean_inference_ms: 4.411590008556114
    mean_raw_obs_processing_ms: 0.3845906383538417
  time_since_restore: 1187.3296282291412
  time_this_iter_s: 26.021512985229492
  time_total_s: 1187.3296282291412
  timers:
    learn_throughput: 8577.678
    learn_time_ms: 18861.98
    sample_throughput: 23583.567
    sample_time_ms: 6860.37
    update_time_ms: 36.492
  timestamp: 1602650114
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     46 |          1187.33 | 7442432 |  248.048 |              296.242 |              136.242 |            808.157 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3276.515614904157
    time_step_min: 2960
  date: 2020-10-14_04-35-40
  done: false
  episode_len_mean: 808.1065222055353
  episode_reward_max: 296.2424242424244
  episode_reward_mean: 248.3415316000598
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 9322
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.48768074065446854
        entropy_coeff: 0.0005000000000000001
        kl: 0.005590466239179174
        model: {}
        policy_loss: -0.011224654345520927
        total_loss: 4.9677205085754395
        vf_explained_var: 0.9889311790466309
        vf_loss: 4.979049324989319
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.351724137931036
    gpu_util_percent0: 0.3213793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479697299162752
    mean_env_wait_ms: 1.202990678760922
    mean_inference_ms: 4.409337787781627
    mean_raw_obs_processing_ms: 0.38448235674683723
  time_since_restore: 1213.0403983592987
  time_this_iter_s: 25.71077013015747
  time_total_s: 1213.0403983592987
  timers:
    learn_throughput: 8583.881
    learn_time_ms: 18848.351
    sample_throughput: 23626.477
    sample_time_ms: 6847.911
    update_time_ms: 35.532
  timestamp: 1602650140
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     47 |          1213.04 | 7604224 |  248.342 |              296.242 |              136.242 |            808.107 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3274.290646114865
    time_step_min: 2960
  date: 2020-10-14_04-36-06
  done: false
  episode_len_mean: 808.0292385359697
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 248.69652987595768
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 9508
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.48659316698710126
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058739522549634176
        model: {}
        policy_loss: -0.011002960692470273
        total_loss: 6.946582198143005
        vf_explained_var: 0.9860231876373291
        vf_loss: 6.957681616147359
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.71
    gpu_util_percent0: 0.35533333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14793378521742498
    mean_env_wait_ms: 1.2030531606802517
    mean_inference_ms: 4.406842320880105
    mean_raw_obs_processing_ms: 0.3843654905941349
  time_since_restore: 1238.5780448913574
  time_this_iter_s: 25.537646532058716
  time_total_s: 1238.5780448913574
  timers:
    learn_throughput: 8590.551
    learn_time_ms: 18833.716
    sample_throughput: 23592.091
    sample_time_ms: 6857.892
    update_time_ms: 35.301
  timestamp: 1602650166
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     48 |          1238.58 | 7766016 |  248.697 |              296.394 |              136.242 |            808.029 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3271.225584027992
    time_step_min: 2960
  date: 2020-10-14_04-36-32
  done: false
  episode_len_mean: 807.914077719676
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 249.17858374579382
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 245
  episodes_total: 9753
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4660199632247289
        entropy_coeff: 0.0005000000000000001
        kl: 0.005540805713584025
        model: {}
        policy_loss: -0.009368036281860745
        total_loss: 7.123373548189799
        vf_explained_var: 0.9875916838645935
        vf_loss: 7.1328361829121905
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.934482758620696
    gpu_util_percent0: 0.3486206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478893821439412
    mean_env_wait_ms: 1.203124193515906
    mean_inference_ms: 4.403703167822463
    mean_raw_obs_processing_ms: 0.3842149417953053
  time_since_restore: 1264.4292039871216
  time_this_iter_s: 25.85115909576416
  time_total_s: 1264.4292039871216
  timers:
    learn_throughput: 8597.578
    learn_time_ms: 18818.323
    sample_throughput: 23571.217
    sample_time_ms: 6863.965
    update_time_ms: 35.427
  timestamp: 1602650192
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     49 |          1264.43 | 7927808 |  249.179 |              296.394 |              136.242 |            807.914 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3268.990218816174
    time_step_min: 2960
  date: 2020-10-14_04-36-58
  done: false
  episode_len_mean: 807.9160052245554
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 249.51915822547792
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 200
  episodes_total: 9953
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.45666811615228653
        entropy_coeff: 0.0005000000000000001
        kl: 0.005614828163137038
        model: {}
        policy_loss: -0.010633054165130792
        total_loss: 7.585811694463094
        vf_explained_var: 0.9846763014793396
        vf_loss: 7.5965326229731245
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.063333333333336
    gpu_util_percent0: 0.32899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14785175473561316
    mean_env_wait_ms: 1.203143082560486
    mean_inference_ms: 4.401132494611177
    mean_raw_obs_processing_ms: 0.3840991935699795
  time_since_restore: 1289.9106843471527
  time_this_iter_s: 25.481480360031128
  time_total_s: 1289.9106843471527
  timers:
    learn_throughput: 8604.193
    learn_time_ms: 18803.854
    sample_throughput: 23553.311
    sample_time_ms: 6869.183
    update_time_ms: 34.658
  timestamp: 1602650218
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     50 |          1289.91 | 8089600 |  249.519 |              296.394 |              136.242 |            807.916 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3267.188231792022
    time_step_min: 2960
  date: 2020-10-14_04-37-24
  done: false
  episode_len_mean: 807.809373146134
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 249.80069330840533
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 161
  episodes_total: 10114
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4684673274556796
        entropy_coeff: 0.0005000000000000001
        kl: 0.005773888862070938
        model: {}
        policy_loss: -0.009907488614165535
        total_loss: 6.344697276751201
        vf_explained_var: 0.9859089851379395
        vf_loss: 6.354694485664368
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.068965517241384
    gpu_util_percent0: 0.38999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782393013823195
    mean_env_wait_ms: 1.2031681360455855
    mean_inference_ms: 4.399167385467646
    mean_raw_obs_processing_ms: 0.3840085885340416
  time_since_restore: 1315.597377538681
  time_this_iter_s: 25.68669319152832
  time_total_s: 1315.597377538681
  timers:
    learn_throughput: 8600.782
    learn_time_ms: 18811.313
    sample_throughput: 23568.323
    sample_time_ms: 6864.808
    update_time_ms: 34.716
  timestamp: 1602650244
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     51 |           1315.6 | 8251392 |  249.801 |              296.394 |              136.242 |            807.809 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3264.671492421298
    time_step_min: 2960
  date: 2020-10-14_04-37-50
  done: false
  episode_len_mean: 807.5570294345468
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 250.17332406168575
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 214
  episodes_total: 10328
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4603114575147629
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055629881874968605
        model: {}
        policy_loss: -0.009810302134913703
        total_loss: 6.591102838516235
        vf_explained_var: 0.9873318672180176
        vf_loss: 6.601004242897034
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.913333333333334
    gpu_util_percent0: 0.326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14778825775666407
    mean_env_wait_ms: 1.2032076118042145
    mean_inference_ms: 4.396653317339464
    mean_raw_obs_processing_ms: 0.383889904320731
  time_since_restore: 1341.3213968276978
  time_this_iter_s: 25.724019289016724
  time_total_s: 1341.3213968276978
  timers:
    learn_throughput: 8614.166
    learn_time_ms: 18782.086
    sample_throughput: 23608.884
    sample_time_ms: 6853.013
    update_time_ms: 34.511
  timestamp: 1602650270
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     52 |          1341.32 | 8413184 |  250.173 |              296.394 |              136.242 |            807.557 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3262.337414578588
    time_step_min: 2960
  date: 2020-10-14_04-38-16
  done: false
  episode_len_mean: 807.3285092697693
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 250.56256950893723
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 244
  episodes_total: 10572
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.436347837249438
        entropy_coeff: 0.0005000000000000001
        kl: 0.005471786794563134
        model: {}
        policy_loss: -0.006899951782543212
        total_loss: 6.8829309940338135
        vf_explained_var: 0.9876399636268616
        vf_loss: 6.889912207921346
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583333333333336
    gpu_util_percent0: 0.373
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14774925967672572
    mean_env_wait_ms: 1.203241223395196
    mean_inference_ms: 4.393918211480122
    mean_raw_obs_processing_ms: 0.38376492735147927
  time_since_restore: 1367.0577018260956
  time_this_iter_s: 25.736304998397827
  time_total_s: 1367.0577018260956
  timers:
    learn_throughput: 8616.321
    learn_time_ms: 18777.387
    sample_throughput: 23667.127
    sample_time_ms: 6836.149
    update_time_ms: 34.439
  timestamp: 1602650296
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     53 |          1367.06 | 8574976 |  250.563 |              296.394 |              136.242 |            807.329 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3260.346750093388
    time_step_min: 2960
  date: 2020-10-14_04-38-42
  done: false
  episode_len_mean: 807.1897803425168
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 250.84764435118115
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 172
  episodes_total: 10744
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.44254865994056064
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053455687981719775
        model: {}
        policy_loss: -0.011176405164102713
        total_loss: 5.371111472447713
        vf_explained_var: 0.9878227710723877
        vf_loss: 5.382375359535217
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.117241379310347
    gpu_util_percent0: 0.39620689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14772146135659558
    mean_env_wait_ms: 1.2032587472829281
    mean_inference_ms: 4.392010670771665
    mean_raw_obs_processing_ms: 0.3836790521291076
  time_since_restore: 1392.7620322704315
  time_this_iter_s: 25.704330444335938
  time_total_s: 1392.7620322704315
  timers:
    learn_throughput: 8632.125
    learn_time_ms: 18743.008
    sample_throughput: 23651.12
    sample_time_ms: 6840.776
    update_time_ms: 36.196
  timestamp: 1602650322
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     54 |          1392.76 | 8736768 |  250.848 |              296.394 |              136.242 |             807.19 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3258.4320602276903
    time_step_min: 2960
  date: 2020-10-14_04-39-08
  done: false
  episode_len_mean: 806.9568081991215
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 251.12959204046314
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 184
  episodes_total: 10928
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4463835855325063
        entropy_coeff: 0.0005000000000000001
        kl: 0.005551510878528158
        model: {}
        policy_loss: -0.011200966138858348
        total_loss: 5.981295307477315
        vf_explained_var: 0.9874823689460754
        vf_loss: 5.992580413818359
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.869999999999997
    gpu_util_percent0: 0.34266666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769455847179883
    mean_env_wait_ms: 1.2032803616680694
    mean_inference_ms: 4.390088033925834
    mean_raw_obs_processing_ms: 0.3835907233905885
  time_since_restore: 1418.4506378173828
  time_this_iter_s: 25.688605546951294
  time_total_s: 1418.4506378173828
  timers:
    learn_throughput: 8638.159
    learn_time_ms: 18729.916
    sample_throughput: 23604.332
    sample_time_ms: 6854.335
    update_time_ms: 35.577
  timestamp: 1602650348
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     55 |          1418.45 | 8898560 |   251.13 |              296.394 |              136.242 |            806.957 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3256.056419009972
    time_step_min: 2960
  date: 2020-10-14_04-39-34
  done: false
  episode_len_mean: 806.6324885824304
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 251.48377389006023
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 239
  episodes_total: 11167
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42752107481161755
        entropy_coeff: 0.0005000000000000001
        kl: 0.005104644185242553
        model: {}
        policy_loss: -0.010616361068969127
        total_loss: 7.0546257098515825
        vf_explained_var: 0.9876191020011902
        vf_loss: 7.065328319867452
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.653333333333343
    gpu_util_percent0: 0.2813333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147659580797148
    mean_env_wait_ms: 1.203312258669073
    mean_inference_ms: 4.387619018853048
    mean_raw_obs_processing_ms: 0.3834735072660125
  time_since_restore: 1444.288244009018
  time_this_iter_s: 25.837606191635132
  time_total_s: 1444.288244009018
  timers:
    learn_throughput: 8642.105
    learn_time_ms: 18721.364
    sample_throughput: 23634.694
    sample_time_ms: 6845.53
    update_time_ms: 34.289
  timestamp: 1602650374
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     56 |          1444.29 | 9060352 |  251.484 |              296.394 |              136.242 |            806.632 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3253.932074805928
    time_step_min: 2960
  date: 2020-10-14_04-39-59
  done: false
  episode_len_mean: 806.4540098487513
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 251.82604802865086
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 205
  episodes_total: 11372
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.41268759469191235
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642210831865668
        model: {}
        policy_loss: -0.011997847468592227
        total_loss: 4.267285346984863
        vf_explained_var: 0.9909687638282776
        vf_loss: 4.279348532358806
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.42413793103448
    gpu_util_percent0: 0.32965517241379305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476294426898053
    mean_env_wait_ms: 1.2033217391798539
    mean_inference_ms: 4.385563874322796
    mean_raw_obs_processing_ms: 0.3833838247514867
  time_since_restore: 1469.7688946723938
  time_this_iter_s: 25.480650663375854
  time_total_s: 1469.7688946723938
  timers:
    learn_throughput: 8654.388
    learn_time_ms: 18694.795
    sample_throughput: 23625.808
    sample_time_ms: 6848.105
    update_time_ms: 33.976
  timestamp: 1602650399
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     57 |          1469.77 | 9222144 |  251.826 |              296.394 |              136.242 |            806.454 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3252.293227853603
    time_step_min: 2960
  date: 2020-10-14_04-40-26
  done: false
  episode_len_mean: 806.2417887165266
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 252.07888924779468
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 167
  episodes_total: 11539
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42186784744262695
        entropy_coeff: 0.0005000000000000001
        kl: 0.006096464271346728
        model: {}
        policy_loss: -0.010270674087223597
        total_loss: 4.845337271690369
        vf_explained_var: 0.9890034198760986
        vf_loss: 4.855666438738505
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.683333333333334
    gpu_util_percent0: 0.30400000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760754042769755
    mean_env_wait_ms: 1.2033302014083878
    mean_inference_ms: 4.383950965534726
    mean_raw_obs_processing_ms: 0.38331102518519766
  time_since_restore: 1495.7401950359344
  time_this_iter_s: 25.97130036354065
  time_total_s: 1495.7401950359344
  timers:
    learn_throughput: 8640.297
    learn_time_ms: 18725.282
    sample_throughput: 23590.818
    sample_time_ms: 6858.261
    update_time_ms: 35.496
  timestamp: 1602650426
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     58 |          1495.74 | 9383936 |  252.079 |              296.394 |              136.242 |            806.242 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3250.10774611841
    time_step_min: 2960
  date: 2020-10-14_04-40-52
  done: false
  episode_len_mean: 805.9806939955774
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 252.39672537588845
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 219
  episodes_total: 11758
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4129921222726504
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053271743236109614
        model: {}
        policy_loss: -0.011034174482726181
        total_loss: 5.47857944170634
        vf_explained_var: 0.9897108674049377
        vf_loss: 5.489686965942383
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.756666666666668
    gpu_util_percent0: 0.2843333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757853559667733
    mean_env_wait_ms: 1.2033493641016728
    mean_inference_ms: 4.381910645187172
    mean_raw_obs_processing_ms: 0.3832155617848996
  time_since_restore: 1521.7157394886017
  time_this_iter_s: 25.975544452667236
  time_total_s: 1521.7157394886017
  timers:
    learn_throughput: 8632.316
    learn_time_ms: 18742.594
    sample_throughput: 23596.858
    sample_time_ms: 6856.506
    update_time_ms: 33.982
  timestamp: 1602650452
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     59 |          1521.72 | 9545728 |  252.397 |              296.394 |              136.242 |            805.981 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3247.9415405201976
    time_step_min: 2960
  date: 2020-10-14_04-41-18
  done: false
  episode_len_mean: 805.7597765363129
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 252.7291248177598
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 235
  episodes_total: 11993
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3986821745832761
        entropy_coeff: 0.0005000000000000001
        kl: 0.005131128787373503
        model: {}
        policy_loss: -0.010990443173795938
        total_loss: 4.98269510269165
        vf_explained_var: 0.9906366467475891
        vf_loss: 4.993756691614787
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69333333333333
    gpu_util_percent0: 0.32633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754751105514
    mean_env_wait_ms: 1.2033626444566419
    mean_inference_ms: 4.379785805167235
    mean_raw_obs_processing_ms: 0.38312140860438454
  time_since_restore: 1547.5642569065094
  time_this_iter_s: 25.848517417907715
  time_total_s: 1547.5642569065094
  timers:
    learn_throughput: 8614.912
    learn_time_ms: 18780.46
    sample_throughput: 23604.57
    sample_time_ms: 6854.266
    update_time_ms: 34.75
  timestamp: 1602650478
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     60 |          1547.56 | 9707520 |  252.729 |              296.394 |              136.242 |             805.76 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3246.39505358615
    time_step_min: 2960
  date: 2020-10-14_04-41-44
  done: false
  episode_len_mean: 805.6408844320237
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 252.97985443785205
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 173
  episodes_total: 12166
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3998661811153094
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056130079707751674
        model: {}
        policy_loss: -0.008932064840337262
        total_loss: 5.2044451634089155
        vf_explained_var: 0.988438069820404
        vf_loss: 5.213436643282573
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.065517241379315
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689655
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752542584198972
    mean_env_wait_ms: 1.203359816345104
    mean_inference_ms: 4.378233643939856
    mean_raw_obs_processing_ms: 0.3830499387132799
  time_since_restore: 1573.1756868362427
  time_this_iter_s: 25.611429929733276
  time_total_s: 1573.1756868362427
  timers:
    learn_throughput: 8613.552
    learn_time_ms: 18783.425
    sample_throughput: 23639.756
    sample_time_ms: 6844.064
    update_time_ms: 33.406
  timestamp: 1602650504
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     61 |          1573.18 | 9869312 |   252.98 |              296.394 |              136.242 |            805.641 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3244.4254161591557
    time_step_min: 2960
  date: 2020-10-14_04-42-10
  done: false
  episode_len_mean: 805.5567970204842
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 253.26912800582946
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 12351
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.40204547842343646
        entropy_coeff: 0.0005000000000000001
        kl: 0.005185766688858469
        model: {}
        policy_loss: -0.009803345756760487
        total_loss: 4.718565662701924
        vf_explained_var: 0.9900150299072266
        vf_loss: 4.728440443674724
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94333333333333
    gpu_util_percent0: 0.3163333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750302955206113
    mean_env_wait_ms: 1.2033616503894442
    mean_inference_ms: 4.376670333268738
    mean_raw_obs_processing_ms: 0.38297894467542876
  time_since_restore: 1598.7957849502563
  time_this_iter_s: 25.620098114013672
  time_total_s: 1598.7957849502563
  timers:
    learn_throughput: 8619.658
    learn_time_ms: 18770.119
    sample_throughput: 23622.511
    sample_time_ms: 6849.06
    update_time_ms: 31.628
  timestamp: 1602650530
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     62 |           1598.8 | 10031104 |  253.269 |              296.394 |              136.242 |            805.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3242.267564122989
    time_step_min: 2960
  date: 2020-10-14_04-42-36
  done: false
  episode_len_mean: 805.4836378077839
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 253.58693768503136
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 239
  episodes_total: 12590
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3832032655676206
        entropy_coeff: 0.0005000000000000001
        kl: 0.004957521761146684
        model: {}
        policy_loss: -0.010417382038819293
        total_loss: 5.678230047225952
        vf_explained_var: 0.9898428320884705
        vf_loss: 5.688715140024821
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.30666666666667
    gpu_util_percent0: 0.40199999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747519461649863
    mean_env_wait_ms: 1.2033586206181428
    mean_inference_ms: 4.374675427715462
    mean_raw_obs_processing_ms: 0.3828858425956549
  time_since_restore: 1624.4103996753693
  time_this_iter_s: 25.614614725112915
  time_total_s: 1624.4103996753693
  timers:
    learn_throughput: 8624.272
    learn_time_ms: 18760.075
    sample_throughput: 23638.702
    sample_time_ms: 6844.369
    update_time_ms: 31.333
  timestamp: 1602650556
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     63 |          1624.41 | 10192896 |  253.587 |              296.394 |              136.242 |            805.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3240.7313479623826
    time_step_min: 2960
  date: 2020-10-14_04-43-02
  done: false
  episode_len_mean: 805.4840575179744
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 253.83679558953077
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 12796
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3708825930953026
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047113581870992976
        model: {}
        policy_loss: -0.01054179862209518
        total_loss: 5.086221973101298
        vf_explained_var: 0.9899683594703674
        vf_loss: 5.096890250841777
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.382758620689657
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474503887698415
    mean_env_wait_ms: 1.2033470187123667
    mean_inference_ms: 4.373014061153682
    mean_raw_obs_processing_ms: 0.38281310110370015
  time_since_restore: 1650.1080584526062
  time_this_iter_s: 25.69765877723694
  time_total_s: 1650.1080584526062
  timers:
    learn_throughput: 8621.909
    learn_time_ms: 18765.219
    sample_throughput: 23655.379
    sample_time_ms: 6839.544
    update_time_ms: 29.44
  timestamp: 1602650582
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     64 |          1650.11 | 10354688 |  253.837 |              296.394 |              136.242 |            805.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3239.3298259187623
    time_step_min: 2960
  date: 2020-10-14_04-43-28
  done: false
  episode_len_mean: 805.4105393102384
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 254.04562405164208
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 165
  episodes_total: 12961
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.37956344336271286
        entropy_coeff: 0.0005000000000000001
        kl: 0.005822447400229673
        model: {}
        policy_loss: -0.009251177175125727
        total_loss: 4.42275853951772
        vf_explained_var: 0.9900858998298645
        vf_loss: 4.432163159052531
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.766666666666673
    gpu_util_percent0: 0.37999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743192827125792
    mean_env_wait_ms: 1.2033374094728457
    mean_inference_ms: 4.3717116020725415
    mean_raw_obs_processing_ms: 0.3827520808195808
  time_since_restore: 1676.0516519546509
  time_this_iter_s: 25.943593502044678
  time_total_s: 1676.0516519546509
  timers:
    learn_throughput: 8612.351
    learn_time_ms: 18786.044
    sample_throughput: 23645.58
    sample_time_ms: 6842.378
    update_time_ms: 30.255
  timestamp: 1602650608
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     65 |          1676.05 | 10516480 |  254.046 |              296.394 |              136.242 |            805.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3237.417313841937
    time_step_min: 2960
  date: 2020-10-14_04-43-54
  done: false
  episode_len_mean: 805.2220956719818
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 254.32208186650095
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 209
  episodes_total: 13170
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3738606820503871
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052025926221782965
        model: {}
        policy_loss: -0.009261313234067833
        total_loss: 5.5259010791778564
        vf_explained_var: 0.9895394444465637
        vf_loss: 5.535316904385884
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15806451612903
    gpu_util_percent0: 0.3148387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740971199310554
    mean_env_wait_ms: 1.2033290375474983
    mean_inference_ms: 4.370123959512515
    mean_raw_obs_processing_ms: 0.38267968589452306
  time_since_restore: 1702.285659790039
  time_this_iter_s: 26.234007835388184
  time_total_s: 1702.285659790039
  timers:
    learn_throughput: 8604.004
    learn_time_ms: 18804.269
    sample_throughput: 23604.498
    sample_time_ms: 6854.287
    update_time_ms: 30.721
  timestamp: 1602650634
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     66 |          1702.29 | 10678272 |  254.322 |              296.394 |              136.242 |            805.222 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3235.4910300493348
    time_step_min: 2960
  date: 2020-10-14_04-44-21
  done: false
  episode_len_mean: 805.0401073505293
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 254.61971888257855
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 244
  episodes_total: 13414
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.35225145022074383
        entropy_coeff: 0.0005000000000000001
        kl: 0.004920171923004091
        model: {}
        policy_loss: -0.01008309266762808
        total_loss: 5.4109346469243365
        vf_explained_var: 0.9899579882621765
        vf_loss: 5.421163002649943
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203333333333333
    gpu_util_percent0: 0.35933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738204796833992
    mean_env_wait_ms: 1.2033078557334298
    mean_inference_ms: 4.368317276857047
    mean_raw_obs_processing_ms: 0.38259549961060646
  time_since_restore: 1728.450202703476
  time_this_iter_s: 26.16454291343689
  time_total_s: 1728.450202703476
  timers:
    learn_throughput: 8580.139
    learn_time_ms: 18856.571
    sample_throughput: 23557.704
    sample_time_ms: 6867.902
    update_time_ms: 32.235
  timestamp: 1602650661
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     67 |          1728.45 | 10840064 |   254.62 |              296.394 |              136.242 |             805.04 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3233.98686540732
    time_step_min: 2960
  date: 2020-10-14_04-44-47
  done: false
  episode_len_mean: 804.9040329702678
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 254.83588014379882
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 174
  episodes_total: 13588
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3531215637922287
        entropy_coeff: 0.0005000000000000001
        kl: 0.005108428847355147
        model: {}
        policy_loss: -0.011081730267809084
        total_loss: 4.278166015942891
        vf_explained_var: 0.9902045726776123
        vf_loss: 4.289408286412557
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.040000000000003
    gpu_util_percent0: 0.39999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736342868525804
    mean_env_wait_ms: 1.2032942999220202
    mean_inference_ms: 4.36705442363236
    mean_raw_obs_processing_ms: 0.3825383767185041
  time_since_restore: 1754.3962082862854
  time_this_iter_s: 25.94600558280945
  time_total_s: 1754.3962082862854
  timers:
    learn_throughput: 8575.252
    learn_time_ms: 18867.318
    sample_throughput: 23605.655
    sample_time_ms: 6853.951
    update_time_ms: 32.538
  timestamp: 1602650687
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     68 |           1754.4 | 11001856 |  254.836 |              296.394 |              136.242 |            804.904 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3232.6620302942033
    time_step_min: 2959
  date: 2020-10-14_04-45-13
  done: false
  episode_len_mean: 804.7653253922139
  episode_reward_max: 296.3939393939395
  episode_reward_mean: 255.04309069779723
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 13768
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.35613494863112766
        entropy_coeff: 0.0005000000000000001
        kl: 0.005562700525236626
        model: {}
        policy_loss: -0.009131224146888902
        total_loss: 5.495855371157329
        vf_explained_var: 0.9884891510009766
        vf_loss: 5.5051471789677935
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03448275862069
    gpu_util_percent0: 0.3627586206896551
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734440014052372
    mean_env_wait_ms: 1.2032709536620578
    mean_inference_ms: 4.3657874086975745
    mean_raw_obs_processing_ms: 0.3824787738185289
  time_since_restore: 1780.0963215827942
  time_this_iter_s: 25.70011329650879
  time_total_s: 1780.0963215827942
  timers:
    learn_throughput: 8585.2
    learn_time_ms: 18845.455
    sample_throughput: 23637.295
    sample_time_ms: 6844.776
    update_time_ms: 34.445
  timestamp: 1602650713
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     69 |           1780.1 | 11163648 |  255.043 |              296.394 |              136.242 |            804.765 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3230.70392900594
    time_step_min: 2959
  date: 2020-10-14_04-45-39
  done: false
  episode_len_mean: 804.658219715897
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 255.34220209086365
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 241
  episodes_total: 14009
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3406384338935216
        entropy_coeff: 0.0005000000000000001
        kl: 0.005524209622914593
        model: {}
        policy_loss: -0.009801516970280014
        total_loss: 5.314028779665629
        vf_explained_var: 0.9903772473335266
        vf_loss: 5.323983470598857
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.836666666666666
    gpu_util_percent0: 0.3406666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8066666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473211574553092
    mean_env_wait_ms: 1.2032612385437655
    mean_inference_ms: 4.36414099580324
    mean_raw_obs_processing_ms: 0.3824018406664521
  time_since_restore: 1805.887692451477
  time_this_iter_s: 25.79137086868286
  time_total_s: 1805.887692451477
  timers:
    learn_throughput: 8592.709
    learn_time_ms: 18828.987
    sample_throughput: 23604.871
    sample_time_ms: 6854.178
    update_time_ms: 35.084
  timestamp: 1602650739
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     70 |          1805.89 | 11325440 |  255.342 |              297.909 |              136.242 |            804.658 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3229.0798843686102
    time_step_min: 2959
  date: 2020-10-14_04-46-05
  done: false
  episode_len_mean: 804.5398410577396
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 255.60027236284353
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 210
  episodes_total: 14219
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3335581570863724
        entropy_coeff: 0.0005000000000000001
        kl: 0.005118343203018109
        model: {}
        policy_loss: -0.008664251750208981
        total_loss: 3.489299555619558
        vf_explained_var: 0.992800235748291
        vf_loss: 3.4981147050857544
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.286666666666665
    gpu_util_percent0: 0.3323333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.860000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729942707514249
    mean_env_wait_ms: 1.203235277673638
    mean_inference_ms: 4.362732064892704
    mean_raw_obs_processing_ms: 0.3823374886770509
  time_since_restore: 1831.6849389076233
  time_this_iter_s: 25.79724645614624
  time_total_s: 1831.6849389076233
  timers:
    learn_throughput: 8585.552
    learn_time_ms: 18844.683
    sample_throughput: 23600.818
    sample_time_ms: 6855.356
    update_time_ms: 35.515
  timestamp: 1602650765
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     71 |          1831.68 | 11487232 |    255.6 |              297.909 |              136.242 |             804.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3227.9840384749423
    time_step_min: 2959
  date: 2020-10-14_04-46-31
  done: false
  episode_len_mean: 804.4161162483488
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 255.77989377189817
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 164
  episodes_total: 14383
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.34026538332303363
        entropy_coeff: 0.0005000000000000001
        kl: 0.005363866764431198
        model: {}
        policy_loss: -0.009909541860300427
        total_loss: 5.410627643267314
        vf_explained_var: 0.9876885414123535
        vf_loss: 5.420690417289734
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.617241379310347
    gpu_util_percent0: 0.4013793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472834104727278
    mean_env_wait_ms: 1.2032161215181782
    mean_inference_ms: 4.361652770930242
    mean_raw_obs_processing_ms: 0.38228514957063636
  time_since_restore: 1857.3869428634644
  time_this_iter_s: 25.702003955841064
  time_total_s: 1857.3869428634644
  timers:
    learn_throughput: 8577.615
    learn_time_ms: 18862.12
    sample_throughput: 23643.525
    sample_time_ms: 6842.973
    update_time_ms: 37.561
  timestamp: 1602650791
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     72 |          1857.39 | 11649024 |   255.78 |              297.909 |              136.242 |            804.416 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3226.49241227769
    time_step_min: 2959
  date: 2020-10-14_04-46-57
  done: false
  episode_len_mean: 804.2507706007261
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 256.00546322184783
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 216
  episodes_total: 14599
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3373646264274915
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051965160528197885
        model: {}
        policy_loss: -0.010510957625228912
        total_loss: 6.452934543291728
        vf_explained_var: 0.9878680109977722
        vf_loss: 6.463597814242045
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.750000000000004
    gpu_util_percent0: 0.3096666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726391305876352
    mean_env_wait_ms: 1.203197670262095
    mean_inference_ms: 4.360281787473423
    mean_raw_obs_processing_ms: 0.3822196731126714
  time_since_restore: 1883.2475612163544
  time_this_iter_s: 25.860618352890015
  time_total_s: 1883.2475612163544
  timers:
    learn_throughput: 8568.644
    learn_time_ms: 18881.869
    sample_throughput: 23619.625
    sample_time_ms: 6849.897
    update_time_ms: 37.128
  timestamp: 1602650817
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     73 |          1883.25 | 11810816 |  256.005 |              297.909 |              136.242 |            804.251 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3224.846081081081
    time_step_min: 2959
  date: 2020-10-14_04-47-24
  done: false
  episode_len_mean: 804.0986114855756
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 256.2684461220454
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 237
  episodes_total: 14836
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3218430131673813
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052020570340876775
        model: {}
        policy_loss: -0.009475928905885667
        total_loss: 4.777470310529073
        vf_explained_var: 0.9910323619842529
        vf_loss: 4.787090698877971
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.880645161290325
    gpu_util_percent0: 0.3258064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724095082084523
    mean_env_wait_ms: 1.2031707655388137
    mean_inference_ms: 4.358806368032341
    mean_raw_obs_processing_ms: 0.38214857666190283
  time_since_restore: 1909.3446929454803
  time_this_iter_s: 26.097131729125977
  time_total_s: 1909.3446929454803
  timers:
    learn_throughput: 8550.006
    learn_time_ms: 18923.027
    sample_throughput: 23626.61
    sample_time_ms: 6847.872
    update_time_ms: 36.818
  timestamp: 1602650844
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     74 |          1909.34 | 11972608 |  256.268 |              297.909 |              136.242 |            804.099 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3223.6562040870845
    time_step_min: 2959
  date: 2020-10-14_04-47-50
  done: false
  episode_len_mean: 804.0159227181879
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 256.4541820604445
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 174
  episodes_total: 15010
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3240286832054456
        entropy_coeff: 0.0005000000000000001
        kl: 0.005266964125136535
        model: {}
        policy_loss: -0.009812670527026057
        total_loss: 4.1972536245981855
        vf_explained_var: 0.9906252026557922
        vf_loss: 4.207211852073669
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.137931034482758
    gpu_util_percent0: 0.34068965517241384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722539416614103
    mean_env_wait_ms: 1.2031484652876703
    mean_inference_ms: 4.3577484638965265
    mean_raw_obs_processing_ms: 0.38209814911852336
  time_since_restore: 1935.1086547374725
  time_this_iter_s: 25.763961791992188
  time_total_s: 1935.1086547374725
  timers:
    learn_throughput: 8553.232
    learn_time_ms: 18915.891
    sample_throughput: 23679.422
    sample_time_ms: 6832.599
    update_time_ms: 39.089
  timestamp: 1602650870
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     75 |          1935.11 | 12134400 |  256.454 |              297.909 |              136.242 |            804.016 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3222.457783641161
    time_step_min: 2959
  date: 2020-10-14_04-48-16
  done: false
  episode_len_mean: 803.9075414582785
  episode_reward_max: 297.90909090909076
  episode_reward_mean: 256.63038518908485
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 186
  episodes_total: 15196
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.32788879921038944
        entropy_coeff: 0.0005000000000000001
        kl: 0.005106039112433791
        model: {}
        policy_loss: -0.010778997612457411
        total_loss: 4.95270053545634
        vf_explained_var: 0.9898748993873596
        vf_loss: 4.963627576828003
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.316666666666663
    gpu_util_percent0: 0.32399999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14720922899028482
    mean_env_wait_ms: 1.2031215386760303
    mean_inference_ms: 4.356651242377476
    mean_raw_obs_processing_ms: 0.38204551262505243
  time_since_restore: 1961.0943298339844
  time_this_iter_s: 25.98567509651184
  time_total_s: 1961.0943298339844
  timers:
    learn_throughput: 8548.505
    learn_time_ms: 18926.349
    sample_throughput: 23775.72
    sample_time_ms: 6804.925
    update_time_ms: 38.939
  timestamp: 1602650896
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     76 |          1961.09 | 12296192 |   256.63 |              297.909 |              136.242 |            803.908 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3220.8537567374506
    time_step_min: 2938
  date: 2020-10-14_04-48-42
  done: false
  episode_len_mean: 803.8274700356333
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 256.8621982703615
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 239
  episodes_total: 15435
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.31383611261844635
        entropy_coeff: 0.0005000000000000001
        kl: 0.004971800837665796
        model: {}
        policy_loss: -0.007888384411974888
        total_loss: 5.696129163106282
        vf_explained_var: 0.9900109171867371
        vf_loss: 5.7041590213775635
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.326666666666668
    gpu_util_percent0: 0.39233333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147188347509413
    mean_env_wait_ms: 1.2031020998778756
    mean_inference_ms: 4.355260440623738
    mean_raw_obs_processing_ms: 0.38197337700786105
  time_since_restore: 1986.8464925289154
  time_this_iter_s: 25.75216269493103
  time_total_s: 1986.8464925289154
  timers:
    learn_throughput: 8564.271
    learn_time_ms: 18891.509
    sample_throughput: 23825.458
    sample_time_ms: 6790.719
    update_time_ms: 45.307
  timestamp: 1602650922
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     77 |          1986.85 | 12457984 |  256.862 |              299.576 |              136.242 |            803.827 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3219.6433835309194
    time_step_min: 2938
  date: 2020-10-14_04-49-08
  done: false
  episode_len_mean: 803.7618438718752
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.043397984706
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 15641
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.30528302987416583
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049380083025122685
        model: {}
        policy_loss: -0.009833232945917795
        total_loss: 5.408826470375061
        vf_explained_var: 0.9891956448554993
        vf_loss: 5.4188045263290405
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.710344827586212
    gpu_util_percent0: 0.3617241379310344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471706882479091
    mean_env_wait_ms: 1.2030699262096853
    mean_inference_ms: 4.354094918991205
    mean_raw_obs_processing_ms: 0.3819183672884376
  time_since_restore: 2012.3627526760101
  time_this_iter_s: 25.516260147094727
  time_total_s: 2012.3627526760101
  timers:
    learn_throughput: 8589.167
    learn_time_ms: 18836.75
    sample_throughput: 23782.968
    sample_time_ms: 6802.852
    update_time_ms: 43.777
  timestamp: 1602650948
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     78 |          2012.36 | 12619776 |  257.043 |              299.576 |              136.242 |            803.762 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3218.7050168072556
    time_step_min: 2938
  date: 2020-10-14_04-49-34
  done: false
  episode_len_mean: 803.7270138581282
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.19698983123646
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 162
  episodes_total: 15803
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3131253769000371
        entropy_coeff: 0.0005000000000000001
        kl: 0.004959821739854912
        model: {}
        policy_loss: -0.011568373551805658
        total_loss: 4.384406864643097
        vf_explained_var: 0.9902186393737793
        vf_loss: 4.396127879619598
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5448275862069
    gpu_util_percent0: 0.3268965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715721903785506
    mean_env_wait_ms: 1.2030479035752388
    mean_inference_ms: 4.353190953660678
    mean_raw_obs_processing_ms: 0.38187270807525525
  time_since_restore: 2037.8849980831146
  time_this_iter_s: 25.522245407104492
  time_total_s: 2037.8849980831146
  timers:
    learn_throughput: 8599.535
    learn_time_ms: 18814.04
    sample_throughput: 23759.808
    sample_time_ms: 6809.483
    update_time_ms: 41.81
  timestamp: 1602650974
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     79 |          2037.88 | 12781568 |  257.197 |              299.576 |              136.242 |            803.727 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3217.264409537518
    time_step_min: 2938
  date: 2020-10-14_04-50-00
  done: false
  episode_len_mean: 803.6711208242273
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.4079792618662
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 212
  episodes_total: 16015
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.3097456494967143
        entropy_coeff: 0.0005000000000000001
        kl: 0.00532760863037159
        model: {}
        policy_loss: -0.009199318175281709
        total_loss: 3.949576715628306
        vf_explained_var: 0.9924254417419434
        vf_loss: 3.958928942680359
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77333333333333
    gpu_util_percent0: 0.328
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714096438955918
    mean_env_wait_ms: 1.2030197024643436
    mean_inference_ms: 4.352055649420605
    mean_raw_obs_processing_ms: 0.38181542544711533
  time_since_restore: 2063.4871883392334
  time_this_iter_s: 25.602190256118774
  time_total_s: 2063.4871883392334
  timers:
    learn_throughput: 8605.103
    learn_time_ms: 18801.866
    sample_throughput: 23778.061
    sample_time_ms: 6804.255
    update_time_ms: 39.025
  timestamp: 1602651000
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     80 |          2063.49 | 12943360 |  257.408 |              299.576 |              136.242 |            803.671 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3215.9624468150705
    time_step_min: 2938
  date: 2020-10-14_04-50-26
  done: false
  episode_len_mean: 803.6356364978773
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.60361816652954
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 238
  episodes_total: 16253
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29288844267527264
        entropy_coeff: 0.0005000000000000001
        kl: 0.005010667955502868
        model: {}
        policy_loss: -0.009869467292446643
        total_loss: 4.766925930976868
        vf_explained_var: 0.9914345741271973
        vf_loss: 4.776940027872722
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.693103448275863
    gpu_util_percent0: 0.3720689655172413
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712149909582742
    mean_env_wait_ms: 1.2029901483463337
    mean_inference_ms: 4.350795196794201
    mean_raw_obs_processing_ms: 0.3817519910940216
  time_since_restore: 2088.8674380779266
  time_this_iter_s: 25.380249738693237
  time_total_s: 2088.8674380779266
  timers:
    learn_throughput: 8623.174
    learn_time_ms: 18762.464
    sample_throughput: 23783.446
    sample_time_ms: 6802.715
    update_time_ms: 37.964
  timestamp: 1602651026
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     81 |          2088.87 | 13105152 |  257.604 |              299.576 |              136.242 |            803.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3214.9252256647965
    time_step_min: 2938
  date: 2020-10-14_04-50-52
  done: false
  episode_len_mean: 803.6209225900682
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.759493670886
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 179
  episodes_total: 16432
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29277368386586505
        entropy_coeff: 0.0005000000000000001
        kl: 0.005134436922768752
        model: {}
        policy_loss: -0.00982299484894611
        total_loss: 3.6141475240389505
        vf_explained_var: 0.9921143054962158
        vf_loss: 3.6241148511568704
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.000000000000004
    gpu_util_percent0: 0.3246666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710773155332343
    mean_env_wait_ms: 1.2029617951334106
    mean_inference_ms: 4.349859337138486
    mean_raw_obs_processing_ms: 0.3817056333792481
  time_since_restore: 2114.503834247589
  time_this_iter_s: 25.636396169662476
  time_total_s: 2114.503834247589
  timers:
    learn_throughput: 8641.79
    learn_time_ms: 18722.047
    sample_throughput: 23691.222
    sample_time_ms: 6829.196
    update_time_ms: 35.871
  timestamp: 1602651052
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     82 |           2114.5 | 13266944 |  257.759 |              299.576 |              136.242 |            803.621 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3213.8907324725474
    time_step_min: 2938
  date: 2020-10-14_04-51-18
  done: false
  episode_len_mean: 803.6116797110175
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 257.9272709028879
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 178
  episodes_total: 16610
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.30325743556022644
        entropy_coeff: 0.0005000000000000001
        kl: 0.005625793749156098
        model: {}
        policy_loss: -0.01004063745494932
        total_loss: 4.162868936856587
        vf_explained_var: 0.9911107420921326
        vf_loss: 4.173058927059174
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793103448275858
    gpu_util_percent0: 0.3375862068965516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709423950209183
    mean_env_wait_ms: 1.2029353326758443
    mean_inference_ms: 4.348964683475418
    mean_raw_obs_processing_ms: 0.38166117141638345
  time_since_restore: 2140.020399093628
  time_this_iter_s: 25.51656484603882
  time_total_s: 2140.020399093628
  timers:
    learn_throughput: 8660.21
    learn_time_ms: 18682.226
    sample_throughput: 23679.98
    sample_time_ms: 6832.438
    update_time_ms: 36.739
  timestamp: 1602651078
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     83 |          2140.02 | 13428736 |  257.927 |              299.576 |              136.242 |            803.612 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3212.459305092813
    time_step_min: 2938
  date: 2020-10-14_04-51-43
  done: false
  episode_len_mean: 803.5714200902398
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.1452239085223
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 234
  episodes_total: 16844
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29385703553756076
        entropy_coeff: 0.0005000000000000001
        kl: 0.005531733234723409
        model: {}
        policy_loss: -0.007117972457005332
        total_loss: 4.653864701588948
        vf_explained_var: 0.9917592406272888
        vf_loss: 4.66112740834554
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.08275862068966
    gpu_util_percent0: 0.3110344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470770512959857
    mean_env_wait_ms: 1.2029111195970164
    mean_inference_ms: 4.347794063807392
    mean_raw_obs_processing_ms: 0.3815992610657303
  time_since_restore: 2165.3381848335266
  time_this_iter_s: 25.31778573989868
  time_total_s: 2165.3381848335266
  timers:
    learn_throughput: 8694.396
    learn_time_ms: 18608.767
    sample_throughput: 23698.558
    sample_time_ms: 6827.082
    update_time_ms: 36.867
  timestamp: 1602651103
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     84 |          2165.34 | 13590528 |  258.145 |              299.576 |              136.242 |            803.571 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3211.1603712624096
    time_step_min: 2938
  date: 2020-10-14_04-52-09
  done: false
  episode_len_mean: 803.5214256404244
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.35177911952627
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 215
  episodes_total: 17059
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.28310780475536984
        entropy_coeff: 0.0005000000000000001
        kl: 0.006362205099624892
        model: {}
        policy_loss: -0.009414531959919259
        total_loss: 3.906277616818746
        vf_explained_var: 0.9923078417778015
        vf_loss: 3.9158312678337097
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.863333333333333
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706059122937076
    mean_env_wait_ms: 1.202868369814428
    mean_inference_ms: 4.346750389444374
    mean_raw_obs_processing_ms: 0.3815475218969213
  time_since_restore: 2190.709169149399
  time_this_iter_s: 25.370984315872192
  time_total_s: 2190.709169149399
  timers:
    learn_throughput: 8711.676
    learn_time_ms: 18571.858
    sample_throughput: 23693.924
    sample_time_ms: 6828.417
    update_time_ms: 33.469
  timestamp: 1602651129
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     85 |          2190.71 | 13752320 |  258.352 |              299.576 |              136.242 |            803.521 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3210.3102565594277
    time_step_min: 2938
  date: 2020-10-14_04-52-35
  done: false
  episode_len_mean: 803.5009579100146
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.4939174033513
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 166
  episodes_total: 17225
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.28725717465082806
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050820055960988
        model: {}
        policy_loss: -0.01056079539800218
        total_loss: 3.478187382221222
        vf_explained_var: 0.9922289848327637
        vf_loss: 3.4888898134231567
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.572413793103454
    gpu_util_percent0: 0.44758620689655165
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704918928893
    mean_env_wait_ms: 1.202840543097969
    mean_inference_ms: 4.345951066052614
    mean_raw_obs_processing_ms: 0.38150687963704255
  time_since_restore: 2216.0446145534515
  time_this_iter_s: 25.335445404052734
  time_total_s: 2216.0446145534515
  timers:
    learn_throughput: 8743.064
    learn_time_ms: 18505.183
    sample_throughput: 23684.663
    sample_time_ms: 6831.087
    update_time_ms: 31.551
  timestamp: 1602651155
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     86 |          2216.04 | 13914112 |  258.494 |              299.576 |              136.242 |            803.501 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3208.9796963073736
    time_step_min: 2938
  date: 2020-10-14_04-53-00
  done: false
  episode_len_mean: 803.4781884973023
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.67730281810174
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 197
  episodes_total: 17422
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.2891300842165947
        entropy_coeff: 0.0005000000000000001
        kl: 0.005348453375821312
        model: {}
        policy_loss: -0.009791116928681731
        total_loss: 4.792139728864034
        vf_explained_var: 0.9905940890312195
        vf_loss: 4.8020734786987305
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58275862068966
    gpu_util_percent0: 0.3234482758620689
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470352610967965
    mean_env_wait_ms: 1.2028071910477753
    mean_inference_ms: 4.345032630568961
    mean_raw_obs_processing_ms: 0.3814591685187386
  time_since_restore: 2241.5601370334625
  time_this_iter_s: 25.515522480010986
  time_total_s: 2241.5601370334625
  timers:
    learn_throughput: 8752.196
    learn_time_ms: 18485.874
    sample_throughput: 23668.502
    sample_time_ms: 6835.752
    update_time_ms: 22.747
  timestamp: 1602651180
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     87 |          2241.56 | 14075904 |  258.677 |              299.576 |              136.242 |            803.478 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3207.562911277513
    time_step_min: 2938
  date: 2020-10-14_04-53-26
  done: false
  episode_len_mean: 803.4110620471015
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 258.8833974939613
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 242
  episodes_total: 17664
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.27116962025562924
        entropy_coeff: 0.0005000000000000001
        kl: 0.004562675875301163
        model: {}
        policy_loss: -0.00924643718462903
        total_loss: 4.395590861638387
        vf_explained_var: 0.9922053217887878
        vf_loss: 4.40497100353241
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.259999999999998
    gpu_util_percent0: 0.38700000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701931189809797
    mean_env_wait_ms: 1.202775613122866
    mean_inference_ms: 4.343931561329129
    mean_raw_obs_processing_ms: 0.38140290154374457
  time_since_restore: 2267.091924905777
  time_this_iter_s: 25.531787872314453
  time_total_s: 2267.091924905777
  timers:
    learn_throughput: 8758.39
    learn_time_ms: 18472.802
    sample_throughput: 23620.993
    sample_time_ms: 6849.5
    update_time_ms: 22.354
  timestamp: 1602651206
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     88 |          2267.09 | 14237696 |  258.883 |              299.576 |              136.242 |            803.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3206.521495117297
    time_step_min: 2938
  date: 2020-10-14_04-53-52
  done: false
  episode_len_mean: 803.3562787050521
  episode_reward_max: 299.5757575757577
  episode_reward_mean: 259.0520229742252
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 190
  episodes_total: 17854
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.2679477706551552
        entropy_coeff: 0.0005000000000000001
        kl: 0.005150679304885368
        model: {}
        policy_loss: -0.011429781695672622
        total_loss: 3.701389968395233
        vf_explained_var: 0.992232084274292
        vf_loss: 3.7129526138305664
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.37241379310345
    gpu_util_percent0: 0.3617241379310345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470059589172084
    mean_env_wait_ms: 1.2027407937706234
    mean_inference_ms: 4.343070305040461
    mean_raw_obs_processing_ms: 0.381359015883314
  time_since_restore: 2292.758987426758
  time_this_iter_s: 25.667062520980835
  time_total_s: 2292.758987426758
  timers:
    learn_throughput: 8747.899
    learn_time_ms: 18494.955
    sample_throughput: 23647.558
    sample_time_ms: 6841.806
    update_time_ms: 21.698
  timestamp: 1602651232
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     89 |          2292.76 | 14399488 |  259.052 |              299.576 |              136.242 |            803.356 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3205.331999110518
    time_step_min: 2938
  date: 2020-10-14_04-54-18
  done: false
  episode_len_mean: 803.2800710164225
  episode_reward_max: 301.0909090909089
  episode_reward_mean: 259.23077983563996
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 170
  episodes_total: 18024
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.27451568841934204
        entropy_coeff: 0.0005000000000000001
        kl: 0.005116381294404467
        model: {}
        policy_loss: -0.009435072677054753
        total_loss: 3.6329266826311746
        vf_explained_var: 0.9919072985649109
        vf_loss: 3.642497976620992
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.636666666666667
    gpu_util_percent0: 0.345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699509669947222
    mean_env_wait_ms: 1.2027124641140832
    mean_inference_ms: 4.342333998838549
    mean_raw_obs_processing_ms: 0.3813220835887648
  time_since_restore: 2317.958030939102
  time_this_iter_s: 25.19904351234436
  time_total_s: 2317.958030939102
  timers:
    learn_throughput: 8766.994
    learn_time_ms: 18454.671
    sample_throughput: 23648.445
    sample_time_ms: 6841.549
    update_time_ms: 21.486
  timestamp: 1602651258
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     90 |          2317.96 | 14561280 |  259.231 |              301.091 |              136.242 |             803.28 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3203.874499259178
    time_step_min: 2938
  date: 2020-10-14_04-54-44
  done: false
  episode_len_mean: 803.1628785804261
  episode_reward_max: 301.0909090909089
  episode_reward_mean: 259.44230740506543
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 235
  episodes_total: 18259
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.2648405184348424
        entropy_coeff: 0.0005000000000000001
        kl: 0.00434200547169894
        model: {}
        policy_loss: -0.009165082970866933
        total_loss: 4.123057206471761
        vf_explained_var: 0.9925705790519714
        vf_loss: 4.132353901863098
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.91034482758621
    gpu_util_percent0: 0.3431034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469797223841062
    mean_env_wait_ms: 1.202676208904217
    mean_inference_ms: 4.341301009523895
    mean_raw_obs_processing_ms: 0.38126372811366066
  time_since_restore: 2343.580926179886
  time_this_iter_s: 25.62289524078369
  time_total_s: 2343.580926179886
  timers:
    learn_throughput: 8759.966
    learn_time_ms: 18469.478
    sample_throughput: 23627.613
    sample_time_ms: 6847.581
    update_time_ms: 23.452
  timestamp: 1602651284
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     91 |          2343.58 | 14723072 |  259.442 |              301.091 |              136.242 |            803.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3202.5689963671853
    time_step_min: 2938
  date: 2020-10-14_04-55-10
  done: false
  episode_len_mean: 803.095297364576
  episode_reward_max: 301.0909090909089
  episode_reward_mean: 259.6464684728118
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 18479
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.24779311940073967
        entropy_coeff: 0.0005000000000000001
        kl: 0.004587585067686935
        model: {}
        policy_loss: -0.00810739710383738
        total_loss: 3.9748273293177285
        vf_explained_var: 0.9924125075340271
        vf_loss: 3.983058234055837
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280000000000005
    gpu_util_percent0: 0.3056666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696528205269777
    mean_env_wait_ms: 1.2026383958358824
    mean_inference_ms: 4.340370723086301
    mean_raw_obs_processing_ms: 0.3812166795587613
  time_since_restore: 2369.1722753047943
  time_this_iter_s: 25.591349124908447
  time_total_s: 2369.1722753047943
  timers:
    learn_throughput: 8749.061
    learn_time_ms: 18492.499
    sample_throughput: 23702.565
    sample_time_ms: 6825.928
    update_time_ms: 25.138
  timestamp: 1602651310
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     92 |          2369.17 | 14884864 |  259.646 |              301.091 |              136.242 |            803.095 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3201.5148030734513
    time_step_min: 2938
  date: 2020-10-14_04-55-36
  done: false
  episode_len_mean: 803.0435458786936
  episode_reward_max: 301.0909090909089
  episode_reward_mean: 259.7961764911408
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 168
  episodes_total: 18647
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2504790226618449
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047872335417196155
        model: {}
        policy_loss: -0.00997145293149515
        total_loss: 3.077739735444387
        vf_explained_var: 0.9931519031524658
        vf_loss: 3.0878362456957498
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.927586206896557
    gpu_util_percent0: 0.34275862068965524
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695528509171035
    mean_env_wait_ms: 1.2026084330222493
    mean_inference_ms: 4.339674070993923
    mean_raw_obs_processing_ms: 0.3811809025842142
  time_since_restore: 2394.5282788276672
  time_this_iter_s: 25.356003522872925
  time_total_s: 2394.5282788276672
  timers:
    learn_throughput: 8750.944
    learn_time_ms: 18488.52
    sample_throughput: 23743.236
    sample_time_ms: 6814.236
    update_time_ms: 24.361
  timestamp: 1602651336
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     93 |          2394.53 | 15046656 |  259.796 |              301.091 |              136.242 |            803.044 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3200.1766081871347
    time_step_min: 2918
  date: 2020-10-14_04-56-02
  done: false
  episode_len_mean: 802.9568608723337
  episode_reward_max: 302.6060606060604
  episode_reward_mean: 259.99529037590156
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 18846
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.25324588765700656
        entropy_coeff: 0.0005000000000000001
        kl: 0.004991384611154596
        model: {}
        policy_loss: -0.01023354880453553
        total_loss: 3.584429462750753
        vf_explained_var: 0.9929077625274658
        vf_loss: 3.5947894851366677
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.806666666666672
    gpu_util_percent0: 0.4223333333333332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694307890469
    mean_env_wait_ms: 1.2025751652096253
    mean_inference_ms: 4.338877187589009
    mean_raw_obs_processing_ms: 0.3811384129879067
  time_since_restore: 2420.3094267845154
  time_this_iter_s: 25.781147956848145
  time_total_s: 2420.3094267845154
  timers:
    learn_throughput: 8729.484
    learn_time_ms: 18533.97
    sample_throughput: 23747.408
    sample_time_ms: 6813.038
    update_time_ms: 25.989
  timestamp: 1602651362
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     94 |          2420.31 | 15208448 |  259.995 |              302.606 |              136.242 |            802.957 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3198.641012179756
    time_step_min: 2918
  date: 2020-10-14_04-56-28
  done: false
  episode_len_mean: 802.8586774261162
  episode_reward_max: 302.6060606060604
  episode_reward_mean: 260.22600877777984
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 238
  episodes_total: 19084
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2388700358569622
        entropy_coeff: 0.0005000000000000001
        kl: 0.005095000029541552
        model: {}
        policy_loss: -0.008878010413657952
        total_loss: 3.757145663102468
        vf_explained_var: 0.9932150840759277
        vf_loss: 3.766142984231313
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.549999999999997
    gpu_util_percent0: 0.3546666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692937554499302
    mean_env_wait_ms: 1.2025376111528012
    mean_inference_ms: 4.337921241031912
    mean_raw_obs_processing_ms: 0.3810879553670911
  time_since_restore: 2445.9639980793
  time_this_iter_s: 25.654571294784546
  time_total_s: 2445.9639980793
  timers:
    learn_throughput: 8717.143
    learn_time_ms: 18560.209
    sample_throughput: 23755.485
    sample_time_ms: 6810.722
    update_time_ms: 27.68
  timestamp: 1602651388
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     95 |          2445.96 | 15370240 |  260.226 |              302.606 |              136.242 |            802.859 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3197.4089704277326
    time_step_min: 2918
  date: 2020-10-14_04-56-54
  done: false
  episode_len_mean: 802.7863775483737
  episode_reward_max: 302.6060606060604
  episode_reward_mean: 260.4168918525924
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 193
  episodes_total: 19277
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2320684939622879
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047873918665573
        model: {}
        policy_loss: -0.00876318987381334
        total_loss: 3.3747625946998596
        vf_explained_var: 0.9928036332130432
        vf_loss: 3.3836416999499
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.434482758620696
    gpu_util_percent0: 0.3813793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691767872486483
    mean_env_wait_ms: 1.2024984055173278
    mean_inference_ms: 4.337153500050552
    mean_raw_obs_processing_ms: 0.3810485626740967
  time_since_restore: 2471.253231048584
  time_this_iter_s: 25.289232969284058
  time_total_s: 2471.253231048584
  timers:
    learn_throughput: 8720.543
    learn_time_ms: 18552.972
    sample_throughput: 23754.113
    sample_time_ms: 6811.115
    update_time_ms: 29.164
  timestamp: 1602651414
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     96 |          2471.25 | 15532032 |  260.417 |              302.606 |              136.242 |            802.786 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3196.3131729431766
    time_step_min: 2918
  date: 2020-10-14_04-57-20
  done: false
  episode_len_mean: 802.7279271867126
  episode_reward_max: 302.6060606060604
  episode_reward_mean: 260.57517946991896
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 170
  episodes_total: 19447
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.24688522890210152
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048849678520734114
        model: {}
        policy_loss: -0.008778546199512979
        total_loss: 4.057907541592916
        vf_explained_var: 0.9914454817771912
        vf_loss: 4.066809415817261
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.62333333333333
    gpu_util_percent0: 0.318
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690813669344469
    mean_env_wait_ms: 1.202466642611047
    mean_inference_ms: 4.336511507523338
    mean_raw_obs_processing_ms: 0.38101553551166
  time_since_restore: 2496.8879940509796
  time_this_iter_s: 25.63476300239563
  time_total_s: 2496.8879940509796
  timers:
    learn_throughput: 8714.503
    learn_time_ms: 18565.833
    sample_throughput: 23764.627
    sample_time_ms: 6808.102
    update_time_ms: 29.679
  timestamp: 1602651440
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     97 |          2496.89 | 15693824 |  260.575 |              302.606 |              136.242 |            802.728 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3194.8206903573973
    time_step_min: 2918
  date: 2020-10-14_04-57-45
  done: false
  episode_len_mean: 802.6548429718467
  episode_reward_max: 303.060606060606
  episode_reward_mean: 260.79570940628963
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 231
  episodes_total: 19678
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2386202352742354
        entropy_coeff: 0.0005000000000000001
        kl: 0.004923707262302439
        model: {}
        policy_loss: -0.00832096382509917
        total_loss: 3.9133440057436624
        vf_explained_var: 0.992855966091156
        vf_loss: 3.921784241994222
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73448275862069
    gpu_util_percent0: 0.4089655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689547994636767
    mean_env_wait_ms: 1.2024309914723832
    mean_inference_ms: 4.335627432131521
    mean_raw_obs_processing_ms: 0.38096453388137763
  time_since_restore: 2522.314104318619
  time_this_iter_s: 25.42611026763916
  time_total_s: 2522.314104318619
  timers:
    learn_throughput: 8708.069
    learn_time_ms: 18579.549
    sample_throughput: 23852.583
    sample_time_ms: 6782.997
    update_time_ms: 29.614
  timestamp: 1602651465
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     98 |          2522.31 | 15855616 |  260.796 |              303.061 |              136.242 |            802.655 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3193.3644648071695
    time_step_min: 2918
  date: 2020-10-14_04-58-11
  done: false
  episode_len_mean: 802.6171474520053
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.01054468699454
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 220
  episodes_total: 19898
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.22250954930981
        entropy_coeff: 0.0005000000000000001
        kl: 0.004387328478818138
        model: {}
        policy_loss: -0.009187571772296602
        total_loss: 3.756307045618693
        vf_explained_var: 0.9926998615264893
        vf_loss: 3.7656057874361673
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62333333333334
    gpu_util_percent0: 0.3996666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468828681195805
    mean_env_wait_ms: 1.202386257236009
    mean_inference_ms: 4.334808132050192
    mean_raw_obs_processing_ms: 0.3809232305589456
  time_since_restore: 2547.8544273376465
  time_this_iter_s: 25.54032301902771
  time_total_s: 2547.8544273376465
  timers:
    learn_throughput: 8715.506
    learn_time_ms: 18563.695
    sample_throughput: 23845.506
    sample_time_ms: 6785.01
    update_time_ms: 29.899
  timestamp: 1602651491
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |     99 |          2547.85 | 16017408 |  261.011 |              303.061 |              136.242 |            802.617 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3192.304347826087
    time_step_min: 2918
  date: 2020-10-14_04-58-37
  done: false
  episode_len_mean: 802.5968907269919
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.1705660924356
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 171
  episodes_total: 20069
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.22806943828860918
        entropy_coeff: 0.0005000000000000001
        kl: 0.004183990609211226
        model: {}
        policy_loss: -0.011238508105937703
        total_loss: 2.9428491592407227
        vf_explained_var: 0.993543803691864
        vf_loss: 2.9542016983032227
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.71379310344828
    gpu_util_percent0: 0.37482758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687379256539954
    mean_env_wait_ms: 1.2023521368332362
    mean_inference_ms: 4.334187146621512
    mean_raw_obs_processing_ms: 0.3808913391259956
  time_since_restore: 2573.192626476288
  time_this_iter_s: 25.338199138641357
  time_total_s: 2573.192626476288
  timers:
    learn_throughput: 8705.227
    learn_time_ms: 18585.616
    sample_throughput: 23877.088
    sample_time_ms: 6776.036
    update_time_ms: 30.058
  timestamp: 1602651517
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    100 |          2573.19 | 16179200 |  261.171 |              303.061 |              136.242 |            802.597 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3190.9797350731515
    time_step_min: 2918
  date: 2020-10-14_04-59-03
  done: false
  episode_len_mean: 802.558219853957
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.3628858149284
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 20268
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2327657255033652
        entropy_coeff: 0.0005000000000000001
        kl: 0.004338842933066189
        model: {}
        policy_loss: -0.008990125914957995
        total_loss: 3.1982950965563455
        vf_explained_var: 0.9936439990997314
        vf_loss: 3.2074015935262046
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46
    gpu_util_percent0: 0.35299999999999987
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468628228954511
    mean_env_wait_ms: 1.2023148085112407
    mean_inference_ms: 4.3334792906322495
    mean_raw_obs_processing_ms: 0.3808523959976327
  time_since_restore: 2598.5770881175995
  time_this_iter_s: 25.384461641311646
  time_total_s: 2598.5770881175995
  timers:
    learn_throughput: 8722.683
    learn_time_ms: 18548.421
    sample_throughput: 23831.981
    sample_time_ms: 6788.861
    update_time_ms: 30.187
  timestamp: 1602651543
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    101 |          2598.58 | 16340992 |  261.363 |              303.061 |              136.242 |            802.558 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3189.4686309000294
    time_step_min: 2918
  date: 2020-10-14_04-59-29
  done: false
  episode_len_mean: 802.5222905082431
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.59390510312363
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 234
  episodes_total: 20502
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2161901444196701
        entropy_coeff: 0.0005000000000000001
        kl: 0.004484265538242956
        model: {}
        policy_loss: -0.009577901513694087
        total_loss: 3.2329789996147156
        vf_explained_var: 0.9940950870513916
        vf_loss: 3.2426650722821555
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4551724137931
    gpu_util_percent0: 0.34448275862068967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468515500262763
    mean_env_wait_ms: 1.202272697648442
    mean_inference_ms: 4.332653782424042
    mean_raw_obs_processing_ms: 0.38080775579972326
  time_since_restore: 2624.1580414772034
  time_this_iter_s: 25.580953359603882
  time_total_s: 2624.1580414772034
  timers:
    learn_throughput: 8722.806
    learn_time_ms: 18548.161
    sample_throughput: 23836.03
    sample_time_ms: 6787.708
    update_time_ms: 29.293
  timestamp: 1602651569
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    102 |          2624.16 | 16502784 |  261.594 |              303.061 |              136.242 |            802.522 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3188.174645432983
    time_step_min: 2918
  date: 2020-10-14_04-59-55
  done: false
  episode_len_mean: 802.4784247402754
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.7829881321062
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 193
  episodes_total: 20695
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.2085278940697511
        entropy_coeff: 0.0005000000000000001
        kl: 0.004920714107962946
        model: {}
        policy_loss: -0.009173751110211015
        total_loss: 2.8402439753214517
        vf_explained_var: 0.9939279556274414
        vf_loss: 2.8495219945907593
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793333333333337
    gpu_util_percent0: 0.35066666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684091336805657
    mean_env_wait_ms: 1.2022266680605342
    mean_inference_ms: 4.331975467840187
    mean_raw_obs_processing_ms: 0.38077180657997367
  time_since_restore: 2649.442456007004
  time_this_iter_s: 25.284414529800415
  time_total_s: 2649.442456007004
  timers:
    learn_throughput: 8729.749
    learn_time_ms: 18533.407
    sample_throughput: 23810.38
    sample_time_ms: 6795.02
    update_time_ms: 28.011
  timestamp: 1602651595
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    103 |          2649.44 | 16664576 |  261.783 |              303.061 |              136.242 |            802.478 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3187.109105745692
    time_step_min: 2918
  date: 2020-10-14_05-00-21
  done: false
  episode_len_mean: 802.4490871627773
  episode_reward_max: 303.060606060606
  episode_reward_mean: 261.95087682614627
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 174
  episodes_total: 20869
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.21678480009237924
        entropy_coeff: 0.0005000000000000001
        kl: 0.004601365653797984
        model: {}
        policy_loss: -0.008773865294642746
        total_loss: 2.8860333363215127
        vf_explained_var: 0.9937410950660706
        vf_loss: 2.894915541013082
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.475862068965515
    gpu_util_percent0: 0.3524137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683270071892138
    mean_env_wait_ms: 1.202190358859805
    mean_inference_ms: 4.331391308530343
    mean_raw_obs_processing_ms: 0.3807409300999393
  time_since_restore: 2674.807585477829
  time_this_iter_s: 25.365129470825195
  time_total_s: 2674.807585477829
  timers:
    learn_throughput: 8752.995
    learn_time_ms: 18484.187
    sample_throughput: 23782.531
    sample_time_ms: 6802.976
    update_time_ms: 26.012
  timestamp: 1602651621
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    104 |          2674.81 | 16826368 |  261.951 |              303.061 |              136.242 |            802.449 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3185.767542401064
    time_step_min: 2918
  date: 2020-10-14_05-00-46
  done: false
  episode_len_mean: 802.4271282902538
  episode_reward_max: 303.96969696969666
  episode_reward_mean: 262.1531607275026
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 216
  episodes_total: 21085
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.21538700411717096
        entropy_coeff: 0.0005000000000000001
        kl: 0.00448287259011219
        model: {}
        policy_loss: -0.00921534831286408
        total_loss: 3.572128971417745
        vf_explained_var: 0.993457555770874
        vf_loss: 3.5814519921938577
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.299999999999997
    gpu_util_percent0: 0.36758620689655175
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468220243541184
    mean_env_wait_ms: 1.2021485168837736
    mean_inference_ms: 4.330676834599378
    mean_raw_obs_processing_ms: 0.38069815583578714
  time_since_restore: 2700.2707529067993
  time_this_iter_s: 25.463167428970337
  time_total_s: 2700.2707529067993
  timers:
    learn_throughput: 8759.034
    learn_time_ms: 18471.444
    sample_throughput: 23813.808
    sample_time_ms: 6794.041
    update_time_ms: 26.53
  timestamp: 1602651646
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    105 |          2700.27 | 16988160 |  262.153 |               303.97 |              136.242 |            802.427 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3184.1904314315257
    time_step_min: 2918
  date: 2020-10-14_05-01-12
  done: false
  episode_len_mean: 802.385145913484
  episode_reward_max: 303.96969696969666
  episode_reward_mean: 262.3873041193581
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 229
  episodes_total: 21314
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.20282008747259775
        entropy_coeff: 0.0005000000000000001
        kl: 0.005946500886542101
        model: {}
        policy_loss: -0.008542639310083663
        total_loss: 2.7954265077908835
        vf_explained_var: 0.9945156574249268
        vf_loss: 2.804070512453715
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280000000000005
    gpu_util_percent0: 0.3216666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681068411156084
    mean_env_wait_ms: 1.2020952172885135
    mean_inference_ms: 4.329926125658952
    mean_raw_obs_processing_ms: 0.3806594197125268
  time_since_restore: 2725.5281817913055
  time_this_iter_s: 25.257428884506226
  time_total_s: 2725.5281817913055
  timers:
    learn_throughput: 8762.087
    learn_time_ms: 18465.008
    sample_throughput: 23801.603
    sample_time_ms: 6797.525
    update_time_ms: 25.568
  timestamp: 1602651672
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    106 |          2725.53 | 17149952 |  262.387 |               303.97 |              136.242 |            802.385 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3183.0718314454853
    time_step_min: 2918
  date: 2020-10-14_05-01-38
  done: false
  episode_len_mean: 802.3614407371214
  episode_reward_max: 303.96969696969666
  episode_reward_mean: 262.55677534806387
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 175
  episodes_total: 21489
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.20489118744929632
        entropy_coeff: 0.0005000000000000001
        kl: 0.00440191892751803
        model: {}
        policy_loss: -0.010943801268391931
        total_loss: 2.446466783682505
        vf_explained_var: 0.9945385456085205
        vf_loss: 2.4575130343437195
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32758620689655
    gpu_util_percent0: 0.356551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468022305324511
    mean_env_wait_ms: 1.202054656890637
    mean_inference_ms: 4.329358066647722
    mean_raw_obs_processing_ms: 0.38062787638524403
  time_since_restore: 2751.1310369968414
  time_this_iter_s: 25.60285520553589
  time_total_s: 2751.1310369968414
  timers:
    learn_throughput: 8765.743
    learn_time_ms: 18457.307
    sample_throughput: 23786.377
    sample_time_ms: 6801.877
    update_time_ms: 24.886
  timestamp: 1602651698
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    107 |          2751.13 | 17311744 |  262.557 |               303.97 |              136.242 |            802.361 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3181.844966970019
    time_step_min: 2918
  date: 2020-10-14_05-02-04
  done: false
  episode_len_mean: 802.3012498270534
  episode_reward_max: 304.27272727272697
  episode_reward_mean: 262.74305523528403
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 194
  episodes_total: 21683
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.21210420752565065
        entropy_coeff: 0.0005000000000000001
        kl: 0.004266667800645034
        model: {}
        policy_loss: -0.009544812882571327
        total_loss: 3.534186601638794
        vf_explained_var: 0.9929177761077881
        vf_loss: 3.5438374876976013
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32666666666667
    gpu_util_percent0: 0.31500000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679365471138478
    mean_env_wait_ms: 1.2020125770040055
    mean_inference_ms: 4.328757718829419
    mean_raw_obs_processing_ms: 0.3805945053187243
  time_since_restore: 2776.5920419692993
  time_this_iter_s: 25.461004972457886
  time_total_s: 2776.5920419692993
  timers:
    learn_throughput: 8765.815
    learn_time_ms: 18457.153
    sample_throughput: 23804.952
    sample_time_ms: 6796.569
    update_time_ms: 26.714
  timestamp: 1602651724
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    108 |          2776.59 | 17473536 |  262.743 |              304.273 |              136.242 |            802.301 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3180.3410409907233
    time_step_min: 2918
  date: 2020-10-14_05-02-30
  done: false
  episode_len_mean: 802.232492358228
  episode_reward_max: 304.27272727272697
  episode_reward_mean: 262.97783575063556
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 236
  episodes_total: 21919
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.19921182716886202
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045283739843095345
        model: {}
        policy_loss: -0.007905332871208278
        total_loss: 2.8254926999409995
        vf_explained_var: 0.9947331547737122
        vf_loss: 2.833497623602549
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.903448275862072
    gpu_util_percent0: 0.3458620689655173
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678242829864366
    mean_env_wait_ms: 1.201958469347217
    mean_inference_ms: 4.328001854061734
    mean_raw_obs_processing_ms: 0.3805501140478177
  time_since_restore: 2802.0165843963623
  time_this_iter_s: 25.42454242706299
  time_total_s: 2802.0165843963623
  timers:
    learn_throughput: 8768.998
    learn_time_ms: 18450.455
    sample_throughput: 23826.657
    sample_time_ms: 6790.378
    update_time_ms: 27.121
  timestamp: 1602651750
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    109 |          2802.02 | 17635328 |  262.978 |              304.273 |              136.242 |            802.232 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3178.937867946744
    time_step_min: 2918
  date: 2020-10-14_05-02-56
  done: false
  episode_len_mean: 802.1858215028484
  episode_reward_max: 304.27272727272697
  episode_reward_mean: 263.19224161316566
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 199
  episodes_total: 22118
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.1900289791325728
        entropy_coeff: 0.0005000000000000001
        kl: 0.005051623952264587
        model: {}
        policy_loss: -0.009107721309798459
        total_loss: 2.154778629541397
        vf_explained_var: 0.9953210353851318
        vf_loss: 2.1639813085397086
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.537931034482764
    gpu_util_percent0: 0.3475862068965518
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677270722202138
    mean_env_wait_ms: 1.2019092194414618
    mean_inference_ms: 4.3273889077706436
    mean_raw_obs_processing_ms: 0.38051747806951003
  time_since_restore: 2827.1555352211
  time_this_iter_s: 25.13895082473755
  time_total_s: 2827.1555352211
  timers:
    learn_throughput: 8777.428
    learn_time_ms: 18432.734
    sample_throughput: 23840.816
    sample_time_ms: 6786.345
    update_time_ms: 27.084
  timestamp: 1602651776
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    110 |          2827.16 | 17797120 |  263.192 |              304.273 |              136.242 |            802.186 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3177.7709316435216
    time_step_min: 2918
  date: 2020-10-14_05-03-22
  done: false
  episode_len_mean: 802.1407547000493
  episode_reward_max: 304.27272727272697
  episode_reward_mean: 263.3683272352002
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 169
  episodes_total: 22287
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.19913217052817345
        entropy_coeff: 0.0005000000000000001
        kl: 0.004601351063077648
        model: {}
        policy_loss: -0.008912130123159537
        total_loss: 2.6408274372418723
        vf_explained_var: 0.9941586852073669
        vf_loss: 2.6498391032218933
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203333333333333
    gpu_util_percent0: 0.3203333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467655562791475
    mean_env_wait_ms: 1.2018666870626573
    mean_inference_ms: 4.326884135067745
    mean_raw_obs_processing_ms: 0.380490777855809
  time_since_restore: 2852.8066794872284
  time_this_iter_s: 25.65114426612854
  time_total_s: 2852.8066794872284
  timers:
    learn_throughput: 8759.834
    learn_time_ms: 18469.757
    sample_throughput: 23898.738
    sample_time_ms: 6769.897
    update_time_ms: 31.338
  timestamp: 1602651802
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    111 |          2852.81 | 17958912 |  263.368 |              304.273 |              136.242 |            802.141 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3176.2522249911
    time_step_min: 2918
  date: 2020-10-14_05-03-47
  done: false
  episode_len_mean: 802.0701528345477
  episode_reward_max: 304.57575757575717
  episode_reward_mean: 263.5940594859201
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 221
  episodes_total: 22508
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.19686042765776315
        entropy_coeff: 0.0005000000000000001
        kl: 0.00463165482506156
        model: {}
        policy_loss: -0.007648821745533496
        total_loss: 2.591915726661682
        vf_explained_var: 0.9950248599052429
        vf_loss: 2.599662959575653
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.910344827586208
    gpu_util_percent0: 0.3437931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467559116715827
    mean_env_wait_ms: 1.201816141737424
    mean_inference_ms: 4.326220740810682
    mean_raw_obs_processing_ms: 0.3804496909366755
  time_since_restore: 2878.2519261837006
  time_this_iter_s: 25.445246696472168
  time_total_s: 2878.2519261837006
  timers:
    learn_throughput: 8766.851
    learn_time_ms: 18454.972
    sample_throughput: 23897.011
    sample_time_ms: 6770.386
    update_time_ms: 31.149
  timestamp: 1602651827
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    112 |          2878.25 | 18120704 |  263.594 |              304.576 |              136.242 |             802.07 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3174.638121668649
    time_step_min: 2896
  date: 2020-10-14_05-04-13
  done: false
  episode_len_mean: 802.019395698641
  episode_reward_max: 305.9393939393938
  episode_reward_mean: 263.8269713895785
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 229
  episodes_total: 22737
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.17844737445314726
        entropy_coeff: 0.0005000000000000001
        kl: 0.004137515226223816
        model: {}
        policy_loss: -0.008909039684416106
        total_loss: 2.859405517578125
        vf_explained_var: 0.994447648525238
        vf_loss: 2.8684038519859314
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79310344827586
    gpu_util_percent0: 0.3175862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674574252573402
    mean_env_wait_ms: 1.201758882336295
    mean_inference_ms: 4.3255657943348265
    mean_raw_obs_processing_ms: 0.380413859850895
  time_since_restore: 2903.7331869602203
  time_this_iter_s: 25.481260776519775
  time_total_s: 2903.7331869602203
  timers:
    learn_throughput: 8754.089
    learn_time_ms: 18481.878
    sample_throughput: 23929.95
    sample_time_ms: 6761.067
    update_time_ms: 32.881
  timestamp: 1602651853
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    113 |          2903.73 | 18282496 |  263.827 |              305.939 |              136.242 |            802.019 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3173.51374863388
    time_step_min: 2896
  date: 2020-10-14_05-04-39
  done: false
  episode_len_mean: 801.98694950024
  episode_reward_max: 305.9393939393938
  episode_reward_mean: 264.004793251356
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 174
  episodes_total: 22911
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.17948132505019507
        entropy_coeff: 0.0005000000000000001
        kl: 0.004640274525930484
        model: {}
        policy_loss: -0.008332632889505476
        total_loss: 2.306395868460337
        vf_explained_var: 0.994772732257843
        vf_loss: 2.314818243185679
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.599999999999998
    gpu_util_percent0: 0.3906666666666668
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673809525590067
    mean_env_wait_ms: 1.2017136577095864
    mean_inference_ms: 4.3250663518730645
    mean_raw_obs_processing_ms: 0.3803860853677426
  time_since_restore: 2929.2429897785187
  time_this_iter_s: 25.50980281829834
  time_total_s: 2929.2429897785187
  timers:
    learn_throughput: 8756.361
    learn_time_ms: 18477.083
    sample_throughput: 23867.982
    sample_time_ms: 6778.621
    update_time_ms: 33.172
  timestamp: 1602651879
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    114 |          2929.24 | 18444288 |  264.005 |              305.939 |              136.242 |            801.987 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3172.1408084571726
    time_step_min: 2896
  date: 2020-10-14_05-05-05
  done: false
  episode_len_mean: 801.9254228489856
  episode_reward_max: 305.9393939393938
  episode_reward_mean: 264.21694384691307
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 23117
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.1882436399658521
        entropy_coeff: 0.0005000000000000001
        kl: 0.004217337370694925
        model: {}
        policy_loss: -0.00838335547450697
        total_loss: 2.469933728377024
        vf_explained_var: 0.9949347972869873
        vf_loss: 2.4784111976623535
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.203333333333337
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672964958492815
    mean_env_wait_ms: 1.2016639206799706
    mean_inference_ms: 4.324500356542126
    mean_raw_obs_processing_ms: 0.3803530830292597
  time_since_restore: 2954.762298345566
  time_this_iter_s: 25.51930856704712
  time_total_s: 2954.762298345566
  timers:
    learn_throughput: 8755.97
    learn_time_ms: 18477.907
    sample_throughput: 23835.095
    sample_time_ms: 6787.974
    update_time_ms: 30.867
  timestamp: 1602651905
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    115 |          2954.76 | 18606080 |  264.217 |              305.939 |              136.242 |            801.925 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3170.632310662948
    time_step_min: 2896
  date: 2020-10-14_05-05-32
  done: false
  episode_len_mean: 801.8785399083158
  episode_reward_max: 305.9393939393938
  episode_reward_mean: 264.44285319239253
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 224
  episodes_total: 23341
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.1726313754916191
        entropy_coeff: 0.0005000000000000001
        kl: 0.003985262029649069
        model: {}
        policy_loss: -0.007977147310157306
        total_loss: 2.51504514614741
        vf_explained_var: 0.9953476786613464
        vf_loss: 2.5231085618336997
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.003333333333337
    gpu_util_percent0: 0.29600000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672062514420078
    mean_env_wait_ms: 1.2016046963709508
    mean_inference_ms: 4.32386549516129
    mean_raw_obs_processing_ms: 0.3803154748401359
  time_since_restore: 2980.622045278549
  time_this_iter_s: 25.8597469329834
  time_total_s: 2980.622045278549
  timers:
    learn_throughput: 8738.523
    learn_time_ms: 18514.8
    sample_throughput: 23755.761
    sample_time_ms: 6810.643
    update_time_ms: 29.901
  timestamp: 1602651932
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    116 |          2980.62 | 18767872 |  264.443 |              305.939 |              136.242 |            801.879 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3169.4152091578367
    time_step_min: 2896
  date: 2020-10-14_05-05-58
  done: false
  episode_len_mean: 801.8584661142978
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 264.629571688845
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 194
  episodes_total: 23535
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.16437324633200964
        entropy_coeff: 0.0005000000000000001
        kl: 0.003923273393108199
        model: {}
        policy_loss: -0.005855827068444341
        total_loss: 2.5756904681523642
        vf_explained_var: 0.9947084784507751
        vf_loss: 2.5816285212834678
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.736666666666668
    gpu_util_percent0: 0.34633333333333327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467120357997459
    mean_env_wait_ms: 1.2015527170282068
    mean_inference_ms: 4.323338458539242
    mean_raw_obs_processing_ms: 0.3802862558854474
  time_since_restore: 3006.331263065338
  time_this_iter_s: 25.70921778678894
  time_total_s: 3006.331263065338
  timers:
    learn_throughput: 8739.293
    learn_time_ms: 18513.169
    sample_throughput: 23724.528
    sample_time_ms: 6819.609
    update_time_ms: 31.772
  timestamp: 1602651958
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    117 |          3006.33 | 18929664 |   264.63 |                  307 |              136.242 |            801.858 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3168.2917001055966
    time_step_min: 2896
  date: 2020-10-14_05-06-24
  done: false
  episode_len_mean: 801.8232465944077
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 264.8048610605229
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 176
  episodes_total: 23711
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.17207426329453787
        entropy_coeff: 0.0005000000000000001
        kl: 0.004950701802348097
        model: {}
        policy_loss: -0.006777488432514171
        total_loss: 2.1098340153694153
        vf_explained_var: 0.995398223400116
        vf_loss: 2.1166975696881614
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74666666666667
    gpu_util_percent0: 0.361
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670552157436992
    mean_env_wait_ms: 1.2015025053251243
    mean_inference_ms: 4.322872129145186
    mean_raw_obs_processing_ms: 0.3802606480309284
  time_since_restore: 3031.998636007309
  time_this_iter_s: 25.667372941970825
  time_total_s: 3031.998636007309
  timers:
    learn_throughput: 8734.986
    learn_time_ms: 18522.297
    sample_throughput: 23664.623
    sample_time_ms: 6836.872
    update_time_ms: 31.847
  timestamp: 1602651984
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    118 |             3032 | 19091456 |  264.805 |                  307 |              136.242 |            801.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3166.895505523937
    time_step_min: 2896
  date: 2020-10-14_05-06-50
  done: false
  episode_len_mean: 801.7650426207588
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 265.02321223263885
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 221
  episodes_total: 23932
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.17282702773809433
        entropy_coeff: 0.0005000000000000001
        kl: 0.003968695527873933
        model: {}
        policy_loss: -0.011567316275128784
        total_loss: 2.639465550581614
        vf_explained_var: 0.9949531555175781
        vf_loss: 2.651119291782379
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78333333333334
    gpu_util_percent0: 0.32499999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669685127621412
    mean_env_wait_ms: 1.201448049856617
    mean_inference_ms: 4.322285471219328
    mean_raw_obs_processing_ms: 0.38022365765952976
  time_since_restore: 3057.705794811249
  time_this_iter_s: 25.70715880393982
  time_total_s: 3057.705794811249
  timers:
    learn_throughput: 8727.784
    learn_time_ms: 18537.581
    sample_throughput: 23625.455
    sample_time_ms: 6848.207
    update_time_ms: 32.803
  timestamp: 1602652010
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    119 |          3057.71 | 19253248 |  265.023 |                  307 |              136.242 |            801.765 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3165.3603648424546
    time_step_min: 2896
  date: 2020-10-14_05-07-16
  done: false
  episode_len_mean: 801.6945686371915
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 265.25369943849813
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 224
  episodes_total: 24156
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.15604355682929358
        entropy_coeff: 0.0005000000000000001
        kl: 0.003540336872295787
        model: {}
        policy_loss: -0.008982419360108906
        total_loss: 2.360484302043915
        vf_explained_var: 0.9954218864440918
        vf_loss: 2.369544724623362
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43
    gpu_util_percent0: 0.30033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466878241461709
    mean_env_wait_ms: 1.2013848937415388
    mean_inference_ms: 4.321706606488704
    mean_raw_obs_processing_ms: 0.3801914494698078
  time_since_restore: 3083.0092606544495
  time_this_iter_s: 25.303465843200684
  time_total_s: 3083.0092606544495
  timers:
    learn_throughput: 8726.51
    learn_time_ms: 18540.287
    sample_throughput: 23583.822
    sample_time_ms: 6860.296
    update_time_ms: 34.385
  timestamp: 1602652036
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    120 |          3083.01 | 19415040 |  265.254 |                  307 |              136.242 |            801.695 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3164.1995308062724
    time_step_min: 2896
  date: 2020-10-14_05-07-42
  done: false
  episode_len_mean: 801.6312415238565
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 265.4239696932335
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 177
  episodes_total: 24333
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.15993553648392358
        entropy_coeff: 0.0005000000000000001
        kl: 0.003991852553250889
        model: {}
        policy_loss: -0.009676661349658389
        total_loss: 2.419548451900482
        vf_explained_var: 0.9946107864379883
        vf_loss: 2.429305096467336
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.67586206896552
    gpu_util_percent0: 0.3751724137931034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466809453139455
    mean_env_wait_ms: 1.2013339200900515
    mean_inference_ms: 4.3212574856372825
    mean_raw_obs_processing_ms: 0.3801652434523398
  time_since_restore: 3108.298241853714
  time_this_iter_s: 25.288981199264526
  time_total_s: 3108.298241853714
  timers:
    learn_throughput: 8742.002
    learn_time_ms: 18507.43
    sample_throughput: 23574.419
    sample_time_ms: 6863.032
    update_time_ms: 28.046
  timestamp: 1602652062
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    121 |           3108.3 | 19576832 |  265.424 |                  307 |              136.242 |            801.631 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3162.7913316736726
    time_step_min: 2896
  date: 2020-10-14_05-08-08
  done: false
  episode_len_mean: 801.5469660540365
  episode_reward_max: 306.99999999999983
  episode_reward_mean: 265.62996689252844
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 206
  episodes_total: 24539
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.17219198991854986
        entropy_coeff: 0.0005000000000000001
        kl: 0.0064288977688799305
        model: {}
        policy_loss: -0.010138675029641794
        total_loss: 2.83526748418808
        vf_explained_var: 0.9942933917045593
        vf_loss: 2.8454922835032144
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43333333333333
    gpu_util_percent0: 0.308
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466734206773072
    mean_env_wait_ms: 1.201281998049857
    mean_inference_ms: 4.320754455923153
    mean_raw_obs_processing_ms: 0.38013451397341846
  time_since_restore: 3133.7536985874176
  time_this_iter_s: 25.455456733703613
  time_total_s: 3133.7536985874176
  timers:
    learn_throughput: 8743.281
    learn_time_ms: 18504.725
    sample_throughput: 23564.812
    sample_time_ms: 6865.83
    update_time_ms: 28.97
  timestamp: 1602652088
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    122 |          3133.75 | 19738624 |   265.63 |                  307 |              136.242 |            801.547 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3161.3080934670115
    time_step_min: 2896
  date: 2020-10-14_05-08-34
  done: false
  episode_len_mean: 801.4363394154691
  episode_reward_max: 308.36363636363603
  episode_reward_mean: 265.8587799519496
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 233
  episodes_total: 24772
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.16056357572476068
        entropy_coeff: 0.0005000000000000001
        kl: 0.004299966036342084
        model: {}
        policy_loss: -0.008023323675539965
        total_loss: 2.824439525604248
        vf_explained_var: 0.9946678280830383
        vf_loss: 2.832543134689331
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.031034482758628
    gpu_util_percent0: 0.3620689655172413
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146664818427631
    mean_env_wait_ms: 1.2012122323483925
    mean_inference_ms: 4.320169009140888
    mean_raw_obs_processing_ms: 0.3800997432489327
  time_since_restore: 3159.086953639984
  time_this_iter_s: 25.33325505256653
  time_total_s: 3159.086953639984
  timers:
    learn_throughput: 8752.532
    learn_time_ms: 18485.165
    sample_throughput: 23554.697
    sample_time_ms: 6868.779
    update_time_ms: 29.314
  timestamp: 1602652114
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    123 |          3159.09 | 19900416 |  265.859 |              308.364 |              136.242 |            801.436 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3160.1154663991974
    time_step_min: 2896
  date: 2020-10-14_05-09-00
  done: false
  episode_len_mean: 801.3312367292978
  episode_reward_max: 308.36363636363603
  episode_reward_mean: 266.0337593312232
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 189
  episodes_total: 24961
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.14547821134328842
        entropy_coeff: 0.0005000000000000001
        kl: 0.003972978583381821
        model: {}
        policy_loss: -0.009451755729969591
        total_loss: 2.3672469655672708
        vf_explained_var: 0.9947699904441833
        vf_loss: 2.3767714699109397
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.689999999999998
    gpu_util_percent0: 0.35800000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665715672477733
    mean_env_wait_ms: 1.201159989094689
    mean_inference_ms: 4.319710728206374
    mean_raw_obs_processing_ms: 0.38007353178168257
  time_since_restore: 3184.574334859848
  time_this_iter_s: 25.48738121986389
  time_total_s: 3184.574334859848
  timers:
    learn_throughput: 8749.376
    learn_time_ms: 18491.833
    sample_throughput: 23616.25
    sample_time_ms: 6850.876
    update_time_ms: 36.725
  timestamp: 1602652140
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    124 |          3184.57 | 20062208 |  266.034 |              308.364 |              136.242 |            801.331 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3158.922651053491
    time_step_min: 2880
  date: 2020-10-14_05-09-26
  done: false
  episode_len_mean: 801.2262259873523
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 266.2103953266104
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 182
  episodes_total: 25143
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.15984883656104407
        entropy_coeff: 0.0005000000000000001
        kl: 0.003537013428285718
        model: {}
        policy_loss: -0.009445018314484818
        total_loss: 2.951190233230591
        vf_explained_var: 0.9936904311180115
        vf_loss: 2.960715174674988
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90344827586207
    gpu_util_percent0: 0.4017241379310345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466509644087674
    mean_env_wait_ms: 1.2011073663146568
    mean_inference_ms: 4.3192717770000595
    mean_raw_obs_processing_ms: 0.3800480203048788
  time_since_restore: 3210.181895494461
  time_this_iter_s: 25.607560634613037
  time_total_s: 3210.181895494461
  timers:
    learn_throughput: 8742.697
    learn_time_ms: 18505.96
    sample_throughput: 23639.931
    sample_time_ms: 6844.013
    update_time_ms: 37.381
  timestamp: 1602652166
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    125 |          3210.18 | 20224000 |   266.21 |              308.364 |              136.242 |            801.226 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3157.376045777427
    time_step_min: 2880
  date: 2020-10-14_05-09-52
  done: false
  episode_len_mean: 801.109749369483
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 266.4451330773051
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 233
  episodes_total: 25376
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.1553135278324286
        entropy_coeff: 0.0005000000000000001
        kl: 0.004857060712917398
        model: {}
        policy_loss: -0.007093027389297883
        total_loss: 2.5144524574279785
        vf_explained_var: 0.9952011704444885
        vf_loss: 2.5216230948766074
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.22000000000001
    gpu_util_percent0: 0.34900000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664293069363243
    mean_env_wait_ms: 1.2010486134660707
    mean_inference_ms: 4.31873148667026
    mean_raw_obs_processing_ms: 0.38001352794696597
  time_since_restore: 3235.657311439514
  time_this_iter_s: 25.4754159450531
  time_total_s: 3235.657311439514
  timers:
    learn_throughput: 8752.713
    learn_time_ms: 18484.783
    sample_throughput: 23706.556
    sample_time_ms: 6824.779
    update_time_ms: 38.886
  timestamp: 1602652192
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    126 |          3235.66 | 20385792 |  266.445 |              308.364 |              136.242 |             801.11 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3155.990059097491
    time_step_min: 2880
  date: 2020-10-14_05-10-18
  done: false
  episode_len_mean: 801.043420486966
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 266.65076370457996
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 211
  episodes_total: 25587
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.13924898703893027
        entropy_coeff: 0.0005000000000000001
        kl: 0.003963715261003624
        model: {}
        policy_loss: -0.008442982827546075
        total_loss: 2.1421040892601013
        vf_explained_var: 0.9955690503120422
        vf_loss: 2.150616725285848
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.990000000000002
    gpu_util_percent0: 0.31833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663501578019747
    mean_env_wait_ms: 1.2009891101516272
    mean_inference_ms: 4.3182494507775875
    mean_raw_obs_processing_ms: 0.3799864928188431
  time_since_restore: 3261.0125348567963
  time_this_iter_s: 25.355223417282104
  time_total_s: 3261.0125348567963
  timers:
    learn_throughput: 8765.23
    learn_time_ms: 18458.386
    sample_throughput: 23741.247
    sample_time_ms: 6814.806
    update_time_ms: 39.203
  timestamp: 1602652218
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    127 |          3261.01 | 20547584 |  266.651 |              308.364 |              136.242 |            801.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3154.8632741126617
    time_step_min: 2880
  date: 2020-10-14_05-10-44
  done: false
  episode_len_mean: 800.9954190768275
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 266.82218630263964
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 172
  episodes_total: 25759
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.14556952317555746
        entropy_coeff: 0.0005000000000000001
        kl: 0.004224637368073066
        model: {}
        policy_loss: -0.006321212160401046
        total_loss: 1.7349521120389302
        vf_explained_var: 0.9960880875587463
        vf_loss: 1.7413460910320282
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.879310344827587
    gpu_util_percent0: 0.3124137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466290464975271
    mean_env_wait_ms: 1.2009376557974356
    mean_inference_ms: 4.317852644632016
    mean_raw_obs_processing_ms: 0.3799632199451526
  time_since_restore: 3286.504278421402
  time_this_iter_s: 25.491743564605713
  time_total_s: 3286.504278421402
  timers:
    learn_throughput: 8768.014
    learn_time_ms: 18452.526
    sample_throughput: 23788.804
    sample_time_ms: 6801.183
    update_time_ms: 38.951
  timestamp: 1602652244
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    128 |           3286.5 | 20709376 |  266.822 |              308.364 |              136.242 |            800.995 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3153.5584981303728
    time_step_min: 2880
  date: 2020-10-14_05-11-10
  done: false
  episode_len_mean: 800.9069561535205
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 267.0282592643141
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 218
  episodes_total: 25977
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.1587205578883489
        entropy_coeff: 0.0005000000000000001
        kl: 0.003996756800916046
        model: {}
        policy_loss: -0.00969479835475795
        total_loss: 2.647255778312683
        vf_explained_var: 0.9948437213897705
        vf_loss: 2.657030006249746
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64
    gpu_util_percent0: 0.42199999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662194360730635
    mean_env_wait_ms: 1.2008818847289475
    mean_inference_ms: 4.3173622069646065
    mean_raw_obs_processing_ms: 0.3799323072540254
  time_since_restore: 3311.8609733581543
  time_this_iter_s: 25.35669493675232
  time_total_s: 3311.8609733581543
  timers:
    learn_throughput: 8787.167
    learn_time_ms: 18412.304
    sample_throughput: 23799.509
    sample_time_ms: 6798.123
    update_time_ms: 46.023
  timestamp: 1602652270
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    129 |          3311.86 | 20871168 |  267.028 |              308.364 |              136.242 |            800.907 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3152.1497841121854
    time_step_min: 2880
  date: 2020-10-14_05-11-35
  done: false
  episode_len_mean: 800.7645285610714
  episode_reward_max: 308.36363636363626
  episode_reward_mean: 267.2463198012096
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 230
  episodes_total: 26207
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.14385775104165077
        entropy_coeff: 0.0005000000000000001
        kl: 0.003811001059754441
        model: {}
        policy_loss: -0.007713697210419923
        total_loss: 2.4613351225852966
        vf_explained_var: 0.9952512383460999
        vf_loss: 2.469120721022288
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.141379310344828
    gpu_util_percent0: 0.3544827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661379845656414
    mean_env_wait_ms: 1.2008174567898358
    mean_inference_ms: 4.316861140923843
    mean_raw_obs_processing_ms: 0.37990339815479784
  time_since_restore: 3337.117552280426
  time_this_iter_s: 25.25657892227173
  time_total_s: 3337.117552280426
  timers:
    learn_throughput: 8785.531
    learn_time_ms: 18415.735
    sample_throughput: 23826.291
    sample_time_ms: 6790.482
    update_time_ms: 44.423
  timestamp: 1602652295
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    130 |          3337.12 | 21032960 |  267.246 |              308.364 |              136.242 |            800.765 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3151.0065652157414
    time_step_min: 2878
  date: 2020-10-14_05-12-01
  done: false
  episode_len_mean: 800.6537310039034
  episode_reward_max: 308.66666666666634
  episode_reward_mean: 267.4155570178611
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 180
  episodes_total: 26387
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.14070829252401987
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007642167600958298
        total_loss: .inf
        vf_explained_var: 0.9948969483375549
        vf_loss: 2.270488361517588
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.093333333333337
    gpu_util_percent0: 0.40433333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660776690084218
    mean_env_wait_ms: 1.2007664837424112
    mean_inference_ms: 4.316472238676364
    mean_raw_obs_processing_ms: 0.37988080460238494
  time_since_restore: 3362.653965473175
  time_this_iter_s: 25.536413192749023
  time_total_s: 3362.653965473175
  timers:
    learn_throughput: 8778.481
    learn_time_ms: 18430.524
    sample_throughput: 23795.241
    sample_time_ms: 6799.343
    update_time_ms: 45.644
  timestamp: 1602652321
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: dbd8f_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    131 |          3362.65 | 21194752 |  267.416 |              308.667 |              136.242 |            800.654 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3149.736885708906
    time_step_min: 2878
  date: 2020-10-14_05-12-28
  done: false
  episode_len_mean: 800.5151367003873
  episode_reward_max: 308.66666666666634
  episode_reward_mean: 267.6011432439546
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 204
  episodes_total: 26591
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.263256414560601e-15
        cur_lr: 5.0e-05
        entropy: 0.15472138424714407
        entropy_coeff: 0.0005000000000000001
        kl: 0.003816704615019262
        model: {}
        policy_loss: -0.008889230859495001
        total_loss: 2.7518296043078103
        vf_explained_var: 0.9945402145385742
        vf_loss: 2.760796229044596
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46
    gpu_util_percent0: 0.3526666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660118903027725
    mean_env_wait_ms: 1.2007153048317754
    mean_inference_ms: 4.316044943748274
    mean_raw_obs_processing_ms: 0.3798545772806378
  time_since_restore: 3388.4482169151306
  time_this_iter_s: 25.794251441955566
  time_total_s: 3388.4482169151306
  timers:
    learn_throughput: 8758.122
    learn_time_ms: 18473.366
    sample_throughput: 23830.663
    sample_time_ms: 6789.236
    update_time_ms: 45.093
  timestamp: 1602652348
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    132 |          3388.45 | 21356544 |  267.601 |              308.667 |              136.242 |            800.515 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3148.358044046286
    time_step_min: 2878
  date: 2020-10-14_05-12-54
  done: false
  episode_len_mean: 800.3646835159919
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 267.8082050656417
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 235
  episodes_total: 26826
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1316282072803005e-15
        cur_lr: 5.0e-05
        entropy: 0.14557435611883798
        entropy_coeff: 0.0005000000000000001
        kl: 0.003975215038129439
        model: {}
        policy_loss: -0.011097151092447652
        total_loss: 2.651736537615458
        vf_explained_var: 0.9950762391090393
        vf_loss: 2.662906507651011
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.706896551724142
    gpu_util_percent0: 0.3527586206896552
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659339574831928
    mean_env_wait_ms: 1.2006482763810902
    mean_inference_ms: 4.315542853243204
    mean_raw_obs_processing_ms: 0.3798237149065011
  time_since_restore: 3413.9614791870117
  time_this_iter_s: 25.513262271881104
  time_total_s: 3413.9614791870117
  timers:
    learn_throughput: 8751.052
    learn_time_ms: 18488.291
    sample_throughput: 23816.487
    sample_time_ms: 6793.277
    update_time_ms: 43.401
  timestamp: 1602652374
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    133 |          3413.96 | 21518336 |  267.808 |              308.667 |              136.242 |            800.365 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3147.277888963012
    time_step_min: 2878
  date: 2020-10-14_05-13-20
  done: false
  episode_len_mean: 800.22044562884
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 267.9751355437563
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 192
  episodes_total: 27018
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0658141036401502e-15
        cur_lr: 5.0e-05
        entropy: 0.13573087751865387
        entropy_coeff: 0.0005000000000000001
        kl: 0.003934694066022833
        model: {}
        policy_loss: -0.01063305014395155
        total_loss: 2.2533395489056907
        vf_explained_var: 0.9950329661369324
        vf_loss: 2.2640404105186462
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.240000000000002
    gpu_util_percent0: 0.4216666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658708652688987
    mean_env_wait_ms: 1.200596225116113
    mean_inference_ms: 4.315140542055742
    mean_raw_obs_processing_ms: 0.3798005155448434
  time_since_restore: 3439.430910587311
  time_this_iter_s: 25.469431400299072
  time_total_s: 3439.430910587311
  timers:
    learn_throughput: 8747.868
    learn_time_ms: 18495.021
    sample_throughput: 23825.493
    sample_time_ms: 6790.709
    update_time_ms: 37.941
  timestamp: 1602652400
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    134 |          3439.43 | 21680128 |  267.975 |              308.667 |              136.242 |             800.22 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3146.12739720985
    time_step_min: 2878
  date: 2020-10-14_05-13-46
  done: false
  episode_len_mean: 800.0861302062273
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.13753607835133
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 185
  episodes_total: 27203
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.329070518200751e-16
        cur_lr: 5.0e-05
        entropy: 0.15098275244235992
        entropy_coeff: 0.0005000000000000001
        kl: 0.004871759602489571
        model: {}
        policy_loss: -0.007839497440727428
        total_loss: 3.0314738949139914
        vf_explained_var: 0.9937071800231934
        vf_loss: 3.03938889503479
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.473333333333333
    gpu_util_percent0: 0.3416666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465813283572888
    mean_env_wait_ms: 1.2005496112618632
    mean_inference_ms: 4.314768673208514
    mean_raw_obs_processing_ms: 0.3797786091265434
  time_since_restore: 3465.254315853119
  time_this_iter_s: 25.823405265808105
  time_total_s: 3465.254315853119
  timers:
    learn_throughput: 8744.658
    learn_time_ms: 18501.811
    sample_throughput: 23794.703
    sample_time_ms: 6799.497
    update_time_ms: 39.514
  timestamp: 1602652426
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    135 |          3465.25 | 21841920 |  268.138 |              308.667 |              136.242 |            800.086 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3144.6848135444793
    time_step_min: 2878
  date: 2020-10-14_05-14-12
  done: false
  episode_len_mean: 799.9055098024925
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.35270973712034
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 239
  episodes_total: 27442
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6645352591003756e-16
        cur_lr: 5.0e-05
        entropy: 0.15152055894335112
        entropy_coeff: 0.0005000000000000001
        kl: 0.004096088116057217
        model: {}
        policy_loss: -0.009226406992335493
        total_loss: 2.275760034720103
        vf_explained_var: 0.9956778883934021
        vf_loss: 2.285062253475189
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.18
    gpu_util_percent0: 0.35466666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657381816216203
    mean_env_wait_ms: 1.2004832845450033
    mean_inference_ms: 4.314267876605293
    mean_raw_obs_processing_ms: 0.3797459150738373
  time_since_restore: 3490.9223535060883
  time_this_iter_s: 25.66803765296936
  time_total_s: 3490.9223535060883
  timers:
    learn_throughput: 8734.644
    learn_time_ms: 18523.022
    sample_throughput: 23837.831
    sample_time_ms: 6787.195
    update_time_ms: 45.7
  timestamp: 1602652452
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    136 |          3490.92 | 22003712 |  268.353 |              308.667 |              136.242 |            799.906 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3143.5800130396988
    time_step_min: 2878
  date: 2020-10-14_05-14-38
  done: false
  episode_len_mean: 799.7616119230213
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.52779604758325
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 202
  episodes_total: 27644
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3322676295501878e-16
        cur_lr: 5.0e-05
        entropy: 0.13440921530127525
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033904482261277735
        model: {}
        policy_loss: -0.0074828818324021995
        total_loss: 2.5545804301897683
        vf_explained_var: 0.9946722984313965
        vf_loss: 2.5621305108070374
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46206896551724
    gpu_util_percent0: 0.34103448275862064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656716185400437
    mean_env_wait_ms: 1.200432650243476
    mean_inference_ms: 4.313873548248242
    mean_raw_obs_processing_ms: 0.37972359034543635
  time_since_restore: 3516.3915271759033
  time_this_iter_s: 25.469173669815063
  time_total_s: 3516.3915271759033
  timers:
    learn_throughput: 8726.312
    learn_time_ms: 18540.707
    sample_throughput: 23855.473
    sample_time_ms: 6782.175
    update_time_ms: 44.358
  timestamp: 1602652478
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    137 |          3516.39 | 22165504 |  268.528 |              308.667 |              136.242 |            799.762 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3142.5631679320645
    time_step_min: 2878
  date: 2020-10-14_05-15-04
  done: false
  episode_len_mean: 799.648327164265
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.68401955371434
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 183
  episodes_total: 27827
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.661338147750939e-17
        cur_lr: 5.0e-05
        entropy: 0.14186163991689682
        entropy_coeff: 0.0005000000000000001
        kl: 0.003602027155769368
        model: {}
        policy_loss: -0.008131344140565488
        total_loss: 2.159025381008784
        vf_explained_var: 0.9954721331596375
        vf_loss: 2.167227655649185
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.303333333333338
    gpu_util_percent0: 0.3606666666666668
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656181918526456
    mean_env_wait_ms: 1.2003833299537925
    mean_inference_ms: 4.313512257682656
    mean_raw_obs_processing_ms: 0.37970194976251714
  time_since_restore: 3541.8421754837036
  time_this_iter_s: 25.450648307800293
  time_total_s: 3541.8421754837036
  timers:
    learn_throughput: 8730.552
    learn_time_ms: 18531.702
    sample_throughput: 23831.471
    sample_time_ms: 6789.006
    update_time_ms: 42.809
  timestamp: 1602652504
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    138 |          3541.84 | 22327296 |  268.684 |              308.667 |              136.242 |            799.648 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3141.2774030053183
    time_step_min: 2878
  date: 2020-10-14_05-15-30
  done: false
  episode_len_mean: 799.5094998752362
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 268.87612408979095
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 226
  episodes_total: 28053
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3306690738754695e-17
        cur_lr: 5.0e-05
        entropy: 0.14673407251636186
        entropy_coeff: 0.0005000000000000001
        kl: 0.003811852657236159
        model: {}
        policy_loss: -0.010056069717393257
        total_loss: 2.2437128722667694
        vf_explained_var: 0.9957773089408875
        vf_loss: 2.253842294216156
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.499999999999996
    gpu_util_percent0: 0.34299999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465553995625615
    mean_env_wait_ms: 1.2003266554542429
    mean_inference_ms: 4.313078995663635
    mean_raw_obs_processing_ms: 0.37967387455100726
  time_since_restore: 3567.2409965991974
  time_this_iter_s: 25.398821115493774
  time_total_s: 3567.2409965991974
  timers:
    learn_throughput: 8720.04
    learn_time_ms: 18554.044
    sample_throughput: 23871.532
    sample_time_ms: 6777.613
    update_time_ms: 35.595
  timestamp: 1602652530
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    139 |          3567.24 | 22489088 |  268.876 |              308.667 |              136.242 |            799.509 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3140.0775660551108
    time_step_min: 2878
  date: 2020-10-14_05-15-57
  done: false
  episode_len_mean: 799.365546515741
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 269.067535989538
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 217
  episodes_total: 28270
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6653345369377347e-17
        cur_lr: 5.0e-05
        entropy: 0.12911246965328851
        entropy_coeff: 0.0005000000000000001
        kl: 0.004554758818509678
        model: {}
        policy_loss: -0.009683790704002604
        total_loss: 2.1207492550214133
        vf_explained_var: 0.9956687092781067
        vf_loss: 2.1304975152015686
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.653333333333332
    gpu_util_percent0: 0.3426666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654856234109598
    mean_env_wait_ms: 1.2002712730333367
    mean_inference_ms: 4.312666023311588
    mean_raw_obs_processing_ms: 0.3796496546244907
  time_since_restore: 3593.087182998657
  time_this_iter_s: 25.84618639945984
  time_total_s: 3593.087182998657
  timers:
    learn_throughput: 8705.806
    learn_time_ms: 18584.379
    sample_throughput: 23812.544
    sample_time_ms: 6794.402
    update_time_ms: 44.468
  timestamp: 1602652557
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | RUNNING  | 172.17.0.4:66685 |    140 |          3593.09 | 22650880 |  269.068 |              308.667 |              136.242 |            799.366 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dbd8f_00000:
  custom_metrics:
    time_step_max: 3996
    time_step_mean: 3139.1251627890606
    time_step_min: 2878
  date: 2020-10-14_05-16-23
  done: true
  episode_len_mean: 799.2591485921187
  episode_reward_max: 308.66666666666646
  episode_reward_mean: 269.20810949868485
  episode_reward_min: 136.24242424242374
  episodes_this_iter: 177
  episodes_total: 28447
  experiment_id: e3f3cf6aad5f42429dd85a781a45ee95
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.326672684688674e-18
        cur_lr: 5.0e-05
        entropy: 0.13276775802175203
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034511150248969593
        model: {}
        policy_loss: -0.009753992451199641
        total_loss: 2.0188810427983603
        vf_explained_var: 0.9956676959991455
        vf_loss: 2.028701434532801
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.246666666666666
    gpu_util_percent0: 0.3663333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 66685
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654343943901496
    mean_env_wait_ms: 1.2002258721238375
    mean_inference_ms: 4.312329571121781
    mean_raw_obs_processing_ms: 0.3796296334692978
  time_since_restore: 3618.945284843445
  time_this_iter_s: 25.858101844787598
  time_total_s: 3618.945284843445
  timers:
    learn_throughput: 8691.629
    learn_time_ms: 18614.693
    sample_throughput: 23813.208
    sample_time_ms: 6794.213
    update_time_ms: 44.132
  timestamp: 1602652583
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: dbd8f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | TERMINATED |       |    141 |          3618.95 | 22812672 |  269.208 |              308.667 |              136.242 |            799.259 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 27.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.52 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dbd8f_00000 | TERMINATED |       |    141 |          3618.95 | 22812672 |  269.208 |              308.667 |              136.242 |            799.259 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


