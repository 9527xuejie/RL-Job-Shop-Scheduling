2020-10-15 06:22:21,244	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_c9144_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=43545)[0m 2020-10-15 06:22:24,033	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=43527)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43527)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43523)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43523)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43515)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43515)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43560)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43560)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43555)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43555)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43543)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43543)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43548)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43548)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43547)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43547)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43535)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43535)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43558)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43558)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43551)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43551)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43501)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43501)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43524)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43524)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43568)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43568)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43553)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43553)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43508)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43508)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43532)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43532)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43516)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43516)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43526)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43526)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43538)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43538)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43549)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43549)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43437)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43437)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43465)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43465)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43441)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43441)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43454)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43454)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43436)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43436)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43453)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43453)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43446)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43446)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43500)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43500)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43528)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43528)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43513)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43513)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43561)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43561)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43447)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43447)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43450)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43450)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43562)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43562)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43530)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43530)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43574)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43574)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43497)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43497)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43503)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43503)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43448)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43448)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43498)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43498)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43470)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43470)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43510)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43510)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43504)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43504)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43539)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43539)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43525)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43525)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43461)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43461)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43459)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43459)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43540)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43540)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43457)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43457)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43440)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43440)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43452)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43452)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43476)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43476)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43502)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43502)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43511)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43511)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43522)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43522)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43451)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43451)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43456)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43456)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43567)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43567)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43438)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43438)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43536)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43536)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43443)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43443)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43442)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43442)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43531)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43531)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43472)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43472)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43505)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43505)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43439)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43439)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43514)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43514)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43471)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43471)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43565)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43565)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43474)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43474)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43455)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43455)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43506)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43506)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43542)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43542)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43521)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43521)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43458)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43458)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43445)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43445)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43570)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43570)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=43460)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43460)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3561.048780487805
    time_step_min: 3288
  date: 2020-10-15_06-22-58
  done: false
  episode_len_mean: 897.367088607595
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 205.7902442142946
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1812196671962738
        entropy_coeff: 0.0005000000000000001
        kl: 0.004003119072876871
        model: {}
        policy_loss: -0.007752152906808381
        total_loss: 417.5151901245117
        vf_explained_var: 0.5535116195678711
        vf_loss: 417.5227355957031
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.38529411764706
    gpu_util_percent0: 0.3561764705882353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5558823529411767
    vram_util_percent0: 0.08636872262844136
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17222631489211082
    mean_env_wait_ms: 1.180786251764159
    mean_inference_ms: 6.053450873841029
    mean_raw_obs_processing_ms: 0.4656331102654462
  time_since_restore: 29.140605688095093
  time_this_iter_s: 29.140605688095093
  time_total_s: 29.140605688095093
  timers:
    learn_throughput: 8213.679
    learn_time_ms: 19697.873
    sample_throughput: 17285.448
    sample_time_ms: 9360.012
    update_time_ms: 46.622
  timestamp: 1602742978
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      1 |          29.1406 | 161792 |   205.79 |              273.505 |              128.354 |            897.367 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3569.6227758007117
    time_step_min: 3288
  date: 2020-10-15_06-23-25
  done: false
  episode_len_mean: 896.1867088607595
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 206.45959595959573
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1479040284951527
        entropy_coeff: 0.0005000000000000001
        kl: 0.008545062194267908
        model: {}
        policy_loss: -0.010045574104879051
        total_loss: 102.10054206848145
        vf_explained_var: 0.8203843235969543
        vf_loss: 102.1103064219157
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.7741935483871
    gpu_util_percent0: 0.29096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7548387096774185
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16681431447576028
    mean_env_wait_ms: 1.172388787653594
    mean_inference_ms: 5.727408466481995
    mean_raw_obs_processing_ms: 0.44911997174812773
  time_since_restore: 55.99322533607483
  time_this_iter_s: 26.852619647979736
  time_total_s: 55.99322533607483
  timers:
    learn_throughput: 8303.623
    learn_time_ms: 19484.508
    sample_throughput: 19187.822
    sample_time_ms: 8432.015
    update_time_ms: 39.1
  timestamp: 1602743005
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      2 |          55.9932 | 323584 |   206.46 |              273.505 |              128.354 |            896.187 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3554.0
    time_step_min: 3195
  date: 2020-10-15_06-23-52
  done: false
  episode_len_mean: 889.2383966244726
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 208.2390998593528
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.137634406487147
        entropy_coeff: 0.0005000000000000001
        kl: 0.007693540576534967
        model: {}
        policy_loss: -0.011871370700343201
        total_loss: 50.18517557779948
        vf_explained_var: 0.8948180079460144
        vf_loss: 50.1968469619751
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.422580645161297
    gpu_util_percent0: 0.3090322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1634403652850075
    mean_env_wait_ms: 1.1709000086240093
    mean_inference_ms: 5.497878273739849
    mean_raw_obs_processing_ms: 0.4384917860040682
  time_since_restore: 82.5050106048584
  time_this_iter_s: 26.51178526878357
  time_total_s: 82.5050106048584
  timers:
    learn_throughput: 8324.973
    learn_time_ms: 19434.538
    sample_throughput: 20264.16
    sample_time_ms: 7984.145
    update_time_ms: 40.379
  timestamp: 1602743032
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      3 |           82.505 | 485376 |  208.239 |              273.505 |              128.354 |            889.238 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3551.257956448911
    time_step_min: 3195
  date: 2020-10-15_06-24-18
  done: false
  episode_len_mean: 884.371835443038
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 208.62444060861762
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1220806539058685
        entropy_coeff: 0.0005000000000000001
        kl: 0.008578164658198753
        model: {}
        policy_loss: -0.012210655714928484
        total_loss: 37.28772290547689
        vf_explained_var: 0.9263750910758972
        vf_loss: 37.29963652292887
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.477419354838712
    gpu_util_percent0: 0.35161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16111503435088748
    mean_env_wait_ms: 1.1710372264270408
    mean_inference_ms: 5.336444210777555
    mean_raw_obs_processing_ms: 0.4308148738508426
  time_since_restore: 108.98076725006104
  time_this_iter_s: 26.475756645202637
  time_total_s: 108.98076725006104
  timers:
    learn_throughput: 8338.075
    learn_time_ms: 19404.0
    sample_throughput: 20849.04
    sample_time_ms: 7760.166
    update_time_ms: 37.012
  timestamp: 1602743058
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      4 |          108.981 | 647168 |  208.624 |              273.505 |              128.354 |            884.372 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3548.7311258278146
    time_step_min: 3195
  date: 2020-10-15_06-24-45
  done: false
  episode_len_mean: 878.8354430379746
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 209.16481268379982
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0830539067586262
        entropy_coeff: 0.0005000000000000001
        kl: 0.007865200944555303
        model: {}
        policy_loss: -0.012452944797890572
        total_loss: 32.10198322931925
        vf_explained_var: 0.9422528147697449
        vf_loss: 32.11419185002645
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.400000000000002
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1594053564216873
    mean_env_wait_ms: 1.1724239240519716
    mean_inference_ms: 5.216498629847945
    mean_raw_obs_processing_ms: 0.4249447809316254
  time_since_restore: 135.4411985874176
  time_this_iter_s: 26.460431337356567
  time_total_s: 135.4411985874176
  timers:
    learn_throughput: 8337.011
    learn_time_ms: 19406.475
    sample_throughput: 21277.562
    sample_time_ms: 7603.879
    update_time_ms: 33.185
  timestamp: 1602743085
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      5 |          135.441 | 808960 |  209.165 |              273.505 |              128.354 |            878.835 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3527.9485500467727
    time_step_min: 3187
  date: 2020-10-15_06-25-11
  done: false
  episode_len_mean: 865.7038043478261
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 212.2182696530521
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 314
  episodes_total: 1104
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0722917715708415
        entropy_coeff: 0.0005000000000000001
        kl: 0.008053469937294722
        model: {}
        policy_loss: -0.011176503176102415
        total_loss: 36.43024476369222
        vf_explained_var: 0.9526867270469666
        vf_loss: 36.44115161895752
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.323333333333338
    gpu_util_percent0: 0.32533333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15713744333622992
    mean_env_wait_ms: 1.1763447000702096
    mean_inference_ms: 5.057873694209602
    mean_raw_obs_processing_ms: 0.41746763985872803
  time_since_restore: 161.60977005958557
  time_this_iter_s: 26.16857147216797
  time_total_s: 161.60977005958557
  timers:
    learn_throughput: 8350.961
    learn_time_ms: 19374.057
    sample_throughput: 21627.177
    sample_time_ms: 7480.958
    update_time_ms: 34.643
  timestamp: 1602743111
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      6 |           161.61 | 970752 |  212.218 |              273.505 |              128.354 |            865.704 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3519.5760781122863
    time_step_min: 3187
  date: 2020-10-15_06-25-37
  done: false
  episode_len_mean: 860.2832278481013
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 213.76049258406843
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 160
  episodes_total: 1264
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0584441026051838
        entropy_coeff: 0.0005000000000000001
        kl: 0.007266054550806682
        model: {}
        policy_loss: -0.011817739791392038
        total_loss: 21.57445494333903
        vf_explained_var: 0.95844966173172
        vf_loss: 21.58607578277588
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.903225806451616
    gpu_util_percent0: 0.31806451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15632388236724645
    mean_env_wait_ms: 1.1779798345272263
    mean_inference_ms: 5.000464200694199
    mean_raw_obs_processing_ms: 0.4147573137464784
  time_since_restore: 188.00703525543213
  time_this_iter_s: 26.397265195846558
  time_total_s: 188.00703525543213
  timers:
    learn_throughput: 8354.224
    learn_time_ms: 19366.49
    sample_throughput: 21835.849
    sample_time_ms: 7409.467
    update_time_ms: 34.947
  timestamp: 1602743137
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      7 |          188.007 | 1132544 |   213.76 |              273.505 |              128.354 |            860.283 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3509.0281182408075
    time_step_min: 3120
  date: 2020-10-15_06-26-04
  done: false
  episode_len_mean: 855.4535864978903
  episode_reward_max: 274.5656565656565
  episode_reward_mean: 215.1003281762774
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0401013692220051
        entropy_coeff: 0.0005000000000000001
        kl: 0.007276753972594936
        model: {}
        policy_loss: -0.01209554991995295
        total_loss: 20.411585489908855
        vf_explained_var: 0.9587884545326233
        vf_loss: 20.423472722371418
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.951612903225808
    gpu_util_percent0: 0.3412903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1556425764172188
    mean_env_wait_ms: 1.1795817201148109
    mean_inference_ms: 4.9522350736022345
    mean_raw_obs_processing_ms: 0.4124125795022059
  time_since_restore: 214.59996914863586
  time_this_iter_s: 26.592933893203735
  time_total_s: 214.59996914863586
  timers:
    learn_throughput: 8346.083
    learn_time_ms: 19385.381
    sample_throughput: 21998.461
    sample_time_ms: 7354.696
    update_time_ms: 36.409
  timestamp: 1602743164
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      8 |            214.6 | 1294336 |    215.1 |              274.566 |              128.354 |            855.454 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3501.862783171521
    time_step_min: 3120
  date: 2020-10-15_06-26-31
  done: false
  episode_len_mean: 851.6063291139241
  episode_reward_max: 274.5656565656565
  episode_reward_mean: 216.40263393427938
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0068644384543102
        entropy_coeff: 0.0005000000000000001
        kl: 0.007244308828376234
        model: {}
        policy_loss: -0.012283675799456736
        total_loss: 22.011494477589924
        vf_explained_var: 0.9590733051300049
        vf_loss: 22.023557662963867
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.91935483870968
    gpu_util_percent0: 0.344516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15504900358050108
    mean_env_wait_ms: 1.1811662215434884
    mean_inference_ms: 4.910538871005152
    mean_raw_obs_processing_ms: 0.41033362498377646
  time_since_restore: 241.14428806304932
  time_this_iter_s: 26.544318914413452
  time_total_s: 241.14428806304932
  timers:
    learn_throughput: 8342.5
    learn_time_ms: 19393.707
    sample_throughput: 22112.855
    sample_time_ms: 7316.649
    update_time_ms: 34.574
  timestamp: 1602743191
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |      9 |          241.144 | 1456128 |  216.403 |              274.566 |              128.354 |            851.606 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3485.0683530678148
    time_step_min: 3120
  date: 2020-10-15_06-26-57
  done: false
  episode_len_mean: 845.2070787110407
  episode_reward_max: 275.6262626262626
  episode_reward_mean: 219.26062527013386
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 313
  episodes_total: 1893
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9907859414815903
        entropy_coeff: 0.0005000000000000001
        kl: 0.006813797284848988
        model: {}
        policy_loss: -0.011380279174773023
        total_loss: 25.875819365183514
        vf_explained_var: 0.9651870727539062
        vf_loss: 25.88701327641805
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.700000000000003
    gpu_util_percent0: 0.3203225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15410189698164078
    mean_env_wait_ms: 1.1841663827439421
    mean_inference_ms: 4.844169187322123
    mean_raw_obs_processing_ms: 0.4071179456735068
  time_since_restore: 267.83481526374817
  time_this_iter_s: 26.690527200698853
  time_total_s: 267.83481526374817
  timers:
    learn_throughput: 8336.966
    learn_time_ms: 19406.58
    sample_throughput: 22207.318
    sample_time_ms: 7285.526
    update_time_ms: 41.934
  timestamp: 1602743217
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     10 |          267.835 | 1617920 |  219.261 |              275.626 |              128.354 |            845.207 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3476.02823179792
    time_step_min: 3099
  date: 2020-10-15_06-27-24
  done: false
  episode_len_mean: 842.4542356377799
  episode_reward_max: 277.7474747474749
  episode_reward_mean: 220.41979188181708
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 161
  episodes_total: 2054
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9728940029939016
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065002391347661614
        model: {}
        policy_loss: -0.010479286599244611
        total_loss: 17.72179587682088
        vf_explained_var: 0.9665319323539734
        vf_loss: 17.73211161295573
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.59666666666667
    gpu_util_percent0: 0.38300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1537053696328896
    mean_env_wait_ms: 1.1854140769132497
    mean_inference_ms: 4.816123928520064
    mean_raw_obs_processing_ms: 0.4057527889037697
  time_since_restore: 294.3429479598999
  time_this_iter_s: 26.508132696151733
  time_total_s: 294.3429479598999
  timers:
    learn_throughput: 8345.153
    learn_time_ms: 19387.542
    sample_throughput: 22979.498
    sample_time_ms: 7040.711
    update_time_ms: 40.896
  timestamp: 1602743244
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     11 |          294.343 | 1779712 |   220.42 |              277.747 |              128.354 |            842.454 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3467.6876435461645
    time_step_min: 3099
  date: 2020-10-15_06-27-51
  done: false
  episode_len_mean: 839.7314647377939
  episode_reward_max: 277.7474747474749
  episode_reward_mean: 221.8002310628892
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9636171956857046
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065782947931438684
        model: {}
        policy_loss: -0.012532726783926288
        total_loss: 14.463757356007894
        vf_explained_var: 0.9673574566841125
        vf_loss: 14.47611395517985
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.68387096774193
    gpu_util_percent0: 0.32419354838709674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1533540981058103
    mean_env_wait_ms: 1.1865793823159205
    mean_inference_ms: 4.791376782072699
    mean_raw_obs_processing_ms: 0.40451814521591334
  time_since_restore: 320.9435794353485
  time_this_iter_s: 26.60063147544861
  time_total_s: 320.9435794353485
  timers:
    learn_throughput: 8337.25
    learn_time_ms: 19405.918
    sample_throughput: 23126.534
    sample_time_ms: 6995.947
    update_time_ms: 40.842
  timestamp: 1602743271
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     12 |          320.944 | 1941504 |    221.8 |              277.747 |              128.354 |            839.731 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3456.561816652649
    time_step_min: 3099
  date: 2020-10-15_06-28-17
  done: false
  episode_len_mean: 836.0049730625777
  episode_reward_max: 277.7474747474749
  episode_reward_mean: 223.48370149903505
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 201
  episodes_total: 2413
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9162271420160929
        entropy_coeff: 0.0005000000000000001
        kl: 0.006539546923401455
        model: {}
        policy_loss: -0.010849950471310876
        total_loss: 16.26166319847107
        vf_explained_var: 0.9718396663665771
        vf_loss: 16.272317091623943
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37741935483871
    gpu_util_percent0: 0.3283870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7612903225806447
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15295119779653862
    mean_env_wait_ms: 1.1882010614140344
    mean_inference_ms: 4.763135579169163
    mean_raw_obs_processing_ms: 0.4031125731071747
  time_since_restore: 347.3565127849579
  time_this_iter_s: 26.412933349609375
  time_total_s: 347.3565127849579
  timers:
    learn_throughput: 8332.915
    learn_time_ms: 19416.016
    sample_throughput: 23193.294
    sample_time_ms: 6975.809
    update_time_ms: 40.073
  timestamp: 1602743297
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     13 |          347.357 | 2103296 |  223.484 |              277.747 |              128.354 |            836.005 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3441.1576763485477
    time_step_min: 3054
  date: 2020-10-15_06-28-44
  done: false
  episode_len_mean: 831.4266567386449
  episode_reward_max: 284.56565656565675
  episode_reward_mean: 225.76486006754052
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 273
  episodes_total: 2686
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9097392161687216
        entropy_coeff: 0.0005000000000000001
        kl: 0.005902522980856399
        model: {}
        policy_loss: -0.010862464109474482
        total_loss: 15.4582626024882
        vf_explained_var: 0.9729016423225403
        vf_loss: 15.468989531199137
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.526666666666667
    gpu_util_percent0: 0.31966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15248208134301203
    mean_env_wait_ms: 1.1900530368986768
    mean_inference_ms: 4.730000463375264
    mean_raw_obs_processing_ms: 0.40144195313801945
  time_since_restore: 373.7608504295349
  time_this_iter_s: 26.404337644577026
  time_total_s: 373.7608504295349
  timers:
    learn_throughput: 8330.658
    learn_time_ms: 19421.276
    sample_throughput: 23235.29
    sample_time_ms: 6963.201
    update_time_ms: 39.457
  timestamp: 1602743324
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     14 |          373.761 | 2265088 |  225.765 |              284.566 |              128.354 |            831.427 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3433.8907084371663
    time_step_min: 3054
  date: 2020-10-15_06-29-10
  done: false
  episode_len_mean: 829.2387482419128
  episode_reward_max: 284.56565656565675
  episode_reward_mean: 226.9806290755657
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9017603745063146
        entropy_coeff: 0.0005000000000000001
        kl: 0.005951517377980053
        model: {}
        policy_loss: -0.01264588067230458
        total_loss: 10.179479281107584
        vf_explained_var: 0.9765340685844421
        vf_loss: 10.191980838775635
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.622580645161293
    gpu_util_percent0: 0.28516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1522410269524485
    mean_env_wait_ms: 1.1910218728104647
    mean_inference_ms: 4.713052248195017
    mean_raw_obs_processing_ms: 0.4005792885251605
  time_since_restore: 400.1589081287384
  time_this_iter_s: 26.39805769920349
  time_total_s: 400.1589081287384
  timers:
    learn_throughput: 8330.352
    learn_time_ms: 19421.988
    sample_throughput: 23266.435
    sample_time_ms: 6953.88
    update_time_ms: 40.979
  timestamp: 1602743350
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     15 |          400.159 | 2426880 |  226.981 |              284.566 |              128.354 |            829.239 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3426.399663299663
    time_step_min: 3054
  date: 2020-10-15_06-29-37
  done: false
  episode_len_mean: 826.922462562396
  episode_reward_max: 284.56565656565675
  episode_reward_mean: 228.09966554059724
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 161
  episodes_total: 3005
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8762048035860062
        entropy_coeff: 0.0005000000000000001
        kl: 0.006493303109891713
        model: {}
        policy_loss: -0.013002060974637667
        total_loss: 14.075467268625895
        vf_explained_var: 0.9701388478279114
        vf_loss: 14.088258266448975
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.374193548387098
    gpu_util_percent0: 0.26838709677419353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520148593548651
    mean_env_wait_ms: 1.1920290881199578
    mean_inference_ms: 4.697097717914769
    mean_raw_obs_processing_ms: 0.39975733039055317
  time_since_restore: 426.8087031841278
  time_this_iter_s: 26.649795055389404
  time_total_s: 426.8087031841278
  timers:
    learn_throughput: 8317.144
    learn_time_ms: 19452.831
    sample_throughput: 23211.582
    sample_time_ms: 6970.313
    update_time_ms: 40.798
  timestamp: 1602743377
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     16 |          426.809 | 2588672 |    228.1 |              284.566 |              128.354 |            826.922 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3413.207800121877
    time_step_min: 3054
  date: 2020-10-15_06-30-04
  done: false
  episode_len_mean: 822.9445281881218
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 229.984344500172
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 312
  episodes_total: 3317
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.839088092247645
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055505180498585105
        model: {}
        policy_loss: -0.01150182525937756
        total_loss: 16.7764093875885
        vf_explained_var: 0.9746711850166321
        vf_loss: 16.78777551651001
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.387096774193548
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15162121900336312
    mean_env_wait_ms: 1.1939066520550368
    mean_inference_ms: 4.669369154169235
    mean_raw_obs_processing_ms: 0.3983439433730298
  time_since_restore: 453.2975571155548
  time_this_iter_s: 26.488853931427002
  time_total_s: 453.2975571155548
  timers:
    learn_throughput: 8311.828
    learn_time_ms: 19465.274
    sample_throughput: 23222.426
    sample_time_ms: 6967.059
    update_time_ms: 41.266
  timestamp: 1602743404
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     17 |          453.298 | 2750464 |  229.984 |              290.929 |              128.354 |            822.945 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3406.6451612903224
    time_step_min: 3054
  date: 2020-10-15_06-30-30
  done: false
  episode_len_mean: 821.0917721518987
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 230.9796817426276
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 159
  episodes_total: 3476
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8368375897407532
        entropy_coeff: 0.0005000000000000001
        kl: 0.005631982310054203
        model: {}
        policy_loss: -0.01084599293123271
        total_loss: 10.226332902908325
        vf_explained_var: 0.9766200184822083
        vf_loss: 10.23703408241272
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.861290322580647
    gpu_util_percent0: 0.30258064516129035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1514427950120309
    mean_env_wait_ms: 1.1947701363048262
    mean_inference_ms: 4.6568638027944536
    mean_raw_obs_processing_ms: 0.3977045868083803
  time_since_restore: 479.65009474754333
  time_this_iter_s: 26.352537631988525
  time_total_s: 479.65009474754333
  timers:
    learn_throughput: 8322.708
    learn_time_ms: 19439.826
    sample_throughput: 23236.134
    sample_time_ms: 6962.948
    update_time_ms: 45.753
  timestamp: 1602743430
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     18 |           479.65 | 2912256 |   230.98 |              290.929 |              128.354 |            821.092 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3401.0494444444444
    time_step_min: 3054
  date: 2020-10-15_06-30-57
  done: false
  episode_len_mean: 819.3532324621733
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 231.91557945340608
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 159
  episodes_total: 3635
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8324753443400065
        entropy_coeff: 0.0005000000000000001
        kl: 0.006413979882684846
        model: {}
        policy_loss: -0.01057599096869429
        total_loss: 11.734215180079142
        vf_explained_var: 0.9724454283714294
        vf_loss: 11.744565884272257
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.164516129032258
    gpu_util_percent0: 0.3912903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1512744149096638
    mean_env_wait_ms: 1.195623053940904
    mean_inference_ms: 4.645098547658345
    mean_raw_obs_processing_ms: 0.3970925783979414
  time_since_restore: 506.23042821884155
  time_this_iter_s: 26.580333471298218
  time_total_s: 506.23042821884155
  timers:
    learn_throughput: 8325.885
    learn_time_ms: 19432.409
    sample_throughput: 23233.255
    sample_time_ms: 6963.811
    update_time_ms: 54.242
  timestamp: 1602743457
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     19 |           506.23 | 3074048 |  231.916 |              290.929 |              128.354 |            819.353 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3390.977389516958
    time_step_min: 3054
  date: 2020-10-15_06-31-24
  done: false
  episode_len_mean: 816.5324675324675
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 233.36511023142032
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 292
  episodes_total: 3927
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7915658603111903
        entropy_coeff: 0.0005000000000000001
        kl: 0.005714234585563342
        model: {}
        policy_loss: -0.009224428193798909
        total_loss: 16.07810894648234
        vf_explained_var: 0.9758429527282715
        vf_loss: 16.08715844154358
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.958064516129035
    gpu_util_percent0: 0.3745161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7612903225806447
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15099359035817694
    mean_env_wait_ms: 1.1971745944461023
    mean_inference_ms: 4.625402220585347
    mean_raw_obs_processing_ms: 0.3960795113831696
  time_since_restore: 532.8877809047699
  time_this_iter_s: 26.657352685928345
  time_total_s: 532.8877809047699
  timers:
    learn_throughput: 8324.183
    learn_time_ms: 19436.381
    sample_throughput: 23259.5
    sample_time_ms: 6955.954
    update_time_ms: 53.568
  timestamp: 1602743484
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     20 |          532.888 | 3235840 |  233.365 |              290.929 |              128.354 |            816.532 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3385.9975448072673
    time_step_min: 3019
  date: 2020-10-15_06-31-51
  done: false
  episode_len_mean: 815.0759493670886
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 234.15135040767947
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 4108
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7875451942284902
        entropy_coeff: 0.0005000000000000001
        kl: 0.005900586722418666
        model: {}
        policy_loss: -0.011265025086080035
        total_loss: 12.707908233006796
        vf_explained_var: 0.9739854335784912
        vf_loss: 12.718976736068726
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.96451612903226
    gpu_util_percent0: 0.30516129032258066
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15083229942739246
    mean_env_wait_ms: 1.1979809515553186
    mean_inference_ms: 4.614192699558485
    mean_raw_obs_processing_ms: 0.3954982237458153
  time_since_restore: 559.5633132457733
  time_this_iter_s: 26.675532341003418
  time_total_s: 559.5633132457733
  timers:
    learn_throughput: 8320.466
    learn_time_ms: 19445.065
    sample_throughput: 23238.071
    sample_time_ms: 6962.368
    update_time_ms: 54.397
  timestamp: 1602743511
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     21 |          559.563 | 3397632 |  234.151 |              290.929 |              128.354 |            815.076 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3380.8260458520444
    time_step_min: 3010
  date: 2020-10-15_06-32-17
  done: false
  episode_len_mean: 813.7878574777309
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 234.91894330079978
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 4266
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.786624605456988
        entropy_coeff: 0.0005000000000000001
        kl: 0.005671430650788049
        model: {}
        policy_loss: -0.012116239978543794
        total_loss: 10.708203315734863
        vf_explained_var: 0.9747191071510315
        vf_loss: 10.72014570236206
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.363333333333333
    gpu_util_percent0: 0.35500000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15070165169579405
    mean_env_wait_ms: 1.1986924946993163
    mean_inference_ms: 4.6050298896081125
    mean_raw_obs_processing_ms: 0.3950183147934953
  time_since_restore: 585.8947627544403
  time_this_iter_s: 26.331449508666992
  time_total_s: 585.8947627544403
  timers:
    learn_throughput: 8322.143
    learn_time_ms: 19441.147
    sample_throughput: 23319.463
    sample_time_ms: 6938.067
    update_time_ms: 55.286
  timestamp: 1602743537
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     22 |          585.895 | 3559424 |  234.919 |              291.232 |              128.354 |            813.788 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3372.76435246996
    time_step_min: 3010
  date: 2020-10-15_06-32-44
  done: false
  episode_len_mean: 811.6392139545153
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 236.14765004873192
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 263
  episodes_total: 4529
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7510873725016912
        entropy_coeff: 0.0005000000000000001
        kl: 0.005892521197286745
        model: {}
        policy_loss: -0.009724960681827119
        total_loss: 13.870153268178305
        vf_explained_var: 0.9768733978271484
        vf_loss: 13.879664421081543
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.016129032258068
    gpu_util_percent0: 0.31000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1505017871457476
    mean_env_wait_ms: 1.1999233906970688
    mean_inference_ms: 4.5909233778256775
    mean_raw_obs_processing_ms: 0.3942970466964041
  time_since_restore: 612.393580198288
  time_this_iter_s: 26.498817443847656
  time_total_s: 612.393580198288
  timers:
    learn_throughput: 8316.081
    learn_time_ms: 19455.319
    sample_throughput: 23339.059
    sample_time_ms: 6932.242
    update_time_ms: 54.549
  timestamp: 1602743564
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     23 |          612.394 | 3721216 |  236.148 |              291.232 |              128.354 |            811.639 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3367.1287991498407
    time_step_min: 3010
  date: 2020-10-15_06-33-10
  done: false
  episode_len_mean: 810.2875527426161
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 237.05446873801301
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 4740
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7363142818212509
        entropy_coeff: 0.0005000000000000001
        kl: 0.005312932499994834
        model: {}
        policy_loss: -0.010551177416346036
        total_loss: 10.422034819920858
        vf_explained_var: 0.9792265892028809
        vf_loss: 10.432422717412313
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.343333333333337
    gpu_util_percent0: 0.345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15034849002701625
    mean_env_wait_ms: 1.20072473098583
    mean_inference_ms: 4.580369090536278
    mean_raw_obs_processing_ms: 0.3937339403890576
  time_since_restore: 638.7558317184448
  time_this_iter_s: 26.36225152015686
  time_total_s: 638.7558317184448
  timers:
    learn_throughput: 8310.771
    learn_time_ms: 19467.749
    sample_throughput: 23405.965
    sample_time_ms: 6912.426
    update_time_ms: 56.889
  timestamp: 1602743590
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     24 |          638.756 | 3883008 |  237.054 |              291.232 |              128.354 |            810.288 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3363.1451778737405
    time_step_min: 3010
  date: 2020-10-15_06-33-37
  done: false
  episode_len_mean: 809.2658227848101
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 237.67555299833774
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 4898
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.744674930969874
        entropy_coeff: 0.0005000000000000001
        kl: 0.005310616339556873
        model: {}
        policy_loss: -0.01328013397869654
        total_loss: 9.559085130691528
        vf_explained_var: 0.9773495197296143
        vf_loss: 9.572206338246664
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.59677419354839
    gpu_util_percent0: 0.29548387096774187
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15024214377869186
    mean_env_wait_ms: 1.2013283885000878
    mean_inference_ms: 4.572959274770438
    mean_raw_obs_processing_ms: 0.39334611033900474
  time_since_restore: 665.216689825058
  time_this_iter_s: 26.46085810661316
  time_total_s: 665.216689825058
  timers:
    learn_throughput: 8309.801
    learn_time_ms: 19470.02
    sample_throughput: 23400.985
    sample_time_ms: 6913.897
    update_time_ms: 57.92
  timestamp: 1602743617
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     25 |          665.217 | 4044800 |  237.676 |              291.232 |              128.354 |            809.266 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3357.3968940436407
    time_step_min: 3010
  date: 2020-10-15_06-34-03
  done: false
  episode_len_mean: 807.8734869191723
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 238.5942083860865
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 224
  episodes_total: 5122
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7157353411118189
        entropy_coeff: 0.0005000000000000001
        kl: 0.006130007832931976
        model: {}
        policy_loss: -0.010868864017538726
        total_loss: 11.382584492365519
        vf_explained_var: 0.979543924331665
        vf_loss: 11.393198251724243
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89032258064516
    gpu_util_percent0: 0.34483870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15010329829517377
    mean_env_wait_ms: 1.202210315855576
    mean_inference_ms: 4.563032995311463
    mean_raw_obs_processing_ms: 0.39284081217576755
  time_since_restore: 691.6863017082214
  time_this_iter_s: 26.469611883163452
  time_total_s: 691.6863017082214
  timers:
    learn_throughput: 8307.343
    learn_time_ms: 19475.781
    sample_throughput: 23485.423
    sample_time_ms: 6889.039
    update_time_ms: 57.766
  timestamp: 1602743643
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     26 |          691.686 | 4206592 |  238.594 |              291.232 |              128.354 |            807.873 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3351.399850103054
    time_step_min: 3010
  date: 2020-10-15_06-34-30
  done: false
  episode_len_mean: 806.5517498138496
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 239.4420357709635
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 250
  episodes_total: 5372
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6879154394070307
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054661074342827005
        model: {}
        policy_loss: -0.010591972202140218
        total_loss: 10.885743856430054
        vf_explained_var: 0.9807800650596619
        vf_loss: 10.896132946014404
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.020000000000003
    gpu_util_percent0: 0.31666666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1499525628794782
    mean_env_wait_ms: 1.203053657119556
    mean_inference_ms: 4.552736284697862
    mean_raw_obs_processing_ms: 0.3922944448318534
  time_since_restore: 717.9918274879456
  time_this_iter_s: 26.30552577972412
  time_total_s: 717.9918274879456
  timers:
    learn_throughput: 8307.476
    learn_time_ms: 19475.47
    sample_throughput: 23552.442
    sample_time_ms: 6869.436
    update_time_ms: 55.676
  timestamp: 1602743670
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     27 |          717.992 | 4368384 |  239.442 |              291.232 |              128.354 |            806.552 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3348.0032757051868
    time_step_min: 3010
  date: 2020-10-15_06-34-57
  done: false
  episode_len_mean: 805.7877034358047
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 239.87405702595575
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 5530
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6986237665017446
        entropy_coeff: 0.0005000000000000001
        kl: 0.005676805002925296
        model: {}
        policy_loss: -0.011504553685275217
        total_loss: 9.959116458892822
        vf_explained_var: 0.9774243235588074
        vf_loss: 9.970402399698893
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.270967741935486
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1498651717250508
    mean_env_wait_ms: 1.2035730682006192
    mean_inference_ms: 4.546607132354364
    mean_raw_obs_processing_ms: 0.3919762086904028
  time_since_restore: 744.4996194839478
  time_this_iter_s: 26.507791996002197
  time_total_s: 744.4996194839478
  timers:
    learn_throughput: 8292.948
    learn_time_ms: 19509.589
    sample_throughput: 23591.31
    sample_time_ms: 6858.119
    update_time_ms: 48.648
  timestamp: 1602743697
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     28 |            744.5 | 4530176 |  239.874 |              291.232 |              128.354 |            805.788 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3344.2459563994375
    time_step_min: 3010
  date: 2020-10-15_06-35-23
  done: false
  episode_len_mean: 804.8137340555653
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 240.4106785132471
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 193
  episodes_total: 5723
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6785935461521149
        entropy_coeff: 0.0005000000000000001
        kl: 0.006100421228135626
        model: {}
        policy_loss: -0.010813766227026159
        total_loss: 11.620836019515991
        vf_explained_var: 0.978428840637207
        vf_loss: 11.631378571192423
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.529032258064518
    gpu_util_percent0: 0.33516129032258074
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14976493160970591
    mean_env_wait_ms: 1.2042197031927915
    mean_inference_ms: 4.539427594128716
    mean_raw_obs_processing_ms: 0.3916037220668841
  time_since_restore: 771.109424829483
  time_this_iter_s: 26.60980534553528
  time_total_s: 771.109424829483
  timers:
    learn_throughput: 8283.555
    learn_time_ms: 19531.712
    sample_throughput: 23666.727
    sample_time_ms: 6836.264
    update_time_ms: 42.438
  timestamp: 1602743723
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     29 |          771.109 | 4691968 |  240.411 |              291.232 |              128.354 |            804.814 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3339.6116602445973
    time_step_min: 3010
  date: 2020-10-15_06-35-50
  done: false
  episode_len_mean: 803.6532311792139
  episode_reward_max: 291.9898989898989
  episode_reward_mean: 241.11038432290925
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 281
  episodes_total: 6004
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6467319428920746
        entropy_coeff: 0.0005000000000000001
        kl: 0.005211092143629988
        model: {}
        policy_loss: -0.009283149343294403
        total_loss: 11.490628719329834
        vf_explained_var: 0.9812774062156677
        vf_loss: 11.499714136123657
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.276666666666664
    gpu_util_percent0: 0.39333333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496255825000616
    mean_env_wait_ms: 1.2050841074158336
    mean_inference_ms: 4.529701771667242
    mean_raw_obs_processing_ms: 0.39110251861751527
  time_since_restore: 797.5859124660492
  time_this_iter_s: 26.476487636566162
  time_total_s: 797.5859124660492
  timers:
    learn_throughput: 8286.412
    learn_time_ms: 19524.977
    sample_throughput: 23690.252
    sample_time_ms: 6829.476
    update_time_ms: 36.826
  timestamp: 1602743750
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     30 |          797.586 | 4853760 |   241.11 |               291.99 |              128.354 |            803.653 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3336.92802350253
    time_step_min: 3010
  date: 2020-10-15_06-36-17
  done: false
  episode_len_mean: 802.9754949691659
  episode_reward_max: 291.9898989898989
  episode_reward_mean: 241.49382333559547
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 6162
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.654222289721171
        entropy_coeff: 0.0005000000000000001
        kl: 0.005552779068239033
        model: {}
        policy_loss: -0.012495742392881462
        total_loss: 8.75546701749166
        vf_explained_var: 0.9802989959716797
        vf_loss: 8.767734368642172
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.41935483870968
    gpu_util_percent0: 0.3445161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1495519825986757
    mean_env_wait_ms: 1.2055371544639966
    mean_inference_ms: 4.524537873979163
    mean_raw_obs_processing_ms: 0.39083424222040003
  time_since_restore: 823.9980823993683
  time_this_iter_s: 26.412169933319092
  time_total_s: 823.9980823993683
  timers:
    learn_throughput: 8290.833
    learn_time_ms: 19514.566
    sample_throughput: 23743.805
    sample_time_ms: 6814.072
    update_time_ms: 35.373
  timestamp: 1602743777
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     31 |          823.998 | 5015552 |  241.494 |               291.99 |              128.354 |            802.975 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3333.5767338517694
    time_step_min: 3005
  date: 2020-10-15_06-36-43
  done: false
  episode_len_mean: 802.2630997474747
  episode_reward_max: 291.98989898989913
  episode_reward_mean: 241.95017887205387
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 6336
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6424600233634313
        entropy_coeff: 0.0005000000000000001
        kl: 0.005638489732518792
        model: {}
        policy_loss: -0.009538379235891625
        total_loss: 9.407063643137613
        vf_explained_var: 0.9806671738624573
        vf_loss: 9.416359504063925
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.42903225806452
    gpu_util_percent0: 0.35870967741935483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14947459575696506
    mean_env_wait_ms: 1.2060489230831748
    mean_inference_ms: 4.519146403041033
    mean_raw_obs_processing_ms: 0.39055224505391334
  time_since_restore: 850.4518573284149
  time_this_iter_s: 26.45377492904663
  time_total_s: 850.4518573284149
  timers:
    learn_throughput: 8287.523
    learn_time_ms: 19522.36
    sample_throughput: 23760.996
    sample_time_ms: 6809.142
    update_time_ms: 35.546
  timestamp: 1602743803
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     32 |          850.452 | 5177344 |   241.95 |               291.99 |              128.354 |            802.263 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3328.4720242608037
    time_step_min: 2990
  date: 2020-10-15_06-37-10
  done: false
  episode_len_mean: 801.1429864253394
  episode_reward_max: 294.2626262626261
  episode_reward_mean: 242.7336258512729
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 294
  episodes_total: 6630
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6038125157356262
        entropy_coeff: 0.0005000000000000001
        kl: 0.005317067766251664
        model: {}
        policy_loss: -0.008520152943674475
        total_loss: 11.313503424326578
        vf_explained_var: 0.9816839694976807
        vf_loss: 11.321793794631958
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.98333333333334
    gpu_util_percent0: 0.28700000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1493508595565928
    mean_env_wait_ms: 1.2068465080118003
    mean_inference_ms: 4.510389063624788
    mean_raw_obs_processing_ms: 0.39010326083046226
  time_since_restore: 876.761519908905
  time_this_iter_s: 26.309662580490112
  time_total_s: 876.761519908905
  timers:
    learn_throughput: 8296.005
    learn_time_ms: 19502.399
    sample_throughput: 23768.005
    sample_time_ms: 6807.134
    update_time_ms: 36.848
  timestamp: 1602743830
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     33 |          876.762 | 5339136 |  242.734 |              294.263 |              128.354 |            801.143 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3325.7595798195
    time_step_min: 2990
  date: 2020-10-15_06-37-36
  done: false
  episode_len_mean: 800.6223138062996
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 243.15886715253802
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 164
  episodes_total: 6794
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6078718453645706
        entropy_coeff: 0.0005000000000000001
        kl: 0.004984718941462536
        model: {}
        policy_loss: -0.012139975135748196
        total_loss: 8.964349667231241
        vf_explained_var: 0.9798333048820496
        vf_loss: 8.976294914881388
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.419354838709683
    gpu_util_percent0: 0.34580645161290324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14928426861265828
    mean_env_wait_ms: 1.207243759206992
    mean_inference_ms: 4.505744199316842
    mean_raw_obs_processing_ms: 0.3898562355623693
  time_since_restore: 903.1871509552002
  time_this_iter_s: 26.425631046295166
  time_total_s: 903.1871509552002
  timers:
    learn_throughput: 8293.727
    learn_time_ms: 19507.756
    sample_throughput: 23766.497
    sample_time_ms: 6807.566
    update_time_ms: 36.379
  timestamp: 1602743856
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     34 |          903.187 | 5500928 |  243.159 |              297.596 |              128.354 |            800.622 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3323.0561976307426
    time_step_min: 2990
  date: 2020-10-15_06-38-03
  done: false
  episode_len_mean: 800.114417133822
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 243.6076286800737
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 163
  episodes_total: 6957
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6069925874471664
        entropy_coeff: 0.0005000000000000001
        kl: 0.005795620886298518
        model: {}
        policy_loss: -0.010724431369453669
        total_loss: 8.029765844345093
        vf_explained_var: 0.9822105765342712
        vf_loss: 8.04050381978353
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.651612903225814
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14922213036559734
    mean_env_wait_ms: 1.2076450813392314
    mean_inference_ms: 4.501366026455589
    mean_raw_obs_processing_ms: 0.389625651224191
  time_since_restore: 929.6552355289459
  time_this_iter_s: 26.468084573745728
  time_total_s: 929.6552355289459
  timers:
    learn_throughput: 8290.532
    learn_time_ms: 19515.273
    sample_throughput: 23792.179
    sample_time_ms: 6800.218
    update_time_ms: 35.43
  timestamp: 1602743883
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     35 |          929.655 | 5662720 |  243.608 |              297.596 |              128.354 |            800.114 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3317.868691070438
    time_step_min: 2990
  date: 2020-10-15_06-38-30
  done: false
  episode_len_mean: 799.1491651717952
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 244.38338678631214
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 290
  episodes_total: 7247
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.573741133014361
        entropy_coeff: 0.0005000000000000001
        kl: 0.005351585801690817
        model: {}
        policy_loss: -0.010270018964850655
        total_loss: 12.00859030087789
        vf_explained_var: 0.9804748892784119
        vf_loss: 12.018879493077597
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.290322580645157
    gpu_util_percent0: 0.34032258064516124
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14911889906912915
    mean_env_wait_ms: 1.208340047639037
    mean_inference_ms: 4.493941032840828
    mean_raw_obs_processing_ms: 0.3892430752024247
  time_since_restore: 956.2553129196167
  time_this_iter_s: 26.600077390670776
  time_total_s: 956.2553129196167
  timers:
    learn_throughput: 8285.884
    learn_time_ms: 19526.22
    sample_throughput: 23786.131
    sample_time_ms: 6801.947
    update_time_ms: 35.148
  timestamp: 1602743910
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     36 |          956.255 | 5824512 |  244.383 |              297.596 |              128.354 |            799.149 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3314.9324854552833
    time_step_min: 2990
  date: 2020-10-15_06-38-57
  done: false
  episode_len_mean: 798.5587126312954
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 244.84401923898287
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 7426
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5739115178585052
        entropy_coeff: 0.0005000000000000001
        kl: 0.005629358890776833
        model: {}
        policy_loss: -0.010710357882392904
        total_loss: 6.466275771458943
        vf_explained_var: 0.9853144288063049
        vf_loss: 6.476991772651672
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.476666666666674
    gpu_util_percent0: 0.31833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333323
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14905375170156274
    mean_env_wait_ms: 1.2087180324501108
    mean_inference_ms: 4.489505456006636
    mean_raw_obs_processing_ms: 0.3890123576506328
  time_since_restore: 982.7097010612488
  time_this_iter_s: 26.45438814163208
  time_total_s: 982.7097010612488
  timers:
    learn_throughput: 8286.271
    learn_time_ms: 19525.31
    sample_throughput: 23732.784
    sample_time_ms: 6817.236
    update_time_ms: 37.086
  timestamp: 1602743937
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     37 |           982.71 | 5986304 |  244.844 |              297.596 |              128.354 |            798.559 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3312.517410300543
    time_step_min: 2990
  date: 2020-10-15_06-39-23
  done: false
  episode_len_mean: 798.0349235635214
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 245.2278864022406
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 162
  episodes_total: 7588
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5793250352144241
        entropy_coeff: 0.0005000000000000001
        kl: 0.00574223801959306
        model: {}
        policy_loss: -0.0108771194354631
        total_loss: 8.031128525733948
        vf_explained_var: 0.9812270998954773
        vf_loss: 8.04200828075409
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25161290322581
    gpu_util_percent0: 0.297741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14899985884673464
    mean_env_wait_ms: 1.2090664243073204
    mean_inference_ms: 4.485695795205872
    mean_raw_obs_processing_ms: 0.38881290967169185
  time_since_restore: 1008.9683654308319
  time_this_iter_s: 26.25866436958313
  time_total_s: 1008.9683654308319
  timers:
    learn_throughput: 8295.244
    learn_time_ms: 19504.188
    sample_throughput: 23759.176
    sample_time_ms: 6809.664
    update_time_ms: 38.885
  timestamp: 1602743963
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     38 |          1008.97 | 6148096 |  245.228 |              297.596 |              128.354 |            798.035 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3308.2096733027056
    time_step_min: 2988
  date: 2020-10-15_06-39-50
  done: false
  episode_len_mean: 797.2153474780841
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 245.93591485943156
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 283
  episodes_total: 7871
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5480562746524811
        entropy_coeff: 0.0005000000000000001
        kl: 0.005499842731902997
        model: {}
        policy_loss: -0.008937571406325636
        total_loss: 10.389391978581747
        vf_explained_var: 0.9827632904052734
        vf_loss: 10.398328224817911
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60967741935484
    gpu_util_percent0: 0.29
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14891140559058802
    mean_env_wait_ms: 1.20965834678772
    mean_inference_ms: 4.479341676328158
    mean_raw_obs_processing_ms: 0.38848558658706184
  time_since_restore: 1035.532147884369
  time_this_iter_s: 26.563782453536987
  time_total_s: 1035.532147884369
  timers:
    learn_throughput: 8294.051
    learn_time_ms: 19506.995
    sample_throughput: 23763.191
    sample_time_ms: 6808.513
    update_time_ms: 38.731
  timestamp: 1602743990
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     39 |          1035.53 | 6309888 |  245.936 |              297.596 |              128.354 |            797.215 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3305.058456936308
    time_step_min: 2988
  date: 2020-10-15_06-40-16
  done: false
  episode_len_mean: 796.6839166046166
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 246.41884970328752
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 8058
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5393295685450236
        entropy_coeff: 0.0005000000000000001
        kl: 0.005108564626425505
        model: {}
        policy_loss: -0.012748330686008558
        total_loss: 7.700785279273987
        vf_explained_var: 0.9828435778617859
        vf_loss: 7.713547786076863
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95
    gpu_util_percent0: 0.35899999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14885342251100625
    mean_env_wait_ms: 1.2100076425708723
    mean_inference_ms: 4.475300107463432
    mean_raw_obs_processing_ms: 0.38827577220753207
  time_since_restore: 1061.8497931957245
  time_this_iter_s: 26.31764531135559
  time_total_s: 1061.8497931957245
  timers:
    learn_throughput: 8296.851
    learn_time_ms: 19500.411
    sample_throughput: 23795.634
    sample_time_ms: 6799.23
    update_time_ms: 38.662
  timestamp: 1602744016
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     40 |          1061.85 | 6471680 |  246.419 |              297.596 |              128.354 |            796.684 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3302.5930360415396
    time_step_min: 2988
  date: 2020-10-15_06-40-43
  done: false
  episode_len_mean: 796.2091240875912
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 246.79267123792673
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 162
  episodes_total: 8220
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5510972589254379
        entropy_coeff: 0.0005000000000000001
        kl: 0.005666492041200399
        model: {}
        policy_loss: -0.012003632000414655
        total_loss: 8.288190285364786
        vf_explained_var: 0.9800311923027039
        vf_loss: 8.300186077753702
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.683870967741942
    gpu_util_percent0: 0.3335483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488066361663499
    mean_env_wait_ms: 1.2103121102089829
    mean_inference_ms: 4.471986327957602
    mean_raw_obs_processing_ms: 0.38810286683312134
  time_since_restore: 1088.3504309654236
  time_this_iter_s: 26.500637769699097
  time_total_s: 1088.3504309654236
  timers:
    learn_throughput: 8296.511
    learn_time_ms: 19501.21
    sample_throughput: 23774.461
    sample_time_ms: 6805.286
    update_time_ms: 39.39
  timestamp: 1602744043
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     41 |          1088.35 | 6633472 |  246.793 |              297.596 |              128.354 |            796.209 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3298.630501656413
    time_step_min: 2988
  date: 2020-10-15_06-41-10
  done: false
  episode_len_mean: 795.5553199010251
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 247.40916767533946
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 267
  episodes_total: 8487
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5244901676972707
        entropy_coeff: 0.0005000000000000001
        kl: 0.00515878254858156
        model: {}
        policy_loss: -0.011081617277037973
        total_loss: 10.254097620646158
        vf_explained_var: 0.9823076725006104
        vf_loss: 10.265183846155802
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7258064516129
    gpu_util_percent0: 0.3393548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1487331206826334
    mean_env_wait_ms: 1.2108023493163944
    mean_inference_ms: 4.46668025347126
    mean_raw_obs_processing_ms: 0.3878287708629814
  time_since_restore: 1114.8169264793396
  time_this_iter_s: 26.466495513916016
  time_total_s: 1114.8169264793396
  timers:
    learn_throughput: 8297.933
    learn_time_ms: 19497.869
    sample_throughput: 23730.874
    sample_time_ms: 6817.785
    update_time_ms: 38.676
  timestamp: 1602744070
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     42 |          1114.82 | 6795264 |  247.409 |              297.596 |              128.354 |            795.555 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3295.431426920855
    time_step_min: 2984
  date: 2020-10-15_06-41-36
  done: false
  episode_len_mean: 795.1115074798619
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 247.86582743429693
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 203
  episodes_total: 8690
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5074903890490532
        entropy_coeff: 0.0005000000000000001
        kl: 0.005213672256407638
        model: {}
        policy_loss: -0.011148126039188355
        total_loss: 7.967456380526225
        vf_explained_var: 0.9832329750061035
        vf_loss: 7.9785975615183515
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.767741935483876
    gpu_util_percent0: 0.2648387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14867703789979855
    mean_env_wait_ms: 1.2111344610371422
    mean_inference_ms: 4.4627812691963475
    mean_raw_obs_processing_ms: 0.3876275959002308
  time_since_restore: 1141.467958688736
  time_this_iter_s: 26.651032209396362
  time_total_s: 1141.467958688736
  timers:
    learn_throughput: 8282.496
    learn_time_ms: 19534.209
    sample_throughput: 23741.645
    sample_time_ms: 6814.692
    update_time_ms: 38.602
  timestamp: 1602744096
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     43 |          1141.47 | 6957056 |  247.866 |              297.596 |              128.354 |            795.112 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3292.996483267158
    time_step_min: 2984
  date: 2020-10-15_06-42-03
  done: false
  episode_len_mean: 794.7411299435029
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 248.2568567026194
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 160
  episodes_total: 8850
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5157928119103113
        entropy_coeff: 0.0005000000000000001
        kl: 0.005249064958964785
        model: {}
        policy_loss: -0.011975913619001707
        total_loss: 6.316017548243205
        vf_explained_var: 0.9843130707740784
        vf_loss: 6.327988902727763
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43333333333334
    gpu_util_percent0: 0.36733333333333346
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486354919034657
    mean_env_wait_ms: 1.2113937766371914
    mean_inference_ms: 4.459853550468318
    mean_raw_obs_processing_ms: 0.3874763399134506
  time_since_restore: 1167.77397108078
  time_this_iter_s: 26.306012392044067
  time_total_s: 1167.77397108078
  timers:
    learn_throughput: 8286.164
    learn_time_ms: 19525.561
    sample_throughput: 23757.16
    sample_time_ms: 6810.242
    update_time_ms: 38.579
  timestamp: 1602744123
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     44 |          1167.77 | 7118848 |  248.257 |              297.596 |              128.354 |            794.741 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3288.906484340538
    time_step_min: 2984
  date: 2020-10-15_06-42-30
  done: false
  episode_len_mean: 794.1748873997583
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 248.82141307616422
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 253
  episodes_total: 9103
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4996359671155612
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045946843844528
        model: {}
        policy_loss: -0.010852797965829572
        total_loss: 11.265838940938314
        vf_explained_var: 0.9799129366874695
        vf_loss: 11.276711622873941
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98709677419355
    gpu_util_percent0: 0.36677419354838703
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14857497897349564
    mean_env_wait_ms: 1.211799275411155
    mean_inference_ms: 4.45538755304863
    mean_raw_obs_processing_ms: 0.38725046324346746
  time_since_restore: 1194.3222379684448
  time_this_iter_s: 26.548266887664795
  time_total_s: 1194.3222379684448
  timers:
    learn_throughput: 8290.809
    learn_time_ms: 19514.621
    sample_throughput: 23694.622
    sample_time_ms: 6828.216
    update_time_ms: 39.339
  timestamp: 1602744150
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     45 |          1194.32 | 7280640 |  248.821 |              305.323 |              128.354 |            794.175 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3285.604070205664
    time_step_min: 2984
  date: 2020-10-15_06-42-56
  done: false
  episode_len_mean: 793.6486805406565
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 249.288631866834
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 9322
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.48180339982112247
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053388803110768395
        model: {}
        policy_loss: -0.009326927965351691
        total_loss: 7.430981198946635
        vf_explained_var: 0.9844884276390076
        vf_loss: 7.440415342648824
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.403225806451612
    gpu_util_percent0: 0.3909677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14852012390830735
    mean_env_wait_ms: 1.212116817702557
    mean_inference_ms: 4.451640123343742
    mean_raw_obs_processing_ms: 0.3870520446730571
  time_since_restore: 1220.7099645137787
  time_this_iter_s: 26.387726545333862
  time_total_s: 1220.7099645137787
  timers:
    learn_throughput: 8309.724
    learn_time_ms: 19470.202
    sample_throughput: 23618.41
    sample_time_ms: 6850.249
    update_time_ms: 39.466
  timestamp: 1602744176
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     46 |          1220.71 | 7442432 |  249.289 |              305.323 |              128.354 |            793.649 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3283.188988883007
    time_step_min: 2984
  date: 2020-10-15_06-43-23
  done: false
  episode_len_mean: 793.2549578059072
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 249.6373385756297
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 9480
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4932329977552096
        entropy_coeff: 0.0005000000000000001
        kl: 0.005842235443803172
        model: {}
        policy_loss: -0.012003136109948779
        total_loss: 6.401867707570394
        vf_explained_var: 0.983512818813324
        vf_loss: 6.413971463839213
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970967741935485
    gpu_util_percent0: 0.2887096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419344
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14848351382324745
    mean_env_wait_ms: 1.212341627547794
    mean_inference_ms: 4.4490452245882395
    mean_raw_obs_processing_ms: 0.3869178938445444
  time_since_restore: 1247.4428293704987
  time_this_iter_s: 26.73286485671997
  time_total_s: 1247.4428293704987
  timers:
    learn_throughput: 8297.247
    learn_time_ms: 19499.48
    sample_throughput: 23628.529
    sample_time_ms: 6847.316
    update_time_ms: 39.565
  timestamp: 1602744203
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     47 |          1247.44 | 7604224 |  249.637 |              305.323 |              128.354 |            793.255 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3279.770158795628
    time_step_min: 2975
  date: 2020-10-15_06-43-50
  done: false
  episode_len_mean: 792.6430699681496
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 250.1550758795185
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 253
  episodes_total: 9733
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4747646301984787
        entropy_coeff: 0.0005000000000000001
        kl: 0.00540254928637296
        model: {}
        policy_loss: -0.008263791542655477
        total_loss: 8.609153588612875
        vf_explained_var: 0.9846184253692627
        vf_loss: 8.617519617080688
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.045161290322575
    gpu_util_percent0: 0.315483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14842836089144604
    mean_env_wait_ms: 1.212707084362077
    mean_inference_ms: 4.445043978147971
    mean_raw_obs_processing_ms: 0.3867191167018729
  time_since_restore: 1273.8708279132843
  time_this_iter_s: 26.427998542785645
  time_total_s: 1273.8708279132843
  timers:
    learn_throughput: 8301.192
    learn_time_ms: 19490.212
    sample_throughput: 23539.162
    sample_time_ms: 6873.312
    update_time_ms: 38.917
  timestamp: 1602744230
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     48 |          1273.87 | 7766016 |  250.155 |              305.323 |              128.354 |            792.643 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3276.672245186007
    time_step_min: 2968
  date: 2020-10-15_06-44-17
  done: false
  episode_len_mean: 792.1457705445047
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 250.63314275972505
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 221
  episodes_total: 9954
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.45495350658893585
        entropy_coeff: 0.0005000000000000001
        kl: 0.005297816319701572
        model: {}
        policy_loss: -0.011545950949463682
        total_loss: 6.367646296819051
        vf_explained_var: 0.9864680767059326
        vf_loss: 6.379287123680115
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.377419354838707
    gpu_util_percent0: 0.3316129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483796143812816
    mean_env_wait_ms: 1.2129939034878738
    mean_inference_ms: 4.44167802569579
    mean_raw_obs_processing_ms: 0.3865413394319363
  time_since_restore: 1300.6001439094543
  time_this_iter_s: 26.729315996170044
  time_total_s: 1300.6001439094543
  timers:
    learn_throughput: 8306.65
    learn_time_ms: 19477.407
    sample_throughput: 23464.736
    sample_time_ms: 6895.113
    update_time_ms: 39.117
  timestamp: 1602744257
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     49 |           1300.6 | 7927808 |  250.633 |              305.323 |              128.354 |            792.146 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3274.7784282595753
    time_step_min: 2950
  date: 2020-10-15_06-44-43
  done: false
  episode_len_mean: 791.7395431622664
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 250.94052959137503
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 159
  episodes_total: 10113
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4651120329896609
        entropy_coeff: 0.0005000000000000001
        kl: 0.005508953550209601
        model: {}
        policy_loss: -0.011178916155283028
        total_loss: 6.434786359469096
        vf_explained_var: 0.9834200739860535
        vf_loss: 6.446059942245483
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.213333333333335
    gpu_util_percent0: 0.3623333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14834678289439843
    mean_env_wait_ms: 1.2131988158291849
    mean_inference_ms: 4.43934520336909
    mean_raw_obs_processing_ms: 0.38642159298039286
  time_since_restore: 1326.8804845809937
  time_this_iter_s: 26.280340671539307
  time_total_s: 1326.8804845809937
  timers:
    learn_throughput: 8308.771
    learn_time_ms: 19472.434
    sample_throughput: 23465.551
    sample_time_ms: 6894.873
    update_time_ms: 38.628
  timestamp: 1602744283
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     50 |          1326.88 | 8089600 |  250.941 |              305.323 |              128.354 |             791.74 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3271.786591854503
    time_step_min: 2950
  date: 2020-10-15_06-45-10
  done: false
  episode_len_mean: 791.0770343231778
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 251.40680230768928
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 259
  episodes_total: 10372
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.44740036129951477
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051830766024068
        model: {}
        policy_loss: -0.011204511916730553
        total_loss: 10.451243956883749
        vf_explained_var: 0.9811357855796814
        vf_loss: 10.46254269282023
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.929032258064517
    gpu_util_percent0: 0.2645161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482951567489294
    mean_env_wait_ms: 1.2135328627447433
    mean_inference_ms: 4.435614210347321
    mean_raw_obs_processing_ms: 0.3862352712113342
  time_since_restore: 1353.5764980316162
  time_this_iter_s: 26.69601345062256
  time_total_s: 1353.5764980316162
  timers:
    learn_throughput: 8300.063
    learn_time_ms: 19492.864
    sample_throughput: 23471.288
    sample_time_ms: 6893.188
    update_time_ms: 38.641
  timestamp: 1602744310
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     51 |          1353.58 | 8251392 |  251.407 |              305.323 |              128.354 |            791.077 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3269.3174106719744
    time_step_min: 2950
  date: 2020-10-15_06-45-37
  done: false
  episode_len_mean: 790.5274891365955
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 251.79537391676067
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 214
  episodes_total: 10586
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42545434335867566
        entropy_coeff: 0.0005000000000000001
        kl: 0.005011683058304091
        model: {}
        policy_loss: -0.01152935406571487
        total_loss: 5.990321119626363
        vf_explained_var: 0.986952006816864
        vf_loss: 6.0019378662109375
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.948387096774194
    gpu_util_percent0: 0.34064516129032263
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14825324001614434
    mean_env_wait_ms: 1.2137987363719165
    mean_inference_ms: 4.432705719014587
    mean_raw_obs_processing_ms: 0.3860856502070045
  time_since_restore: 1380.210322380066
  time_this_iter_s: 26.633824348449707
  time_total_s: 1380.210322380066
  timers:
    learn_throughput: 8290.777
    learn_time_ms: 19514.698
    sample_throughput: 23493.097
    sample_time_ms: 6886.789
    update_time_ms: 39.223
  timestamp: 1602744337
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     52 |          1380.21 | 8413184 |  251.795 |              305.323 |              128.354 |            790.527 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3267.739962651727
    time_step_min: 2950
  date: 2020-10-15_06-46-04
  done: false
  episode_len_mean: 790.1010702652396
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 252.04978589994877
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 159
  episodes_total: 10745
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4393023227651914
        entropy_coeff: 0.0005000000000000001
        kl: 0.005606093637955685
        model: {}
        policy_loss: -0.010608289807957286
        total_loss: 5.841294089953105
        vf_explained_var: 0.9850214123725891
        vf_loss: 5.85198179880778
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.66875
    gpu_util_percent0: 0.3775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14822373915425727
    mean_env_wait_ms: 1.2139914170336699
    mean_inference_ms: 4.430596627129045
    mean_raw_obs_processing_ms: 0.3859787961087253
  time_since_restore: 1406.9090583324432
  time_this_iter_s: 26.69873595237732
  time_total_s: 1406.9090583324432
  timers:
    learn_throughput: 8296.993
    learn_time_ms: 19500.077
    sample_throughput: 23463.833
    sample_time_ms: 6895.378
    update_time_ms: 40.189
  timestamp: 1602744364
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     53 |          1406.91 | 8574976 |   252.05 |              305.323 |              128.354 |            790.101 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3264.7645290581163
    time_step_min: 2950
  date: 2020-10-15_06-46-31
  done: false
  episode_len_mean: 789.3747389448833
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 252.48673606123896
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 268
  episodes_total: 11013
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4164704183737437
        entropy_coeff: 0.0005000000000000001
        kl: 0.004910058225505054
        model: {}
        policy_loss: -0.008640341703236723
        total_loss: 8.891689221064249
        vf_explained_var: 0.9841923713684082
        vf_loss: 8.900414943695068
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.783333333333328
    gpu_util_percent0: 0.3493333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14817552379872984
    mean_env_wait_ms: 1.2143191979103942
    mean_inference_ms: 4.4271378110193655
    mean_raw_obs_processing_ms: 0.38580707028974337
  time_since_restore: 1433.2742712497711
  time_this_iter_s: 26.36521291732788
  time_total_s: 1433.2742712497711
  timers:
    learn_throughput: 8294.697
    learn_time_ms: 19505.473
    sample_throughput: 23465.637
    sample_time_ms: 6894.848
    update_time_ms: 40.634
  timestamp: 1602744391
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     54 |          1433.27 | 8736768 |  252.487 |              305.323 |              128.354 |            789.375 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3262.4488956451755
    time_step_min: 2950
  date: 2020-10-15_06-46-57
  done: false
  episode_len_mean: 788.8435550008915
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 252.84319482937775
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 205
  episodes_total: 11218
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4035197471578916
        entropy_coeff: 0.0005000000000000001
        kl: 0.005553202470764518
        model: {}
        policy_loss: -0.009300931472656279
        total_loss: 5.939739346504211
        vf_explained_var: 0.9866606593132019
        vf_loss: 5.949172576268514
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.880645161290328
    gpu_util_percent0: 0.2932258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14813867243974882
    mean_env_wait_ms: 1.2145579525482533
    mean_inference_ms: 4.4245703700136705
    mean_raw_obs_processing_ms: 0.3856777232264334
  time_since_restore: 1459.656973361969
  time_this_iter_s: 26.382702112197876
  time_total_s: 1459.656973361969
  timers:
    learn_throughput: 8291.675
    learn_time_ms: 19512.584
    sample_throughput: 23549.361
    sample_time_ms: 6870.335
    update_time_ms: 40.373
  timestamp: 1602744417
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     55 |          1459.66 | 8898560 |  252.843 |              305.323 |              128.354 |            788.844 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3260.695372410754
    time_step_min: 2950
  date: 2020-10-15_06-47-24
  done: false
  episode_len_mean: 788.4505272407733
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 253.1275274715521
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 162
  episodes_total: 11380
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4184493770202001
        entropy_coeff: 0.0005000000000000001
        kl: 0.005551290077467759
        model: {}
        policy_loss: -0.011252332527268058
        total_loss: 5.403757611910502
        vf_explained_var: 0.9860562682151794
        vf_loss: 5.415149927139282
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.45161290322581
    gpu_util_percent0: 0.3387096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14811105574176137
    mean_env_wait_ms: 1.2147450951910663
    mean_inference_ms: 4.422630131589867
    mean_raw_obs_processing_ms: 0.385580473462049
  time_since_restore: 1486.1371405124664
  time_this_iter_s: 26.480167150497437
  time_total_s: 1486.1371405124664
  timers:
    learn_throughput: 8277.573
    learn_time_ms: 19545.826
    sample_throughput: 23635.828
    sample_time_ms: 6845.201
    update_time_ms: 40.838
  timestamp: 1602744444
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     56 |          1486.14 | 9060352 |  253.128 |              305.323 |              128.354 |            788.451 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3257.6070906118234
    time_step_min: 2950
  date: 2020-10-15_06-47-51
  done: false
  episode_len_mean: 787.7989018531229
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 253.62370444319654
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 276
  episodes_total: 11656
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.39494340121746063
        entropy_coeff: 0.0005000000000000001
        kl: 0.005176450125873089
        model: {}
        policy_loss: -0.010596645918364326
        total_loss: 7.448789477348328
        vf_explained_var: 0.9863360524177551
        vf_loss: 7.459518949190776
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.045161290322582
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806635466858006
    mean_env_wait_ms: 1.215066505576636
    mean_inference_ms: 4.4194191509910725
    mean_raw_obs_processing_ms: 0.3854203111787025
  time_since_restore: 1512.6553988456726
  time_this_iter_s: 26.518258333206177
  time_total_s: 1512.6553988456726
  timers:
    learn_throughput: 8285.699
    learn_time_ms: 19526.656
    sample_throughput: 23644.741
    sample_time_ms: 6842.621
    update_time_ms: 40.261
  timestamp: 1602744471
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     57 |          1512.66 | 9222144 |  253.624 |              305.323 |              128.354 |            787.799 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3255.520694033009
    time_step_min: 2950
  date: 2020-10-15_06-48-18
  done: false
  episode_len_mean: 787.3697046413503
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 253.92976601457616
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 194
  episodes_total: 11850
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.38347873588403064
        entropy_coeff: 0.0005000000000000001
        kl: 0.004819899913854897
        model: {}
        policy_loss: -0.010946293051044146
        total_loss: 6.16417129834493
        vf_explained_var: 0.98579341173172
        vf_loss: 6.175249099731445
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.596774193548388
    gpu_util_percent0: 0.26870967741935486
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14803420988646307
    mean_env_wait_ms: 1.2152650658750306
    mean_inference_ms: 4.417164007916668
    mean_raw_obs_processing_ms: 0.38530838808484336
  time_since_restore: 1539.0284161567688
  time_this_iter_s: 26.37301731109619
  time_total_s: 1539.0284161567688
  timers:
    learn_throughput: 8281.381
    learn_time_ms: 19536.837
    sample_throughput: 23704.702
    sample_time_ms: 6825.313
    update_time_ms: 41.359
  timestamp: 1602744498
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     58 |          1539.03 | 9383936 |   253.93 |              305.323 |              128.354 |             787.37 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3253.8100642576983
    time_step_min: 2950
  date: 2020-10-15_06-48-44
  done: false
  episode_len_mean: 787.0014977533699
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 254.19643766673224
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 12018
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.39791788657506305
        entropy_coeff: 0.0005000000000000001
        kl: 0.005529815602737169
        model: {}
        policy_loss: -0.011331992524598414
        total_loss: 6.490147113800049
        vf_explained_var: 0.9842641353607178
        vf_loss: 6.501643260320027
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36129032258065
    gpu_util_percent0: 0.26129032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14800906249432627
    mean_env_wait_ms: 1.2154463564600533
    mean_inference_ms: 4.415340179169895
    mean_raw_obs_processing_ms: 0.38521738314068255
  time_since_restore: 1565.6154265403748
  time_this_iter_s: 26.587010383605957
  time_total_s: 1565.6154265403748
  timers:
    learn_throughput: 8275.611
    learn_time_ms: 19550.459
    sample_throughput: 23771.586
    sample_time_ms: 6806.109
    update_time_ms: 39.425
  timestamp: 1602744524
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     59 |          1565.62 | 9545728 |  254.196 |              305.323 |              128.354 |            787.001 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3250.8934920893817
    time_step_min: 2950
  date: 2020-10-15_06-49-11
  done: false
  episode_len_mean: 786.4229486866716
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 254.6451659803697
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 279
  episodes_total: 12297
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3765448456009229
        entropy_coeff: 0.0005000000000000001
        kl: 0.005654264163846771
        model: {}
        policy_loss: -0.009280701555932561
        total_loss: 7.544934113820394
        vf_explained_var: 0.9866731762886047
        vf_loss: 7.554367701212565
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.980645161290326
    gpu_util_percent0: 0.32870967741935475
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479671760568651
    mean_env_wait_ms: 1.21574470599145
    mean_inference_ms: 4.412367264553587
    mean_raw_obs_processing_ms: 0.3850723832958191
  time_since_restore: 1592.2401251792908
  time_this_iter_s: 26.624698638916016
  time_total_s: 1592.2401251792908
  timers:
    learn_throughput: 8261.243
    learn_time_ms: 19584.463
    sample_throughput: 23771.133
    sample_time_ms: 6806.239
    update_time_ms: 39.621
  timestamp: 1602744551
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     60 |          1592.24 | 9707520 |  254.645 |              305.323 |              128.354 |            786.423 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3249.0492488149753
    time_step_min: 2950
  date: 2020-10-15_06-49-38
  done: false
  episode_len_mean: 786.0809966351546
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 254.93048494883135
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 185
  episodes_total: 12482
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3720836068193118
        entropy_coeff: 0.0005000000000000001
        kl: 0.005019095997946958
        model: {}
        policy_loss: -0.011203962999085585
        total_loss: 5.8692318598429365
        vf_explained_var: 0.9862847328186035
        vf_loss: 5.880590558052063
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.99
    gpu_util_percent0: 0.366
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147939444456677
    mean_env_wait_ms: 1.2159197198784273
    mean_inference_ms: 4.4103959220956765
    mean_raw_obs_processing_ms: 0.3849743663557929
  time_since_restore: 1618.5853605270386
  time_this_iter_s: 26.345235347747803
  time_total_s: 1618.5853605270386
  timers:
    learn_throughput: 8271.058
    learn_time_ms: 19561.222
    sample_throughput: 23812.134
    sample_time_ms: 6794.519
    update_time_ms: 37.94
  timestamp: 1602744578
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     61 |          1618.59 | 9869312 |   254.93 |              305.323 |              128.354 |            786.081 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3247.1057136064665
    time_step_min: 2950
  date: 2020-10-15_06-50-05
  done: false
  episode_len_mean: 785.7563616247827
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 255.2223180117917
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 172
  episodes_total: 12654
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3821555897593498
        entropy_coeff: 0.0005000000000000001
        kl: 0.005336747388355434
        model: {}
        policy_loss: -0.011190150883824876
        total_loss: 5.106372197469075
        vf_explained_var: 0.9875199198722839
        vf_loss: 5.117720087369283
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.832258064516132
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14791605433425767
    mean_env_wait_ms: 1.216092528464719
    mean_inference_ms: 4.408695247627222
    mean_raw_obs_processing_ms: 0.38489044865580424
  time_since_restore: 1645.261358499527
  time_this_iter_s: 26.675997972488403
  time_total_s: 1645.261358499527
  timers:
    learn_throughput: 8274.148
    learn_time_ms: 19553.916
    sample_throughput: 23774.95
    sample_time_ms: 6805.146
    update_time_ms: 37.385
  timestamp: 1602744605
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     62 |          1645.26 | 10031104 |  255.222 |              305.323 |              128.354 |            785.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3244.266020922123
    time_step_min: 2950
  date: 2020-10-15_06-50-32
  done: false
  episode_len_mean: 785.2134466769706
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 255.66669398779135
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 286
  episodes_total: 12940
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3558207228779793
        entropy_coeff: 0.0005000000000000001
        kl: 0.004898939708558221
        model: {}
        policy_loss: -0.009166850621113554
        total_loss: 7.341542323430379
        vf_explained_var: 0.9869981408119202
        vf_loss: 7.35085650285085
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.045161290322582
    gpu_util_percent0: 0.27161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787573615647534
    mean_env_wait_ms: 1.2163645060469923
    mean_inference_ms: 4.405878522176861
    mean_raw_obs_processing_ms: 0.3847504240699375
  time_since_restore: 1671.6946902275085
  time_this_iter_s: 26.433331727981567
  time_total_s: 1671.6946902275085
  timers:
    learn_throughput: 8279.477
    learn_time_ms: 19541.33
    sample_throughput: 23785.963
    sample_time_ms: 6801.995
    update_time_ms: 34.698
  timestamp: 1602744632
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     63 |          1671.69 | 10192896 |  255.667 |              305.323 |              128.354 |            785.213 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3242.5269516018043
    time_step_min: 2950
  date: 2020-10-15_06-50-58
  done: false
  episode_len_mean: 784.9008692999847
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 255.9368374918932
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 13114
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3503326053420703
        entropy_coeff: 0.0005000000000000001
        kl: 0.005159190545479457
        model: {}
        policy_loss: -0.010730534287480017
        total_loss: 4.31449298063914
        vf_explained_var: 0.9890246391296387
        vf_loss: 4.325382471084595
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.816129032258072
    gpu_util_percent0: 0.2903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14785195695639228
    mean_env_wait_ms: 1.2165194912409352
    mean_inference_ms: 4.404177843728697
    mean_raw_obs_processing_ms: 0.3846669589762416
  time_since_restore: 1698.1127746105194
  time_this_iter_s: 26.418084383010864
  time_total_s: 1698.1127746105194
  timers:
    learn_throughput: 8285.593
    learn_time_ms: 19526.908
    sample_throughput: 23727.277
    sample_time_ms: 6818.819
    update_time_ms: 34.532
  timestamp: 1602744658
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     64 |          1698.11 | 10354688 |  255.937 |              305.323 |              128.354 |            784.901 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3240.8319251659627
    time_step_min: 2950
  date: 2020-10-15_06-51-25
  done: false
  episode_len_mean: 784.6386276427658
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 256.18322416095344
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 177
  episodes_total: 13291
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3594164252281189
        entropy_coeff: 0.0005000000000000001
        kl: 0.005062125584421058
        model: {}
        policy_loss: -0.011500527684499199
        total_loss: 5.815953413645427
        vf_explained_var: 0.9867269396781921
        vf_loss: 5.827617843945821
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43870967741936
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782945308264217
    mean_env_wait_ms: 1.2166835264827336
    mean_inference_ms: 4.402553046267549
    mean_raw_obs_processing_ms: 0.3845856443218917
  time_since_restore: 1724.73073220253
  time_this_iter_s: 26.617957592010498
  time_total_s: 1724.73073220253
  timers:
    learn_throughput: 8278.379
    learn_time_ms: 19543.922
    sample_throughput: 23709.262
    sample_time_ms: 6824.0
    update_time_ms: 34.486
  timestamp: 1602744685
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     65 |          1724.73 | 10516480 |  256.183 |              305.323 |              128.354 |            784.639 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3238.2538779731126
    time_step_min: 2950
  date: 2020-10-15_06-51-52
  done: false
  episode_len_mean: 784.2552862300155
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 256.57828636322705
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 282
  episodes_total: 13573
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3380468636751175
        entropy_coeff: 0.0005000000000000001
        kl: 0.004767467073785762
        model: {}
        policy_loss: -0.011347987786090622
        total_loss: 7.793144861857097
        vf_explained_var: 0.9862980842590332
        vf_loss: 7.804647008577983
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.743333333333332
    gpu_util_percent0: 0.38400000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14779276346160847
    mean_env_wait_ms: 1.2169284481671514
    mean_inference_ms: 4.40001241543565
    mean_raw_obs_processing_ms: 0.3844579876652777
  time_since_restore: 1751.081869840622
  time_this_iter_s: 26.35113763809204
  time_total_s: 1751.081869840622
  timers:
    learn_throughput: 8285.964
    learn_time_ms: 19526.033
    sample_throughput: 23695.413
    sample_time_ms: 6827.988
    update_time_ms: 34.453
  timestamp: 1602744712
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     66 |          1751.08 | 10678272 |  256.578 |              305.323 |              128.354 |            784.255 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3236.714681642477
    time_step_min: 2950
  date: 2020-10-15_06-52-19
  done: false
  episode_len_mean: 783.9962170813327
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 256.80994581343776
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 173
  episodes_total: 13746
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3312930141886075
        entropy_coeff: 0.0005000000000000001
        kl: 0.004902967523473005
        model: {}
        policy_loss: -0.010100232805901518
        total_loss: 4.8077118794123335
        vf_explained_var: 0.9883539080619812
        vf_loss: 4.8179701169331866
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.448387096774194
    gpu_util_percent0: 0.3412903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7935483870967746
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477708204572562
    mean_env_wait_ms: 1.2170674625210247
    mean_inference_ms: 4.3984626117236205
    mean_raw_obs_processing_ms: 0.3843808453568491
  time_since_restore: 1777.7064468860626
  time_this_iter_s: 26.624577045440674
  time_total_s: 1777.7064468860626
  timers:
    learn_throughput: 8281.69
    learn_time_ms: 19536.11
    sample_throughput: 23695.345
    sample_time_ms: 6828.008
    update_time_ms: 34.046
  timestamp: 1602744739
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     67 |          1777.71 | 10840064 |   256.81 |              305.323 |              128.354 |            783.996 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3235.0881887498203
    time_step_min: 2950
  date: 2020-10-15_06-52-46
  done: false
  episode_len_mean: 783.689746717371
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 257.0585303418051
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 13937
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3444381132721901
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056744702548409505
        model: {}
        policy_loss: -0.012320043751969934
        total_loss: 5.890604615211487
        vf_explained_var: 0.9868749976158142
        vf_loss: 5.903092543284099
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.821875
    gpu_util_percent0: 0.2990625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.778125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14774906138262758
    mean_env_wait_ms: 1.2172288899151493
    mean_inference_ms: 4.396866406415344
    mean_raw_obs_processing_ms: 0.3843029023780026
  time_since_restore: 1804.730907201767
  time_this_iter_s: 27.024460315704346
  time_total_s: 1804.730907201767
  timers:
    learn_throughput: 8266.759
    learn_time_ms: 19571.395
    sample_throughput: 23598.794
    sample_time_ms: 6855.944
    update_time_ms: 33.429
  timestamp: 1602744766
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     68 |          1804.73 | 11001856 |  257.059 |              305.323 |              128.354 |             783.69 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3232.8164244391137
    time_step_min: 2950
  date: 2020-10-15_06-53-13
  done: false
  episode_len_mean: 783.2805264269125
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 257.41160141068656
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 272
  episodes_total: 14209
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3151191423336665
        entropy_coeff: 0.0005000000000000001
        kl: 0.004452489976150294
        model: {}
        policy_loss: -0.009892117969381312
        total_loss: 7.8788909912109375
        vf_explained_var: 0.9861714243888855
        vf_loss: 7.888937473297119
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.887096774193548
    gpu_util_percent0: 0.3348387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771551520205348
    mean_env_wait_ms: 1.2174431306793556
    mean_inference_ms: 4.394604840000055
    mean_raw_obs_processing_ms: 0.384185916128206
  time_since_restore: 1831.1056385040283
  time_this_iter_s: 26.374731302261353
  time_total_s: 1831.1056385040283
  timers:
    learn_throughput: 8278.837
    learn_time_ms: 19542.841
    sample_throughput: 23587.561
    sample_time_ms: 6859.209
    update_time_ms: 36.229
  timestamp: 1602744793
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     69 |          1831.11 | 11163648 |  257.412 |              305.323 |              128.354 |            783.281 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3231.107648330196
    time_step_min: 2950
  date: 2020-10-15_06-53-39
  done: false
  episode_len_mean: 783.0469467241619
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 257.67121134842654
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 169
  episodes_total: 14378
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.3170602818330129
        entropy_coeff: 0.0005000000000000001
        kl: 0.004908886000824471
        model: {}
        policy_loss: -0.010506820544833317
        total_loss: 3.9420782128969827
        vf_explained_var: 0.9898332953453064
        vf_loss: 3.952741583188375
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.074193548387097
    gpu_util_percent0: 0.3503225806451612
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769564460136209
    mean_env_wait_ms: 1.2175639445479212
    mean_inference_ms: 4.393215491526075
    mean_raw_obs_processing_ms: 0.38411394699950757
  time_since_restore: 1857.6173901557922
  time_this_iter_s: 26.511751651763916
  time_total_s: 1857.6173901557922
  timers:
    learn_throughput: 8286.266
    learn_time_ms: 19525.321
    sample_throughput: 23576.258
    sample_time_ms: 6862.497
    update_time_ms: 35.516
  timestamp: 1602744819
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     70 |          1857.62 | 11325440 |  257.671 |              305.323 |              128.354 |            783.047 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3229.4403795901526
    time_step_min: 2950
  date: 2020-10-15_06-54-07
  done: false
  episode_len_mean: 782.7946765452425
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 257.92329136185896
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 14577
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.32189908375342685
        entropy_coeff: 0.0005000000000000001
        kl: 0.004997135916103919
        model: {}
        policy_loss: -0.010790806454527532
        total_loss: 6.395708521207173
        vf_explained_var: 0.9863850474357605
        vf_loss: 6.406659245491028
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.609677419354835
    gpu_util_percent0: 0.3774193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14767415897921304
    mean_env_wait_ms: 1.2177163239106155
    mean_inference_ms: 4.391669204263689
    mean_raw_obs_processing_ms: 0.38403769119964437
  time_since_restore: 1884.3040928840637
  time_this_iter_s: 26.686702728271484
  time_total_s: 1884.3040928840637
  timers:
    learn_throughput: 8277.015
    learn_time_ms: 19547.144
    sample_throughput: 23548.321
    sample_time_ms: 6870.638
    update_time_ms: 37.276
  timestamp: 1602744847
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     71 |           1884.3 | 11487232 |  257.923 |              305.323 |              128.354 |            782.795 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3227.0247839005942
    time_step_min: 2950
  date: 2020-10-15_06-54-33
  done: false
  episode_len_mean: 782.4831233578118
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 258.2973105031314
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 266
  episodes_total: 14843
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.29725851863622665
        entropy_coeff: 0.0005000000000000001
        kl: 0.004627004925472041
        model: {}
        policy_loss: -0.008652853925013915
        total_loss: 5.9916768074035645
        vf_explained_var: 0.9891205430030823
        vf_loss: 6.000477910041809
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.945161290322584
    gpu_util_percent0: 0.257741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.861290322580646
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14764397548810623
    mean_env_wait_ms: 1.2179042621572014
    mean_inference_ms: 4.3896425866073026
    mean_raw_obs_processing_ms: 0.38392881462641193
  time_since_restore: 1910.6995069980621
  time_this_iter_s: 26.395414113998413
  time_total_s: 1910.6995069980621
  timers:
    learn_throughput: 8285.492
    learn_time_ms: 19527.144
    sample_throughput: 23609.147
    sample_time_ms: 6852.937
    update_time_ms: 37.86
  timestamp: 1602744873
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     72 |           1910.7 | 11649024 |  258.297 |              305.323 |              128.354 |            782.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3225.64714524207
    time_step_min: 2950
  date: 2020-10-15_06-55-00
  done: false
  episode_len_mean: 782.2940706195869
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 258.5215142766775
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 15010
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.29669157912333805
        entropy_coeff: 0.0005000000000000001
        kl: 0.004874422408950825
        model: {}
        policy_loss: -0.011252956717119863
        total_loss: 4.834938367207845
        vf_explained_var: 0.988092839717865
        vf_loss: 4.8463393449783325
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53548387096775
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762578337879317
    mean_env_wait_ms: 1.2180098678953086
    mean_inference_ms: 4.3883602523379555
    mean_raw_obs_processing_ms: 0.3838605557380058
  time_since_restore: 1937.3475360870361
  time_this_iter_s: 26.648029088974
  time_total_s: 1937.3475360870361
  timers:
    learn_throughput: 8281.275
    learn_time_ms: 19537.087
    sample_throughput: 23580.214
    sample_time_ms: 6861.346
    update_time_ms: 40.438
  timestamp: 1602744900
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     73 |          1937.35 | 11810816 |  258.522 |              305.323 |              128.354 |            782.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3223.793014827018
    time_step_min: 2950
  date: 2020-10-15_06-55-27
  done: false
  episode_len_mean: 782.0633793556871
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 258.80502261271494
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 200
  episodes_total: 15210
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.30901283274094266
        entropy_coeff: 0.0005000000000000001
        kl: 0.005666652228683233
        model: {}
        policy_loss: -0.010412109596624456
        total_loss: 5.1122108697891235
        vf_explained_var: 0.9888952374458313
        vf_loss: 5.122777422269185
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14516129032258
    gpu_util_percent0: 0.32290322580645164
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760517083581334
    mean_env_wait_ms: 1.2181434088673997
    mean_inference_ms: 4.3869057382050025
    mean_raw_obs_processing_ms: 0.38378693219251603
  time_since_restore: 1963.8969905376434
  time_this_iter_s: 26.5494544506073
  time_total_s: 1963.8969905376434
  timers:
    learn_throughput: 8278.609
    learn_time_ms: 19543.38
    sample_throughput: 23552.982
    sample_time_ms: 6869.279
    update_time_ms: 40.385
  timestamp: 1602744927
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     74 |           1963.9 | 11972608 |  258.805 |              305.323 |              128.354 |            782.063 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3221.2739167044497
    time_step_min: 2950
  date: 2020-10-15_06-55-54
  done: false
  episode_len_mean: 781.8125242341993
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 259.1772435483176
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 264
  episodes_total: 15474
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.2824097474416097
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046757976136480766
        model: {}
        policy_loss: -0.010624428505252581
        total_loss: 5.228386839230855
        vf_explained_var: 0.990327775478363
        vf_loss: 5.239152232805888
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.677419354838705
    gpu_util_percent0: 0.29709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757785212043736
    mean_env_wait_ms: 1.2183127600596333
    mean_inference_ms: 4.385048072466326
    mean_raw_obs_processing_ms: 0.38368463223457056
  time_since_restore: 1990.5741724967957
  time_this_iter_s: 26.67718195915222
  time_total_s: 1990.5741724967957
  timers:
    learn_throughput: 8287.727
    learn_time_ms: 19521.879
    sample_throughput: 23456.449
    sample_time_ms: 6897.549
    update_time_ms: 38.734
  timestamp: 1602744954
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     75 |          1990.57 | 12134400 |  259.177 |              305.323 |              128.354 |            781.813 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3219.722432241943
    time_step_min: 2950
  date: 2020-10-15_06-56-21
  done: false
  episode_len_mean: 781.6298427311085
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 259.40917227511017
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 15642
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2869689092040062
        entropy_coeff: 0.0005000000000000001
        kl: 0.004641822228829066
        model: {}
        policy_loss: -0.00899923139756235
        total_loss: 3.954155921936035
        vf_explained_var: 0.9898896217346191
        vf_loss: 3.963298499584198
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63225806451613
    gpu_util_percent0: 0.2996774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756071574978705
    mean_env_wait_ms: 1.2184059686952375
    mean_inference_ms: 4.383854622902416
    mean_raw_obs_processing_ms: 0.3836200385598551
  time_since_restore: 2017.1447286605835
  time_this_iter_s: 26.570556163787842
  time_total_s: 2017.1447286605835
  timers:
    learn_throughput: 8284.847
    learn_time_ms: 19528.664
    sample_throughput: 23409.66
    sample_time_ms: 6911.335
    update_time_ms: 38.469
  timestamp: 1602744981
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     76 |          2017.14 | 12296192 |  259.409 |              305.323 |              128.354 |             781.63 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3217.8160374375516
    time_step_min: 2950
  date: 2020-10-15_06-56-48
  done: false
  episode_len_mean: 781.4249116607774
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 259.69880595454805
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 206
  episodes_total: 15848
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.30046174426873523
        entropy_coeff: 0.0005000000000000001
        kl: 0.005141240156566103
        model: {}
        policy_loss: -0.010082557108641291
        total_loss: 4.380263129870097
        vf_explained_var: 0.9905273914337158
        vf_loss: 4.390495856602986
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.88
    gpu_util_percent0: 0.37533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475408472143943
    mean_env_wait_ms: 1.218527909918255
    mean_inference_ms: 4.382471536292015
    mean_raw_obs_processing_ms: 0.3835498836106033
  time_since_restore: 2043.5558156967163
  time_this_iter_s: 26.411087036132812
  time_total_s: 2043.5558156967163
  timers:
    learn_throughput: 8289.1
    learn_time_ms: 19518.644
    sample_throughput: 23449.711
    sample_time_ms: 6899.531
    update_time_ms: 37.413
  timestamp: 1602745008
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     77 |          2043.56 | 12457984 |  259.699 |              305.323 |              128.354 |            781.425 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3215.422457231726
    time_step_min: 2950
  date: 2020-10-15_06-57-15
  done: false
  episode_len_mean: 781.1581626319056
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 260.05554615051824
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 262
  episodes_total: 16110
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.27083553870519
        entropy_coeff: 0.0005000000000000001
        kl: 0.004197654585974912
        model: {}
        policy_loss: -0.0105372149652491
        total_loss: 4.557687163352966
        vf_explained_var: 0.9912907481193542
        vf_loss: 4.568359653155009
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.822580645161295
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14751579075840007
    mean_env_wait_ms: 1.2186749149567295
    mean_inference_ms: 4.380736058377766
    mean_raw_obs_processing_ms: 0.38345283221870713
  time_since_restore: 2069.9670870304108
  time_this_iter_s: 26.411271333694458
  time_total_s: 2069.9670870304108
  timers:
    learn_throughput: 8304.425
    learn_time_ms: 19482.624
    sample_throughput: 23528.026
    sample_time_ms: 6876.565
    update_time_ms: 35.64
  timestamp: 1602745035
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     78 |          2069.97 | 12619776 |  260.056 |              305.323 |              128.354 |            781.158 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3213.8581193423242
    time_step_min: 2950
  date: 2020-10-15_06-57-41
  done: false
  episode_len_mean: 781.0087870222441
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 260.28060995850103
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 164
  episodes_total: 16274
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.27802975724140805
        entropy_coeff: 0.0005000000000000001
        kl: 0.004632183428232868
        model: {}
        policy_loss: -0.009663733314179504
        total_loss: 3.6808159351348877
        vf_explained_var: 0.9904757142066956
        vf_loss: 3.690618614355723
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.777419354838717
    gpu_util_percent0: 0.3258064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749997367883966
    mean_env_wait_ms: 1.2187550044309992
    mean_inference_ms: 4.379660228409253
    mean_raw_obs_processing_ms: 0.3833932996252899
  time_since_restore: 2096.441958665848
  time_this_iter_s: 26.47487163543701
  time_total_s: 2096.441958665848
  timers:
    learn_throughput: 8298.006
    learn_time_ms: 19497.697
    sample_throughput: 23542.881
    sample_time_ms: 6872.226
    update_time_ms: 34.506
  timestamp: 1602745061
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     79 |          2096.44 | 12781568 |  260.281 |              305.323 |              128.354 |            781.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3211.932871214885
    time_step_min: 2950
  date: 2020-10-15_06-58-08
  done: false
  episode_len_mean: 780.8264061646745
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 260.5720312156208
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 207
  episodes_total: 16481
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2814795871575673
        entropy_coeff: 0.0005000000000000001
        kl: 0.005099617992527783
        model: {}
        policy_loss: -0.009346065638965229
        total_loss: 4.120864848295848
        vf_explained_var: 0.9910848140716553
        vf_loss: 4.130351642767589
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.980645161290326
    gpu_util_percent0: 0.297741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748227137400366
    mean_env_wait_ms: 1.2188638859109853
    mean_inference_ms: 4.378362626520621
    mean_raw_obs_processing_ms: 0.38332626639610456
  time_since_restore: 2122.8916313648224
  time_this_iter_s: 26.44967269897461
  time_total_s: 2122.8916313648224
  timers:
    learn_throughput: 8301.333
    learn_time_ms: 19489.883
    sample_throughput: 23532.455
    sample_time_ms: 6875.271
    update_time_ms: 34.775
  timestamp: 1602745088
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     80 |          2122.89 | 12943360 |  260.572 |              305.323 |              128.354 |            780.826 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3209.74371408046
    time_step_min: 2950
  date: 2020-10-15_06-58-35
  done: false
  episode_len_mean: 780.627934763128
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 260.9192456255005
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 258
  episodes_total: 16739
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.25749700516462326
        entropy_coeff: 0.0005000000000000001
        kl: 0.004436231645134588
        model: {}
        policy_loss: -0.007659326685825363
        total_loss: 3.9238884647687278
        vf_explained_var: 0.9924590587615967
        vf_loss: 3.9316764871279397
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.619354838709675
    gpu_util_percent0: 0.30548387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745835473085417
    mean_env_wait_ms: 1.2189896723103488
    mean_inference_ms: 4.3767563766441935
    mean_raw_obs_processing_ms: 0.38323572506566256
  time_since_restore: 2149.4468812942505
  time_this_iter_s: 26.5552499294281
  time_total_s: 2149.4468812942505
  timers:
    learn_throughput: 8305.46
    learn_time_ms: 19480.197
    sample_throughput: 23537.305
    sample_time_ms: 6873.854
    update_time_ms: 33.587
  timestamp: 1602745115
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     81 |          2149.45 | 13105152 |  260.919 |              305.323 |              128.354 |            780.628 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3208.274553968348
    time_step_min: 2950
  date: 2020-10-15_06-59-02
  done: false
  episode_len_mean: 780.4979297290903
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 261.1398152828415
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 16906
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.26211660355329514
        entropy_coeff: 0.0005000000000000001
        kl: 0.004696245305240154
        model: {}
        policy_loss: -0.011184754732918615
        total_loss: 3.641374329725901
        vf_explained_var: 0.9907317161560059
        vf_loss: 3.65269011259079
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.361290322580643
    gpu_util_percent0: 0.30870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474434301963015
    mean_env_wait_ms: 1.219060649076814
    mean_inference_ms: 4.375733444387198
    mean_raw_obs_processing_ms: 0.3831784927089467
  time_since_restore: 2176.0216357707977
  time_this_iter_s: 26.57475447654724
  time_total_s: 2176.0216357707977
  timers:
    learn_throughput: 8300.779
    learn_time_ms: 19491.183
    sample_throughput: 23491.306
    sample_time_ms: 6887.314
    update_time_ms: 33.764
  timestamp: 1602745142
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     82 |          2176.02 | 13266944 |   261.14 |              305.323 |              128.354 |            780.498 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3206.417432535269
    time_step_min: 2950
  date: 2020-10-15_06-59-29
  done: false
  episode_len_mean: 780.3190793316976
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 261.42058864140887
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 17118
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2694035818179448
        entropy_coeff: 0.0005000000000000001
        kl: 0.005037655821070075
        model: {}
        policy_loss: -0.009216082204754153
        total_loss: 4.942888617515564
        vf_explained_var: 0.989602267742157
        vf_loss: 4.952239553133647
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77096774193549
    gpu_util_percent0: 0.2887096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14742655952177627
    mean_env_wait_ms: 1.219157571548136
    mean_inference_ms: 4.374483549081889
    mean_raw_obs_processing_ms: 0.3831128989017988
  time_since_restore: 2202.460642337799
  time_this_iter_s: 26.439006567001343
  time_total_s: 2202.460642337799
  timers:
    learn_throughput: 8308.346
    learn_time_ms: 19473.431
    sample_throughput: 23501.561
    sample_time_ms: 6884.309
    update_time_ms: 32.655
  timestamp: 1602745169
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     83 |          2202.46 | 13428736 |  261.421 |              305.323 |              128.354 |            780.319 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3204.318354868482
    time_step_min: 2950
  date: 2020-10-15_06-59-56
  done: false
  episode_len_mean: 780.1321167463013
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 261.7379203351226
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 253
  episodes_total: 17371
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2436151554187139
        entropy_coeff: 0.0005000000000000001
        kl: 0.004467451595701277
        model: {}
        policy_loss: -0.010677221541603407
        total_loss: 4.291302442550659
        vf_explained_var: 0.9917826056480408
        vf_loss: 4.302101532618205
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39032258064516
    gpu_util_percent0: 0.29129032258064513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740444854396215
    mean_env_wait_ms: 1.2192642422589357
    mean_inference_ms: 4.37302148808822
    mean_raw_obs_processing_ms: 0.3830296521383294
  time_since_restore: 2229.073293209076
  time_this_iter_s: 26.612650871276855
  time_total_s: 2229.073293209076
  timers:
    learn_throughput: 8307.058
    learn_time_ms: 19476.45
    sample_throughput: 23493.282
    sample_time_ms: 6886.735
    update_time_ms: 31.916
  timestamp: 1602745196
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     84 |          2229.07 | 13590528 |  261.738 |              305.323 |              128.354 |            780.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3202.995200822716
    time_step_min: 2950
  date: 2020-10-15_07-00-23
  done: false
  episode_len_mean: 780.0091230470978
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 261.9464176489493
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 17538
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2509919007619222
        entropy_coeff: 0.0005000000000000001
        kl: 0.005146100263421734
        model: {}
        policy_loss: -0.008820091335413357
        total_loss: 3.578421711921692
        vf_explained_var: 0.9908131957054138
        vf_loss: 3.5873672564824424
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5258064516129
    gpu_util_percent0: 0.29096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473906012124796
    mean_env_wait_ms: 1.2193254945978305
    mean_inference_ms: 4.37205563054892
    mean_raw_obs_processing_ms: 0.382974948785774
  time_since_restore: 2255.6990237236023
  time_this_iter_s: 26.625730514526367
  time_total_s: 2255.6990237236023
  timers:
    learn_throughput: 8306.02
    learn_time_ms: 19478.884
    sample_throughput: 23526.872
    sample_time_ms: 6876.902
    update_time_ms: 32.069
  timestamp: 1602745223
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     85 |           2255.7 | 13752320 |  261.946 |              305.323 |              128.354 |            780.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3201.1250070545743
    time_step_min: 2950
  date: 2020-10-15_07-00-50
  done: false
  episode_len_mean: 779.8487101498254
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 262.2342143981211
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 216
  episodes_total: 17754
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.25614098211129505
        entropy_coeff: 0.0005000000000000001
        kl: 0.004949440364725888
        model: {}
        policy_loss: -0.009649161210594078
        total_loss: 3.7816648284594216
        vf_explained_var: 0.9918193817138672
        vf_loss: 3.7914421359697976
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.822580645161295
    gpu_util_percent0: 0.36612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737433955326198
    mean_env_wait_ms: 1.219410055029384
    mean_inference_ms: 4.370857811704321
    mean_raw_obs_processing_ms: 0.3829102123051602
  time_since_restore: 2282.306848526001
  time_this_iter_s: 26.60782480239868
  time_total_s: 2282.306848526001
  timers:
    learn_throughput: 8306.503
    learn_time_ms: 19477.752
    sample_throughput: 23515.082
    sample_time_ms: 6880.35
    update_time_ms: 32.14
  timestamp: 1602745250
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     86 |          2282.31 | 13914112 |  262.234 |              305.323 |              128.354 |            779.849 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3198.94557898837
    time_step_min: 2950
  date: 2020-10-15_07-01-17
  done: false
  episode_len_mean: 779.6709430189936
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 262.55848050649786
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 252
  episodes_total: 18006
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.23144945750633875
        entropy_coeff: 0.0005000000000000001
        kl: 0.004349789000116289
        model: {}
        policy_loss: -0.010432317008962855
        total_loss: 3.5066633025805154
        vf_explained_var: 0.9931366443634033
        vf_loss: 3.517211357752482
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.667741935483868
    gpu_util_percent0: 0.3238709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735369577035076
    mean_env_wait_ms: 1.2195009353510124
    mean_inference_ms: 4.3694863169012885
    mean_raw_obs_processing_ms: 0.38283260508383754
  time_since_restore: 2309.0344319343567
  time_this_iter_s: 26.727583408355713
  time_total_s: 2309.0344319343567
  timers:
    learn_throughput: 8303.28
    learn_time_ms: 19485.311
    sample_throughput: 23439.942
    sample_time_ms: 6902.406
    update_time_ms: 33.9
  timestamp: 1602745277
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     87 |          2309.03 | 14075904 |  262.558 |              305.323 |              128.354 |            779.671 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3197.705723423026
    time_step_min: 2950
  date: 2020-10-15_07-01-44
  done: false
  episode_len_mean: 779.5422376313907
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 262.7615664653802
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 165
  episodes_total: 18171
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.23796684419115385
        entropy_coeff: 0.0005000000000000001
        kl: 0.004746708087623119
        model: {}
        policy_loss: -0.010158667995331902
        total_loss: 3.2070122758547464
        vf_explained_var: 0.9915556907653809
        vf_loss: 3.217290004094442
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.78125
    gpu_util_percent0: 0.34968750000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734073697433492
    mean_env_wait_ms: 1.2195528240728695
    mean_inference_ms: 4.368601046234462
    mean_raw_obs_processing_ms: 0.38278253861969436
  time_since_restore: 2335.766270875931
  time_this_iter_s: 26.731838941574097
  time_total_s: 2335.766270875931
  timers:
    learn_throughput: 8299.298
    learn_time_ms: 19494.66
    sample_throughput: 23374.522
    sample_time_ms: 6921.725
    update_time_ms: 36.516
  timestamp: 1602745304
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     88 |          2335.77 | 14237696 |  262.762 |              305.323 |              128.354 |            779.542 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3195.9664886660853
    time_step_min: 2950
  date: 2020-10-15_07-02-11
  done: false
  episode_len_mean: 779.4080600424213
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 263.02331192492716
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 216
  episodes_total: 18387
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.24343065917491913
        entropy_coeff: 0.0005000000000000001
        kl: 0.004554607129345338
        model: {}
        policy_loss: -0.011246437119552866
        total_loss: 3.7148165504137673
        vf_explained_var: 0.9921318888664246
        vf_loss: 3.726184686024984
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.200000000000003
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732552774094385
    mean_env_wait_ms: 1.2196252329352149
    mean_inference_ms: 4.367466600804559
    mean_raw_obs_processing_ms: 0.3827197109842071
  time_since_restore: 2362.4908821582794
  time_this_iter_s: 26.724611282348633
  time_total_s: 2362.4908821582794
  timers:
    learn_throughput: 8296.008
    learn_time_ms: 19502.392
    sample_throughput: 23314.076
    sample_time_ms: 6939.67
    update_time_ms: 34.6
  timestamp: 1602745331
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     89 |          2362.49 | 14399488 |  263.023 |              305.323 |              128.354 |            779.408 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3194.079243051449
    time_step_min: 2950
  date: 2020-10-15_07-02-38
  done: false
  episode_len_mean: 779.2437754883022
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 263.3326487671304
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 249
  episodes_total: 18636
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.22417493412892023
        entropy_coeff: 0.0005000000000000001
        kl: 0.005401874853608509
        model: {}
        policy_loss: -0.009299968970784297
        total_loss: 3.656243145465851
        vf_explained_var: 0.992896556854248
        vf_loss: 3.665655255317688
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.21935483870968
    gpu_util_percent0: 0.3267741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730665841311105
    mean_env_wait_ms: 1.2197029421519083
    mean_inference_ms: 4.366207758941424
    mean_raw_obs_processing_ms: 0.38264835955381116
  time_since_restore: 2389.3772218227386
  time_this_iter_s: 26.88633966445923
  time_total_s: 2389.3772218227386
  timers:
    learn_throughput: 8290.003
    learn_time_ms: 19516.518
    sample_throughput: 23219.364
    sample_time_ms: 6967.977
    update_time_ms: 34.605
  timestamp: 1602745358
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     90 |          2389.38 | 14561280 |  263.333 |              305.323 |              128.354 |            779.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3192.787126338786
    time_step_min: 2950
  date: 2020-10-15_07-03-05
  done: false
  episode_len_mean: 779.1524305924902
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 263.5286940245987
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 166
  episodes_total: 18802
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.23339983324209848
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044905396255974965
        model: {}
        policy_loss: -0.006208823120687157
        total_loss: 2.6869796911875405
        vf_explained_var: 0.9930952191352844
        vf_loss: 2.6933053135871887
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.254838709677426
    gpu_util_percent0: 0.3483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472943499154305
    mean_env_wait_ms: 1.2197458352373376
    mean_inference_ms: 4.365360765879978
    mean_raw_obs_processing_ms: 0.3826007655345064
  time_since_restore: 2415.675392150879
  time_this_iter_s: 26.29817032814026
  time_total_s: 2415.675392150879
  timers:
    learn_throughput: 8299.472
    learn_time_ms: 19494.252
    sample_throughput: 23260.784
    sample_time_ms: 6955.57
    update_time_ms: 34.023
  timestamp: 1602745385
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     91 |          2415.68 | 14723072 |  263.529 |              305.323 |              128.354 |            779.152 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3191.1694406404717
    time_step_min: 2950
  date: 2020-10-15_07-03-32
  done: false
  episode_len_mean: 779.0157720414279
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 263.7814037541707
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 19021
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.2426673285663128
        entropy_coeff: 0.0005000000000000001
        kl: 0.004914539594513674
        model: {}
        policy_loss: -0.009997313551139086
        total_loss: 3.930317997932434
        vf_explained_var: 0.9916749000549316
        vf_loss: 3.9404366612434387
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.474193548387102
    gpu_util_percent0: 0.39741935483870955
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728001417284467
    mean_env_wait_ms: 1.2198126933443578
    mean_inference_ms: 4.364288423682018
    mean_raw_obs_processing_ms: 0.38254236308067324
  time_since_restore: 2442.2044689655304
  time_this_iter_s: 26.52907681465149
  time_total_s: 2442.2044689655304
  timers:
    learn_throughput: 8297.414
    learn_time_ms: 19499.088
    sample_throughput: 23319.174
    sample_time_ms: 6938.153
    update_time_ms: 33.428
  timestamp: 1602745412
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     92 |           2442.2 | 14884864 |  263.781 |              305.323 |              128.354 |            779.016 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3189.2560049911613
    time_step_min: 2950
  date: 2020-10-15_07-03-59
  done: false
  episode_len_mean: 778.8726970782085
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.06427029126706
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 248
  episodes_total: 19269
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.21617051089803377
        entropy_coeff: 0.0005000000000000001
        kl: 0.00392264958160619
        model: {}
        policy_loss: -0.009051351774057062
        total_loss: 3.663463215033213
        vf_explained_var: 0.9928447604179382
        vf_loss: 3.6726226607958474
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490322580645163
    gpu_util_percent0: 0.3122580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726195004423362
    mean_env_wait_ms: 1.2198733727151092
    mean_inference_ms: 4.363089995265131
    mean_raw_obs_processing_ms: 0.38247434630604443
  time_since_restore: 2468.6577472686768
  time_this_iter_s: 26.453278303146362
  time_total_s: 2468.6577472686768
  timers:
    learn_throughput: 8291.717
    learn_time_ms: 19512.484
    sample_throughput: 23362.317
    sample_time_ms: 6925.34
    update_time_ms: 33.356
  timestamp: 1602745439
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     93 |          2468.66 | 15046656 |  264.064 |              305.323 |              128.354 |            778.873 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3188.015670103093
    time_step_min: 2950
  date: 2020-10-15_07-04-26
  done: false
  episode_len_mean: 778.7855930023154
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.2532087013692
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 166
  episodes_total: 19435
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.22326836735010147
        entropy_coeff: 0.0005000000000000001
        kl: 0.004347114940173924
        model: {}
        policy_loss: -0.008071325634470364
        total_loss: 3.4326427777608237
        vf_explained_var: 0.9912821650505066
        vf_loss: 3.4408257007598877
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.113333333333337
    gpu_util_percent0: 0.2866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14725028920675748
    mean_env_wait_ms: 1.2199113120708402
    mean_inference_ms: 4.3622970530085095
    mean_raw_obs_processing_ms: 0.382429129518466
  time_since_restore: 2494.9694244861603
  time_this_iter_s: 26.31167721748352
  time_total_s: 2494.9694244861603
  timers:
    learn_throughput: 8294.503
    learn_time_ms: 19505.931
    sample_throughput: 23444.621
    sample_time_ms: 6901.029
    update_time_ms: 33.931
  timestamp: 1602745466
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     94 |          2494.97 | 15208448 |  264.253 |              305.323 |              128.354 |            778.786 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3186.414801977675
    time_step_min: 2950
  date: 2020-10-15_07-04-53
  done: false
  episode_len_mean: 778.6659204233235
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.50424567235393
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 19654
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.23257563263177872
        entropy_coeff: 0.0005000000000000001
        kl: 0.00485941837541759
        model: {}
        policy_loss: -0.010026904977470016
        total_loss: 3.708245118459066
        vf_explained_var: 0.9921152591705322
        vf_loss: 3.718388338883718
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.596875
    gpu_util_percent0: 0.31125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14723688211187555
    mean_env_wait_ms: 1.2199670426366245
    mean_inference_ms: 4.361288452174163
    mean_raw_obs_processing_ms: 0.38237314800630007
  time_since_restore: 2521.556752204895
  time_this_iter_s: 26.58732771873474
  time_total_s: 2521.556752204895
  timers:
    learn_throughput: 8291.098
    learn_time_ms: 19513.942
    sample_throughput: 23492.371
    sample_time_ms: 6887.002
    update_time_ms: 36.589
  timestamp: 1602745493
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     95 |          2521.56 | 15370240 |  264.504 |              305.323 |              128.354 |            778.666 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3184.50981576563
    time_step_min: 2950
  date: 2020-10-15_07-05-20
  done: false
  episode_len_mean: 778.5404753529973
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.79224788968014
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 247
  episodes_total: 19901
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.2100596303741137
        entropy_coeff: 0.0005000000000000001
        kl: 0.004346414158741633
        model: {}
        policy_loss: -0.008264342187127719
        total_loss: 3.1842907468477883
        vf_explained_var: 0.9937360286712646
        vf_loss: 3.19266011317571
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77741935483871
    gpu_util_percent0: 0.3441935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721997849327462
    mean_env_wait_ms: 1.220016631563988
    mean_inference_ms: 4.360159050114895
    mean_raw_obs_processing_ms: 0.3823100808817782
  time_since_restore: 2548.274396419525
  time_this_iter_s: 26.717644214630127
  time_total_s: 2548.274396419525
  timers:
    learn_throughput: 8288.7
    learn_time_ms: 19519.588
    sample_throughput: 23476.347
    sample_time_ms: 6891.703
    update_time_ms: 36.592
  timestamp: 1602745520
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     96 |          2548.27 | 15532032 |  264.792 |              305.323 |              128.354 |             778.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3183.2542931309904
    time_step_min: 2950
  date: 2020-10-15_07-05-47
  done: false
  episode_len_mean: 778.4958887726118
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.97838755321186
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 166
  episodes_total: 20067
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.2108885571360588
        entropy_coeff: 0.0005000000000000001
        kl: 0.004517972702160478
        model: {}
        policy_loss: -0.010164422759165367
        total_loss: 2.8915368715922036
        vf_explained_var: 0.9927234053611755
        vf_loss: 2.901806732018789
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.349999999999998
    gpu_util_percent0: 0.375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14720911042270374
    mean_env_wait_ms: 1.220047274515378
    mean_inference_ms: 4.359417856192861
    mean_raw_obs_processing_ms: 0.38226769104169156
  time_since_restore: 2575.2405667304993
  time_this_iter_s: 26.96617031097412
  time_total_s: 2575.2405667304993
  timers:
    learn_throughput: 8282.873
    learn_time_ms: 19533.319
    sample_throughput: 23475.842
    sample_time_ms: 6891.851
    update_time_ms: 42.469
  timestamp: 1602745547
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     97 |          2575.24 | 15693824 |  264.978 |              305.323 |              128.354 |            778.496 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3181.6712112230784
    time_step_min: 2950
  date: 2020-10-15_07-06-14
  done: false
  episode_len_mean: 778.4248237092559
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 265.21741304758217
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 20279
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.22524654741088548
        entropy_coeff: 0.0005000000000000001
        kl: 0.004906056371207039
        model: {}
        policy_loss: -0.009739923446128765
        total_loss: 3.3801024158795676
        vf_explained_var: 0.9928431510925293
        vf_loss: 3.3899550437927246
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.345161290322586
    gpu_util_percent0: 0.3138709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719720338523884
    mean_env_wait_ms: 1.2200904431785455
    mean_inference_ms: 4.35851136824955
    mean_raw_obs_processing_ms: 0.38221801387761406
  time_since_restore: 2601.92964887619
  time_this_iter_s: 26.689082145690918
  time_total_s: 2601.92964887619
  timers:
    learn_throughput: 8279.542
    learn_time_ms: 19541.178
    sample_throughput: 23517.701
    sample_time_ms: 6879.584
    update_time_ms: 41.706
  timestamp: 1602745574
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     98 |          2601.93 | 15855616 |  265.217 |              305.323 |              128.354 |            778.425 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3179.816247865333
    time_step_min: 2950
  date: 2020-10-15_07-06-41
  done: false
  episode_len_mean: 778.3734047735022
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 265.5078844952201
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 251
  episodes_total: 20530
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.19802668442328772
        entropy_coeff: 0.0005000000000000001
        kl: 0.004424594924785197
        model: {}
        policy_loss: -0.010245248946982125
        total_loss: 3.3691912094751992
        vf_explained_var: 0.9935426115989685
        vf_loss: 3.3795355558395386
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.358064516129033
    gpu_util_percent0: 0.3061290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718098250011308
    mean_env_wait_ms: 1.220127510080168
    mean_inference_ms: 4.357433326102603
    mean_raw_obs_processing_ms: 0.38215643205023253
  time_since_restore: 2628.539039373398
  time_this_iter_s: 26.60939049720764
  time_total_s: 2628.539039373398
  timers:
    learn_throughput: 8285.196
    learn_time_ms: 19527.843
    sample_throughput: 23516.455
    sample_time_ms: 6879.948
    update_time_ms: 42.379
  timestamp: 1602745601
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |     99 |          2628.54 | 16017408 |  265.508 |              305.323 |              128.354 |            778.373 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3178.577263708077
    time_step_min: 2950
  date: 2020-10-15_07-07-08
  done: false
  episode_len_mean: 778.3550584597546
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 265.69789156420717
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 20698
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.19598001117507616
        entropy_coeff: 0.0005000000000000001
        kl: 0.004606316331773996
        model: {}
        policy_loss: -0.010173791376776839
        total_loss: 2.12760066986084
        vf_explained_var: 0.994718074798584
        vf_loss: 2.1378723978996277
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.754838709677422
    gpu_util_percent0: 0.3425806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717086497748227
    mean_env_wait_ms: 1.220149264438003
    mean_inference_ms: 4.3567226240077686
    mean_raw_obs_processing_ms: 0.3821154735451887
  time_since_restore: 2655.136007785797
  time_this_iter_s: 26.596968412399292
  time_total_s: 2655.136007785797
  timers:
    learn_throughput: 8291.954
    learn_time_ms: 19511.926
    sample_throughput: 23558.485
    sample_time_ms: 6867.674
    update_time_ms: 40.408
  timestamp: 1602745628
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    100 |          2655.14 | 16179200 |  265.698 |              305.323 |              128.354 |            778.355 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3177.1539383479553
    time_step_min: 2950
  date: 2020-10-15_07-07-35
  done: false
  episode_len_mean: 778.3401454963147
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 265.9129260442078
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 196
  episodes_total: 20894
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.21070735653241476
        entropy_coeff: 0.0005000000000000001
        kl: 0.004244183112556736
        model: {}
        policy_loss: -0.00905375556612853
        total_loss: 2.441717485586802
        vf_explained_var: 0.9947071075439453
        vf_loss: 2.4508766531944275
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63125
    gpu_util_percent0: 0.29875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471603118641464
    mean_env_wait_ms: 1.220176616724261
    mean_inference_ms: 4.3559390876473945
    mean_raw_obs_processing_ms: 0.38207306307491523
  time_since_restore: 2681.874209880829
  time_this_iter_s: 26.73820209503174
  time_total_s: 2681.874209880829
  timers:
    learn_throughput: 8285.6
    learn_time_ms: 19526.889
    sample_throughput: 23434.43
    sample_time_ms: 6904.03
    update_time_ms: 40.632
  timestamp: 1602745655
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    101 |          2681.87 | 16340992 |  265.913 |              305.323 |              128.354 |             778.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3175.3479043334123
    time_step_min: 2950
  date: 2020-10-15_07-08-03
  done: false
  episode_len_mean: 778.3208037825059
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 266.18831578193283
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 256
  episodes_total: 21150
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.19386547058820724
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038890320186813674
        model: {}
        policy_loss: -0.008659424861737838
        total_loss: 2.954138239224752
        vf_explained_var: 0.9946532845497131
        vf_loss: 2.962894638379415
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.696774193548396
    gpu_util_percent0: 0.3548387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471453313633686
    mean_env_wait_ms: 1.2202028860252765
    mean_inference_ms: 4.354896981908789
    mean_raw_obs_processing_ms: 0.38201326482646136
  time_since_restore: 2708.616040945053
  time_this_iter_s: 26.741831064224243
  time_total_s: 2708.616040945053
  timers:
    learn_throughput: 8279.437
    learn_time_ms: 19541.425
    sample_throughput: 23413.529
    sample_time_ms: 6910.193
    update_time_ms: 40.779
  timestamp: 1602745683
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    102 |          2708.62 | 16502784 |  266.188 |              305.323 |              128.354 |            778.321 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3174.0594975346326
    time_step_min: 2950
  date: 2020-10-15_07-08-30
  done: false
  episode_len_mean: 778.315611814346
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 266.3841509326741
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 180
  episodes_total: 21330
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.18441305682063103
        entropy_coeff: 0.0005000000000000001
        kl: 0.003973086772020906
        model: {}
        policy_loss: -0.008704686028067954
        total_loss: 2.045116752386093
        vf_explained_var: 0.9951183199882507
        vf_loss: 2.0539136230945587
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4741935483871
    gpu_util_percent0: 0.32838709677419353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713460150805827
    mean_env_wait_ms: 1.220216841089973
    mean_inference_ms: 4.354185483179388
    mean_raw_obs_processing_ms: 0.38197219693519835
  time_since_restore: 2734.94358754158
  time_this_iter_s: 26.3275465965271
  time_total_s: 2734.94358754158
  timers:
    learn_throughput: 8284.472
    learn_time_ms: 19529.549
    sample_throughput: 23419.433
    sample_time_ms: 6908.451
    update_time_ms: 41.225
  timestamp: 1602745710
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    103 |          2734.94 | 16664576 |  266.384 |              305.323 |              128.354 |            778.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3172.7255924391266
    time_step_min: 2950
  date: 2020-10-15_07-08-57
  done: false
  episode_len_mean: 778.3073812401227
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 266.5851942310527
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 21514
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.19824964553117752
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040229161774429185
        model: {}
        policy_loss: -0.007387646634015255
        total_loss: 2.607377211252848
        vf_explained_var: 0.9942156672477722
        vf_loss: 2.6148639718691506
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01612903225807
    gpu_util_percent0: 0.3754838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712484505716222
    mean_env_wait_ms: 1.220236119621191
    mean_inference_ms: 4.353501432018631
    mean_raw_obs_processing_ms: 0.3819348578166098
  time_since_restore: 2761.5544703006744
  time_this_iter_s: 26.61088275909424
  time_total_s: 2761.5544703006744
  timers:
    learn_throughput: 8278.174
    learn_time_ms: 19544.406
    sample_throughput: 23402.59
    sample_time_ms: 6913.423
    update_time_ms: 41.712
  timestamp: 1602745737
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    104 |          2761.55 | 16826368 |  266.585 |              305.323 |              128.354 |            778.307 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3170.8945988222304
    time_step_min: 2950
  date: 2020-10-15_07-09-24
  done: false
  episode_len_mean: 778.332414680079
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 266.85784954408354
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 257
  episodes_total: 21771
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.191275334606568
        entropy_coeff: 0.0005000000000000001
        kl: 0.004122633875037233
        model: {}
        policy_loss: -0.008579501551139401
        total_loss: 3.101557950178782
        vf_explained_var: 0.994422435760498
        vf_loss: 3.110233167807261
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.448387096774194
    gpu_util_percent0: 0.3225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711117172592542
    mean_env_wait_ms: 1.2202488318400935
    mean_inference_ms: 4.352506667386385
    mean_raw_obs_processing_ms: 0.38187890403182795
  time_since_restore: 2788.182864189148
  time_this_iter_s: 26.62839388847351
  time_total_s: 2788.182864189148
  timers:
    learn_throughput: 8269.791
    learn_time_ms: 19564.218
    sample_throughput: 23457.508
    sample_time_ms: 6897.237
    update_time_ms: 40.343
  timestamp: 1602745764
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    105 |          2788.18 | 16988160 |  266.858 |              305.323 |              128.354 |            778.332 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3169.5796962648787
    time_step_min: 2950
  date: 2020-10-15_07-09-51
  done: false
  episode_len_mean: 778.3297969219561
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.0641525904708
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 21962
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.17079847181836763
        entropy_coeff: 0.0005000000000000001
        kl: 0.00390712955656151
        model: {}
        policy_loss: -0.0103188930467392
        total_loss: 1.982936531305313
        vf_explained_var: 0.9954614043235779
        vf_loss: 1.9933408399422963
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.174193548387098
    gpu_util_percent0: 0.34580645161290324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710041770003557
    mean_env_wait_ms: 1.2202514489960223
    mean_inference_ms: 4.3517790916926975
    mean_raw_obs_processing_ms: 0.38183624929333
  time_since_restore: 2814.721257686615
  time_this_iter_s: 26.53839349746704
  time_total_s: 2814.721257686615
  timers:
    learn_throughput: 8265.492
    learn_time_ms: 19574.395
    sample_throughput: 23554.981
    sample_time_ms: 6868.696
    update_time_ms: 40.296
  timestamp: 1602745791
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    106 |          2814.72 | 17149952 |  267.064 |              305.323 |              128.354 |             778.33 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3168.3918099547514
    time_step_min: 2950
  date: 2020-10-15_07-10-18
  done: false
  episode_len_mean: 778.3448836683984
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.24122864059615
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 173
  episodes_total: 22135
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.18805952370166779
        entropy_coeff: 0.0005000000000000001
        kl: 0.004009904999596377
        model: {}
        policy_loss: -0.008801199272663022
        total_loss: 2.853523035844167
        vf_explained_var: 0.9935711026191711
        vf_loss: 2.8624181946118674
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.648387096774197
    gpu_util_percent0: 0.3067741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709183906960097
    mean_env_wait_ms: 1.2202598688401276
    mean_inference_ms: 4.351172963536076
    mean_raw_obs_processing_ms: 0.3818027377263189
  time_since_restore: 2841.6561913490295
  time_this_iter_s: 26.93493366241455
  time_total_s: 2841.6561913490295
  timers:
    learn_throughput: 8266.526
    learn_time_ms: 19571.947
    sample_throughput: 23531.616
    sample_time_ms: 6875.516
    update_time_ms: 34.549
  timestamp: 1602745818
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    107 |          2841.66 | 17311744 |  267.241 |              305.323 |              128.354 |            778.345 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3166.675227059192
    time_step_min: 2950
  date: 2020-10-15_07-10-45
  done: false
  episode_len_mean: 778.3826051996783
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.506597738305
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 251
  episodes_total: 22386
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.18865067387620607
        entropy_coeff: 0.0005000000000000001
        kl: 0.006582954782061279
        model: {}
        policy_loss: -0.007514678523875773
        total_loss: 2.3213423093159995
        vf_explained_var: 0.995720386505127
        vf_loss: 2.3289512991905212
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.806451612903228
    gpu_util_percent0: 0.30483870967741933
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470795778694045
    mean_env_wait_ms: 1.220260317981489
    mean_inference_ms: 4.3502399328033645
    mean_raw_obs_processing_ms: 0.38175104624490697
  time_since_restore: 2868.047112941742
  time_this_iter_s: 26.390921592712402
  time_total_s: 2868.047112941742
  timers:
    learn_throughput: 8272.907
    learn_time_ms: 19556.851
    sample_throughput: 23582.446
    sample_time_ms: 6860.696
    update_time_ms: 33.661
  timestamp: 1602745845
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    108 |          2868.05 | 17473536 |  267.507 |              305.323 |              128.354 |            778.383 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3165.2506538990115
    time_step_min: 2950
  date: 2020-10-15_07-11-12
  done: false
  episode_len_mean: 778.4289571529745
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.71874374052135
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 206
  episodes_total: 22592
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.16685236866275469
        entropy_coeff: 0.0005000000000000001
        kl: 0.003932366632701208
        model: {}
        policy_loss: -0.011335260981771475
        total_loss: 1.7410776615142822
        vf_explained_var: 0.9963397979736328
        vf_loss: 1.752496321996053
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.358064516129033
    gpu_util_percent0: 0.34129032258064523
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470684340147377
    mean_env_wait_ms: 1.220254587440554
    mean_inference_ms: 4.349504936847386
    mean_raw_obs_processing_ms: 0.3817077090290594
  time_since_restore: 2894.34685754776
  time_this_iter_s: 26.299744606018066
  time_total_s: 2894.34685754776
  timers:
    learn_throughput: 8278.816
    learn_time_ms: 19542.892
    sample_throughput: 23638.606
    sample_time_ms: 6844.397
    update_time_ms: 32.713
  timestamp: 1602745872
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    109 |          2894.35 | 17635328 |  267.719 |              305.323 |              128.354 |            778.429 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3164.0901689843336
    time_step_min: 2950
  date: 2020-10-15_07-11-39
  done: false
  episode_len_mean: 778.4599499099257
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.89233252601593
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 22759
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.17553378393252692
        entropy_coeff: 0.0005000000000000001
        kl: 0.004073347450078775
        model: {}
        policy_loss: -0.0077541398131870665
        total_loss: 1.8975201845169067
        vf_explained_var: 0.9955175518989563
        vf_loss: 1.9053620994091034
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.283870967741937
    gpu_util_percent0: 0.27580645161290324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470601821293187
    mean_env_wait_ms: 1.2202496823188858
    mean_inference_ms: 4.348928732923022
    mean_raw_obs_processing_ms: 0.38167456851967235
  time_since_restore: 2921.028750181198
  time_this_iter_s: 26.68189263343811
  time_total_s: 2921.028750181198
  timers:
    learn_throughput: 8270.549
    learn_time_ms: 19562.427
    sample_throughput: 23688.844
    sample_time_ms: 6829.882
    update_time_ms: 35.001
  timestamp: 1602745899
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    110 |          2921.03 | 17797120 |  267.892 |              305.323 |              128.354 |             778.46 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3162.607293799834
    time_step_min: 2950
  date: 2020-10-15_07-12-06
  done: false
  episode_len_mean: 778.5004350474202
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.1214507381305
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 227
  episodes_total: 22986
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.1821675499280294
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037739647280735276
        model: {}
        policy_loss: -0.010040990697840849
        total_loss: 2.495137890179952
        vf_explained_var: 0.9952752590179443
        vf_loss: 2.5052700638771057
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.621875000000003
    gpu_util_percent0: 0.30843750000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470500799176449
    mean_env_wait_ms: 1.2202387011528524
    mean_inference_ms: 4.348140009411369
    mean_raw_obs_processing_ms: 0.3816300067399178
  time_since_restore: 2947.652496099472
  time_this_iter_s: 26.623745918273926
  time_total_s: 2947.652496099472
  timers:
    learn_throughput: 8265.812
    learn_time_ms: 19573.636
    sample_throughput: 23782.404
    sample_time_ms: 6803.013
    update_time_ms: 36.97
  timestamp: 1602745926
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    111 |          2947.65 | 17958912 |  268.121 |              305.323 |              128.354 |              778.5 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3161.15826251995
    time_step_min: 2950
  date: 2020-10-15_07-12-33
  done: false
  episode_len_mean: 778.5304505125334
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.34338126723344
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 232
  episodes_total: 23218
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.15971247230966887
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036957418778911233
        model: {}
        policy_loss: -0.008545937540475279
        total_loss: 2.6411733428637185
        vf_explained_var: 0.9948610663414001
        vf_loss: 2.6497991482416787
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.909677419354843
    gpu_util_percent0: 0.3390322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703807669561994
    mean_env_wait_ms: 1.2202258471900254
    mean_inference_ms: 4.347354554691665
    mean_raw_obs_processing_ms: 0.3815853688672986
  time_since_restore: 2974.013768196106
  time_this_iter_s: 26.36127209663391
  time_total_s: 2974.013768196106
  timers:
    learn_throughput: 8279.72
    learn_time_ms: 19540.758
    sample_throughput: 23777.86
    sample_time_ms: 6804.313
    update_time_ms: 36.853
  timestamp: 1602745953
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    112 |          2974.01 | 18120704 |  268.343 |              305.323 |              128.354 |             778.53 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3160.030915474865
    time_step_min: 2950
  date: 2020-10-15_07-12-59
  done: false
  episode_len_mean: 778.5465389713113
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.51551903661874
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 171
  episodes_total: 23389
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.16117296740412712
        entropy_coeff: 0.0005000000000000001
        kl: 0.004501701216213405
        model: {}
        policy_loss: -0.007391366428540398
        total_loss: 1.9591073592503865
        vf_explained_var: 0.9953795075416565
        vf_loss: 1.9665793180465698
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666672
    gpu_util_percent0: 0.27666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470298874857145
    mean_env_wait_ms: 1.2202110698507107
    mean_inference_ms: 4.346791913871381
    mean_raw_obs_processing_ms: 0.38155100630100275
  time_since_restore: 3000.1751873493195
  time_this_iter_s: 26.1614191532135
  time_total_s: 3000.1751873493195
  timers:
    learn_throughput: 8286.604
    learn_time_ms: 19524.523
    sample_throughput: 23779.779
    sample_time_ms: 6803.764
    update_time_ms: 36.257
  timestamp: 1602745979
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    113 |          3000.18 | 18282496 |  268.516 |              305.323 |              128.354 |            778.547 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3158.648546573308
    time_step_min: 2950
  date: 2020-10-15_07-13-26
  done: false
  episode_len_mean: 778.5567372881355
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.7241568224619
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 23600
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.17645690714319548
        entropy_coeff: 0.0005000000000000001
        kl: 0.00455080372436593
        model: {}
        policy_loss: -0.010527246360046169
        total_loss: 2.439042647679647
        vf_explained_var: 0.995141327381134
        vf_loss: 2.449658155441284
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44375
    gpu_util_percent0: 0.30656249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702111127934223
    mean_env_wait_ms: 1.2201934114803248
    mean_inference_ms: 4.346094608974112
    mean_raw_obs_processing_ms: 0.3815114960333842
  time_since_restore: 3026.6026694774628
  time_this_iter_s: 26.42748212814331
  time_total_s: 3026.6026694774628
  timers:
    learn_throughput: 8294.268
    learn_time_ms: 19506.484
    sample_throughput: 23747.409
    sample_time_ms: 6813.038
    update_time_ms: 33.947
  timestamp: 1602746006
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    114 |           3026.6 | 18444288 |  268.724 |              305.323 |              128.354 |            778.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3157.0572100642667
    time_step_min: 2950
  date: 2020-10-15_07-13-53
  done: false
  episode_len_mean: 778.5681989765959
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.9621697217117
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 242
  episodes_total: 23842
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.16001935675740242
        entropy_coeff: 0.0005000000000000001
        kl: 0.005934614804573357
        model: {}
        policy_loss: -0.010901586120477683
        total_loss: 1.85432102282842
        vf_explained_var: 0.9965662956237793
        vf_loss: 1.865302582581838
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.796774193548387
    gpu_util_percent0: 0.3212903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700943657891552
    mean_env_wait_ms: 1.2201697203852837
    mean_inference_ms: 4.345324934273064
    mean_raw_obs_processing_ms: 0.3814666908792615
  time_since_restore: 3052.9867749214172
  time_this_iter_s: 26.384105443954468
  time_total_s: 3052.9867749214172
  timers:
    learn_throughput: 8310.239
    learn_time_ms: 19468.995
    sample_throughput: 23726.13
    sample_time_ms: 6819.148
    update_time_ms: 39.811
  timestamp: 1602746033
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    115 |          3052.99 | 18606080 |  268.962 |              305.323 |              128.354 |            778.568 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3155.9518825834966
    time_step_min: 2950
  date: 2020-10-15_07-14-21
  done: false
  episode_len_mean: 778.5784411691232
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.13100780475247
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 176
  episodes_total: 24018
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.15443184599280357
        entropy_coeff: 0.0005000000000000001
        kl: 0.00397810017845283
        model: {}
        policy_loss: -0.007802540688620259
        total_loss: 1.61911674340566
        vf_explained_var: 0.9962174892425537
        vf_loss: 1.6269965370496113
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.774193548387096
    gpu_util_percent0: 0.2951612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470013336795748
    mean_env_wait_ms: 1.2201467590088135
    mean_inference_ms: 4.344763771988982
    mean_raw_obs_processing_ms: 0.38143379100325914
  time_since_restore: 3079.732397556305
  time_this_iter_s: 26.745622634887695
  time_total_s: 3079.732397556305
  timers:
    learn_throughput: 8307.141
    learn_time_ms: 19476.255
    sample_throughput: 23683.647
    sample_time_ms: 6831.38
    update_time_ms: 39.901
  timestamp: 1602746061
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    116 |          3079.73 | 18767872 |  269.131 |              305.323 |              128.354 |            778.578 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3154.771683831741
    time_step_min: 2950
  date: 2020-10-15_07-14-48
  done: false
  episode_len_mean: 778.5959854617545
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.3097103531599
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 194
  episodes_total: 24212
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.1701481839021047
        entropy_coeff: 0.0005000000000000001
        kl: 0.00412475304134811
        model: {}
        policy_loss: -0.009199916142582273
        total_loss: 1.8399411340554555
        vf_explained_var: 0.9962038993835449
        vf_loss: 1.8492261171340942
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3875
    gpu_util_percent0: 0.3296875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699277783801615
    mean_env_wait_ms: 1.2201229183633064
    mean_inference_ms: 4.344160600177373
    mean_raw_obs_processing_ms: 0.3814000486764216
  time_since_restore: 3106.5999326705933
  time_this_iter_s: 26.86753511428833
  time_total_s: 3106.5999326705933
  timers:
    learn_throughput: 8303.537
    learn_time_ms: 19484.708
    sample_throughput: 23740.56
    sample_time_ms: 6815.004
    update_time_ms: 40.502
  timestamp: 1602746088
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    117 |           3106.6 | 18929664 |   269.31 |              305.323 |              128.354 |            778.596 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3153.1925170903432
    time_step_min: 2950
  date: 2020-10-15_07-15-15
  done: false
  episode_len_mean: 778.6361183780249
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.5497581273824
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 252
  episodes_total: 24464
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.156033077587684
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037198655190877616
        model: {}
        policy_loss: -0.009029158842774146
        total_loss: 1.5604484577973683
        vf_explained_var: 0.9971835613250732
        vf_loss: 1.569555660088857
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43870967741936
    gpu_util_percent0: 0.3680645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698255183677497
    mean_env_wait_ms: 1.2200911987929124
    mean_inference_ms: 4.343394102682492
    mean_raw_obs_processing_ms: 0.3813550271255343
  time_since_restore: 3133.093611717224
  time_this_iter_s: 26.49367904663086
  time_total_s: 3133.093611717224
  timers:
    learn_throughput: 8298.083
    learn_time_ms: 19497.515
    sample_throughput: 23782.926
    sample_time_ms: 6802.863
    update_time_ms: 48.146
  timestamp: 1602746115
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    118 |          3133.09 | 19091456 |   269.55 |              305.323 |              128.354 |            778.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3152.048632836306
    time_step_min: 2950
  date: 2020-10-15_07-15-42
  done: false
  episode_len_mean: 778.6690603700098
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.7223783600366
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 24648
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.13847396398584047
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038921852440883717
        model: {}
        policy_loss: -0.006751392104585345
        total_loss: 1.3265824615955353
        vf_explained_var: 0.9970529079437256
        vf_loss: 1.3334031005700429
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.480645161290326
    gpu_util_percent0: 0.2793548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697403437323192
    mean_env_wait_ms: 1.2200598524286324
    mean_inference_ms: 4.342831172384234
    mean_raw_obs_processing_ms: 0.3813220258616258
  time_since_restore: 3160.039802312851
  time_this_iter_s: 26.94619059562683
  time_total_s: 3160.039802312851
  timers:
    learn_throughput: 8269.961
    learn_time_ms: 19563.817
    sample_throughput: 23802.48
    sample_time_ms: 6797.275
    update_time_ms: 50.251
  timestamp: 1602746142
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    119 |          3160.04 | 19253248 |  269.722 |              305.323 |              128.354 |            778.669 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3150.913443310612
    time_step_min: 2950
  date: 2020-10-15_07-16-10
  done: false
  episode_len_mean: 778.7131464475592
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.8922123604337
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 180
  episodes_total: 24828
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.15610838681459427
        entropy_coeff: 0.0005000000000000001
        kl: 0.003907249813588957
        model: {}
        policy_loss: -0.008552311808064891
        total_loss: 1.3009407420953114
        vf_explained_var: 0.9971628785133362
        vf_loss: 1.309571127096812
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.196875
    gpu_util_percent0: 0.36343749999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696663447304928
    mean_env_wait_ms: 1.2200313608327706
    mean_inference_ms: 4.342307176203268
    mean_raw_obs_processing_ms: 0.38129282070408416
  time_since_restore: 3186.7160346508026
  time_this_iter_s: 26.67623233795166
  time_total_s: 3186.7160346508026
  timers:
    learn_throughput: 8269.806
    learn_time_ms: 19564.183
    sample_throughput: 23832.478
    sample_time_ms: 6788.719
    update_time_ms: 48.513
  timestamp: 1602746170
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    120 |          3186.72 | 19415040 |  269.892 |              305.323 |              128.354 |            778.713 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3149.3702431817273
    time_step_min: 2950
  date: 2020-10-15_07-16-37
  done: false
  episode_len_mean: 778.7751814339262
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.1260294144894
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 250
  episodes_total: 25078
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.15299376845359802
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039689391463374095
        model: {}
        policy_loss: -0.008984444139059633
        total_loss: 1.3363647560278575
        vf_explained_var: 0.9976133704185486
        vf_loss: 1.3454256852467854
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.638709677419357
    gpu_util_percent0: 0.25709677419354837
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695727986752727
    mean_env_wait_ms: 1.2199920851069828
    mean_inference_ms: 4.341573983947807
    mean_raw_obs_processing_ms: 0.381249845469126
  time_since_restore: 3213.2858316898346
  time_this_iter_s: 26.569797039031982
  time_total_s: 3213.2858316898346
  timers:
    learn_throughput: 8269.371
    learn_time_ms: 19565.213
    sample_throughput: 23855.04
    sample_time_ms: 6782.298
    update_time_ms: 48.49
  timestamp: 1602746197
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    121 |          3213.29 | 19576832 |  270.126 |              305.323 |              128.354 |            778.775 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3148.1567959434296
    time_step_min: 2950
  date: 2020-10-15_07-17-04
  done: false
  episode_len_mean: 778.8494342906876
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.31145420499803
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 200
  episodes_total: 25278
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.13192211215694746
        entropy_coeff: 0.0005000000000000001
        kl: 0.003562251280527562
        model: {}
        policy_loss: -0.006472938703761126
        total_loss: 1.2065467437108357
        vf_explained_var: 0.9975103735923767
        vf_loss: 1.2130856414635975
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.945161290322584
    gpu_util_percent0: 0.3483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694812805954457
    mean_env_wait_ms: 1.2199484307941937
    mean_inference_ms: 4.340994683244947
    mean_raw_obs_processing_ms: 0.3812148917271656
  time_since_restore: 3239.849322795868
  time_this_iter_s: 26.563491106033325
  time_total_s: 3239.849322795868
  timers:
    learn_throughput: 8266.719
    learn_time_ms: 19571.489
    sample_throughput: 23807.772
    sample_time_ms: 6795.764
    update_time_ms: 48.609
  timestamp: 1602746224
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    122 |          3239.85 | 19738624 |  270.311 |              305.323 |              128.354 |            778.849 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3147.1301011373025
    time_step_min: 2950
  date: 2020-10-15_07-17-31
  done: false
  episode_len_mean: 778.9041106657235
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.4646190744988
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 25446
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.13830991089344025
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043364263062054915
        model: {}
        policy_loss: -0.008275023118282357
        total_loss: 1.0009029010931652
        vf_explained_var: 0.9977321028709412
        vf_loss: 1.0092470596234004
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.234375
    gpu_util_percent0: 0.3340625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694157242471842
    mean_env_wait_ms: 1.2199152862281921
    mean_inference_ms: 4.340527888828526
    mean_raw_obs_processing_ms: 0.3811879556812206
  time_since_restore: 3266.4504442214966
  time_this_iter_s: 26.601121425628662
  time_total_s: 3266.4504442214966
  timers:
    learn_throughput: 8252.28
    learn_time_ms: 19605.733
    sample_throughput: 23789.034
    sample_time_ms: 6801.117
    update_time_ms: 49.169
  timestamp: 1602746251
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    123 |          3266.45 | 19900416 |  270.465 |              305.323 |              128.354 |            778.904 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3145.8145041741436
    time_step_min: 2950
  date: 2020-10-15_07-17-58
  done: false
  episode_len_mean: 779.0039347072344
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.6624635855615
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 223
  episodes_total: 25669
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.15040000155568123
        entropy_coeff: 0.0005000000000000001
        kl: 0.004205987846944481
        model: {}
        policy_loss: -0.0084898002751288
        total_loss: 1.413439432779948
        vf_explained_var: 0.9973837733268738
        vf_loss: 1.4220044513543446
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.383870967741938
    gpu_util_percent0: 0.25451612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469335406222289
    mean_env_wait_ms: 1.2198676757370295
    mean_inference_ms: 4.339891941888979
    mean_raw_obs_processing_ms: 0.38115207512342425
  time_since_restore: 3293.217169046402
  time_this_iter_s: 26.766724824905396
  time_total_s: 3293.217169046402
  timers:
    learn_throughput: 8233.979
    learn_time_ms: 19649.309
    sample_throughput: 23831.989
    sample_time_ms: 6788.859
    update_time_ms: 51.039
  timestamp: 1602746278
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    124 |          3293.22 | 20062208 |  270.662 |              305.323 |              128.354 |            779.004 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3144.483414520993
    time_step_min: 2950
  date: 2020-10-15_07-18-25
  done: false
  episode_len_mean: 779.0900737423266
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.8570481464192
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 232
  episodes_total: 25901
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.13230709110697111
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037244013510644436
        model: {}
        policy_loss: -0.007667488719259079
        total_loss: 1.4335866769154866
        vf_explained_var: 0.9973209500312805
        vf_loss: 1.4413203001022339
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74193548387097
    gpu_util_percent0: 0.3216129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692382005653842
    mean_env_wait_ms: 1.219814596391552
    mean_inference_ms: 4.3392668969452615
    mean_raw_obs_processing_ms: 0.3811144478853987
  time_since_restore: 3319.857436656952
  time_this_iter_s: 26.640267610549927
  time_total_s: 3319.857436656952
  timers:
    learn_throughput: 8223.991
    learn_time_ms: 19673.174
    sample_throughput: 23810.645
    sample_time_ms: 6794.944
    update_time_ms: 45.448
  timestamp: 1602746305
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    125 |          3319.86 | 20224000 |  270.857 |              305.323 |              128.354 |             779.09 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3143.5189338658884
    time_step_min: 2950
  date: 2020-10-15_07-18-52
  done: false
  episode_len_mean: 779.1518045487669
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.0074185648918
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 172
  episodes_total: 26073
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.12917267034451166
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038988196562665203
        model: {}
        policy_loss: -0.006992035099149992
        total_loss: 1.0980620334545772
        vf_explained_var: 0.9975457787513733
        vf_loss: 1.1051186919212341
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.6
    gpu_util_percent0: 0.28
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691692664879294
    mean_env_wait_ms: 1.219771046713104
    mean_inference_ms: 4.3387977167755185
    mean_raw_obs_processing_ms: 0.3810861846314219
  time_since_restore: 3346.5418524742126
  time_this_iter_s: 26.684415817260742
  time_total_s: 3346.5418524742126
  timers:
    learn_throughput: 8223.046
    learn_time_ms: 19675.435
    sample_throughput: 23838.488
    sample_time_ms: 6787.008
    update_time_ms: 43.858
  timestamp: 1602746332
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    126 |          3346.54 | 20385792 |  271.007 |              305.323 |              128.354 |            779.152 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3142.3872393947477
    time_step_min: 2950
  date: 2020-10-15_07-19-20
  done: false
  episode_len_mean: 779.214486906212
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.17589721822367
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 26272
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.1453334242105484
        entropy_coeff: 0.0005000000000000001
        kl: 0.004310628632083535
        model: {}
        policy_loss: -0.009597847509818772
        total_loss: 1.1674810647964478
        vf_explained_var: 0.9976624846458435
        vf_loss: 1.1771516005198162
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.529032258064518
    gpu_util_percent0: 0.26516129032258057
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469103269823275
    mean_env_wait_ms: 1.2197260415115259
    mean_inference_ms: 4.338275395765625
    mean_raw_obs_processing_ms: 0.3810571107446247
  time_since_restore: 3373.530199289322
  time_this_iter_s: 26.988346815109253
  time_total_s: 3373.530199289322
  timers:
    learn_throughput: 8221.954
    learn_time_ms: 19678.048
    sample_throughput: 23806.04
    sample_time_ms: 6796.258
    update_time_ms: 43.251
  timestamp: 1602746360
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    127 |          3373.53 | 20547584 |  271.176 |              305.323 |              128.354 |            779.214 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3141.077867150032
    time_step_min: 2950
  date: 2020-10-15_07-19-47
  done: false
  episode_len_mean: 779.2942374415447
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.373592997405
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 244
  episodes_total: 26516
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.13515925034880638
        entropy_coeff: 0.0005000000000000001
        kl: 0.003592020172315339
        model: {}
        policy_loss: -0.006902218621689826
        total_loss: 1.8067791163921356
        vf_explained_var: 0.9968041777610779
        vf_loss: 1.8137489358584087
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44516129032258
    gpu_util_percent0: 0.2680645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690117349974804
    mean_env_wait_ms: 1.2196627064587078
    mean_inference_ms: 4.337647924007461
    mean_raw_obs_processing_ms: 0.3810187198039464
  time_since_restore: 3399.9870438575745
  time_this_iter_s: 26.456844568252563
  time_total_s: 3399.9870438575745
  timers:
    learn_throughput: 8222.534
    learn_time_ms: 19676.659
    sample_throughput: 23788.461
    sample_time_ms: 6801.281
    update_time_ms: 34.911
  timestamp: 1602746387
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    128 |          3399.99 | 20709376 |  271.374 |              305.323 |              128.354 |            779.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3140.0637468126592
    time_step_min: 2950
  date: 2020-10-15_07-20-14
  done: false
  episode_len_mean: 779.3489495562296
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.52590618010237
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 26703
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.12362479977309704
        entropy_coeff: 0.0005000000000000001
        kl: 0.004788789587716262
        model: {}
        policy_loss: -0.01017092047065186
        total_loss: 1.0793224076430004
        vf_explained_var: 0.9976506233215332
        vf_loss: 1.0895551145076752
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.509375
    gpu_util_percent0: 0.2990625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468935087643719
    mean_env_wait_ms: 1.2196064873519978
    mean_inference_ms: 4.337151580177423
    mean_raw_obs_processing_ms: 0.3809883691937055
  time_since_restore: 3426.511312484741
  time_this_iter_s: 26.524268627166748
  time_total_s: 3426.511312484741
  timers:
    learn_throughput: 8239.246
    learn_time_ms: 19636.749
    sample_throughput: 23797.484
    sample_time_ms: 6798.702
    update_time_ms: 35.032
  timestamp: 1602746414
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    129 |          3426.51 | 20871168 |  271.526 |              305.323 |              128.354 |            779.349 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3139.122322792118
    time_step_min: 2950
  date: 2020-10-15_07-20-42
  done: false
  episode_len_mean: 779.4044341938844
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.66934541456527
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 26882
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.13681315382321677
        entropy_coeff: 0.0005000000000000001
        kl: 0.004381068516522646
        model: {}
        policy_loss: -0.008273542117725205
        total_loss: 1.340931475162506
        vf_explained_var: 0.9971432685852051
        vf_loss: 1.3492734134197235
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.325806451612912
    gpu_util_percent0: 0.32806451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688746493449392
    mean_env_wait_ms: 1.2195588802442079
    mean_inference_ms: 4.336714911314017
    mean_raw_obs_processing_ms: 0.38096394421722707
  time_since_restore: 3453.322854757309
  time_this_iter_s: 26.81154227256775
  time_total_s: 3453.322854757309
  timers:
    learn_throughput: 8230.185
    learn_time_ms: 19658.368
    sample_throughput: 23804.143
    sample_time_ms: 6796.8
    update_time_ms: 36.964
  timestamp: 1602746442
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    130 |          3453.32 | 21032960 |  271.669 |              305.323 |              128.354 |            779.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3137.8412018751615
    time_step_min: 2950
  date: 2020-10-15_07-21-09
  done: false
  episode_len_mean: 779.4864336798644
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.86411672576236
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 244
  episodes_total: 27126
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.13891911258300146
        entropy_coeff: 0.0005000000000000001
        kl: 0.004638209706172347
        model: {}
        policy_loss: -0.010382649403860947
        total_loss: 1.242403397957484
        vf_explained_var: 0.9978165626525879
        vf_loss: 1.2528554797172546
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203225806451613
    gpu_util_percent0: 0.36709677419354836
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687917354331165
    mean_env_wait_ms: 1.2194906130240029
    mean_inference_ms: 4.336104579152607
    mean_raw_obs_processing_ms: 0.3809266247012939
  time_since_restore: 3480.0322875976562
  time_this_iter_s: 26.70943284034729
  time_total_s: 3480.0322875976562
  timers:
    learn_throughput: 8236.32
    learn_time_ms: 19643.723
    sample_throughput: 23701.195
    sample_time_ms: 6826.322
    update_time_ms: 35.078
  timestamp: 1602746469
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    131 |          3480.03 | 21194752 |  271.864 |              305.323 |              128.354 |            779.486 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3136.780297479484
    time_step_min: 2950
  date: 2020-10-15_07-21-36
  done: false
  episode_len_mean: 779.5484248655373
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.0277514451529
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 205
  episodes_total: 27331
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.11882328987121582
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033434471115469933
        model: {}
        policy_loss: -0.007724979077465832
        total_loss: 0.9796513219674429
        vf_explained_var: 0.9980270266532898
        vf_loss: 0.9874357034762701
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93870967741936
    gpu_util_percent0: 0.2512903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687094925916522
    mean_env_wait_ms: 1.2194227416269348
    mean_inference_ms: 4.3355801258987645
    mean_raw_obs_processing_ms: 0.38089424893568974
  time_since_restore: 3506.574964761734
  time_this_iter_s: 26.54267716407776
  time_total_s: 3506.574964761734
  timers:
    learn_throughput: 8231.169
    learn_time_ms: 19656.018
    sample_throughput: 23752.237
    sample_time_ms: 6811.653
    update_time_ms: 33.707
  timestamp: 1602746496
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    132 |          3506.57 | 21356544 |  272.028 |              305.323 |              128.354 |            779.548 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3135.9012925541597
    time_step_min: 2950
  date: 2020-10-15_07-22-03
  done: false
  episode_len_mean: 779.5916
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.1607199265381
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 169
  episodes_total: 27500
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.12584660512705645
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036025882000103593
        model: {}
        policy_loss: -0.0094398159320311
        total_loss: 0.8635007490714391
        vf_explained_var: 0.9980620741844177
        vf_loss: 0.8730034927527109
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36774193548387
    gpu_util_percent0: 0.3390322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686514089431013
    mean_env_wait_ms: 1.219371439227161
    mean_inference_ms: 4.3351844911466975
    mean_raw_obs_processing_ms: 0.38087015448219397
  time_since_restore: 3533.115668296814
  time_this_iter_s: 26.540703535079956
  time_total_s: 3533.115668296814
  timers:
    learn_throughput: 8235.155
    learn_time_ms: 19646.504
    sample_throughput: 23737.548
    sample_time_ms: 6815.868
    update_time_ms: 34.003
  timestamp: 1602746523
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    133 |          3533.12 | 21518336 |  272.161 |              305.323 |              128.354 |            779.592 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3134.736258577104
    time_step_min: 2950
  date: 2020-10-15_07-22-30
  done: false
  episode_len_mean: 779.646420198377
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.3386598172892
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 225
  episodes_total: 27725
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.13572877024610838
        entropy_coeff: 0.0005000000000000001
        kl: 0.004482988268136978
        model: {}
        policy_loss: -0.008557878199402088
        total_loss: 0.9369617253541946
        vf_explained_var: 0.9982648491859436
        vf_loss: 0.9455874810616175
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.225
    gpu_util_percent0: 0.32218749999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468577352356403
    mean_env_wait_ms: 1.219300386715835
    mean_inference_ms: 4.334628104260294
    mean_raw_obs_processing_ms: 0.38083789668557066
  time_since_restore: 3559.828266143799
  time_this_iter_s: 26.712597846984863
  time_total_s: 3559.828266143799
  timers:
    learn_throughput: 8240.63
    learn_time_ms: 19633.449
    sample_throughput: 23740.246
    sample_time_ms: 6815.094
    update_time_ms: 32.801
  timestamp: 1602746550
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    134 |          3559.83 | 21680128 |  272.339 |              305.323 |              128.354 |            779.646 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3133.533039647577
    time_step_min: 2950
  date: 2020-10-15_07-22-57
  done: false
  episode_len_mean: 779.701745600229
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.51982805592047
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 231
  episodes_total: 27956
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.12009631469845772
        entropy_coeff: 0.0005000000000000001
        kl: 0.00511722903077801
        model: {}
        policy_loss: -0.008499198534991592
        total_loss: 0.8839535117149353
        vf_explained_var: 0.9983601570129395
        vf_loss: 0.8925127486387888
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29677419354839
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684981707720943
    mean_env_wait_ms: 1.2192271134715642
    mean_inference_ms: 4.334104707907359
    mean_raw_obs_processing_ms: 0.3808050112880006
  time_since_restore: 3586.4673988819122
  time_this_iter_s: 26.639132738113403
  time_total_s: 3586.4673988819122
  timers:
    learn_throughput: 8236.412
    learn_time_ms: 19643.506
    sample_throughput: 23771.931
    sample_time_ms: 6806.01
    update_time_ms: 31.247
  timestamp: 1602746577
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    135 |          3586.47 | 21841920 |   272.52 |              305.323 |              128.354 |            779.702 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3132.6958812431026
    time_step_min: 2950
  date: 2020-10-15_07-23-24
  done: false
  episode_len_mean: 779.7487022683638
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.64552551038366
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 170
  episodes_total: 28126
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.11492656357586384
        entropy_coeff: 0.0005000000000000001
        kl: 0.003911605939113845
        model: {}
        policy_loss: -0.007973914422715703
        total_loss: 1.179173767566681
        vf_explained_var: 0.9974175095558167
        vf_loss: 1.1872051457564037
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.76451612903226
    gpu_util_percent0: 0.3458064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684397090888737
    mean_env_wait_ms: 1.2191677505906633
    mean_inference_ms: 4.33371558089279
    mean_raw_obs_processing_ms: 0.38078084903454745
  time_since_restore: 3612.985851287842
  time_this_iter_s: 26.518452405929565
  time_total_s: 3612.985851287842
  timers:
    learn_throughput: 8251.211
    learn_time_ms: 19608.273
    sample_throughput: 23732.76
    sample_time_ms: 6817.243
    update_time_ms: 33.278
  timestamp: 1602746604
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    136 |          3612.99 | 22003712 |  272.646 |              305.323 |              128.354 |            779.749 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3131.6837975399408
    time_step_min: 2950
  date: 2020-10-15_07-23-52
  done: false
  episode_len_mean: 779.8297384121156
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.799817998533
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 201
  episodes_total: 28327
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.1291513635466496
        entropy_coeff: 0.0005000000000000001
        kl: 0.00461435845742623
        model: {}
        policy_loss: -0.009190612530801445
        total_loss: 0.7973249008258184
        vf_explained_var: 0.9984477162361145
        vf_loss: 0.8065801113843918
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.153125000000003
    gpu_util_percent0: 0.31124999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468379868015758
    mean_env_wait_ms: 1.2191024640738262
    mean_inference_ms: 4.33325928928743
    mean_raw_obs_processing_ms: 0.38075480833211867
  time_since_restore: 3639.6503229141235
  time_this_iter_s: 26.66447162628174
  time_total_s: 3639.6503229141235
  timers:
    learn_throughput: 8258.292
    learn_time_ms: 19591.461
    sample_throughput: 23794.325
    sample_time_ms: 6799.605
    update_time_ms: 33.282
  timestamp: 1602746632
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    137 |          3639.65 | 22165504 |    272.8 |              305.323 |              128.354 |             779.83 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3130.490659937616
    time_step_min: 2950
  date: 2020-10-15_07-24-19
  done: false
  episode_len_mean: 779.9121044525343
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.98024737715997
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 241
  episodes_total: 28568
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.1212281882762909
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037809794109004238
        model: {}
        policy_loss: -0.008161952447456619
        total_loss: 0.944248616695404
        vf_explained_var: 0.9984046816825867
        vf_loss: 0.9524711966514587
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.025806451612908
    gpu_util_percent0: 0.31387096774193557
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683031159980836
    mean_env_wait_ms: 1.219018882667448
    mean_inference_ms: 4.332729070161373
    mean_raw_obs_processing_ms: 0.3807215435874042
  time_since_restore: 3666.4217097759247
  time_this_iter_s: 26.771386861801147
  time_total_s: 3666.4217097759247
  timers:
    learn_throughput: 8249.272
    learn_time_ms: 19612.883
    sample_throughput: 23762.538
    sample_time_ms: 6808.7
    update_time_ms: 33.815
  timestamp: 1602746659
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    138 |          3666.42 | 22327296 |   272.98 |              305.323 |              128.354 |            779.912 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3129.5729406030223
    time_step_min: 2950
  date: 2020-10-15_07-24-46
  done: false
  episode_len_mean: 779.9720415898738
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 273.11859914301056
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 189
  episodes_total: 28757
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.388131789017202e-22
        cur_lr: 5.0e-05
        entropy: 0.10731145739555359
        entropy_coeff: 0.0005000000000000001
        kl: 0.003778786980547011
        model: {}
        policy_loss: -0.008710089595600342
        total_loss: 0.731962040066719
        vf_explained_var: 0.9984782338142395
        vf_loss: 0.7407257854938507
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.190624999999997
    gpu_util_percent0: 0.3228125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682348606400347
    mean_env_wait_ms: 1.2189460499583746
    mean_inference_ms: 4.332304719018422
    mean_raw_obs_processing_ms: 0.3806945260640634
  time_since_restore: 3693.1063144207
  time_this_iter_s: 26.68460464477539
  time_total_s: 3693.1063144207
  timers:
    learn_throughput: 8244.61
    learn_time_ms: 19623.973
    sample_throughput: 23751.953
    sample_time_ms: 6811.734
    update_time_ms: 34.067
  timestamp: 1602746686
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    139 |          3693.11 | 22489088 |  273.119 |              305.323 |              128.354 |            779.972 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3128.7116362755614
    time_step_min: 2950
  date: 2020-10-15_07-25-13
  done: false
  episode_len_mean: 780.0298244401438
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 273.250685595239
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 28936
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.694065894508601e-22
        cur_lr: 5.0e-05
        entropy: 0.11463576244811217
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034035745775327086
        model: {}
        policy_loss: -0.007327661733143032
        total_loss: 0.7629700700441996
        vf_explained_var: 0.9984410405158997
        vf_loss: 0.7703550557295481
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.393548387096775
    gpu_util_percent0: 0.2935483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681826081232477
    mean_env_wait_ms: 1.2188818819506806
    mean_inference_ms: 4.331925298501139
    mean_raw_obs_processing_ms: 0.38067254231108977
  time_since_restore: 3719.8605947494507
  time_this_iter_s: 26.75428032875061
  time_total_s: 3719.8605947494507
  timers:
    learn_throughput: 8255.557
    learn_time_ms: 19597.95
    sample_throughput: 23679.57
    sample_time_ms: 6832.557
    update_time_ms: 31.908
  timestamp: 1602746713
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    140 |          3719.86 | 22650880 |  273.251 |              305.323 |              128.354 |             780.03 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3127.5655217376384
    time_step_min: 2950
  date: 2020-10-15_07-25-41
  done: false
  episode_len_mean: 780.1098773048187
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 273.42166472456415
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 242
  episodes_total: 29178
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.470329472543005e-23
        cur_lr: 5.0e-05
        entropy: 0.12350944057106972
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038897688888634243
        model: {}
        policy_loss: -0.008290650245423118
        total_loss: 1.3516164223353069
        vf_explained_var: 0.9977133870124817
        vf_loss: 1.3599688311417897
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.115625
    gpu_util_percent0: 0.2884375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468108025424126
    mean_env_wait_ms: 1.218793619940974
    mean_inference_ms: 4.3314101995124386
    mean_raw_obs_processing_ms: 0.3806413726274456
  time_since_restore: 3746.7048144340515
  time_this_iter_s: 26.84421968460083
  time_total_s: 3746.7048144340515
  timers:
    learn_throughput: 8256.857
    learn_time_ms: 19594.865
    sample_throughput: 23658.186
    sample_time_ms: 6838.732
    update_time_ms: 33.466
  timestamp: 1602746741
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    141 |           3746.7 | 22812672 |  273.422 |              305.323 |              128.354 |             780.11 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3126.6009268093226
    time_step_min: 2950
  date: 2020-10-15_07-26-08
  done: false
  episode_len_mean: 780.1824184052003
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 273.5649645555373
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 205
  episodes_total: 29383
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2351647362715025e-23
        cur_lr: 5.0e-05
        entropy: 0.1076820157468319
        entropy_coeff: 0.0005000000000000001
        kl: 0.004129853623453528
        model: {}
        policy_loss: -0.006553034939770441
        total_loss: 0.7742396344741186
        vf_explained_var: 0.9984902739524841
        vf_loss: 0.7808465162913004
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.910000000000004
    gpu_util_percent0: 0.24699999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680393582674225
    mean_env_wait_ms: 1.218710773248947
    mean_inference_ms: 4.330962691972558
    mean_raw_obs_processing_ms: 0.3806129562645218
  time_since_restore: 3772.8944425582886
  time_this_iter_s: 26.18962812423706
  time_total_s: 3772.8944425582886
  timers:
    learn_throughput: 8273.796
    learn_time_ms: 19554.748
    sample_throughput: 23652.89
    sample_time_ms: 6840.264
    update_time_ms: 34.831
  timestamp: 1602746768
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    142 |          3772.89 | 22974464 |  273.565 |              305.323 |              128.354 |            780.182 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3125.8185421041935
    time_step_min: 2950
  date: 2020-10-15_07-26-34
  done: false
  episode_len_mean: 780.23639070271
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 273.68533902820195
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 29557
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1175823681357513e-23
        cur_lr: 5.0e-05
        entropy: 0.10905477466682593
        entropy_coeff: 0.0005000000000000001
        kl: 0.003966478417472293
        model: {}
        policy_loss: -0.009192176742847854
        total_loss: 0.8003581861654917
        vf_explained_var: 0.9983252882957458
        vf_loss: 0.8096048583587011
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65161290322581
    gpu_util_percent0: 0.2664516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679878306256158
    mean_env_wait_ms: 1.2186444007403552
    mean_inference_ms: 4.330619791791043
    mean_raw_obs_processing_ms: 0.38059167709131336
  time_since_restore: 3799.180959224701
  time_this_iter_s: 26.286516666412354
  time_total_s: 3799.180959224701
  timers:
    learn_throughput: 8283.52
    learn_time_ms: 19531.794
    sample_throughput: 23657.012
    sample_time_ms: 6839.072
    update_time_ms: 33.034
  timestamp: 1602746794
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    143 |          3799.18 | 23136256 |  273.685 |              305.323 |              128.354 |            780.236 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3124.855643750841
    time_step_min: 2950
  date: 2020-10-15_07-27-01
  done: false
  episode_len_mean: 780.3154835892095
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 273.8302988225385
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 210
  episodes_total: 29767
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0587911840678756e-23
        cur_lr: 5.0e-05
        entropy: 0.12360756347576778
        entropy_coeff: 0.0005000000000000001
        kl: 0.004550003524248798
        model: {}
        policy_loss: -0.007976514882708821
        total_loss: 0.8405757447083791
        vf_explained_var: 0.998445451259613
        vf_loss: 0.8486140668392181
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63870967741936
    gpu_util_percent0: 0.354516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679278922370037
    mean_env_wait_ms: 1.2185616249872495
    mean_inference_ms: 4.330177386571338
    mean_raw_obs_processing_ms: 0.3805655289006359
  time_since_restore: 3825.529425382614
  time_this_iter_s: 26.348466157913208
  time_total_s: 3825.529425382614
  timers:
    learn_throughput: 8298.604
    learn_time_ms: 19496.291
    sample_throughput: 23636.753
    sample_time_ms: 6844.933
    update_time_ms: 34.375
  timestamp: 1602746821
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    144 |          3825.53 | 23298048 |   273.83 |              305.323 |              128.354 |            780.315 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3123.8270764507624
    time_step_min: 2950
  date: 2020-10-15_07-27-28
  done: false
  episode_len_mean: 780.410905939604
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 273.98425593175944
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 235
  episodes_total: 30002
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.293955920339378e-24
        cur_lr: 5.0e-05
        entropy: 0.11361611882845561
        entropy_coeff: 0.0005000000000000001
        kl: 0.003843607109350463
        model: {}
        policy_loss: -0.006866424422090252
        total_loss: 1.2257683078447978
        vf_explained_var: 0.9978378415107727
        vf_loss: 1.2326915164788563
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.467741935483872
    gpu_util_percent0: 0.33612903225806456
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678570095918422
    mean_env_wait_ms: 1.2184686063674977
    mean_inference_ms: 4.3297174011564294
    mean_raw_obs_processing_ms: 0.3805364764548169
  time_since_restore: 3851.918698787689
  time_this_iter_s: 26.389273405075073
  time_total_s: 3851.918698787689
  timers:
    learn_throughput: 8311.834
    learn_time_ms: 19465.258
    sample_throughput: 23619.611
    sample_time_ms: 6849.901
    update_time_ms: 35.654
  timestamp: 1602746848
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    145 |          3851.92 | 23459840 |  273.984 |              305.323 |              128.354 |            780.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3123.040668745439
    time_step_min: 2950
  date: 2020-10-15_07-27-55
  done: false
  episode_len_mean: 780.4993207647195
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 274.1056139741405
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 30181
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.646977960169689e-24
        cur_lr: 5.0e-05
        entropy: 0.1051303247610728
        entropy_coeff: 0.0005000000000000001
        kl: 0.004605054467295607
        model: {}
        policy_loss: -0.007170514938479755
        total_loss: 0.5464567939440409
        vf_explained_var: 0.9988390803337097
        vf_loss: 0.5536798586448034
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.8741935483871
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467803369751929
    mean_env_wait_ms: 1.2183922336929622
    mean_inference_ms: 4.3293577140272514
    mean_raw_obs_processing_ms: 0.3805140511305846
  time_since_restore: 3878.271066427231
  time_this_iter_s: 26.352367639541626
  time_total_s: 3878.271066427231
  timers:
    learn_throughput: 8313.866
    learn_time_ms: 19460.502
    sample_throughput: 23648.538
    sample_time_ms: 6841.522
    update_time_ms: 35.027
  timestamp: 1602746875
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    146 |          3878.27 | 23621632 |  274.106 |              305.323 |              128.354 |            780.499 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3122.2181782217385
    time_step_min: 2950
  date: 2020-10-15_07-28-22
  done: false
  episode_len_mean: 780.5956928345627
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 274.2304352800928
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 30368
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3234889800848445e-24
        cur_lr: 5.0e-05
        entropy: 0.1115204927821954
        entropy_coeff: 0.0005000000000000001
        kl: 0.004320211824961007
        model: {}
        policy_loss: -0.008147362398934396
        total_loss: 0.6426206827163696
        vf_explained_var: 0.9987303614616394
        vf_loss: 0.6508238017559052
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.612903225806456
    gpu_util_percent0: 0.29064516129032253
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467757113152911
    mean_env_wait_ms: 1.218317117459649
    mean_inference_ms: 4.328993847426802
    mean_raw_obs_processing_ms: 0.38049319495321526
  time_since_restore: 3904.5807995796204
  time_this_iter_s: 26.309733152389526
  time_total_s: 3904.5807995796204
  timers:
    learn_throughput: 8330.144
    learn_time_ms: 19422.474
    sample_throughput: 23632.821
    sample_time_ms: 6846.072
    update_time_ms: 33.067
  timestamp: 1602746902
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    147 |          3904.58 | 23783424 |   274.23 |              305.323 |              128.354 |            780.596 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3121.185627371451
    time_step_min: 2950
  date: 2020-10-15_07-28-49
  done: false
  episode_len_mean: 780.6971281079492
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 274.38805805630386
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 239
  episodes_total: 30607
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.617444900424223e-25
        cur_lr: 5.0e-05
        entropy: 0.11291346823175748
        entropy_coeff: 0.0005000000000000001
        kl: 0.004657148305947582
        model: {}
        policy_loss: -0.0077766883332515135
        total_loss: 0.7319181660811106
        vf_explained_var: 0.9987514019012451
        vf_loss: 0.7397512992223104
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.567741935483873
    gpu_util_percent0: 0.3338709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676911789459687
    mean_env_wait_ms: 1.2182153394428297
    mean_inference_ms: 4.328531057044461
    mean_raw_obs_processing_ms: 0.38046395138838135
  time_since_restore: 3930.743370294571
  time_this_iter_s: 26.16257071495056
  time_total_s: 3930.743370294571
  timers:
    learn_throughput: 8354.924
    learn_time_ms: 19364.868
    sample_throughput: 23643.28
    sample_time_ms: 6843.044
    update_time_ms: 32.234
  timestamp: 1602746929
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    148 |          3930.74 | 23945216 |  274.388 |              305.323 |              128.354 |            780.697 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3120.340200825399
    time_step_min: 2950
  date: 2020-10-15_07-29-16
  done: false
  episode_len_mean: 780.7775902363022
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 274.517587587115
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 201
  episodes_total: 30808
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3087224502121113e-25
        cur_lr: 5.0e-05
        entropy: 0.09846524645884831
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034358943812549114
        model: {}
        policy_loss: -0.006402378542892014
        total_loss: 0.6536095490058264
        vf_explained_var: 0.9987220168113708
        vf_loss: 0.6600611706574758
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45161290322581
    gpu_util_percent0: 0.3009677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676285478085152
    mean_env_wait_ms: 1.2181249038132718
    mean_inference_ms: 4.3281472693183805
    mean_raw_obs_processing_ms: 0.38043886134757493
  time_since_restore: 3957.225383043289
  time_this_iter_s: 26.48201274871826
  time_total_s: 3957.225383043289
  timers:
    learn_throughput: 8367.12
    learn_time_ms: 19336.642
    sample_throughput: 23612.785
    sample_time_ms: 6851.881
    update_time_ms: 30.546
  timestamp: 1602746956
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    149 |          3957.23 | 24107008 |  274.518 |              305.323 |              128.354 |            780.778 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3119.6060429794798
    time_step_min: 2950
  date: 2020-10-15_07-29-43
  done: false
  episode_len_mean: 780.8283408650742
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 274.6266147596038
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 172
  episodes_total: 30980
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6543612251060557e-25
        cur_lr: 5.0e-05
        entropy: 0.10112810879945755
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031137283270557723
        model: {}
        policy_loss: -0.006787225632554812
        total_loss: 0.7272495925426483
        vf_explained_var: 0.9984807372093201
        vf_loss: 0.7340873777866364
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.996774193548386
    gpu_util_percent0: 0.30709677419354836
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675846686398258
    mean_env_wait_ms: 1.2180509434414626
    mean_inference_ms: 4.327836548433285
    mean_raw_obs_processing_ms: 0.38041967801968973
  time_since_restore: 3983.3822791576385
  time_this_iter_s: 26.156896114349365
  time_total_s: 3983.3822791576385
  timers:
    learn_throughput: 8385.327
    learn_time_ms: 19294.656
    sample_throughput: 23676.451
    sample_time_ms: 6833.457
    update_time_ms: 30.512
  timestamp: 1602746983
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    150 |          3983.38 | 24268800 |  274.627 |              305.323 |              128.354 |            780.828 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3118.6708267830218
    time_step_min: 2950
  date: 2020-10-15_07-30-10
  done: false
  episode_len_mean: 780.906550442251
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 274.7688502121588
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 224
  episodes_total: 31204
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.271806125530278e-26
        cur_lr: 5.0e-05
        entropy: 0.11039948152999084
        entropy_coeff: 0.0005000000000000001
        kl: 0.004013253763938944
        model: {}
        policy_loss: -0.006520074695193519
        total_loss: 0.8020197053750356
        vf_explained_var: 0.9986333250999451
        vf_loss: 0.8085949917634329
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.974193548387102
    gpu_util_percent0: 0.34451612903225803
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675317823259576
    mean_env_wait_ms: 1.217953674344784
    mean_inference_ms: 4.3274147733169865
    mean_raw_obs_processing_ms: 0.3803947819427978
  time_since_restore: 4009.725756406784
  time_this_iter_s: 26.343477249145508
  time_total_s: 4009.725756406784
  timers:
    learn_throughput: 8392.965
    learn_time_ms: 19277.097
    sample_throughput: 23767.865
    sample_time_ms: 6807.174
    update_time_ms: 31.982
  timestamp: 1602747010
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    151 |          4009.73 | 24430592 |  274.769 |              305.323 |              128.354 |            780.907 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3117.7155779214577
    time_step_min: 2950
  date: 2020-10-15_07-30-36
  done: false
  episode_len_mean: 780.9919826927971
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 274.9140710361441
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 228
  episodes_total: 31432
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.135903062765139e-26
        cur_lr: 5.0e-05
        entropy: 0.10905105993151665
        entropy_coeff: 0.0005000000000000001
        kl: 0.010684365561852852
        model: {}
        policy_loss: -0.009505328636199314
        total_loss: 0.4807838251193364
        vf_explained_var: 0.9991396069526672
        vf_loss: 0.4903436650832494
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94
    gpu_util_percent0: 0.286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674646138471584
    mean_env_wait_ms: 1.2178496525997524
    mean_inference_ms: 4.326998706720436
    mean_raw_obs_processing_ms: 0.3803677089003085
  time_since_restore: 4035.8347158432007
  time_this_iter_s: 26.108959436416626
  time_total_s: 4035.8347158432007
  timers:
    learn_throughput: 8397.325
    learn_time_ms: 19267.088
    sample_throughput: 23759.519
    sample_time_ms: 6809.565
    update_time_ms: 32.233
  timestamp: 1602747036
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    152 |          4035.83 | 24592384 |  274.914 |              305.323 |              128.354 |            780.992 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3117.0951189382663
    time_step_min: 2950
  date: 2020-10-15_07-31-03
  done: false
  episode_len_mean: 781.0432196418402
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.0031310382826
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 31606
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.135903062765139e-26
        cur_lr: 5.0e-05
        entropy: 0.132445078343153
        entropy_coeff: 0.0005000000000000001
        kl: 0.00533253897447139
        model: {}
        policy_loss: -0.008557429622063259
        total_loss: 1.5195555984973907
        vf_explained_var: 0.9965251088142395
        vf_loss: 1.5281791885693867
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36774193548387
    gpu_util_percent0: 0.3096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674158680729027
    mean_env_wait_ms: 1.217769889176603
    mean_inference_ms: 4.326689078736281
    mean_raw_obs_processing_ms: 0.3803480980561041
  time_since_restore: 4062.116585969925
  time_this_iter_s: 26.281870126724243
  time_total_s: 4062.116585969925
  timers:
    learn_throughput: 8397.264
    learn_time_ms: 19267.228
    sample_throughput: 23769.44
    sample_time_ms: 6806.723
    update_time_ms: 33.264
  timestamp: 1602747063
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    153 |          4062.12 | 24754176 |  275.003 |              305.323 |              128.354 |            781.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3116.7958888154376
    time_step_min: 2950
  date: 2020-10-15_07-31-30
  done: false
  episode_len_mean: 781.0730457203949
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.04825438206984
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 196
  episodes_total: 31802
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.135903062765139e-26
        cur_lr: 5.0e-05
        entropy: 0.1506442887087663
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050797230408837395
        model: {}
        policy_loss: -0.010306316840190751
        total_loss: 3.3014795382817588
        vf_explained_var: 0.9933236241340637
        vf_loss: 3.3118611375490823
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12258064516129
    gpu_util_percent0: 0.3525806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673721327999636
    mean_env_wait_ms: 1.2176868759403223
    mean_inference_ms: 4.326338084090158
    mean_raw_obs_processing_ms: 0.38032770057817167
  time_since_restore: 4088.3326029777527
  time_this_iter_s: 26.21601700782776
  time_total_s: 4088.3326029777527
  timers:
    learn_throughput: 8404.057
    learn_time_ms: 19251.655
    sample_throughput: 23770.472
    sample_time_ms: 6806.428
    update_time_ms: 33.495
  timestamp: 1602747090
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    154 |          4088.33 | 24915968 |  275.048 |              305.323 |              128.354 |            781.073 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3116.361614394602
    time_step_min: 2950
  date: 2020-10-15_07-31-57
  done: false
  episode_len_mean: 781.132368084376
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.1247359859398
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 245
  episodes_total: 32047
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.135903062765139e-26
        cur_lr: 5.0e-05
        entropy: 0.132830282052358
        entropy_coeff: 0.0005000000000000001
        kl: 0.004480111102263133
        model: {}
        policy_loss: -0.009170213102455213
        total_loss: 3.0288339257240295
        vf_explained_var: 0.9947267174720764
        vf_loss: 3.0380706985791526
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.706451612903226
    gpu_util_percent0: 0.33354838709677426
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673121923295956
    mean_env_wait_ms: 1.2175786506094888
    mean_inference_ms: 4.32591609969725
    mean_raw_obs_processing_ms: 0.3803013116010207
  time_since_restore: 4114.77668762207
  time_this_iter_s: 26.444084644317627
  time_total_s: 4114.77668762207
  timers:
    learn_throughput: 8405.392
    learn_time_ms: 19248.596
    sample_throughput: 23741.395
    sample_time_ms: 6814.764
    update_time_ms: 32.248
  timestamp: 1602747117
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    155 |          4114.78 | 25077760 |  275.125 |              305.323 |              128.354 |            781.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3115.8033479300598
    time_step_min: 2950
  date: 2020-10-15_07-32-24
  done: false
  episode_len_mean: 781.1849910032885
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.2081693650534
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 32234
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0679515313825696e-26
        cur_lr: 5.0e-05
        entropy: 0.1101513219376405
        entropy_coeff: 0.0005000000000000001
        kl: 0.004614938943025966
        model: {}
        policy_loss: -0.010342060646507889
        total_loss: 1.510247419277827
        vf_explained_var: 0.9968504309654236
        vf_loss: 1.5206445157527924
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25483870967742
    gpu_util_percent0: 0.32354838709677425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672589892576593
    mean_env_wait_ms: 1.2174888650426188
    mean_inference_ms: 4.3255848458558095
    mean_raw_obs_processing_ms: 0.38027928121603743
  time_since_restore: 4140.824009418488
  time_this_iter_s: 26.047321796417236
  time_total_s: 4140.824009418488
  timers:
    learn_throughput: 8419.162
    learn_time_ms: 19217.114
    sample_throughput: 23738.685
    sample_time_ms: 6815.542
    update_time_ms: 32.485
  timestamp: 1602747144
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    156 |          4140.82 | 25239552 |  275.208 |              305.323 |              128.354 |            781.185 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3115.1534684044723
    time_step_min: 2950
  date: 2020-10-15_07-32-51
  done: false
  episode_len_mean: 781.2417548514485
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.30957338167406
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 32413
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0339757656912848e-26
        cur_lr: 5.0e-05
        entropy: 0.11111485833923022
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036488547339104116
        model: {}
        policy_loss: -0.008742890620093627
        total_loss: 0.9174015919367472
        vf_explained_var: 0.9980797171592712
        vf_loss: 0.9262000322341919
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.822580645161295
    gpu_util_percent0: 0.3345161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672178946040815
    mean_env_wait_ms: 1.2174105966461877
    mean_inference_ms: 4.325294563319269
    mean_raw_obs_processing_ms: 0.38026268819564396
  time_since_restore: 4167.120839595795
  time_this_iter_s: 26.29683017730713
  time_total_s: 4167.120839595795
  timers:
    learn_throughput: 8418.236
    learn_time_ms: 19219.229
    sample_throughput: 23756.792
    sample_time_ms: 6810.347
    update_time_ms: 32.839
  timestamp: 1602747171
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    157 |          4167.12 | 25401344 |   275.31 |              305.323 |              128.354 |            781.242 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3114.296096409187
    time_step_min: 2950
  date: 2020-10-15_07-33-17
  done: false
  episode_len_mean: 781.3200392084789
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.44153072723185
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 233
  episodes_total: 32646
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.169878828456424e-27
        cur_lr: 5.0e-05
        entropy: 0.11004653076330821
        entropy_coeff: 0.0005000000000000001
        kl: 0.003398537422375133
        model: {}
        policy_loss: -0.0070081611920613796
        total_loss: 0.8690818250179291
        vf_explained_var: 0.9985256195068359
        vf_loss: 0.8761450300614039
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.61935483870968
    gpu_util_percent0: 0.3303225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671621927814169
    mean_env_wait_ms: 1.2173025848127694
    mean_inference_ms: 4.324877694649899
    mean_raw_obs_processing_ms: 0.3802367509337763
  time_since_restore: 4193.117198705673
  time_this_iter_s: 25.99635910987854
  time_total_s: 4193.117198705673
  timers:
    learn_throughput: 8428.445
    learn_time_ms: 19195.949
    sample_throughput: 23773.958
    sample_time_ms: 6805.43
    update_time_ms: 34.68
  timestamp: 1602747197
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    158 |          4193.12 | 25563136 |  275.442 |              305.323 |              128.354 |             781.32 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3113.4934190481995
    time_step_min: 2950
  date: 2020-10-15_07-33-44
  done: false
  episode_len_mean: 781.3971452049791
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.5626859335049
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 32857
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.584939414228212e-27
        cur_lr: 5.0e-05
        entropy: 0.09547472062210242
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034435049553091326
        model: {}
        policy_loss: -0.007461272083067645
        total_loss: 0.6019104321797689
        vf_explained_var: 0.9988720417022705
        vf_loss: 0.6094194402297338
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.97096774193549
    gpu_util_percent0: 0.3325806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671079204012571
    mean_env_wait_ms: 1.2172059152841548
    mean_inference_ms: 4.324526705760452
    mean_raw_obs_processing_ms: 0.38021466258007747
  time_since_restore: 4219.417605161667
  time_this_iter_s: 26.300406455993652
  time_total_s: 4219.417605161667
  timers:
    learn_throughput: 8432.544
    learn_time_ms: 19186.618
    sample_throughput: 23806.779
    sample_time_ms: 6796.047
    update_time_ms: 34.379
  timestamp: 1602747224
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    159 |          4219.42 | 25724928 |  275.563 |              305.323 |              128.354 |            781.397 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3112.8345506894984
    time_step_min: 2950
  date: 2020-10-15_07-34-11
  done: false
  episode_len_mean: 781.4613079019074
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.6596344920595
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 173
  episodes_total: 33030
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.292469707114106e-27
        cur_lr: 5.0e-05
        entropy: 0.09339249258240064
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037560786586254835
        model: {}
        policy_loss: -0.007364505392615683
        total_loss: 0.6423476835091909
        vf_explained_var: 0.9986768364906311
        vf_loss: 0.649758905172348
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.659999999999993
    gpu_util_percent0: 0.2453333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467067967244931
    mean_env_wait_ms: 1.2171245737740766
    mean_inference_ms: 4.324248035933989
    mean_raw_obs_processing_ms: 0.3801966956639084
  time_since_restore: 4245.5496661663055
  time_this_iter_s: 26.132061004638672
  time_total_s: 4245.5496661663055
  timers:
    learn_throughput: 8433.283
    learn_time_ms: 19184.938
    sample_throughput: 23817.077
    sample_time_ms: 6793.109
    update_time_ms: 34.45
  timestamp: 1602747251
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    160 |          4245.55 | 25886720 |   275.66 |              305.323 |              128.354 |            781.461 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3112.0829844272416
    time_step_min: 2950
  date: 2020-10-15_07-34-38
  done: false
  episode_len_mean: 781.5385147740266
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.773943624729
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 204
  episodes_total: 33234
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.46234853557053e-28
        cur_lr: 5.0e-05
        entropy: 0.10564454769094785
        entropy_coeff: 0.0005000000000000001
        kl: 0.004967969337788721
        model: {}
        policy_loss: -0.010104932305694092
        total_loss: 0.6688520908355713
        vf_explained_var: 0.9987995028495789
        vf_loss: 0.6790098448594412
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58387096774194
    gpu_util_percent0: 0.2896774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670227941492991
    mean_env_wait_ms: 1.2170320255303424
    mean_inference_ms: 4.323919354928233
    mean_raw_obs_processing_ms: 0.3801781493416427
  time_since_restore: 4271.873421669006
  time_this_iter_s: 26.323755502700806
  time_total_s: 4271.873421669006
  timers:
    learn_throughput: 8431.489
    learn_time_ms: 19189.018
    sample_throughput: 23838.293
    sample_time_ms: 6787.063
    update_time_ms: 32.606
  timestamp: 1602747278
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    161 |          4271.87 | 26048512 |  275.774 |              305.323 |              128.354 |            781.539 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3111.1938754149346
    time_step_min: 2950
  date: 2020-10-15_07-35-05
  done: false
  episode_len_mean: 781.6412439505287
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 275.90720432502116
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 240
  episodes_total: 33474
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.231174267785265e-28
        cur_lr: 5.0e-05
        entropy: 0.09706129940847556
        entropy_coeff: 0.0005000000000000001
        kl: 0.003666744062987467
        model: {}
        policy_loss: -0.0063702424910540385
        total_loss: 0.6391013363997141
        vf_explained_var: 0.9989442825317383
        vf_loss: 0.6455201158920923
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.683870967741935
    gpu_util_percent0: 0.2864516129032259
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669704787560484
    mean_env_wait_ms: 1.2169196110912317
    mean_inference_ms: 4.323535222385227
    mean_raw_obs_processing_ms: 0.3801536013479378
  time_since_restore: 4298.338705301285
  time_this_iter_s: 26.465283632278442
  time_total_s: 4298.338705301285
  timers:
    learn_throughput: 8418.766
    learn_time_ms: 19218.018
    sample_throughput: 23819.99
    sample_time_ms: 6792.278
    update_time_ms: 33.094
  timestamp: 1602747305
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: c9144_00000
  
2020-10-15 07:35:06,411	WARNING util.py:136 -- The `process_trial` operation took 0.5208334922790527 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    162 |          4298.34 | 26210304 |  275.907 |              305.323 |              128.354 |            781.641 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3110.5536718122603
    time_step_min: 2950
  date: 2020-10-15_07-35-32
  done: false
  episode_len_mean: 781.7165735678631
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.005127036949
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 182
  episodes_total: 33656
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6155871338926325e-28
        cur_lr: 5.0e-05
        entropy: 0.08451742678880692
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030414691815773645
        model: {}
        policy_loss: -0.006895586338941939
        total_loss: 0.7513509541749954
        vf_explained_var: 0.9985208511352539
        vf_loss: 0.7582888056834539
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.880645161290328
    gpu_util_percent0: 0.3787096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466922918022772
    mean_env_wait_ms: 1.216827739436515
    mean_inference_ms: 4.323241016380788
    mean_raw_obs_processing_ms: 0.38013427459344545
  time_since_restore: 4324.741077423096
  time_this_iter_s: 26.402372121810913
  time_total_s: 4324.741077423096
  timers:
    learn_throughput: 8412.552
    learn_time_ms: 19232.214
    sample_throughput: 23831.738
    sample_time_ms: 6788.93
    update_time_ms: 33.242
  timestamp: 1602747332
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    163 |          4324.74 | 26372096 |  276.005 |              305.323 |              128.354 |            781.717 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3109.8854277600285
    time_step_min: 2950
  date: 2020-10-15_07-35-59
  done: false
  episode_len_mean: 781.7941724046219
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.1065717907823
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 183
  episodes_total: 33839
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.077935669463162e-29
        cur_lr: 5.0e-05
        entropy: 0.08872558797399203
        entropy_coeff: 0.0005000000000000001
        kl: 0.003271812320842097
        model: {}
        policy_loss: -0.006558249767598075
        total_loss: 0.3587055280804634
        vf_explained_var: 0.9992899894714355
        vf_loss: 0.36530814071496326
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.246875
    gpu_util_percent0: 0.25906250000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668878418214298
    mean_env_wait_ms: 1.2167429812870283
    mean_inference_ms: 4.322975645924138
    mean_raw_obs_processing_ms: 0.3801195178798274
  time_since_restore: 4351.168833971024
  time_this_iter_s: 26.427756547927856
  time_total_s: 4351.168833971024
  timers:
    learn_throughput: 8399.481
    learn_time_ms: 19262.142
    sample_throughput: 23859.402
    sample_time_ms: 6781.058
    update_time_ms: 32.384
  timestamp: 1602747359
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    164 |          4351.17 | 26533888 |  276.107 |              305.323 |              128.354 |            781.794 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3109.046124919208
    time_step_min: 2950
  date: 2020-10-15_07-36-27
  done: false
  episode_len_mean: 781.8949021219147
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.23317256739614
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 234
  episodes_total: 34073
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.038967834731581e-29
        cur_lr: 5.0e-05
        entropy: 0.09700794083376725
        entropy_coeff: 0.0005000000000000001
        kl: 0.004915821831673384
        model: {}
        policy_loss: -0.006786611270702754
        total_loss: 0.5086634109417597
        vf_explained_var: 0.9991665482521057
        vf_loss: 0.5154985239108404
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33870967741936
    gpu_util_percent0: 0.31967741935483873
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466833790893431
    mean_env_wait_ms: 1.2166261766354765
    mean_inference_ms: 4.322593305638112
    mean_raw_obs_processing_ms: 0.3800953780435486
  time_since_restore: 4377.565943002701
  time_this_iter_s: 26.397109031677246
  time_total_s: 4377.565943002701
  timers:
    learn_throughput: 8401.09
    learn_time_ms: 19258.453
    sample_throughput: 23895.594
    sample_time_ms: 6770.788
    update_time_ms: 32.077
  timestamp: 1602747387
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    165 |          4377.57 | 26695680 |  276.233 |              305.323 |              128.354 |            781.895 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3108.339932258818
    time_step_min: 2950
  date: 2020-10-15_07-36-54
  done: false
  episode_len_mean: 781.9864364262171
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.3419190298693
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 210
  episodes_total: 34283
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0194839173657906e-29
        cur_lr: 5.0e-05
        entropy: 0.08662957263489564
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036373414526072643
        model: {}
        policy_loss: -0.006861231406219304
        total_loss: 0.6753063996632894
        vf_explained_var: 0.9987664818763733
        vf_loss: 0.6822109421094259
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26451612903226
    gpu_util_percent0: 0.34838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667852994410607
    mean_env_wait_ms: 1.216521974160211
    mean_inference_ms: 4.322269226801101
    mean_raw_obs_processing_ms: 0.3800738468684035
  time_since_restore: 4404.059243440628
  time_this_iter_s: 26.493300437927246
  time_total_s: 4404.059243440628
  timers:
    learn_throughput: 8376.308
    learn_time_ms: 19315.43
    sample_throughput: 23942.517
    sample_time_ms: 6757.518
    update_time_ms: 32.138
  timestamp: 1602747414
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    166 |          4404.06 | 26857472 |  276.342 |              305.323 |              128.354 |            781.986 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3107.775895632972
    time_step_min: 2950
  date: 2020-10-15_07-37-21
  done: false
  episode_len_mean: 782.0575583420411
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.431150586323
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 169
  episodes_total: 34452
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0097419586828953e-29
        cur_lr: 5.0e-05
        entropy: 0.08406032932301362
        entropy_coeff: 0.0005000000000000001
        kl: 0.003267441235948354
        model: {}
        policy_loss: -0.007933800768417617
        total_loss: 0.4392632345358531
        vf_explained_var: 0.9991467595100403
        vf_loss: 0.44723907113075256
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.661290322580648
    gpu_util_percent0: 0.33258064516129027
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466748493924508
    mean_env_wait_ms: 1.2164379138237393
    mean_inference_ms: 4.32202139871746
    mean_raw_obs_processing_ms: 0.3800579980195468
  time_since_restore: 4430.355807065964
  time_this_iter_s: 26.296563625335693
  time_total_s: 4430.355807065964
  timers:
    learn_throughput: 8379.668
    learn_time_ms: 19307.686
    sample_throughput: 23917.128
    sample_time_ms: 6764.692
    update_time_ms: 31.51
  timestamp: 1602747441
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    167 |          4430.36 | 27019264 |  276.431 |              305.323 |              128.354 |            782.058 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3107.065368728155
    time_step_min: 2950
  date: 2020-10-15_07-37-47
  done: false
  episode_len_mean: 782.1527962140012
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.5387944779357
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 202
  episodes_total: 34654
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.0487097934144765e-30
        cur_lr: 5.0e-05
        entropy: 0.09033316063384215
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034665580994139114
        model: {}
        policy_loss: -0.00839943444589153
        total_loss: 0.4082765728235245
        vf_explained_var: 0.9992680549621582
        vf_loss: 0.41672117014726
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.903225806451612
    gpu_util_percent0: 0.35096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667083275572648
    mean_env_wait_ms: 1.2163379051679497
    mean_inference_ms: 4.321716526709019
    mean_raw_obs_processing_ms: 0.38003990493201056
  time_since_restore: 4456.471494436264
  time_this_iter_s: 26.115687370300293
  time_total_s: 4456.471494436264
  timers:
    learn_throughput: 8372.81
    learn_time_ms: 19323.501
    sample_throughput: 23914.916
    sample_time_ms: 6765.317
    update_time_ms: 33.082
  timestamp: 1602747467
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    168 |          4456.47 | 27181056 |  276.539 |              305.323 |              128.354 |            782.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3106.2419391853127
    time_step_min: 2950
  date: 2020-10-15_07-38-15
  done: false
  episode_len_mean: 782.254219802264
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.6608295304383
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 241
  episodes_total: 34895
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5243548967072383e-30
        cur_lr: 5.0e-05
        entropy: 0.08781133902569611
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037850608156683543
        model: {}
        policy_loss: -0.0070219648914644495
        total_loss: 0.6503189553817114
        vf_explained_var: 0.9989774227142334
        vf_loss: 0.6573848277330399
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20645161290323
    gpu_util_percent0: 0.3267741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466662280203121
    mean_env_wait_ms: 1.2162186282404854
    mean_inference_ms: 4.321368929168178
    mean_raw_obs_processing_ms: 0.38001805385076026
  time_since_restore: 4482.778646945953
  time_this_iter_s: 26.30715250968933
  time_total_s: 4482.778646945953
  timers:
    learn_throughput: 8372.148
    learn_time_ms: 19325.028
    sample_throughput: 23928.968
    sample_time_ms: 6761.345
    update_time_ms: 35.137
  timestamp: 1602747495
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: c9144_00000
  
2020-10-15 07:38:15,717	WARNING util.py:136 -- The `process_trial` operation took 0.5477762222290039 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    169 |          4482.78 | 27342848 |  276.661 |              305.323 |              128.354 |            782.254 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3105.6099760301336
    time_step_min: 2950
  date: 2020-10-15_07-38-41
  done: false
  episode_len_mean: 782.3335613900055
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.7546939505376
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 35079
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2621774483536191e-30
        cur_lr: 5.0e-05
        entropy: 0.07410980636874835
        entropy_coeff: 0.0005000000000000001
        kl: 0.003370630923503389
        model: {}
        policy_loss: -0.008089037701211055
        total_loss: 0.5034877707560858
        vf_explained_var: 0.9990183711051941
        vf_loss: 0.5116138507922491
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.258064516129032
    gpu_util_percent0: 0.34741935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666163140878033
    mean_env_wait_ms: 1.2161198765476526
    mean_inference_ms: 4.321098137254046
    mean_raw_obs_processing_ms: 0.37999971037465113
  time_since_restore: 4508.941184997559
  time_this_iter_s: 26.162538051605225
  time_total_s: 4508.941184997559
  timers:
    learn_throughput: 8373.941
    learn_time_ms: 19320.891
    sample_throughput: 23911.425
    sample_time_ms: 6766.305
    update_time_ms: 37.912
  timestamp: 1602747521
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: c9144_00000
  
2020-10-15 07:38:42,594	WARNING util.py:136 -- The `process_trial` operation took 0.5497119426727295 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    170 |          4508.94 | 27504640 |  276.755 |              305.323 |              128.354 |            782.334 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3105.0061037928685
    time_step_min: 2950
  date: 2020-10-15_07-39-08
  done: false
  episode_len_mean: 782.4046910008792
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.8472833499635
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 180
  episodes_total: 35259
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.310887241768096e-31
        cur_lr: 5.0e-05
        entropy: 0.08126670556763808
        entropy_coeff: 0.0005000000000000001
        kl: 0.003353585566704472
        model: {}
        policy_loss: -0.007144051691284403
        total_loss: 0.3491665894786517
        vf_explained_var: 0.9993372559547424
        vf_loss: 0.35635127623875934
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.467741935483872
    gpu_util_percent0: 0.355483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665834063070426
    mean_env_wait_ms: 1.2160304089118188
    mean_inference_ms: 4.320855188276897
    mean_raw_obs_processing_ms: 0.3799860745567137
  time_since_restore: 4535.100158452988
  time_this_iter_s: 26.158973455429077
  time_total_s: 4535.100158452988
  timers:
    learn_throughput: 8380.174
    learn_time_ms: 19306.52
    sample_throughput: 23920.552
    sample_time_ms: 6763.724
    update_time_ms: 38.021
  timestamp: 1602747548
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: c9144_00000
  
2020-10-15 07:39:09,456	WARNING util.py:136 -- The `process_trial` operation took 0.5353741645812988 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    171 |           4535.1 | 27666432 |  276.847 |              305.323 |              128.354 |            782.405 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3104.2358698104695
    time_step_min: 2950
  date: 2020-10-15_07-39-35
  done: false
  episode_len_mean: 782.4973091769745
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 276.96109954181014
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 232
  episodes_total: 35491
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.155443620884048e-31
        cur_lr: 5.0e-05
        entropy: 0.08903657769163449
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030247716155524054
        model: {}
        policy_loss: -0.007562650610149528
        total_loss: 0.8009070058663686
        vf_explained_var: 0.9987195134162903
        vf_loss: 0.8085141529639562
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10322580645162
    gpu_util_percent0: 0.2761290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665362315295255
    mean_env_wait_ms: 1.2159086948674886
    mean_inference_ms: 4.320508251176375
    mean_raw_obs_processing_ms: 0.379963786679541
  time_since_restore: 4561.291475057602
  time_this_iter_s: 26.191316604614258
  time_total_s: 4561.291475057602
  timers:
    learn_throughput: 8388.277
    learn_time_ms: 19287.87
    sample_throughput: 23946.913
    sample_time_ms: 6756.278
    update_time_ms: 36.593
  timestamp: 1602747575
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    172 |          4561.29 | 27828224 |  276.961 |              305.323 |              128.354 |            782.497 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3103.5284124358723
    time_step_min: 2950
  date: 2020-10-15_07-40-02
  done: false
  episode_len_mean: 782.586540077298
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.06553775021257
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 215
  episodes_total: 35706
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.577721810442024e-31
        cur_lr: 5.0e-05
        entropy: 0.08006275321046512
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050843175655851764
        model: {}
        policy_loss: -0.007473760958722171
        total_loss: 0.5455773423115412
        vf_explained_var: 0.9990174770355225
        vf_loss: 0.5530911286671957
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.461290322580652
    gpu_util_percent0: 0.3551612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466489870825345
    mean_env_wait_ms: 1.2157955755820584
    mean_inference_ms: 4.320207083369193
    mean_raw_obs_processing_ms: 0.37994358934588585
  time_since_restore: 4587.651536226273
  time_this_iter_s: 26.360061168670654
  time_total_s: 4587.651536226273
  timers:
    learn_throughput: 8390.519
    learn_time_ms: 19282.716
    sample_throughput: 23970.635
    sample_time_ms: 6749.592
    update_time_ms: 35.237
  timestamp: 1602747602
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: c9144_00000
  
2020-10-15 07:40:03,512	WARNING util.py:136 -- The `process_trial` operation took 0.5220975875854492 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    173 |          4587.65 | 27990016 |  277.066 |              305.323 |              128.354 |            782.587 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3103.022126733447
    time_step_min: 2950
  date: 2020-10-15_07-40-29
  done: false
  episode_len_mean: 782.6564642916876
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.1447591823909
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 35874
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.577721810442024e-31
        cur_lr: 5.0e-05
        entropy: 0.07878222006062667
        entropy_coeff: 0.0005000000000000001
        kl: 0.004325724497903138
        model: {}
        policy_loss: -0.00855581923557717
        total_loss: 0.649313266078631
        vf_explained_var: 0.9987664222717285
        vf_loss: 0.6579084694385529
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.393333333333338
    gpu_util_percent0: 0.35533333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664567150087765
    mean_env_wait_ms: 1.2157077707474877
    mean_inference_ms: 4.319982572997916
    mean_raw_obs_processing_ms: 0.3799291641789564
  time_since_restore: 4613.834419488907
  time_this_iter_s: 26.182883262634277
  time_total_s: 4613.834419488907
  timers:
    learn_throughput: 8400.796
    learn_time_ms: 19259.128
    sample_throughput: 23970.617
    sample_time_ms: 6749.597
    update_time_ms: 33.548
  timestamp: 1602747629
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: c9144_00000
  
2020-10-15 07:40:30,447	WARNING util.py:136 -- The `process_trial` operation took 0.5240192413330078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    174 |          4613.83 | 28151808 |  277.145 |              305.323 |              128.354 |            782.656 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3102.407613973751
    time_step_min: 2950
  date: 2020-10-15_07-40-56
  done: false
  episode_len_mean: 782.7332427787326
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.23960036132235
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 200
  episodes_total: 36074
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.88860905221012e-32
        cur_lr: 5.0e-05
        entropy: 0.08449518804748853
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00861350451305043
        total_loss: .inf
        vf_explained_var: 0.9989232420921326
        vf_loss: 0.6311278194189072
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26451612903226
    gpu_util_percent0: 0.28774193548387095
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466421645740913
    mean_env_wait_ms: 1.2156038430894835
    mean_inference_ms: 4.319709220842616
    mean_raw_obs_processing_ms: 0.3799131263246097
  time_since_restore: 4640.124822378159
  time_this_iter_s: 26.29040288925171
  time_total_s: 4640.124822378159
  timers:
    learn_throughput: 8401.911
    learn_time_ms: 19256.571
    sample_throughput: 23977.637
    sample_time_ms: 6747.621
    update_time_ms: 33.297
  timestamp: 1602747656
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:40:57,496	WARNING util.py:136 -- The `process_trial` operation took 0.5434525012969971 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    175 |          4640.12 | 28313600 |   277.24 |              305.323 |              128.354 |            782.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3101.661769570011
    time_step_min: 2950
  date: 2020-10-15_07-41-23
  done: false
  episode_len_mean: 782.8289136720364
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.3530179949015
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 241
  episodes_total: 36315
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1832913578315177e-31
        cur_lr: 5.0e-05
        entropy: 0.08941567689180374
        entropy_coeff: 0.0005000000000000001
        kl: 0.00553335074800998
        model: {}
        policy_loss: -0.00839980494917351
        total_loss: 0.7818872978289922
        vf_explained_var: 0.9987068772315979
        vf_loss: 0.7903318057457606
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.416129032258063
    gpu_util_percent0: 0.2974193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466377962071207
    mean_env_wait_ms: 1.21547779580037
    mean_inference_ms: 4.319389888245325
    mean_raw_obs_processing_ms: 0.37989201696896674
  time_since_restore: 4666.199853897095
  time_this_iter_s: 26.075031518936157
  time_total_s: 4666.199853897095
  timers:
    learn_throughput: 8420.544
    learn_time_ms: 19213.961
    sample_throughput: 23980.727
    sample_time_ms: 6746.751
    update_time_ms: 33.052
  timestamp: 1602747683
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: c9144_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    176 |           4666.2 | 28475392 |  277.353 |              305.323 |              128.354 |            782.829 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3101.114002852128
    time_step_min: 2950
  date: 2020-10-15_07-41-50
  done: false
  episode_len_mean: 782.901695936875
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.43532837899807
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 36499
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1832913578315177e-31
        cur_lr: 5.0e-05
        entropy: 0.07379694903890292
        entropy_coeff: 0.0005000000000000001
        kl: 0.003362211359975239
        model: {}
        policy_loss: -0.008518818950202936
        total_loss: 0.5716538230578104
        vf_explained_var: 0.9988901019096375
        vf_loss: 0.5802095433076223
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.648387096774194
    gpu_util_percent0: 0.3474193548387098
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663364326137837
    mean_env_wait_ms: 1.2153752103379813
    mean_inference_ms: 4.319134806525777
    mean_raw_obs_processing_ms: 0.37987478610156666
  time_since_restore: 4692.701669692993
  time_this_iter_s: 26.501815795898438
  time_total_s: 4692.701669692993
  timers:
    learn_throughput: 8410.324
    learn_time_ms: 19237.308
    sample_throughput: 23996.27
    sample_time_ms: 6742.381
    update_time_ms: 34.323
  timestamp: 1602747710
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: c9144_00000
  
2020-10-15 07:41:51,417	WARNING util.py:136 -- The `process_trial` operation took 0.5173871517181396 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    177 |           4692.7 | 28637184 |  277.435 |              305.323 |              128.354 |            782.902 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3100.5559406210773
    time_step_min: 2950
  date: 2020-10-15_07-42-17
  done: false
  episode_len_mean: 782.9791717783048
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.5200842976258
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 182
  episodes_total: 36681
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.916456789157588e-32
        cur_lr: 5.0e-05
        entropy: 0.07806453915933768
        entropy_coeff: 0.0005000000000000001
        kl: 0.003283521354508897
        model: {}
        policy_loss: -0.008394053892213075
        total_loss: 0.5323159818847975
        vf_explained_var: 0.9989646077156067
        vf_loss: 0.5407490755120913
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50322580645161
    gpu_util_percent0: 0.29096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663075050264118
    mean_env_wait_ms: 1.2152811777435462
    mean_inference_ms: 4.318911570447587
    mean_raw_obs_processing_ms: 0.3798621580915435
  time_since_restore: 4719.178343057632
  time_this_iter_s: 26.476673364639282
  time_total_s: 4719.178343057632
  timers:
    learn_throughput: 8398.83
    learn_time_ms: 19263.635
    sample_throughput: 23946.528
    sample_time_ms: 6756.387
    update_time_ms: 31.229
  timestamp: 1602747737
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: c9144_00000
  
2020-10-15 07:42:18,671	WARNING util.py:136 -- The `process_trial` operation took 0.5224471092224121 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    178 |          4719.18 | 28798976 |   277.52 |              305.323 |              128.354 |            782.979 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3099.884769756468
    time_step_min: 2950
  date: 2020-10-15_07-42-45
  done: false
  episode_len_mean: 783.0675174076783
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.62186277962917
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 228
  episodes_total: 36909
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.958228394578794e-32
        cur_lr: 5.0e-05
        entropy: 0.08127165585756302
        entropy_coeff: 0.0005000000000000001
        kl: 0.0029146873081723848
        model: {}
        policy_loss: -0.008366045706983035
        total_loss: 0.7713397890329361
        vf_explained_var: 0.9988241195678711
        vf_loss: 0.7797464728355408
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.549999999999997
    gpu_util_percent0: 0.30374999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662618367364955
    mean_env_wait_ms: 1.215156558237205
    mean_inference_ms: 4.31859283955091
    mean_raw_obs_processing_ms: 0.37984118428507924
  time_since_restore: 4745.644561290741
  time_this_iter_s: 26.46621823310852
  time_total_s: 4745.644561290741
  timers:
    learn_throughput: 8398.037
    learn_time_ms: 19265.454
    sample_throughput: 23899.646
    sample_time_ms: 6769.64
    update_time_ms: 31.194
  timestamp: 1602747765
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: c9144_00000
  
2020-10-15 07:42:45,998	WARNING util.py:136 -- The `process_trial` operation took 0.520336389541626 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    179 |          4745.64 | 28960768 |  277.622 |              305.323 |              128.354 |            783.068 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3099.2218807289983
    time_step_min: 2950
  date: 2020-10-15_07-43-12
  done: false
  episode_len_mean: 783.1655399035742
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.72189098135175
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 218
  episodes_total: 37127
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.479114197289397e-32
        cur_lr: 5.0e-05
        entropy: 0.0734778077652057
        entropy_coeff: 0.0005000000000000001
        kl: 0.004329283876965444
        model: {}
        policy_loss: -0.006818539748564945
        total_loss: 0.2901948740084966
        vf_explained_var: 0.9994789958000183
        vf_loss: 0.29705015073219937
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.732258064516135
    gpu_util_percent0: 0.337741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662210379098964
    mean_env_wait_ms: 1.2150380033785488
    mean_inference_ms: 4.318319116159637
    mean_raw_obs_processing_ms: 0.37982238359580855
  time_since_restore: 4771.943668842316
  time_this_iter_s: 26.299107551574707
  time_total_s: 4771.943668842316
  timers:
    learn_throughput: 8400.351
    learn_time_ms: 19260.147
    sample_throughput: 23830.583
    sample_time_ms: 6789.259
    update_time_ms: 28.455
  timestamp: 1602747792
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: c9144_00000
  
2020-10-15 07:43:13,203	WARNING util.py:136 -- The `process_trial` operation took 0.5353567600250244 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    180 |          4771.94 | 29122560 |  277.722 |              305.323 |              128.354 |            783.166 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3098.7421294183955
    time_step_min: 2950
  date: 2020-10-15_07-43-39
  done: false
  episode_len_mean: 783.2424518689334
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.79525642004864
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 37294
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.395570986446985e-33
        cur_lr: 5.0e-05
        entropy: 0.0715114710231622
        entropy_coeff: 0.0005000000000000001
        kl: 0.003195387738135954
        model: {}
        policy_loss: -0.0064687159028835595
        total_loss: 0.514264759918054
        vf_explained_var: 0.9989972114562988
        vf_loss: 0.5207692111531893
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73870967741936
    gpu_util_percent0: 0.3593548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661889426027588
    mean_env_wait_ms: 1.214947743126369
    mean_inference_ms: 4.31811294441845
    mean_raw_obs_processing_ms: 0.37980920129109763
  time_since_restore: 4798.381065607071
  time_this_iter_s: 26.43739676475525
  time_total_s: 4798.381065607071
  timers:
    learn_throughput: 8396.264
    learn_time_ms: 19269.523
    sample_throughput: 23759.267
    sample_time_ms: 6809.638
    update_time_ms: 26.816
  timestamp: 1602747819
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: c9144_00000
  
2020-10-15 07:43:40,429	WARNING util.py:136 -- The `process_trial` operation took 0.5801572799682617 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    181 |          4798.38 | 29284352 |  277.795 |              305.323 |              128.354 |            783.242 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3098.1905067008383
    time_step_min: 2950
  date: 2020-10-15_07-44-06
  done: false
  episode_len_mean: 783.3292614621396
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.8802914591195
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 37493
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.697785493223493e-33
        cur_lr: 5.0e-05
        entropy: 0.0737106731782357
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007537641688334891
        total_loss: .inf
        vf_explained_var: 0.9990618228912354
        vf_loss: 0.532352457443873
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93225806451613
    gpu_util_percent0: 0.33903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661541773967224
    mean_env_wait_ms: 1.2148410429287322
    mean_inference_ms: 4.317865363624323
    mean_raw_obs_processing_ms: 0.3797934292876633
  time_since_restore: 4824.76819062233
  time_this_iter_s: 26.38712501525879
  time_total_s: 4824.76819062233
  timers:
    learn_throughput: 8394.875
    learn_time_ms: 19272.712
    sample_throughput: 23704.88
    sample_time_ms: 6825.261
    update_time_ms: 25.591
  timestamp: 1602747846
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:44:07,609	WARNING util.py:136 -- The `process_trial` operation took 0.50048828125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    182 |          4824.77 | 29446144 |   277.88 |              305.323 |              128.354 |            783.329 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3097.485743083738
    time_step_min: 2950
  date: 2020-10-15_07-44-34
  done: false
  episode_len_mean: 783.4524856900572
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 277.9884832531377
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 243
  episodes_total: 37736
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.546678239835241e-33
        cur_lr: 5.0e-05
        entropy: 0.0735589141647021
        entropy_coeff: 0.0005000000000000001
        kl: 0.004632070350150268
        model: {}
        policy_loss: -0.00610719282606927
        total_loss: 0.32083046187957126
        vf_explained_var: 0.9994897842407227
        vf_loss: 0.32697442670663196
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53125
    gpu_util_percent0: 0.280625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466116483830365
    mean_env_wait_ms: 1.2147090399416112
    mean_inference_ms: 4.317568786692039
    mean_raw_obs_processing_ms: 0.3797740540025068
  time_since_restore: 4851.173286676407
  time_this_iter_s: 26.40509605407715
  time_total_s: 4851.173286676407
  timers:
    learn_throughput: 8395.395
    learn_time_ms: 19271.518
    sample_throughput: 23661.185
    sample_time_ms: 6837.865
    update_time_ms: 25.747
  timestamp: 1602747874
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: c9144_00000
  
2020-10-15 07:44:34,872	WARNING util.py:136 -- The `process_trial` operation took 0.5152006149291992 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    183 |          4851.17 | 29607936 |  277.988 |              305.323 |              128.354 |            783.452 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3096.956341559984
    time_step_min: 2950
  date: 2020-10-15_07-45-01
  done: false
  episode_len_mean: 783.5554852320676
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.06958429229
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 37920
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7733391199176204e-33
        cur_lr: 5.0e-05
        entropy: 0.06612569404145081
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006352165461673091
        total_loss: .inf
        vf_explained_var: 0.9994874000549316
        vf_loss: 0.2731761187314987
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.822580645161292
    gpu_util_percent0: 0.3532258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660792361857417
    mean_env_wait_ms: 1.2146049743675296
    mean_inference_ms: 4.317335434987566
    mean_raw_obs_processing_ms: 0.3797580451656196
  time_since_restore: 4877.48820567131
  time_this_iter_s: 26.314918994903564
  time_total_s: 4877.48820567131
  timers:
    learn_throughput: 8399.765
    learn_time_ms: 19261.491
    sample_throughput: 23615.727
    sample_time_ms: 6851.028
    update_time_ms: 32.772
  timestamp: 1602747901
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:45:01,956	WARNING util.py:136 -- The `process_trial` operation took 0.5421507358551025 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    184 |          4877.49 | 29769728 |   278.07 |              305.323 |              128.354 |            783.555 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3096.4633614460995
    time_step_min: 2950
  date: 2020-10-15_07-45-28
  done: false
  episode_len_mean: 783.6304598908022
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.1417935656438
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 176
  episodes_total: 38096
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.160008679876429e-33
        cur_lr: 5.0e-05
        entropy: 0.07236136433978875
        entropy_coeff: 0.0005000000000000001
        kl: 0.00367399943449224
        model: {}
        policy_loss: -0.007123773510102183
        total_loss: 0.9323802342017492
        vf_explained_var: 0.9982280731201172
        vf_loss: 0.9395401875178019
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78709677419354
    gpu_util_percent0: 0.34258064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466049516602501
    mean_env_wait_ms: 1.2145097337065163
    mean_inference_ms: 4.317129165854858
    mean_raw_obs_processing_ms: 0.37974587279859645
  time_since_restore: 4903.840673685074
  time_this_iter_s: 26.352468013763428
  time_total_s: 4903.840673685074
  timers:
    learn_throughput: 8399.829
    learn_time_ms: 19261.344
    sample_throughput: 23597.527
    sample_time_ms: 6856.312
    update_time_ms: 35.023
  timestamp: 1602747928
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: c9144_00000
  
2020-10-15 07:45:29,043	WARNING util.py:136 -- The `process_trial` operation took 0.5517210960388184 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    185 |          4903.84 | 29931520 |  278.142 |              305.323 |              128.354 |             783.63 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3095.856788317354
    time_step_min: 2950
  date: 2020-10-15_07-45-55
  done: false
  episode_len_mean: 783.7259226392441
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.23365671118444
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 218
  episodes_total: 38314
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0800043399382145e-33
        cur_lr: 5.0e-05
        entropy: 0.07488355164726575
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008230563544202596
        total_loss: .inf
        vf_explained_var: 0.9991938471794128
        vf_loss: 0.5295515904823939
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29032258064516
    gpu_util_percent0: 0.31967741935483873
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660125992909362
    mean_env_wait_ms: 1.2143881756026988
    mean_inference_ms: 4.316858697637497
    mean_raw_obs_processing_ms: 0.37972799369308435
  time_since_restore: 4930.14937543869
  time_this_iter_s: 26.308701753616333
  time_total_s: 4930.14937543869
  timers:
    learn_throughput: 8394.02
    learn_time_ms: 19274.675
    sample_throughput: 23563.415
    sample_time_ms: 6866.237
    update_time_ms: 34.778
  timestamp: 1602747955
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:45:56,066	WARNING util.py:136 -- The `process_trial` operation took 0.5322446823120117 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    186 |          4930.15 | 30093312 |  278.234 |              305.323 |              128.354 |            783.726 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3095.2072762211433
    time_step_min: 2950
  date: 2020-10-15_07-46-22
  done: false
  episode_len_mean: 783.8454493565795
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.3326776482131
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 230
  episodes_total: 38544
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.120006509907322e-33
        cur_lr: 5.0e-05
        entropy: 0.06864633535345395
        entropy_coeff: 0.0005000000000000001
        kl: 0.005976866465061903
        model: {}
        policy_loss: -0.006719584117187575
        total_loss: 0.2650476209819317
        vf_explained_var: 0.9995550513267517
        vf_loss: 0.27180153752366704
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46451612903226
    gpu_util_percent0: 0.33032258064516123
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659757537900803
    mean_env_wait_ms: 1.2142611090729345
    mean_inference_ms: 4.316593955035797
    mean_raw_obs_processing_ms: 0.37970957963080865
  time_since_restore: 4956.484137296677
  time_this_iter_s: 26.33476185798645
  time_total_s: 4956.484137296677
  timers:
    learn_throughput: 8400.819
    learn_time_ms: 19259.074
    sample_throughput: 23569.257
    sample_time_ms: 6864.535
    update_time_ms: 34.144
  timestamp: 1602747982
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: c9144_00000
  
2020-10-15 07:46:23,148	WARNING util.py:136 -- The `process_trial` operation took 0.5681190490722656 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    187 |          4956.48 | 30255104 |  278.333 |              305.323 |              128.354 |            783.845 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3094.744829369183
    time_step_min: 2950
  date: 2020-10-15_07-46-49
  done: false
  episode_len_mean: 783.9221232080589
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.39943800656704
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 171
  episodes_total: 38715
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.120006509907322e-33
        cur_lr: 5.0e-05
        entropy: 0.08571012504398823
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010331035700801294
        total_loss: .inf
        vf_explained_var: 0.9981878399848938
        vf_loss: 0.8784174968798956
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809375
    gpu_util_percent0: 0.2940625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659436313300736
    mean_env_wait_ms: 1.214166187005226
    mean_inference_ms: 4.316397725856963
    mean_raw_obs_processing_ms: 0.3796970278253227
  time_since_restore: 4983.056465625763
  time_this_iter_s: 26.572328329086304
  time_total_s: 4983.056465625763
  timers:
    learn_throughput: 8394.212
    learn_time_ms: 19274.233
    sample_throughput: 23600.989
    sample_time_ms: 6855.306
    update_time_ms: 35.533
  timestamp: 1602748009
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:46:50,630	WARNING util.py:136 -- The `process_trial` operation took 0.5523254871368408 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    188 |          4983.06 | 30416896 |  278.399 |              305.323 |              128.354 |            783.922 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3094.53224437299
    time_step_min: 2950
  date: 2020-10-15_07-47-17
  done: false
  episode_len_mean: 783.9370598817785
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.4280624284479
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 195
  episodes_total: 38910
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6800097648609846e-33
        cur_lr: 5.0e-05
        entropy: 0.09044411219656467
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0097089833264666
        total_loss: .inf
        vf_explained_var: 0.9932078719139099
        vf_loss: 3.7116207480430603
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.448387096774198
    gpu_util_percent0: 0.37193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659123954756587
    mean_env_wait_ms: 1.2140617262228488
    mean_inference_ms: 4.3161850258596015
    mean_raw_obs_processing_ms: 0.379684279317942
  time_since_restore: 5009.601269483566
  time_this_iter_s: 26.544803857803345
  time_total_s: 5009.601269483566
  timers:
    learn_throughput: 8390.065
    learn_time_ms: 19283.761
    sample_throughput: 23604.266
    sample_time_ms: 6854.354
    update_time_ms: 33.243
  timestamp: 1602748037
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:47:17,926	WARNING util.py:136 -- The `process_trial` operation took 0.5517287254333496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    189 |           5009.6 | 30578688 |  278.428 |              305.323 |              128.354 |            783.937 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3094.0394312893163
    time_step_min: 2950
  date: 2020-10-15_07-47-44
  done: false
  episode_len_mean: 784.0365856774226
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.512997685911
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 231
  episodes_total: 39141
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.020014647291474e-33
        cur_lr: 5.0e-05
        entropy: 0.06806577928364277
        entropy_coeff: 0.0005000000000000001
        kl: 0.003294165168578426
        model: {}
        policy_loss: -0.00825881139220049
        total_loss: 0.7310702502727509
        vf_explained_var: 0.9988231658935547
        vf_loss: 0.739363099137942
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95483870967742
    gpu_util_percent0: 0.36129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658794011021786
    mean_env_wait_ms: 1.2139331490199627
    mean_inference_ms: 4.315914964322395
    mean_raw_obs_processing_ms: 0.37966604644823143
  time_since_restore: 5035.916478872299
  time_this_iter_s: 26.31520938873291
  time_total_s: 5035.916478872299
  timers:
    learn_throughput: 8381.862
    learn_time_ms: 19302.633
    sample_throughput: 23673.441
    sample_time_ms: 6834.325
    update_time_ms: 34.618
  timestamp: 1602748064
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: c9144_00000
  
2020-10-15 07:47:45,032	WARNING util.py:136 -- The `process_trial` operation took 0.5910308361053467 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    190 |          5035.92 | 30740480 |  278.513 |              305.323 |              128.354 |            784.037 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3093.530059788831
    time_step_min: 2950
  date: 2020-10-15_07-48-11
  done: false
  episode_len_mean: 784.1344941535333
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.59213127718465
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 39340
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.510007323645737e-33
        cur_lr: 5.0e-05
        entropy: 0.06211088007936875
        entropy_coeff: 0.0005000000000000001
        kl: 0.003982092835940421
        model: {}
        policy_loss: -0.006694547531272595
        total_loss: 0.3271636019150416
        vf_explained_var: 0.9993844628334045
        vf_loss: 0.3338892037669818
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.284375
    gpu_util_percent0: 0.3265625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465842221862086
    mean_env_wait_ms: 1.2138204851454457
    mean_inference_ms: 4.315680891003324
    mean_raw_obs_processing_ms: 0.37964981797475056
  time_since_restore: 5062.499386310577
  time_this_iter_s: 26.5829074382782
  time_total_s: 5062.499386310577
  timers:
    learn_throughput: 8374.988
    learn_time_ms: 19318.475
    sample_throughput: 23720.837
    sample_time_ms: 6820.67
    update_time_ms: 35.703
  timestamp: 1602748091
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: c9144_00000
  
2020-10-15 07:48:12,365	WARNING util.py:136 -- The `process_trial` operation took 0.5635583400726318 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    191 |           5062.5 | 30902272 |  278.592 |              305.323 |              128.354 |            784.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3093.0718392988324
    time_step_min: 2950
  date: 2020-10-15_07-48-38
  done: false
  episode_len_mean: 784.2117584531281
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.6616695912353
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 172
  episodes_total: 39512
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7550036618228685e-33
        cur_lr: 5.0e-05
        entropy: 0.06162939282755057
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007936739554376496
        total_loss: .inf
        vf_explained_var: 0.9994046688079834
        vf_loss: 0.30797653645277023
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.435483870967747
    gpu_util_percent0: 0.34322580645161294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658149510487367
    mean_env_wait_ms: 1.2137260585121243
    mean_inference_ms: 4.315493804135806
    mean_raw_obs_processing_ms: 0.3796378580698964
  time_since_restore: 5088.846879720688
  time_this_iter_s: 26.347493410110474
  time_total_s: 5088.846879720688
  timers:
    learn_throughput: 8369.992
    learn_time_ms: 19330.006
    sample_throughput: 23786.025
    sample_time_ms: 6801.977
    update_time_ms: 37.785
  timestamp: 1602748118
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:48:39,467	WARNING util.py:136 -- The `process_trial` operation took 0.5684018135070801 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    192 |          5088.85 | 31064064 |  278.662 |              305.323 |              128.354 |            784.212 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3092.5336273151065
    time_step_min: 2950
  date: 2020-10-15_07-49-05
  done: false
  episode_len_mean: 784.3132930513596
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.742614971467
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 208
  episodes_total: 39720
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6325054927343025e-33
        cur_lr: 5.0e-05
        entropy: 0.06361879035830498
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033390840593104563
        model: {}
        policy_loss: -0.007759507764906933
        total_loss: 0.4031963621576627
        vf_explained_var: 0.9993359446525574
        vf_loss: 0.41098766525586444
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06129032258065
    gpu_util_percent0: 0.3538709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657854722303404
    mean_env_wait_ms: 1.2136098192004252
    mean_inference_ms: 4.315256573245857
    mean_raw_obs_processing_ms: 0.3796220499291696
  time_since_restore: 5115.180636644363
  time_this_iter_s: 26.333756923675537
  time_total_s: 5115.180636644363
  timers:
    learn_throughput: 8373.804
    learn_time_ms: 19321.208
    sample_throughput: 23780.552
    sample_time_ms: 6803.543
    update_time_ms: 37.487
  timestamp: 1602748145
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: c9144_00000
  
2020-10-15 07:49:06,561	WARNING util.py:136 -- The `process_trial` operation took 0.5765223503112793 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    193 |          5115.18 | 31225856 |  278.743 |              305.323 |              128.354 |            784.313 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3091.943566365253
    time_step_min: 2950
  date: 2020-10-15_07-49-32
  done: false
  episode_len_mean: 784.4187396766605
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.82975508121916
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 238
  episodes_total: 39958
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3162527463671513e-33
        cur_lr: 5.0e-05
        entropy: 0.06678113527595997
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006632720633812521
        total_loss: .inf
        vf_explained_var: 0.998708188533783
        vf_loss: 0.8012447605530421
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34516129032258
    gpu_util_percent0: 0.2903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657475358469504
    mean_env_wait_ms: 1.213477995120951
    mean_inference_ms: 4.315004231286321
    mean_raw_obs_processing_ms: 0.3796048240157434
  time_since_restore: 5141.612325191498
  time_this_iter_s: 26.4316885471344
  time_total_s: 5141.612325191498
  timers:
    learn_throughput: 8359.533
    learn_time_ms: 19354.192
    sample_throughput: 23834.834
    sample_time_ms: 6788.048
    update_time_ms: 32.885
  timestamp: 1602748172
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:49:33,745	WARNING util.py:136 -- The `process_trial` operation took 0.5659482479095459 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    194 |          5141.61 | 31387648 |   278.83 |              305.323 |              128.354 |            784.419 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3091.496321420555
    time_step_min: 2950
  date: 2020-10-15_07-50-00
  done: false
  episode_len_mean: 784.5090451510017
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.89768284861975
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 40132
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9743791195507277e-33
        cur_lr: 5.0e-05
        entropy: 0.05743734228114287
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007244088032166474
        total_loss: .inf
        vf_explained_var: 0.9994306564331055
        vf_loss: 0.2972281326850255
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02258064516129
    gpu_util_percent0: 0.3393548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657169963665806
    mean_env_wait_ms: 1.2133795556911542
    mean_inference_ms: 4.314813348549775
    mean_raw_obs_processing_ms: 0.37959206126280887
  time_since_restore: 5167.994710922241
  time_this_iter_s: 26.382385730743408
  time_total_s: 5167.994710922241
  timers:
    learn_throughput: 8356.066
    learn_time_ms: 19362.221
    sample_throughput: 23855.174
    sample_time_ms: 6782.26
    update_time_ms: 32.881
  timestamp: 1602748200
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:50:01,008	WARNING util.py:136 -- The `process_trial` operation took 0.5998218059539795 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    195 |          5167.99 | 31549440 |  278.898 |              305.323 |              128.354 |            784.509 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3091.0104259755735
    time_step_min: 2950
  date: 2020-10-15_07-50-27
  done: false
  episode_len_mean: 784.5997916614996
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 278.96990014733507
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 40319
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9615686793260908e-33
        cur_lr: 5.0e-05
        entropy: 0.05998825499167045
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006757120960780109
        total_loss: .inf
        vf_explained_var: 0.9993411898612976
        vf_loss: 0.378958302239577
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.896874999999998
    gpu_util_percent0: 0.25125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465690807924921
    mean_env_wait_ms: 1.2132785504744654
    mean_inference_ms: 4.314631645411826
    mean_raw_obs_processing_ms: 0.3795806749505515
  time_since_restore: 5194.198009967804
  time_this_iter_s: 26.203299045562744
  time_total_s: 5194.198009967804
  timers:
    learn_throughput: 8359.325
    learn_time_ms: 19354.674
    sample_throughput: 23863.141
    sample_time_ms: 6779.996
    update_time_ms: 31.393
  timestamp: 1602748227
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:50:28,187	WARNING util.py:136 -- The `process_trial` operation took 0.6089882850646973 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    196 |           5194.2 | 31711232 |   278.97 |              305.323 |              128.354 |              784.6 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3090.4706201856607
    time_step_min: 2950
  date: 2020-10-15_07-50-54
  done: false
  episode_len_mean: 784.7002146081551
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.05168461047987
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 220
  episodes_total: 40539
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.442353018989137e-33
        cur_lr: 5.0e-05
        entropy: 0.06714940381546815
        entropy_coeff: 0.0005000000000000001
        kl: 0.00501680129673332
        model: {}
        policy_loss: -0.0068234831899947794
        total_loss: 0.5648053338130316
        vf_explained_var: 0.9991629719734192
        vf_loss: 0.5716624036431313
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.335483870967746
    gpu_util_percent0: 0.315483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465659754349051
    mean_env_wait_ms: 1.2131543234681683
    mean_inference_ms: 4.314391984830088
    mean_raw_obs_processing_ms: 0.37956458828769335
  time_since_restore: 5220.4491584300995
  time_this_iter_s: 26.251148462295532
  time_total_s: 5220.4491584300995
  timers:
    learn_throughput: 8370.621
    learn_time_ms: 19328.555
    sample_throughput: 23818.091
    sample_time_ms: 6792.82
    update_time_ms: 30.705
  timestamp: 1602748254
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: c9144_00000
  
2020-10-15 07:50:55,317	WARNING util.py:136 -- The `process_trial` operation took 0.607839822769165 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    197 |          5220.45 | 31873024 |  279.052 |              305.323 |              128.354 |              784.7 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3089.939254057505
    time_step_min: 2950
  date: 2020-10-15_07-51-21
  done: false
  episode_len_mean: 784.8113929640351
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.1355359690819
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 223
  episodes_total: 40762
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.442353018989137e-33
        cur_lr: 5.0e-05
        entropy: 0.06368117375920217
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032170334015972912
        model: {}
        policy_loss: -0.008662800527721023
        total_loss: 0.4113843118151029
        vf_explained_var: 0.9993184208869934
        vf_loss: 0.42007894814014435
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.532258064516128
    gpu_util_percent0: 0.32677419354838705
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465622538014692
    mean_env_wait_ms: 1.2130271579812903
    mean_inference_ms: 4.314150307595074
    mean_raw_obs_processing_ms: 0.3795474955075688
  time_since_restore: 5246.700553894043
  time_this_iter_s: 26.25139546394348
  time_total_s: 5246.700553894043
  timers:
    learn_throughput: 8388.025
    learn_time_ms: 19288.451
    sample_throughput: 23820.803
    sample_time_ms: 6792.046
    update_time_ms: 30.518
  timestamp: 1602748281
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: c9144_00000
  
2020-10-15 07:51:22,328	WARNING util.py:136 -- The `process_trial` operation took 0.572458028793335 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    198 |           5246.7 | 32034816 |  279.136 |              305.323 |              128.354 |            784.811 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3089.532412275339
    time_step_min: 2950
  date: 2020-10-15_07-51-48
  done: false
  episode_len_mean: 784.891790862448
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.1974780791053
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 40930
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2211765094945684e-33
        cur_lr: 5.0e-05
        entropy: 0.0599502669647336
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005882557423319668
        total_loss: .inf
        vf_explained_var: 0.9993043541908264
        vf_loss: 0.37960734218358994
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.083870967741937
    gpu_util_percent0: 0.3241935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655963413153206
    mean_env_wait_ms: 1.212933219809036
    mean_inference_ms: 4.3139791969582175
    mean_raw_obs_processing_ms: 0.37953619914377834
  time_since_restore: 5272.994870424271
  time_this_iter_s: 26.29431653022766
  time_total_s: 5272.994870424271
  timers:
    learn_throughput: 8399.266
    learn_time_ms: 19262.635
    sample_throughput: 23852.598
    sample_time_ms: 6782.993
    update_time_ms: 31.745
  timestamp: 1602748308
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:51:49,394	WARNING util.py:136 -- The `process_trial` operation took 0.5715377330780029 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    199 |          5272.99 | 32196608 |  279.197 |              305.323 |              128.354 |            784.892 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3089.044415780379
    time_step_min: 2950
  date: 2020-10-15_07-52-15
  done: false
  episode_len_mean: 784.99623091139
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.2710376304628
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 194
  episodes_total: 41124
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.331764764241853e-33
        cur_lr: 5.0e-05
        entropy: 0.05780729247877995
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007835680017403016
        total_loss: .inf
        vf_explained_var: 0.9997252821922302
        vf_loss: 0.16660077249010405
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.570967741935487
    gpu_util_percent0: 0.34258064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655728651478067
    mean_env_wait_ms: 1.212827551186692
    mean_inference_ms: 4.313796559806881
    mean_raw_obs_processing_ms: 0.3795251898409183
  time_since_restore: 5299.2258105278015
  time_this_iter_s: 26.230940103530884
  time_total_s: 5299.2258105278015
  timers:
    learn_throughput: 8407.294
    learn_time_ms: 19244.242
    sample_throughput: 23836.028
    sample_time_ms: 6787.708
    update_time_ms: 30.327
  timestamp: 1602748335
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:52:16,417	WARNING util.py:136 -- The `process_trial` operation took 0.5982177257537842 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    200 |          5299.23 | 32358400 |  279.271 |              305.323 |              128.354 |            784.996 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3088.479597269955
    time_step_min: 2950
  date: 2020-10-15_07-52-42
  done: false
  episode_len_mean: 785.1150339757696
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.35674203891745
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 229
  episodes_total: 41353
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.997647146362778e-33
        cur_lr: 5.0e-05
        entropy: 0.06018569692969322
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005886988869557778
        total_loss: .inf
        vf_explained_var: 0.9996140599250793
        vf_loss: 0.29958725968996686
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.209999999999997
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655390945207392
    mean_env_wait_ms: 1.2126965472165923
    mean_inference_ms: 4.313557507375136
    mean_raw_obs_processing_ms: 0.3795081358180347
  time_since_restore: 5325.017551422119
  time_this_iter_s: 25.791740894317627
  time_total_s: 5325.017551422119
  timers:
    learn_throughput: 8438.985
    learn_time_ms: 19171.974
    sample_throughput: 23829.331
    sample_time_ms: 6789.616
    update_time_ms: 28.973
  timestamp: 1602748362
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:52:42,957	WARNING util.py:136 -- The `process_trial` operation took 0.5492157936096191 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    201 |          5325.02 | 32520192 |  279.357 |              305.323 |              128.354 |            785.115 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3088.042318994171
    time_step_min: 2950
  date: 2020-10-15_07-53-09
  done: false
  episode_len_mean: 785.2059779077323
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.4238004913769
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 200
  episodes_total: 41553
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.496470719544168e-33
        cur_lr: 5.0e-05
        entropy: 0.060491097470124565
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040001367645648616
        model: {}
        policy_loss: -0.007562688709488914
        total_loss: 0.6479777644077936
        vf_explained_var: 0.9988073706626892
        vf_loss: 0.6555706908305486
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.977419354838712
    gpu_util_percent0: 0.3370967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655066663840297
    mean_env_wait_ms: 1.2125814569869213
    mean_inference_ms: 4.31334797378244
    mean_raw_obs_processing_ms: 0.3794932096947402
  time_since_restore: 5351.391099691391
  time_this_iter_s: 26.37354826927185
  time_total_s: 5351.391099691391
  timers:
    learn_throughput: 8437.382
    learn_time_ms: 19175.615
    sample_throughput: 23832.996
    sample_time_ms: 6788.572
    update_time_ms: 27.697
  timestamp: 1602748389
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: c9144_00000
  
2020-10-15 07:53:10,207	WARNING util.py:136 -- The `process_trial` operation took 0.5936644077301025 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    202 |          5351.39 | 32681984 |  279.424 |              305.323 |              128.354 |            785.206 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3087.6790194526397
    time_step_min: 2950
  date: 2020-10-15_07-53-36
  done: false
  episode_len_mean: 785.2936778028088
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.4835444024679
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 173
  episodes_total: 41726
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.748235359772084e-33
        cur_lr: 5.0e-05
        entropy: 0.0500148336092631
        entropy_coeff: 0.0005000000000000001
        kl: 0.0026454742377003035
        model: {}
        policy_loss: -0.006292531795528096
        total_loss: 0.32321450859308243
        vf_explained_var: 0.9994382858276367
        vf_loss: 0.32953204462925595
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.37741935483871
    gpu_util_percent0: 0.33129032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654814913726702
    mean_env_wait_ms: 1.2124849551338177
    mean_inference_ms: 4.313185527796143
    mean_raw_obs_processing_ms: 0.3794825140818007
  time_since_restore: 5377.607403278351
  time_this_iter_s: 26.21630358695984
  time_total_s: 5377.607403278351
  timers:
    learn_throughput: 8438.186
    learn_time_ms: 19173.789
    sample_throughput: 23875.76
    sample_time_ms: 6776.413
    update_time_ms: 27.314
  timestamp: 1602748416
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: c9144_00000
  
2020-10-15 07:53:37,196	WARNING util.py:136 -- The `process_trial` operation took 0.5810916423797607 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    203 |          5377.61 | 32843776 |  279.484 |              305.323 |              128.354 |            785.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3087.1876834944505
    time_step_min: 2950
  date: 2020-10-15_07-54-03
  done: false
  episode_len_mean: 785.3926305747675
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.5576273105488
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 204
  episodes_total: 41930
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.874117679886042e-33
        cur_lr: 5.0e-05
        entropy: 0.05348397046327591
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007374368569192787
        total_loss: .inf
        vf_explained_var: 0.9995326399803162
        vf_loss: 0.2965693362057209
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.690322580645166
    gpu_util_percent0: 0.27838709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654594891647657
    mean_env_wait_ms: 1.2123706153773992
    mean_inference_ms: 4.312979313766138
    mean_raw_obs_processing_ms: 0.37946904449266033
  time_since_restore: 5403.9634001255035
  time_this_iter_s: 26.35599684715271
  time_total_s: 5403.9634001255035
  timers:
    learn_throughput: 8446.787
    learn_time_ms: 19154.265
    sample_throughput: 23838.39
    sample_time_ms: 6787.036
    update_time_ms: 27.213
  timestamp: 1602748443
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:54:04,460	WARNING util.py:136 -- The `process_trial` operation took 0.6053354740142822 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    204 |          5403.96 | 33005568 |  279.558 |              305.323 |              128.354 |            785.393 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3086.634464751958
    time_step_min: 2950
  date: 2020-10-15_07-54-30
  done: false
  episode_len_mean: 785.5065575714455
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.6405750856124
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 235
  episodes_total: 42165
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8111765198290637e-33
        cur_lr: 5.0e-05
        entropy: 0.05512635099391142
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032927243737503886
        model: {}
        policy_loss: -0.006658052259202425
        total_loss: 0.2649196609854698
        vf_explained_var: 0.9996399879455566
        vf_loss: 0.27160527432958287
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.51935483870968
    gpu_util_percent0: 0.26419354838709674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654243268173137
    mean_env_wait_ms: 1.2122364834693837
    mean_inference_ms: 4.3127553463719375
    mean_raw_obs_processing_ms: 0.37945306520725086
  time_since_restore: 5430.012022018433
  time_this_iter_s: 26.048621892929077
  time_total_s: 5430.012022018433
  timers:
    learn_throughput: 8463.78
    learn_time_ms: 19115.809
    sample_throughput: 23824.024
    sample_time_ms: 6791.128
    update_time_ms: 26.995
  timestamp: 1602748470
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: c9144_00000
  
2020-10-15 07:54:31,257	WARNING util.py:136 -- The `process_trial` operation took 0.5567948818206787 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    205 |          5430.01 | 33167360 |  279.641 |              305.323 |              128.354 |            785.507 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3086.204471861779
    time_step_min: 2950
  date: 2020-10-15_07-54-57
  done: false
  episode_len_mean: 785.5996599282071
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.705363668806
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 42344
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4055882599145318e-33
        cur_lr: 5.0e-05
        entropy: 0.05092105610917012
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0039535091976479935
        total_loss: .inf
        vf_explained_var: 0.9996992945671082
        vf_loss: 0.1637899950146675
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.084375
    gpu_util_percent0: 0.28781249999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653956198906465
    mean_env_wait_ms: 1.212133716774322
    mean_inference_ms: 4.312580626411017
    mean_raw_obs_processing_ms: 0.3794412315164448
  time_since_restore: 5456.546366453171
  time_this_iter_s: 26.53434443473816
  time_total_s: 5456.546366453171
  timers:
    learn_throughput: 8453.889
    learn_time_ms: 19138.175
    sample_throughput: 23825.531
    sample_time_ms: 6790.699
    update_time_ms: 28.488
  timestamp: 1602748497
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:54:58,559	WARNING util.py:136 -- The `process_trial` operation took 0.563408374786377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    206 |          5456.55 | 33329152 |  279.705 |              305.323 |              128.354 |              785.6 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3085.7749588138386
    time_step_min: 2950
  date: 2020-10-15_07-55-24
  done: false
  episode_len_mean: 785.693874191652
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.76945112500664
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 42525
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.108382389871797e-33
        cur_lr: 5.0e-05
        entropy: 0.05177450738847256
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007501121760772851
        total_loss: .inf
        vf_explained_var: 0.9996964335441589
        vf_loss: 0.16668491438031197
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.200000000000003
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653735827200617
    mean_env_wait_ms: 1.212033682888243
    mean_inference_ms: 4.312423506735168
    mean_raw_obs_processing_ms: 0.3794310804117667
  time_since_restore: 5482.845908880234
  time_this_iter_s: 26.29954242706299
  time_total_s: 5482.845908880234
  timers:
    learn_throughput: 8443.962
    learn_time_ms: 19160.673
    sample_throughput: 23881.137
    sample_time_ms: 6774.887
    update_time_ms: 30.848
  timestamp: 1602748524
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:55:25,634	WARNING util.py:136 -- The `process_trial` operation took 0.5745272636413574 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    207 |          5482.85 | 33490944 |  279.769 |              305.323 |              128.354 |            785.694 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3085.276799512949
    time_step_min: 2950
  date: 2020-10-15_07-55-51
  done: false
  episode_len_mean: 785.8071406845886
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.84563399134885
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 216
  episodes_total: 42741
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.162573584807696e-33
        cur_lr: 5.0e-05
        entropy: 0.054002219811081886
        entropy_coeff: 0.0005000000000000001
        kl: 0.004653715489742656
        model: {}
        policy_loss: -0.006673100947712858
        total_loss: 0.12898264142374197
        vf_explained_var: 0.9997804760932922
        vf_loss: 0.1356827405591806
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.535483870967745
    gpu_util_percent0: 0.332258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653475019280202
    mean_env_wait_ms: 1.2119108046353422
    mean_inference_ms: 4.312210572785158
    mean_raw_obs_processing_ms: 0.3794162487341517
  time_since_restore: 5508.937906265259
  time_this_iter_s: 26.091997385025024
  time_total_s: 5508.937906265259
  timers:
    learn_throughput: 8444.502
    learn_time_ms: 19159.449
    sample_throughput: 23901.668
    sample_time_ms: 6769.067
    update_time_ms: 30.091
  timestamp: 1602748551
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: c9144_00000
  
2020-10-15 07:55:52,495	WARNING util.py:136 -- The `process_trial` operation took 0.573810338973999 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    208 |          5508.94 | 33652736 |  279.846 |              305.323 |              128.354 |            785.807 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3084.752166216342
    time_step_min: 2950
  date: 2020-10-15_07-56-18
  done: false
  episode_len_mean: 785.9210556939046
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.92379258406675
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 226
  episodes_total: 42967
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.581286792403848e-33
        cur_lr: 5.0e-05
        entropy: 0.05239995755255222
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004339841449109372
        total_loss: .inf
        vf_explained_var: 0.9994184970855713
        vf_loss: 0.3613383745153745
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.809677419354838
    gpu_util_percent0: 0.3754838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653112999904927
    mean_env_wait_ms: 1.2117796856195455
    mean_inference_ms: 4.311993255137695
    mean_raw_obs_processing_ms: 0.3794012171779109
  time_since_restore: 5535.240644693375
  time_this_iter_s: 26.302738428115845
  time_total_s: 5535.240644693375
  timers:
    learn_throughput: 8440.069
    learn_time_ms: 19169.512
    sample_throughput: 23904.25
    sample_time_ms: 6768.336
    update_time_ms: 28.956
  timestamp: 1602748578
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:56:19,633	WARNING util.py:136 -- The `process_trial` operation took 0.6324007511138916 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    209 |          5535.24 | 33814528 |  279.924 |              305.323 |              128.354 |            785.921 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3084.356238110704
    time_step_min: 2950
  date: 2020-10-15_07-56-46
  done: false
  episode_len_mean: 786.0097123386106
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 279.9826366865147
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 43141
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3719301886057722e-33
        cur_lr: 5.0e-05
        entropy: 0.04993097763508558
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0072281876928173006
        total_loss: .inf
        vf_explained_var: 0.999370813369751
        vf_loss: 0.334176408747832
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.596874999999997
    gpu_util_percent0: 0.28312499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652886993870753
    mean_env_wait_ms: 1.2116812603598828
    mean_inference_ms: 4.311848056262659
    mean_raw_obs_processing_ms: 0.3793915169404409
  time_since_restore: 5561.694643735886
  time_this_iter_s: 26.453999042510986
  time_total_s: 5561.694643735886
  timers:
    learn_throughput: 8425.922
    learn_time_ms: 19201.698
    sample_throughput: 23925.503
    sample_time_ms: 6762.324
    update_time_ms: 31.296
  timestamp: 1602748606
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:56:47,030	WARNING util.py:136 -- The `process_trial` operation took 0.5767064094543457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    210 |          5561.69 | 33976320 |  279.983 |              305.323 |              128.354 |             786.01 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3083.9287859188767
    time_step_min: 2950
  date: 2020-10-15_07-57-13
  done: false
  episode_len_mean: 786.1091005608512
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.0469926490422
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 186
  episodes_total: 43327
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5578952829086575e-33
        cur_lr: 5.0e-05
        entropy: 0.04918144146601359
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006311522098258138
        total_loss: .inf
        vf_explained_var: 0.9997596144676208
        vf_loss: 0.15222185229261717
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.69032258064517
    gpu_util_percent0: 0.37064516129032254
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465264570336328
    mean_env_wait_ms: 1.2115774283117677
    mean_inference_ms: 4.311685306483773
    mean_raw_obs_processing_ms: 0.37938125328621697
  time_since_restore: 5588.180523395538
  time_this_iter_s: 26.48587965965271
  time_total_s: 5588.180523395538
  timers:
    learn_throughput: 8404.962
    learn_time_ms: 19249.582
    sample_throughput: 23852.911
    sample_time_ms: 6782.904
    update_time_ms: 31.901
  timestamp: 1602748633
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:57:14,279	WARNING util.py:136 -- The `process_trial` operation took 0.562532901763916 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    211 |          5588.18 | 34138112 |  280.047 |              305.323 |              128.354 |            786.109 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3083.427327251419
    time_step_min: 2950
  date: 2020-10-15_07-57-40
  done: false
  episode_len_mean: 786.2214594048494
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.12227088111086
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 225
  episodes_total: 43552
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.336842924362988e-33
        cur_lr: 5.0e-05
        entropy: 0.05512580337623755
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008972132850127915
        total_loss: .inf
        vf_explained_var: 0.9994284510612488
        vf_loss: 0.3807741502920787
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.465624999999996
    gpu_util_percent0: 0.2878125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652387624497148
    mean_env_wait_ms: 1.211448191010322
    mean_inference_ms: 4.311475402848812
    mean_raw_obs_processing_ms: 0.37936644974735595
  time_since_restore: 5614.838407278061
  time_this_iter_s: 26.657883882522583
  time_total_s: 5614.838407278061
  timers:
    learn_throughput: 8407.521
    learn_time_ms: 19243.722
    sample_throughput: 23771.748
    sample_time_ms: 6806.062
    update_time_ms: 40.403
  timestamp: 1602748660
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:57:41,792	WARNING util.py:136 -- The `process_trial` operation took 0.6518964767456055 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    212 |          5614.84 | 34299904 |  280.122 |              305.323 |              128.354 |            786.221 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3082.959819795097
    time_step_min: 2950
  date: 2020-10-15_07-58-08
  done: false
  episode_len_mean: 786.3306446084592
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.1929437648196
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 43763
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.005264386544479e-33
        cur_lr: 5.0e-05
        entropy: 0.05696206260472536
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005872677866136655
        total_loss: .inf
        vf_explained_var: 0.9997172951698303
        vf_loss: 0.16403467083970705
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08064516129033
    gpu_util_percent0: 0.3441935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652077680964068
    mean_env_wait_ms: 1.2113245704852182
    mean_inference_ms: 4.311283996130529
    mean_raw_obs_processing_ms: 0.3793525063439987
  time_since_restore: 5641.15345287323
  time_this_iter_s: 26.315045595169067
  time_total_s: 5641.15345287323
  timers:
    learn_throughput: 8408.963
    learn_time_ms: 19240.423
    sample_throughput: 23726.799
    sample_time_ms: 6818.956
    update_time_ms: 41.018
  timestamp: 1602748688
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:58:08,886	WARNING util.py:136 -- The `process_trial` operation took 0.5679175853729248 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    213 |          5641.15 | 34461696 |  280.193 |              305.323 |              128.354 |            786.331 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3082.643395667722
    time_step_min: 2950
  date: 2020-10-15_07-58-35
  done: false
  episode_len_mean: 786.3841549456051
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.23550885062565
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 175
  episodes_total: 43938
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2007896579816718e-32
        cur_lr: 5.0e-05
        entropy: 0.09419150898853938
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008216452416187773
        total_loss: .inf
        vf_explained_var: 0.9962122440338135
        vf_loss: 1.844534198443095
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56451612903226
    gpu_util_percent0: 0.35935483870967744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465184179926654
    mean_env_wait_ms: 1.211226082042256
    mean_inference_ms: 4.311139503565771
    mean_raw_obs_processing_ms: 0.3793428545150055
  time_since_restore: 5667.537375450134
  time_this_iter_s: 26.383922576904297
  time_total_s: 5667.537375450134
  timers:
    learn_throughput: 8406.21
    learn_time_ms: 19246.723
    sample_throughput: 23742.276
    sample_time_ms: 6814.511
    update_time_ms: 40.977
  timestamp: 1602748715
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:58:36,123	WARNING util.py:136 -- The `process_trial` operation took 0.6465353965759277 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    214 |          5667.54 | 34623488 |  280.236 |              305.323 |              128.354 |            786.384 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3082.654341419179
    time_step_min: 2950
  date: 2020-10-15_07-59-02
  done: false
  episode_len_mean: 786.3784120511949
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.23118373679023
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 207
  episodes_total: 44145
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.801184486972508e-32
        cur_lr: 5.0e-05
        entropy: 0.12383660487830639
        entropy_coeff: 0.0005000000000000001
        kl: 0.005229551771966119
        model: {}
        policy_loss: -0.01007530624823024
        total_loss: 7.091117103894551
        vf_explained_var: 0.987183153629303
        vf_loss: 7.101254423459371
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0483870967742
    gpu_util_percent0: 0.3319354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651628895301355
    mean_env_wait_ms: 1.2111103240369618
    mean_inference_ms: 4.3109588326573
    mean_raw_obs_processing_ms: 0.3793310923949302
  time_since_restore: 5694.044631719589
  time_this_iter_s: 26.507256269454956
  time_total_s: 5694.044631719589
  timers:
    learn_throughput: 8386.303
    learn_time_ms: 19292.41
    sample_throughput: 23738.764
    sample_time_ms: 6815.519
    update_time_ms: 39.326
  timestamp: 1602748742
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: c9144_00000
  
2020-10-15 07:59:03,562	WARNING util.py:136 -- The `process_trial` operation took 0.6265296936035156 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    215 |          5694.04 | 34785280 |  280.231 |              305.323 |              128.354 |            786.378 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3082.4353718486445
    time_step_min: 2950
  date: 2020-10-15_07-59-29
  done: false
  episode_len_mean: 786.4147044906604
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.2651334780399
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 236
  episodes_total: 44381
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.801184486972508e-32
        cur_lr: 5.0e-05
        entropy: 0.10183153363565604
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007719908491708338
        total_loss: .inf
        vf_explained_var: 0.9935219883918762
        vf_loss: 3.7796889543533325
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.629032258064512
    gpu_util_percent0: 0.31032258064516133
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465130416097981
    mean_env_wait_ms: 1.2109771280747343
    mean_inference_ms: 4.310754771542896
    mean_raw_obs_processing_ms: 0.37931593315635126
  time_since_restore: 5720.469998836517
  time_this_iter_s: 26.4253671169281
  time_total_s: 5720.469998836517
  timers:
    learn_throughput: 8386.864
    learn_time_ms: 19291.12
    sample_throughput: 23739.728
    sample_time_ms: 6815.242
    update_time_ms: 37.646
  timestamp: 1602748769
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 07:59:30,884	WARNING util.py:136 -- The `process_trial` operation took 0.6121537685394287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    216 |          5720.47 | 34947072 |  280.265 |              305.323 |              128.354 |            786.415 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3082.197197017339
    time_step_min: 2950
  date: 2020-10-15_07-59-57
  done: false
  episode_len_mean: 786.4676945173815
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.30275895697025
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 178
  episodes_total: 44559
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7017767304587616e-32
        cur_lr: 5.0e-05
        entropy: 0.08237642173965772
        entropy_coeff: 0.0005000000000000001
        kl: 0.004329368957163145
        model: {}
        policy_loss: -0.008706455768939728
        total_loss: 1.9171506663163502
        vf_explained_var: 0.9961485862731934
        vf_loss: 1.9258982837200165
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.190624999999997
    gpu_util_percent0: 0.38875000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651062598605893
    mean_env_wait_ms: 1.2108758104762214
    mean_inference_ms: 4.310600814268413
    mean_raw_obs_processing_ms: 0.37930558663601543
  time_since_restore: 5747.0271718502045
  time_this_iter_s: 26.557173013687134
  time_total_s: 5747.0271718502045
  timers:
    learn_throughput: 8375.76
    learn_time_ms: 19316.694
    sample_throughput: 23745.177
    sample_time_ms: 6813.678
    update_time_ms: 37.702
  timestamp: 1602748797
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: c9144_00000
  
2020-10-15 07:59:58,474	WARNING util.py:136 -- The `process_trial` operation took 0.6537220478057861 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    217 |          5747.03 | 35108864 |  280.303 |              305.323 |              128.354 |            786.468 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3081.8758080751595
    time_step_min: 2950
  date: 2020-10-15_08-00-24
  done: false
  episode_len_mean: 786.5254582029504
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.35227554941457
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 44740
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3508883652293808e-32
        cur_lr: 5.0e-05
        entropy: 0.0777352595080932
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007854213346339142
        total_loss: .inf
        vf_explained_var: 0.9975881576538086
        vf_loss: 1.2448266247908275
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.648387096774197
    gpu_util_percent0: 0.3887096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465085568416343
    mean_env_wait_ms: 1.2107781192654365
    mean_inference_ms: 4.310459200482228
    mean_raw_obs_processing_ms: 0.37929670312478664
  time_since_restore: 5773.298209190369
  time_this_iter_s: 26.271037340164185
  time_total_s: 5773.298209190369
  timers:
    learn_throughput: 8370.762
    learn_time_ms: 19328.227
    sample_throughput: 23725.0
    sample_time_ms: 6819.473
    update_time_ms: 37.284
  timestamp: 1602748824
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:00:25,570	WARNING util.py:136 -- The `process_trial` operation took 0.6205353736877441 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    218 |           5773.3 | 35270656 |  280.352 |              305.323 |              128.354 |            786.525 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3081.4652317143746
    time_step_min: 2950
  date: 2020-10-15_08-00-51
  done: false
  episode_len_mean: 786.6151775983631
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.41748572668706
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 221
  episodes_total: 44961
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.026332547844071e-32
        cur_lr: 5.0e-05
        entropy: 0.06685111795862515
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033884806713710227
        model: {}
        policy_loss: -0.007780240140467261
        total_loss: 0.8479311615228653
        vf_explained_var: 0.9985880851745605
        vf_loss: 0.8557448387145996
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77741935483871
    gpu_util_percent0: 0.33967741935483864
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650614501820064
    mean_env_wait_ms: 1.210654527355759
    mean_inference_ms: 4.310269472064262
    mean_raw_obs_processing_ms: 0.37928358566761594
  time_since_restore: 5799.669410943985
  time_this_iter_s: 26.371201753616333
  time_total_s: 5799.669410943985
  timers:
    learn_throughput: 8369.612
    learn_time_ms: 19330.883
    sample_throughput: 23721.986
    sample_time_ms: 6820.34
    update_time_ms: 38.977
  timestamp: 1602748851
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: c9144_00000
  
2020-10-15 08:00:52,772	WARNING util.py:136 -- The `process_trial` operation took 0.6242334842681885 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    219 |          5799.67 | 35432448 |  280.417 |              305.323 |              128.354 |            786.615 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3081.012514119914
    time_step_min: 2950
  date: 2020-10-15_08-01-18
  done: false
  episode_len_mean: 786.7182188385269
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.48581333877013
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 223
  episodes_total: 45184
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0131662739220355e-32
        cur_lr: 5.0e-05
        entropy: 0.058017916356523834
        entropy_coeff: 0.0005000000000000001
        kl: 0.002846617232232044
        model: {}
        policy_loss: -0.007839270493680791
        total_loss: 0.4887346799174945
        vf_explained_var: 0.9991560578346252
        vf_loss: 0.49660296986500424
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.929032258064513
    gpu_util_percent0: 0.37451612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146503077626655
    mean_env_wait_ms: 1.2105273750276624
    mean_inference_ms: 4.310079971961341
    mean_raw_obs_processing_ms: 0.37926981310497565
  time_since_restore: 5825.320472002029
  time_this_iter_s: 25.651061058044434
  time_total_s: 5825.320472002029
  timers:
    learn_throughput: 8400.548
    learn_time_ms: 19259.696
    sample_throughput: 23750.451
    sample_time_ms: 6812.166
    update_time_ms: 37.412
  timestamp: 1602748878
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: c9144_00000
  
2020-10-15 08:01:19,407	WARNING util.py:136 -- The `process_trial` operation took 0.6041488647460938 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    220 |          5825.32 | 35594240 |  280.486 |              305.323 |              128.354 |            786.718 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3080.6615259023915
    time_step_min: 2950
  date: 2020-10-15_08-01-45
  done: false
  episode_len_mean: 786.7995767102449
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.53947686926807
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 175
  episodes_total: 45359
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.0658313696101776e-33
        cur_lr: 5.0e-05
        entropy: 0.052406394543747105
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005065112801579137
        total_loss: .inf
        vf_explained_var: 0.9994017481803894
        vf_loss: 0.32241304715474445
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.733333333333334
    gpu_util_percent0: 0.28933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650068378586736
    mean_env_wait_ms: 1.2104298070154567
    mean_inference_ms: 4.30994208623624
    mean_raw_obs_processing_ms: 0.379260471249485
  time_since_restore: 5851.394405841827
  time_this_iter_s: 26.073933839797974
  time_total_s: 5851.394405841827
  timers:
    learn_throughput: 8412.336
    learn_time_ms: 19232.707
    sample_throughput: 23817.201
    sample_time_ms: 6793.074
    update_time_ms: 39.079
  timestamp: 1602748905
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:01:46,395	WARNING util.py:136 -- The `process_trial` operation took 0.6061532497406006 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    221 |          5851.39 | 35756032 |  280.539 |              305.323 |              128.354 |              786.8 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3080.2858429632233
    time_step_min: 2950
  date: 2020-10-15_08-02-12
  done: false
  episode_len_mean: 786.888349834259
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.5978475067448
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 194
  episodes_total: 45553
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.59874705441527e-33
        cur_lr: 5.0e-05
        entropy: 0.0503522145251433
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043153498554602265
        model: {}
        policy_loss: -0.0071292614641909795
        total_loss: 0.3415401304761569
        vf_explained_var: 0.9994111061096191
        vf_loss: 0.3486945703625679
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.990625
    gpu_util_percent0: 0.28
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649916123157322
    mean_env_wait_ms: 1.2103240141464044
    mean_inference_ms: 4.3097898287669905
    mean_raw_obs_processing_ms: 0.37925045030553695
  time_since_restore: 5877.864333868027
  time_this_iter_s: 26.46992802619934
  time_total_s: 5877.864333868027
  timers:
    learn_throughput: 8413.176
    learn_time_ms: 19230.787
    sample_throughput: 23877.767
    sample_time_ms: 6775.843
    update_time_ms: 32.273
  timestamp: 1602748932
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: c9144_00000
  
2020-10-15 08:02:13,718	WARNING util.py:136 -- The `process_trial` operation took 0.6481649875640869 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    222 |          5877.86 | 35917824 |  280.598 |              305.323 |              128.354 |            786.888 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3079.818471964149
    time_step_min: 2950
  date: 2020-10-15_08-02-40
  done: false
  episode_len_mean: 786.9944298820445
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.6682420535632
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 227
  episodes_total: 45780
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.799373527207635e-33
        cur_lr: 5.0e-05
        entropy: 0.05314105407645305
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006636315384336437
        total_loss: .inf
        vf_explained_var: 0.9996911883354187
        vf_loss: 0.21119053289294243
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.183870967741935
    gpu_util_percent0: 0.3258064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464961587534554
    mean_env_wait_ms: 1.2101940066606032
    mean_inference_ms: 4.30960287377029
    mean_raw_obs_processing_ms: 0.3792368682229112
  time_since_restore: 5904.299200773239
  time_this_iter_s: 26.434866905212402
  time_total_s: 5904.299200773239
  timers:
    learn_throughput: 8407.544
    learn_time_ms: 19243.67
    sample_throughput: 23888.202
    sample_time_ms: 6772.883
    update_time_ms: 33.832
  timestamp: 1602748960
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:02:40,981	WARNING util.py:136 -- The `process_trial` operation took 0.6096127033233643 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    223 |           5904.3 | 36079616 |  280.668 |              305.323 |              128.354 |            786.994 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3079.4340567249296
    time_step_min: 2950
  date: 2020-10-15_08-03-07
  done: false
  episode_len_mean: 787.0868496606926
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.7273232586875
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 196
  episodes_total: 45976
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.699060290811451e-33
        cur_lr: 5.0e-05
        entropy: 0.05041625816375017
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006367286780005088
        total_loss: .inf
        vf_explained_var: 0.999671220779419
        vf_loss: 0.21105295171340308
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.867741935483878
    gpu_util_percent0: 0.2587096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649365233718484
    mean_env_wait_ms: 1.2100831101758243
    mean_inference_ms: 4.309443625222255
    mean_raw_obs_processing_ms: 0.37922539184755355
  time_since_restore: 5930.7359256744385
  time_this_iter_s: 26.43672490119934
  time_total_s: 5930.7359256744385
  timers:
    learn_throughput: 8414.421
    learn_time_ms: 19227.942
    sample_throughput: 23824.046
    sample_time_ms: 6791.122
    update_time_ms: 32.153
  timestamp: 1602748987
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:03:08,248	WARNING util.py:136 -- The `process_trial` operation took 0.601355791091919 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    224 |          5930.74 | 36241408 |  280.727 |              305.323 |              128.354 |            787.087 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3079.0836802845
    time_step_min: 2950
  date: 2020-10-15_08-03-34
  done: false
  episode_len_mean: 787.166583605989
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.7802006544613
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 175
  episodes_total: 46151
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.548590436217177e-33
        cur_lr: 5.0e-05
        entropy: 0.054030911376078926
        entropy_coeff: 0.0005000000000000001
        kl: 0.005194992564308147
        model: {}
        policy_loss: -0.006980937692181517
        total_loss: 0.2971293292939663
        vf_explained_var: 0.9994872212409973
        vf_loss: 0.3041372907658418
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.153125
    gpu_util_percent0: 0.3496875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649145487230694
    mean_env_wait_ms: 1.209985982222765
    mean_inference_ms: 4.309313257717069
    mean_raw_obs_processing_ms: 0.3792165866061471
  time_since_restore: 5957.481163978577
  time_this_iter_s: 26.745238304138184
  time_total_s: 5957.481163978577
  timers:
    learn_throughput: 8416.013
    learn_time_ms: 19224.305
    sample_throughput: 23760.94
    sample_time_ms: 6809.158
    update_time_ms: 32.132
  timestamp: 1602749014
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: c9144_00000
  
2020-10-15 08:03:35,844	WARNING util.py:136 -- The `process_trial` operation took 0.6093473434448242 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    225 |          5957.48 | 36403200 |   280.78 |              305.323 |              128.354 |            787.167 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3078.685860483192
    time_step_min: 2950
  date: 2020-10-15_08-04-02
  done: false
  episode_len_mean: 787.2585864687608
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.8403130807558
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 201
  episodes_total: 46352
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.548590436217177e-33
        cur_lr: 5.0e-05
        entropy: 0.0579613900432984
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005644336745414573
        total_loss: .inf
        vf_explained_var: 0.9993662238121033
        vf_loss: 0.37889518092075986
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.587096774193547
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648983826345274
    mean_env_wait_ms: 1.2098763391023937
    mean_inference_ms: 4.309163093347858
    mean_raw_obs_processing_ms: 0.37920651221781937
  time_since_restore: 5984.133565187454
  time_this_iter_s: 26.652401208877563
  time_total_s: 5984.133565187454
  timers:
    learn_throughput: 8420.603
    learn_time_ms: 19213.826
    sample_throughput: 23661.427
    sample_time_ms: 6837.796
    update_time_ms: 34.093
  timestamp: 1602749042
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:04:03,323	WARNING util.py:136 -- The `process_trial` operation took 0.6107220649719238 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    226 |          5984.13 | 36564992 |   280.84 |              305.323 |              128.354 |            787.259 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3078.235095709713
    time_step_min: 2950
  date: 2020-10-15_08-04-29
  done: false
  episode_len_mean: 787.3617277059808
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.90865158389096
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 230
  episodes_total: 46582
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2822885654325768e-32
        cur_lr: 5.0e-05
        entropy: 0.06102301459759474
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051702830630044145
        model: {}
        policy_loss: -0.007044822891960696
        total_loss: 0.25327081854144734
        vf_explained_var: 0.9995868802070618
        vf_loss: 0.26034615313013393
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.99677419354839
    gpu_util_percent0: 0.32903225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464869714074732
    mean_env_wait_ms: 1.2097452397296478
    mean_inference_ms: 4.308985438740649
    mean_raw_obs_processing_ms: 0.37919324161919404
  time_since_restore: 6010.414363145828
  time_this_iter_s: 26.280797958374023
  time_total_s: 6010.414363145828
  timers:
    learn_throughput: 8433.64
    learn_time_ms: 19184.124
    sample_throughput: 23649.355
    sample_time_ms: 6841.286
    update_time_ms: 32.054
  timestamp: 1602749069
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: c9144_00000
  
2020-10-15 08:04:30,485	WARNING util.py:136 -- The `process_trial` operation took 0.6648051738739014 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    227 |          6010.41 | 36726784 |  280.909 |              305.323 |              128.354 |            787.362 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3077.8883872210217
    time_step_min: 2950
  date: 2020-10-15_08-04-57
  done: false
  episode_len_mean: 787.4436794389326
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 280.96134022399843
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 186
  episodes_total: 46768
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2822885654325768e-32
        cur_lr: 5.0e-05
        entropy: 0.058519801745812096
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031809890060685575
        model: {}
        policy_loss: -0.007575220603030175
        total_loss: 0.5017368569970131
        vf_explained_var: 0.9990621209144592
        vf_loss: 0.5093413343032202
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.609375
    gpu_util_percent0: 0.279375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464846034639896
    mean_env_wait_ms: 1.2096385481922927
    mean_inference_ms: 4.308834944164431
    mean_raw_obs_processing_ms: 0.3791825316004393
  time_since_restore: 6036.969615459442
  time_this_iter_s: 26.55525231361389
  time_total_s: 6036.969615459442
  timers:
    learn_throughput: 8427.946
    learn_time_ms: 19197.086
    sample_throughput: 23624.957
    sample_time_ms: 6848.351
    update_time_ms: 31.812
  timestamp: 1602749097
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: c9144_00000
  
2020-10-15 08:04:57,883	WARNING util.py:136 -- The `process_trial` operation took 0.6134445667266846 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    228 |          6036.97 | 36888576 |  280.961 |              305.323 |              128.354 |            787.444 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3077.574406855535
    time_step_min: 2950
  date: 2020-10-15_08-05-24
  done: false
  episode_len_mean: 787.5200869083628
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.0104112741611
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 178
  episodes_total: 46946
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.411442827162884e-33
        cur_lr: 5.0e-05
        entropy: 0.06096265402932962
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01036932690961597
        total_loss: .inf
        vf_explained_var: 0.9990455508232117
        vf_loss: 0.5246335690220197
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.845161290322583
    gpu_util_percent0: 0.34096774193548385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648269202904107
    mean_env_wait_ms: 1.2095409911798631
    mean_inference_ms: 4.3087123486425165
    mean_raw_obs_processing_ms: 0.3791744376614599
  time_since_restore: 6063.276893854141
  time_this_iter_s: 26.307278394699097
  time_total_s: 6063.276893854141
  timers:
    learn_throughput: 8431.724
    learn_time_ms: 19188.485
    sample_throughput: 23619.488
    sample_time_ms: 6849.937
    update_time_ms: 32.187
  timestamp: 1602749124
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:05:25,093	WARNING util.py:136 -- The `process_trial` operation took 0.6766102313995361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    229 |          6063.28 | 37050368 |   281.01 |              305.323 |              128.354 |             787.52 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3077.180693489517
    time_step_min: 2950
  date: 2020-10-15_08-05-51
  done: false
  episode_len_mean: 787.615979982612
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.06944741633777
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 213
  episodes_total: 47159
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.617164240744323e-33
        cur_lr: 5.0e-05
        entropy: 0.0614380935827891
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007838445948436856
        total_loss: .inf
        vf_explained_var: 0.999211847782135
        vf_loss: 0.478101114432017
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63548387096774
    gpu_util_percent0: 0.33064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648091536983912
    mean_env_wait_ms: 1.2094226904042333
    mean_inference_ms: 4.3085535973924705
    mean_raw_obs_processing_ms: 0.3791635546543942
  time_since_restore: 6089.43314409256
  time_this_iter_s: 26.15625023841858
  time_total_s: 6089.43314409256
  timers:
    learn_throughput: 8412.425
    learn_time_ms: 19232.504
    sample_throughput: 23598.717
    sample_time_ms: 6855.966
    update_time_ms: 31.516
  timestamp: 1602749151
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:05:52,104	WARNING util.py:136 -- The `process_trial` operation took 0.6309401988983154 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    230 |          6089.43 | 37212160 |  281.069 |              305.323 |              128.354 |            787.616 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3076.751034803176
    time_step_min: 2950
  date: 2020-10-15_08-06-18
  done: false
  episode_len_mean: 787.7152172536772
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.1341598823186
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 228
  episodes_total: 47387
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4425746361116487e-32
        cur_lr: 5.0e-05
        entropy: 0.058591674702862896
        entropy_coeff: 0.0005000000000000001
        kl: 0.003188601384560267
        model: {}
        policy_loss: -0.0059208421735093
        total_loss: 0.16269304603338242
        vf_explained_var: 0.999724805355072
        vf_loss: 0.16864318524797758
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.012500000000003
    gpu_util_percent0: 0.26593750000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464777681194959
    mean_env_wait_ms: 1.2092929509742893
    mean_inference_ms: 4.308375014801555
    mean_raw_obs_processing_ms: 0.379150137734729
  time_since_restore: 6115.7252378463745
  time_this_iter_s: 26.292093753814697
  time_total_s: 6115.7252378463745
  timers:
    learn_throughput: 8396.109
    learn_time_ms: 19269.878
    sample_throughput: 23640.688
    sample_time_ms: 6843.794
    update_time_ms: 29.472
  timestamp: 1602749178
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: c9144_00000
  
2020-10-15 08:06:19,448	WARNING util.py:136 -- The `process_trial` operation took 0.6609995365142822 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    231 |          6115.73 | 37373952 |  281.134 |              305.323 |              128.354 |            787.715 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3076.415616386475
    time_step_min: 2950
  date: 2020-10-15_08-06-45
  done: false
  episode_len_mean: 787.7946890374669
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.1846310971453
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 175
  episodes_total: 47562
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.212873180558243e-33
        cur_lr: 5.0e-05
        entropy: 0.053067020451029144
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005852551783997721
        total_loss: .inf
        vf_explained_var: 0.9996978640556335
        vf_loss: 0.16133931775887808
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12903225806452
    gpu_util_percent0: 0.3364516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647597841606425
    mean_env_wait_ms: 1.2091941449777854
    mean_inference_ms: 4.308255776014235
    mean_raw_obs_processing_ms: 0.37914156270621596
  time_since_restore: 6142.055414676666
  time_this_iter_s: 26.330176830291748
  time_total_s: 6142.055414676666
  timers:
    learn_throughput: 8399.27
    learn_time_ms: 19262.626
    sample_throughput: 23640.707
    sample_time_ms: 6843.789
    update_time_ms: 29.054
  timestamp: 1602749205
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:06:46,680	WARNING util.py:136 -- The `process_trial` operation took 0.6800029277801514 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    232 |          6142.06 | 37535744 |  281.185 |              305.323 |              128.354 |            787.795 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3076.068263749162
    time_step_min: 2950
  date: 2020-10-15_08-07-12
  done: false
  episode_len_mean: 787.8773954384569
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.23598838406053
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 185
  episodes_total: 47747
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0819309770837366e-32
        cur_lr: 5.0e-05
        entropy: 0.053808458149433136
        entropy_coeff: 0.0005000000000000001
        kl: 0.00448349816724658
        model: {}
        policy_loss: -0.006813580914846777
        total_loss: 0.18800108258922896
        vf_explained_var: 0.9996507167816162
        vf_loss: 0.1948415661851565
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.980645161290326
    gpu_util_percent0: 0.2896774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464739741100931
    mean_env_wait_ms: 1.209092816080314
    mean_inference_ms: 4.3081286872166205
    mean_raw_obs_processing_ms: 0.3791332921426774
  time_since_restore: 6168.289232254028
  time_this_iter_s: 26.23381757736206
  time_total_s: 6168.289232254028
  timers:
    learn_throughput: 8404.35
    learn_time_ms: 19250.984
    sample_throughput: 23665.729
    sample_time_ms: 6836.552
    update_time_ms: 26.942
  timestamp: 1602749232
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: c9144_00000
  
2020-10-15 08:07:13,793	WARNING util.py:136 -- The `process_trial` operation took 0.6605772972106934 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    233 |          6168.29 | 37697536 |  281.236 |              305.323 |              128.354 |            787.877 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3075.668231415994
    time_step_min: 2950
  date: 2020-10-15_08-07-40
  done: false
  episode_len_mean: 787.9693324438143
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.29616580262876
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 47966
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.409654885418683e-33
        cur_lr: 5.0e-05
        entropy: 0.05883967597037554
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00840264706251522
        total_loss: .inf
        vf_explained_var: 0.9991498589515686
        vf_loss: 0.5526870017250379
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.71875
    gpu_util_percent0: 0.260625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647220974925368
    mean_env_wait_ms: 1.20897127399603
    mean_inference_ms: 4.307970403550857
    mean_raw_obs_processing_ms: 0.3791220588501156
  time_since_restore: 6194.967486143112
  time_this_iter_s: 26.678253889083862
  time_total_s: 6194.967486143112
  timers:
    learn_throughput: 8399.799
    learn_time_ms: 19261.414
    sample_throughput: 23647.001
    sample_time_ms: 6841.967
    update_time_ms: 27.877
  timestamp: 1602749260
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:07:41,416	WARNING util.py:136 -- The `process_trial` operation took 0.7127766609191895 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    234 |          6194.97 | 37859328 |  281.296 |              305.323 |              128.354 |            787.969 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3075.2768374488587
    time_step_min: 2950
  date: 2020-10-15_08-08-08
  done: false
  episode_len_mean: 788.0605777611754
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.3563093266119
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 220
  episodes_total: 48186
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.114482328128023e-33
        cur_lr: 5.0e-05
        entropy: 0.05957304065426191
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0053931091679260135
        total_loss: .inf
        vf_explained_var: 0.9994227290153503
        vf_loss: 0.3464401016632716
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.862499999999997
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464694083885976
    mean_env_wait_ms: 1.2088452925719315
    mean_inference_ms: 4.307808911150766
    mean_raw_obs_processing_ms: 0.3791094664086044
  time_since_restore: 6221.575660705566
  time_this_iter_s: 26.608174562454224
  time_total_s: 6221.575660705566
  timers:
    learn_throughput: 8401.333
    learn_time_ms: 19257.897
    sample_throughput: 23660.849
    sample_time_ms: 6837.963
    update_time_ms: 29.401
  timestamp: 1602749288
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:08:09,115	WARNING util.py:136 -- The `process_trial` operation took 0.6974165439605713 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    235 |          6221.58 | 38021120 |  281.356 |              305.323 |              128.354 |            788.061 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3074.9780638218617
    time_step_min: 2950
  date: 2020-10-15_08-08-35
  done: false
  episode_len_mean: 788.1297640465702
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.4014598494405
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 171
  episodes_total: 48357
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2171723492192033e-32
        cur_lr: 5.0e-05
        entropy: 0.0606935607890288
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049083075718954206
        model: {}
        policy_loss: -0.007221459767606575
        total_loss: 0.4330272153019905
        vf_explained_var: 0.9991629719734192
        vf_loss: 0.4402790193756421
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19354838709678
    gpu_util_percent0: 0.33580645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646752444691635
    mean_env_wait_ms: 1.2087502432865327
    mean_inference_ms: 4.307694713811523
    mean_raw_obs_processing_ms: 0.37910161181388785
  time_since_restore: 6247.883404970169
  time_this_iter_s: 26.30774426460266
  time_total_s: 6247.883404970169
  timers:
    learn_throughput: 8403.632
    learn_time_ms: 19252.627
    sample_throughput: 23753.673
    sample_time_ms: 6811.241
    update_time_ms: 27.766
  timestamp: 1602749315
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: c9144_00000
  
2020-10-15 08:08:36,347	WARNING util.py:136 -- The `process_trial` operation took 0.709144115447998 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    236 |          6247.88 | 38182912 |  281.401 |              305.323 |              128.354 |             788.13 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3074.6418368819177
    time_step_min: 2950
  date: 2020-10-15_08-09-02
  done: false
  episode_len_mean: 788.2076330532213
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.4526243652541
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 195
  episodes_total: 48552
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.0858617460960166e-33
        cur_lr: 5.0e-05
        entropy: 0.06154300272464752
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008634033855438853
        total_loss: .inf
        vf_explained_var: 0.9993064403533936
        vf_loss: 0.39868414402008057
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.319354838709685
    gpu_util_percent0: 0.3625806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646592155303306
    mean_env_wait_ms: 1.2086436248890198
    mean_inference_ms: 4.3075659410227916
    mean_raw_obs_processing_ms: 0.3790925611367621
  time_since_restore: 6274.271779537201
  time_this_iter_s: 26.38837456703186
  time_total_s: 6274.271779537201
  timers:
    learn_throughput: 8399.085
    learn_time_ms: 19263.051
    sample_throughput: 23762.52
    sample_time_ms: 6808.706
    update_time_ms: 29.944
  timestamp: 1602749342
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:09:03,751	WARNING util.py:136 -- The `process_trial` operation took 0.7104523181915283 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    237 |          6274.27 | 38344704 |  281.453 |              305.323 |              128.354 |            788.208 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3074.241886513765
    time_step_min: 2950
  date: 2020-10-15_08-09-29
  done: false
  episode_len_mean: 788.3048727988356
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.5129218011898
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 229
  episodes_total: 48781
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.128792619144025e-33
        cur_lr: 5.0e-05
        entropy: 0.061449773920079075
        entropy_coeff: 0.0005000000000000001
        kl: 0.003756474470719695
        model: {}
        policy_loss: -0.00689311501840469
        total_loss: 0.44905178248882294
        vf_explained_var: 0.9992730617523193
        vf_loss: 0.45597562442223233
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.838709677419356
    gpu_util_percent0: 0.31548387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646377771102717
    mean_env_wait_ms: 1.2085157344412985
    mean_inference_ms: 4.307408360991879
    mean_raw_obs_processing_ms: 0.37908104879350096
  time_since_restore: 6300.5135152339935
  time_this_iter_s: 26.241735696792603
  time_total_s: 6300.5135152339935
  timers:
    learn_throughput: 8404.385
    learn_time_ms: 19250.902
    sample_throughput: 23807.653
    sample_time_ms: 6795.798
    update_time_ms: 30.284
  timestamp: 1602749369
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: c9144_00000
  
2020-10-15 08:09:30,951	WARNING util.py:136 -- The `process_trial` operation took 0.6733992099761963 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    238 |          6300.51 | 38506496 |  281.513 |              305.323 |              128.354 |            788.305 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3073.9217677345537
    time_step_min: 2950
  date: 2020-10-15_08-09-57
  done: false
  episode_len_mean: 788.3845729802568
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.5628823402155
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 198
  episodes_total: 48979
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5643963095720124e-33
        cur_lr: 5.0e-05
        entropy: 0.05921915701280037
        entropy_coeff: 0.0005000000000000001
        kl: 0.007893046946264803
        model: {}
        policy_loss: -0.006875674594387722
        total_loss: 0.3524970089395841
        vf_explained_var: 0.9993765354156494
        vf_loss: 0.35940229147672653
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.796875
    gpu_util_percent0: 0.31875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646127197331338
    mean_env_wait_ms: 1.2084030224565654
    mean_inference_ms: 4.307266852099938
    mean_raw_obs_processing_ms: 0.3790704818930984
  time_since_restore: 6327.1135585308075
  time_this_iter_s: 26.600043296813965
  time_total_s: 6327.1135585308075
  timers:
    learn_throughput: 8395.355
    learn_time_ms: 19271.609
    sample_throughput: 23809.093
    sample_time_ms: 6795.387
    update_time_ms: 28.195
  timestamp: 1602749397
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: c9144_00000
  
2020-10-15 08:09:58,461	WARNING util.py:136 -- The `process_trial` operation took 0.6847491264343262 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    239 |          6327.11 | 38668288 |  281.563 |              305.323 |              128.354 |            788.385 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3073.626155286837
    time_step_min: 2950
  date: 2020-10-15_08-10-25
  done: false
  episode_len_mean: 788.4605040991111
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.6089026645815
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 178
  episodes_total: 49157
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5643963095720124e-33
        cur_lr: 5.0e-05
        entropy: 0.056928626261651516
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0078055365932717296
        total_loss: .inf
        vf_explained_var: 0.9994707107543945
        vf_loss: 0.308099905649821
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44375
    gpu_util_percent0: 0.291875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464593741681422
    mean_env_wait_ms: 1.2083053582121974
    mean_inference_ms: 4.307151409939938
    mean_raw_obs_processing_ms: 0.37906243373539467
  time_since_restore: 6353.671204566956
  time_this_iter_s: 26.55764603614807
  time_total_s: 6353.671204566956
  timers:
    learn_throughput: 8379.678
    learn_time_ms: 19307.663
    sample_throughput: 23799.05
    sample_time_ms: 6798.254
    update_time_ms: 28.026
  timestamp: 1602749425
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:10:26,077	WARNING util.py:136 -- The `process_trial` operation took 0.6473250389099121 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    240 |          6353.67 | 38830080 |  281.609 |              305.323 |              128.354 |            788.461 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3073.287327656123
    time_step_min: 2950
  date: 2020-10-15_08-10-52
  done: false
  episode_len_mean: 788.5459223989465
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.65861021316397
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 198
  episodes_total: 49355
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.846594464358018e-33
        cur_lr: 5.0e-05
        entropy: 0.06101408818115791
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006747969659045339
        total_loss: .inf
        vf_explained_var: 0.9990021586418152
        vf_loss: 0.6492032160361608
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.38064516129033
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645811003576392
    mean_env_wait_ms: 1.2081968457384502
    mean_inference_ms: 4.307020611458907
    mean_raw_obs_processing_ms: 0.37905319250893466
  time_since_restore: 6380.172743320465
  time_this_iter_s: 26.50153875350952
  time_total_s: 6380.172743320465
  timers:
    learn_throughput: 8375.472
    learn_time_ms: 19317.359
    sample_throughput: 23763.748
    sample_time_ms: 6808.353
    update_time_ms: 28.016
  timestamp: 1602749452
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:10:53,466	WARNING util.py:136 -- The `process_trial` operation took 0.6635091304779053 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    241 |          6380.17 | 38991872 |  281.659 |              305.323 |              128.354 |            788.546 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3072.902427703671
    time_step_min: 2950
  date: 2020-10-15_08-11-19
  done: false
  episode_len_mean: 788.6428571428571
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.7184531855622
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 233
  episodes_total: 49588
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0269891696537027e-32
        cur_lr: 5.0e-05
        entropy: 0.06146574765443802
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007067822482592116
        total_loss: .inf
        vf_explained_var: 0.9995031356811523
        vf_loss: 0.31362881511449814
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25625
    gpu_util_percent0: 0.3190625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645529059581117
    mean_env_wait_ms: 1.208065189199333
    mean_inference_ms: 4.306862662982664
    mean_raw_obs_processing_ms: 0.37904111352469555
  time_since_restore: 6406.415416479111
  time_this_iter_s: 26.24267315864563
  time_total_s: 6406.415416479111
  timers:
    learn_throughput: 8372.62
    learn_time_ms: 19323.94
    sample_throughput: 23808.706
    sample_time_ms: 6795.497
    update_time_ms: 26.067
  timestamp: 1602749479
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:11:20,798	WARNING util.py:136 -- The `process_trial` operation took 0.6960115432739258 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    242 |          6406.42 | 39153664 |  281.718 |              305.323 |              128.354 |            788.643 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3072.586396171787
    time_step_min: 2950
  date: 2020-10-15_08-11-47
  done: false
  episode_len_mean: 788.714412006992
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.7649383672168
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 183
  episodes_total: 49771
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.540483754480554e-32
        cur_lr: 5.0e-05
        entropy: 0.05693087416390578
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0067788849798186375
        total_loss: .inf
        vf_explained_var: 0.9995381832122803
        vf_loss: 0.24878672634561858
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116129032258065
    gpu_util_percent0: 0.31548387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645336529082942
    mean_env_wait_ms: 1.2079628759431098
    mean_inference_ms: 4.30674312606961
    mean_raw_obs_processing_ms: 0.37903244221555177
  time_since_restore: 6432.947674512863
  time_this_iter_s: 26.53225803375244
  time_total_s: 6432.947674512863
  timers:
    learn_throughput: 8361.197
    learn_time_ms: 19350.34
    sample_throughput: 23806.578
    sample_time_ms: 6796.105
    update_time_ms: 27.668
  timestamp: 1602749507
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:11:48,212	WARNING util.py:136 -- The `process_trial` operation took 0.6477186679840088 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    243 |          6432.95 | 39315456 |  281.765 |              305.323 |              128.354 |            788.714 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3072.2829810678154
    time_step_min: 2950
  date: 2020-10-15_08-12-14
  done: false
  episode_len_mean: 788.7882482482482
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.81092304425636
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 49950
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3107256317208305e-32
        cur_lr: 5.0e-05
        entropy: 0.05875894116858641
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041584062079588575
        model: {}
        policy_loss: -0.008206110496151572
        total_loss: 0.2957907517751058
        vf_explained_var: 0.9994892477989197
        vf_loss: 0.3040262386202812
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.928125
    gpu_util_percent0: 0.3046875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645180112478884
    mean_env_wait_ms: 1.2078652961941858
    mean_inference_ms: 4.306631707147703
    mean_raw_obs_processing_ms: 0.37902457833442416
  time_since_restore: 6459.423848152161
  time_this_iter_s: 26.476173639297485
  time_total_s: 6459.423848152161
  timers:
    learn_throughput: 8358.492
    learn_time_ms: 19356.601
    sample_throughput: 23893.907
    sample_time_ms: 6771.266
    update_time_ms: 28.052
  timestamp: 1602749534
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: c9144_00000
  
2020-10-15 08:12:15,709	WARNING util.py:136 -- The `process_trial` operation took 0.7073266506195068 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    244 |          6459.42 | 39477248 |  281.811 |              305.323 |              128.354 |            788.788 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3071.9351221945135
    time_step_min: 2950
  date: 2020-10-15_08-12-42
  done: false
  episode_len_mean: 788.867663476874
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.8599209801363
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 210
  episodes_total: 50160
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1553628158604153e-32
        cur_lr: 5.0e-05
        entropy: 0.06272915595521529
        entropy_coeff: 0.0005000000000000001
        kl: 0.002930625322430084
        model: {}
        policy_loss: -0.004980833417600176
        total_loss: 0.6312321871519089
        vf_explained_var: 0.9989907145500183
        vf_loss: 0.636244386434555
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.467741935483872
    gpu_util_percent0: 0.32741935483870965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645014284219224
    mean_env_wait_ms: 1.207750459189121
    mean_inference_ms: 4.3064985921815815
    mean_raw_obs_processing_ms: 0.3790154928683939
  time_since_restore: 6485.812679767609
  time_this_iter_s: 26.388831615447998
  time_total_s: 6485.812679767609
  timers:
    learn_throughput: 8363.115
    learn_time_ms: 19345.901
    sample_throughput: 23932.855
    sample_time_ms: 6760.247
    update_time_ms: 27.18
  timestamp: 1602749562
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: c9144_00000
  
2020-10-15 08:12:43,047	WARNING util.py:136 -- The `process_trial` operation took 0.7215938568115234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    245 |          6485.81 | 39639040 |   281.86 |              305.323 |              128.354 |            788.868 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3071.557208397053
    time_step_min: 2950
  date: 2020-10-15_08-13-09
  done: false
  episode_len_mean: 788.957508037947
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.9149292712783
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 226
  episodes_total: 50386
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.776814079302076e-33
        cur_lr: 5.0e-05
        entropy: 0.06055836876233419
        entropy_coeff: 0.0005000000000000001
        kl: 0.004828884537952642
        model: {}
        policy_loss: -0.006474706092073272
        total_loss: 0.42022382964690524
        vf_explained_var: 0.9993200302124023
        vf_loss: 0.4267288148403168
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.596874999999997
    gpu_util_percent0: 0.26749999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644760641875681
    mean_env_wait_ms: 1.2076237078026506
    mean_inference_ms: 4.306349421137068
    mean_raw_obs_processing_ms: 0.37900378249189526
  time_since_restore: 6512.185038328171
  time_this_iter_s: 26.372358560562134
  time_total_s: 6512.185038328171
  timers:
    learn_throughput: 8363.816
    learn_time_ms: 19344.28
    sample_throughput: 23917.935
    sample_time_ms: 6764.464
    update_time_ms: 26.909
  timestamp: 1602749589
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: c9144_00000
  
2020-10-15 08:13:10,533	WARNING util.py:136 -- The `process_trial` operation took 0.7127578258514404 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    246 |          6512.19 | 39800832 |  281.915 |              305.323 |              128.354 |            788.958 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3071.2705970828633
    time_step_min: 2950
  date: 2020-10-15_08-13-37
  done: false
  episode_len_mean: 789.0222490309311
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 281.9584776648696
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 178
  episodes_total: 50564
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.888407039651038e-33
        cur_lr: 5.0e-05
        entropy: 0.059322405916949116
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00765489704645006
        total_loss: .inf
        vf_explained_var: 0.9993243217468262
        vf_loss: 0.3866219719250997
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.180645161290318
    gpu_util_percent0: 0.38161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644577367893324
    mean_env_wait_ms: 1.2075249965849912
    mean_inference_ms: 4.30624130907254
    mean_raw_obs_processing_ms: 0.3789961066379638
  time_since_restore: 6538.739236116409
  time_this_iter_s: 26.554197788238525
  time_total_s: 6538.739236116409
  timers:
    learn_throughput: 8363.438
    learn_time_ms: 19345.154
    sample_throughput: 23860.304
    sample_time_ms: 6780.802
    update_time_ms: 24.61
  timestamp: 1602749617
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:13:37,990	WARNING util.py:136 -- The `process_trial` operation took 0.6729826927185059 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    247 |          6538.74 | 39962624 |  281.958 |              305.323 |              128.354 |            789.022 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3070.9691424965986
    time_step_min: 2950
  date: 2020-10-15_08-14-04
  done: false
  episode_len_mean: 789.0904594892812
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.0017241695008
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 188
  episodes_total: 50752
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3326105594765595e-33
        cur_lr: 5.0e-05
        entropy: 0.06151097764571508
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035911871624800065
        model: {}
        policy_loss: -0.00786093699086147
        total_loss: 0.6027274504303932
        vf_explained_var: 0.9989988803863525
        vf_loss: 0.6106191724538803
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.009375
    gpu_util_percent0: 0.31562500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644418736336737
    mean_env_wait_ms: 1.2074229271975667
    mean_inference_ms: 4.306128643814047
    mean_raw_obs_processing_ms: 0.37898843075295574
  time_since_restore: 6565.457678318024
  time_this_iter_s: 26.71844220161438
  time_total_s: 6565.457678318024
  timers:
    learn_throughput: 8356.585
    learn_time_ms: 19361.02
    sample_throughput: 23783.129
    sample_time_ms: 6802.805
    update_time_ms: 33.75
  timestamp: 1602749644
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: c9144_00000
  
2020-10-15 08:14:05,660	WARNING util.py:136 -- The `process_trial` operation took 0.7239620685577393 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    248 |          6565.46 | 40124416 |  282.002 |              305.323 |              128.354 |             789.09 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3070.6377807444637
    time_step_min: 2950
  date: 2020-10-15_08-14-32
  done: false
  episode_len_mean: 789.1722744305586
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.05451981112657
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 50971
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1663052797382797e-33
        cur_lr: 5.0e-05
        entropy: 0.06127173143128554
        entropy_coeff: 0.0005000000000000001
        kl: 0.003726906046116104
        model: {}
        policy_loss: -0.006749419029802084
        total_loss: 0.37263814608256024
        vf_explained_var: 0.9993978142738342
        vf_loss: 0.3794182017445564
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53125
    gpu_util_percent0: 0.2653125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464424590604419
    mean_env_wait_ms: 1.207303727266522
    mean_inference_ms: 4.305992960150153
    mean_raw_obs_processing_ms: 0.3789790097298846
  time_since_restore: 6592.027551412582
  time_this_iter_s: 26.569873094558716
  time_total_s: 6592.027551412582
  timers:
    learn_throughput: 8361.662
    learn_time_ms: 19349.264
    sample_throughput: 23757.287
    sample_time_ms: 6810.205
    update_time_ms: 35.166
  timestamp: 1602749672
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: c9144_00000
  
2020-10-15 08:14:33,217	WARNING util.py:136 -- The `process_trial` operation took 0.7317099571228027 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    249 |          6592.03 | 40286208 |  282.055 |              305.323 |              128.354 |            789.172 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3070.284887585533
    time_step_min: 2950
  date: 2020-10-15_08-14-59
  done: false
  episode_len_mean: 789.2582201816939
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.1068692196953
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 214
  episodes_total: 51185
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0831526398691399e-33
        cur_lr: 5.0e-05
        entropy: 0.05849279835820198
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006129979514904941
        total_loss: .inf
        vf_explained_var: 0.9995247721672058
        vf_loss: 0.2827887609601021
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.067741935483877
    gpu_util_percent0: 0.3764516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644012609424326
    mean_env_wait_ms: 1.2071840869450963
    mean_inference_ms: 4.305858801620589
    mean_raw_obs_processing_ms: 0.37896826426126423
  time_since_restore: 6618.432164907455
  time_this_iter_s: 26.404613494873047
  time_total_s: 6618.432164907455
  timers:
    learn_throughput: 8369.792
    learn_time_ms: 19330.468
    sample_throughput: 23751.746
    sample_time_ms: 6811.794
    update_time_ms: 36.652
  timestamp: 1602749699
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:15:00,576	WARNING util.py:136 -- The `process_trial` operation took 0.7242965698242188 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    250 |          6618.43 | 40448000 |  282.107 |              305.323 |              128.354 |            789.258 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3070.0133076143716
    time_step_min: 2950
  date: 2020-10-15_08-15-26
  done: false
  episode_len_mean: 789.324597441539
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.14834102035957
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 51359
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6247289598037094e-33
        cur_lr: 5.0e-05
        entropy: 0.05509726578990618
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007658520104693404
        total_loss: .inf
        vf_explained_var: 0.999595582485199
        vf_loss: 0.215130394945542
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.122580645161293
    gpu_util_percent0: 0.362258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464384362647435
    mean_env_wait_ms: 1.2070883334605953
    mean_inference_ms: 4.305756483735051
    mean_raw_obs_processing_ms: 0.3789610439418747
  time_since_restore: 6644.670215368271
  time_this_iter_s: 26.23805046081543
  time_total_s: 6644.670215368271
  timers:
    learn_throughput: 8378.765
    learn_time_ms: 19309.767
    sample_throughput: 23772.491
    sample_time_ms: 6805.85
    update_time_ms: 36.807
  timestamp: 1602749726
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:15:27,820	WARNING util.py:136 -- The `process_trial` operation took 0.7750725746154785 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    251 |          6644.67 | 40609792 |  282.148 |              305.323 |              128.354 |            789.325 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3069.7033967391303
    time_step_min: 2950
  date: 2020-10-15_08-15-54
  done: false
  episode_len_mean: 789.4002133643681
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.19380596773675
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 196
  episodes_total: 51555
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.437093439705564e-33
        cur_lr: 5.0e-05
        entropy: 0.05985869870831569
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007405294403724838
        total_loss: .inf
        vf_explained_var: 0.9994029402732849
        vf_loss: 0.35494623333215714
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.137500000000003
    gpu_util_percent0: 0.37750000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643716769232754
    mean_env_wait_ms: 1.2069835457117932
    mean_inference_ms: 4.305650903368143
    mean_raw_obs_processing_ms: 0.37895387728494634
  time_since_restore: 6671.153382778168
  time_this_iter_s: 26.48316740989685
  time_total_s: 6671.153382778168
  timers:
    learn_throughput: 8379.905
    learn_time_ms: 19307.139
    sample_throughput: 23721.349
    sample_time_ms: 6820.523
    update_time_ms: 38.625
  timestamp: 1602749754
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:15:55,223	WARNING util.py:136 -- The `process_trial` operation took 0.6788492202758789 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    252 |          6671.15 | 40771584 |  282.194 |              305.323 |              128.354 |              789.4 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3069.363246293898
    time_step_min: 2950
  date: 2020-10-15_08-16-21
  done: false
  episode_len_mean: 789.4859002588172
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.24659739122603
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 51774
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.655640159558347e-33
        cur_lr: 5.0e-05
        entropy: 0.061321403520802654
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006833694482338615
        total_loss: .inf
        vf_explained_var: 0.999447762966156
        vf_loss: 0.3614974245429039
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.622580645161296
    gpu_util_percent0: 0.3387096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643520402169552
    mean_env_wait_ms: 1.2068641669651854
    mean_inference_ms: 4.305521539146398
    mean_raw_obs_processing_ms: 0.3789445077852551
  time_since_restore: 6697.642081022263
  time_this_iter_s: 26.48869824409485
  time_total_s: 6697.642081022263
  timers:
    learn_throughput: 8383.324
    learn_time_ms: 19299.267
    sample_throughput: 23712.723
    sample_time_ms: 6823.004
    update_time_ms: 39.204
  timestamp: 1602749781
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:16:22,723	WARNING util.py:136 -- The `process_trial` operation took 0.6872923374176025 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    253 |          6697.64 | 40933376 |  282.247 |              305.323 |              128.354 |            789.486 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3069.049263644239
    time_step_min: 2950
  date: 2020-10-15_08-16-49
  done: false
  episode_len_mean: 789.5654482493267
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.2947248553251
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 206
  episodes_total: 51980
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.483460239337519e-33
        cur_lr: 5.0e-05
        entropy: 0.0575667784238855
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008186907575388128
        total_loss: .inf
        vf_explained_var: 0.9996356964111328
        vf_loss: 0.20478581388791403
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.49375
    gpu_util_percent0: 0.2984375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643315213269703
    mean_env_wait_ms: 1.2067488712874472
    mean_inference_ms: 4.305394687776456
    mean_raw_obs_processing_ms: 0.3789342128742278
  time_since_restore: 6724.262923240662
  time_this_iter_s: 26.620842218399048
  time_total_s: 6724.262923240662
  timers:
    learn_throughput: 8376.619
    learn_time_ms: 19314.714
    sample_throughput: 23720.132
    sample_time_ms: 6820.873
    update_time_ms: 38.132
  timestamp: 1602749809
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:16:50,306	WARNING util.py:136 -- The `process_trial` operation took 0.7207481861114502 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    254 |          6724.26 | 41095168 |  282.295 |              305.323 |              128.354 |            789.565 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3068.77371016328
    time_step_min: 2950
  date: 2020-10-15_08-17-16
  done: false
  episode_len_mean: 789.6323196686735
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.33591969083017
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 52154
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.225190359006279e-33
        cur_lr: 5.0e-05
        entropy: 0.05748722919573387
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006605221698312865
        total_loss: .inf
        vf_explained_var: 0.9996162056922913
        vf_loss: 0.20574240510662398
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.734375
    gpu_util_percent0: 0.25875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464314824619095
    mean_env_wait_ms: 1.2066548029106363
    mean_inference_ms: 4.305298609117587
    mean_raw_obs_processing_ms: 0.3789273956034067
  time_since_restore: 6750.828948020935
  time_this_iter_s: 26.566024780273438
  time_total_s: 6750.828948020935
  timers:
    learn_throughput: 8373.451
    learn_time_ms: 19322.021
    sample_throughput: 23692.89
    sample_time_ms: 6828.715
    update_time_ms: 39.272
  timestamp: 1602749836
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:17:18,032	WARNING util.py:136 -- The `process_trial` operation took 0.740131139755249 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    255 |          6750.83 | 41256960 |  282.336 |              305.323 |              128.354 |            789.632 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3068.455775552328
    time_step_min: 2950
  date: 2020-10-15_08-17-44
  done: false
  episode_len_mean: 789.712637750912
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.38212411168354
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 205
  episodes_total: 52359
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.233778553850942e-32
        cur_lr: 5.0e-05
        entropy: 0.05976334741959969
        entropy_coeff: 0.0005000000000000001
        kl: 0.003379772805298368
        model: {}
        policy_loss: -0.00504392718236583
        total_loss: 0.6891599595546722
        vf_explained_var: 0.9989177584648132
        vf_loss: 0.6942337701718012
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25
    gpu_util_percent0: 0.250625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464303522749826
    mean_env_wait_ms: 1.206545117528428
    mean_inference_ms: 4.305182885013485
    mean_raw_obs_processing_ms: 0.3789190652458052
  time_since_restore: 6777.450902223587
  time_this_iter_s: 26.621954202651978
  time_total_s: 6777.450902223587
  timers:
    learn_throughput: 8368.187
    learn_time_ms: 19334.175
    sample_throughput: 23643.035
    sample_time_ms: 6843.115
    update_time_ms: 39.73
  timestamp: 1602749864
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: c9144_00000
  
2020-10-15 08:17:45,796	WARNING util.py:136 -- The `process_trial` operation took 0.7373034954071045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    256 |          6777.45 | 41418752 |  282.382 |              305.323 |              128.354 |            789.713 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3068.1313160148443
    time_step_min: 2950
  date: 2020-10-15_08-18-12
  done: false
  episode_len_mean: 789.7991821985546
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.4321860675988
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 221
  episodes_total: 52580
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.16889276925471e-33
        cur_lr: 5.0e-05
        entropy: 0.05692138553907474
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007427452316430087
        total_loss: .inf
        vf_explained_var: 0.9996359348297119
        vf_loss: 0.24979795888066292
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.190625
    gpu_util_percent0: 0.32906250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464282787616785
    mean_env_wait_ms: 1.2064245394574813
    mean_inference_ms: 4.305057451898147
    mean_raw_obs_processing_ms: 0.37891032418727444
  time_since_restore: 6804.211837768555
  time_this_iter_s: 26.76093554496765
  time_total_s: 6804.211837768555
  timers:
    learn_throughput: 8365.553
    learn_time_ms: 19340.265
    sample_throughput: 23631.218
    sample_time_ms: 6846.537
    update_time_ms: 41.716
  timestamp: 1602749892
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:18:13,568	WARNING util.py:136 -- The `process_trial` operation took 0.7637979984283447 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    257 |          6804.21 | 41580544 |  282.432 |              305.323 |              128.354 |            789.799 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3067.852608225724
    time_step_min: 2950
  date: 2020-10-15_08-18-40
  done: false
  episode_len_mean: 789.8739672553627
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.4746228678049
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 192
  episodes_total: 52772
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.253339153882065e-33
        cur_lr: 5.0e-05
        entropy: 0.055582914190987744
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00745771035629635
        total_loss: .inf
        vf_explained_var: 0.9996387362480164
        vf_loss: 0.20057272662719092
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11612903225807
    gpu_util_percent0: 0.29451612903225804
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642632911484563
    mean_env_wait_ms: 1.2063177868369699
    mean_inference_ms: 4.3049480505833975
    mean_raw_obs_processing_ms: 0.37890128290829683
  time_since_restore: 6830.908463001251
  time_this_iter_s: 26.696625232696533
  time_total_s: 6830.908463001251
  timers:
    learn_throughput: 8362.211
    learn_time_ms: 19347.992
    sample_throughput: 23639.239
    sample_time_ms: 6844.214
    update_time_ms: 32.715
  timestamp: 1602749920
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:18:41,380	WARNING util.py:136 -- The `process_trial` operation took 0.7774825096130371 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    258 |          6830.91 | 41742336 |  282.475 |              305.323 |              128.354 |            789.874 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3067.6104202887595
    time_step_min: 2950
  date: 2020-10-15_08-19-07
  done: false
  episode_len_mean: 789.9426828577364
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.5129926677018
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 52951
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3880008730823097e-32
        cur_lr: 5.0e-05
        entropy: 0.05507848380754391
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007647013252911468
        total_loss: .inf
        vf_explained_var: 0.9991628527641296
        vf_loss: 0.4673537462949753
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.106250000000003
    gpu_util_percent0: 0.34781249999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642497124239645
    mean_env_wait_ms: 1.206222147113605
    mean_inference_ms: 4.304854127639294
    mean_raw_obs_processing_ms: 0.37889459425671496
  time_since_restore: 6857.384662628174
  time_this_iter_s: 26.476199626922607
  time_total_s: 6857.384662628174
  timers:
    learn_throughput: 8363.786
    learn_time_ms: 19344.351
    sample_throughput: 23634.089
    sample_time_ms: 6845.705
    update_time_ms: 32.917
  timestamp: 1602749947
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:19:08,873	WARNING util.py:136 -- The `process_trial` operation took 0.7715058326721191 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    259 |          6857.38 | 41904128 |  282.513 |              305.323 |              128.354 |            789.943 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3067.3079225252695
    time_step_min: 2950
  date: 2020-10-15_08-19-35
  done: false
  episode_len_mean: 790.0214815093487
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.55924220953756
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 53162
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0820013096234643e-32
        cur_lr: 5.0e-05
        entropy: 0.056136815808713436
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008225841253685454
        total_loss: .inf
        vf_explained_var: 0.9992055296897888
        vf_loss: 0.4853617573777835
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.921875
    gpu_util_percent0: 0.265625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642333490145668
    mean_env_wait_ms: 1.2061086639656613
    mean_inference_ms: 4.304736939407482
    mean_raw_obs_processing_ms: 0.3788864621616922
  time_since_restore: 6883.765465736389
  time_this_iter_s: 26.380803108215332
  time_total_s: 6883.765465736389
  timers:
    learn_throughput: 8364.965
    learn_time_ms: 19341.622
    sample_throughput: 23631.854
    sample_time_ms: 6846.353
    update_time_ms: 31.735
  timestamp: 1602749975
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:19:36,425	WARNING util.py:136 -- The `process_trial` operation took 0.7568416595458984 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    260 |          6883.77 | 42065920 |  282.559 |              305.323 |              128.354 |            790.021 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3066.9804127382804
    time_step_min: 2950
  date: 2020-10-15_08-20-02
  done: false
  episode_len_mean: 790.1026299029708
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.60739148878366
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 224
  episodes_total: 53386
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1230019644351963e-32
        cur_lr: 5.0e-05
        entropy: 0.05663612360755602
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010215395338794528
        total_loss: .inf
        vf_explained_var: 0.9992585778236389
        vf_loss: 0.45906178653240204
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.54516129032258
    gpu_util_percent0: 0.32548387096774195
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642141717408966
    mean_env_wait_ms: 1.2059860759249754
    mean_inference_ms: 4.304610893514057
    mean_raw_obs_processing_ms: 0.37887632317658826
  time_since_restore: 6910.126790046692
  time_this_iter_s: 26.361324310302734
  time_total_s: 6910.126790046692
  timers:
    learn_throughput: 8361.788
    learn_time_ms: 19348.972
    sample_throughput: 23620.818
    sample_time_ms: 6849.551
    update_time_ms: 31.742
  timestamp: 1602750002
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:20:03,793	WARNING util.py:136 -- The `process_trial` operation took 0.7626850605010986 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    261 |          6910.13 | 42227712 |  282.607 |              305.323 |              128.354 |            790.103 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3066.7815991033067
    time_step_min: 2950
  date: 2020-10-15_08-20-30
  done: false
  episode_len_mean: 790.1621207878279
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.64066031358107
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 53565
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6845029466527955e-32
        cur_lr: 5.0e-05
        entropy: 0.05786370951682329
        entropy_coeff: 0.0005000000000000001
        kl: 0.003513625318494936
        model: {}
        policy_loss: -0.010065005432503918
        total_loss: 0.5409265731771787
        vf_explained_var: 0.9989797472953796
        vf_loss: 0.5510204955935478
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14516129032258
    gpu_util_percent0: 0.3290322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464197455541997
    mean_env_wait_ms: 1.2058892540873478
    mean_inference_ms: 4.304519141626593
    mean_raw_obs_processing_ms: 0.37886960062075553
  time_since_restore: 6936.401242494583
  time_this_iter_s: 26.274452447891235
  time_total_s: 6936.401242494583
  timers:
    learn_throughput: 8364.398
    learn_time_ms: 19342.934
    sample_throughput: 23638.543
    sample_time_ms: 6844.415
    update_time_ms: 30.083
  timestamp: 1602750030
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: c9144_00000
  
2020-10-15 08:20:31,181	WARNING util.py:136 -- The `process_trial` operation took 0.7790603637695312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    262 |           6936.4 | 42389504 |  282.641 |              305.323 |              128.354 |            790.162 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3066.5271251442828
    time_step_min: 2950
  date: 2020-10-15_08-20-57
  done: false
  episode_len_mean: 790.2317624513944
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.6808463056207
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 53749
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3422514733263978e-32
        cur_lr: 5.0e-05
        entropy: 0.05483102581153313
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008222622828422269
        total_loss: .inf
        vf_explained_var: 0.9993428587913513
        vf_loss: 0.42207563668489456
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.740625
    gpu_util_percent0: 0.32875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641851398348077
    mean_env_wait_ms: 1.205791605773981
    mean_inference_ms: 4.304426432442788
    mean_raw_obs_processing_ms: 0.37886320235205106
  time_since_restore: 6962.797048091888
  time_this_iter_s: 26.395805597305298
  time_total_s: 6962.797048091888
  timers:
    learn_throughput: 8372.197
    learn_time_ms: 19324.915
    sample_throughput: 23626.581
    sample_time_ms: 6847.88
    update_time_ms: 27.98
  timestamp: 1602750057
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:20:58,630	WARNING util.py:136 -- The `process_trial` operation took 0.8019936084747314 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    263 |           6962.8 | 42551296 |  282.681 |              305.323 |              128.354 |            790.232 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3066.2079848684334
    time_step_min: 2950
  date: 2020-10-15_08-21-25
  done: false
  episode_len_mean: 790.3100515177347
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.72811488368734
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 213
  episodes_total: 53962
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.513377209989596e-32
        cur_lr: 5.0e-05
        entropy: 0.05547247268259525
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008459750655068396
        total_loss: .inf
        vf_explained_var: 0.9996255040168762
        vf_loss: 0.24424013992150626
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96451612903226
    gpu_util_percent0: 0.3635483870967741
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641717660948458
    mean_env_wait_ms: 1.2056780589377194
    mean_inference_ms: 4.304313869160858
    mean_raw_obs_processing_ms: 0.3788549826623121
  time_since_restore: 6989.214248657227
  time_this_iter_s: 26.417200565338135
  time_total_s: 6989.214248657227
  timers:
    learn_throughput: 8379.419
    learn_time_ms: 19308.259
    sample_throughput: 23616.928
    sample_time_ms: 6850.679
    update_time_ms: 29.248
  timestamp: 1602750085
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:21:26,112	WARNING util.py:136 -- The `process_trial` operation took 0.7562599182128906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    264 |          6989.21 | 42713088 |  282.728 |              305.323 |              128.354 |             790.31 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3065.880683287165
    time_step_min: 2950
  date: 2020-10-15_08-21-52
  done: false
  episode_len_mean: 790.39128910215
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.7765278511796
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 223
  episodes_total: 54185
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.270065814984394e-32
        cur_lr: 5.0e-05
        entropy: 0.052856087063749634
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0065992320627567365
        total_loss: .inf
        vf_explained_var: 0.9996504187583923
        vf_loss: 0.21601550156871477
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.040625
    gpu_util_percent0: 0.37187499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464149018508496
    mean_env_wait_ms: 1.2055562339808221
    mean_inference_ms: 4.30418991213274
    mean_raw_obs_processing_ms: 0.3788451820865161
  time_since_restore: 7015.657824277878
  time_this_iter_s: 26.443575620651245
  time_total_s: 7015.657824277878
  timers:
    learn_throughput: 8384.812
    learn_time_ms: 19295.841
    sample_throughput: 23650.718
    sample_time_ms: 6840.892
    update_time_ms: 29.337
  timestamp: 1602750112
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:21:53,592	WARNING util.py:136 -- The `process_trial` operation took 0.793546199798584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    265 |          7015.66 | 42874880 |  282.777 |              305.323 |              128.354 |            790.391 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3065.6414292025624
    time_step_min: 2950
  date: 2020-10-15_08-22-20
  done: false
  episode_len_mean: 790.4532828050553
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.81428367822525
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 54359
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.905098722476593e-32
        cur_lr: 5.0e-05
        entropy: 0.053073897336920105
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007377803354756907
        total_loss: .inf
        vf_explained_var: 0.999647855758667
        vf_loss: 0.19740298887093863
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.912903225806453
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641343102726817
    mean_env_wait_ms: 1.2054628329310004
    mean_inference_ms: 4.304106729045385
    mean_raw_obs_processing_ms: 0.37883878904061724
  time_since_restore: 7042.149609804153
  time_this_iter_s: 26.491785526275635
  time_total_s: 7042.149609804153
  timers:
    learn_throughput: 8384.97
    learn_time_ms: 19295.478
    sample_throughput: 23703.787
    sample_time_ms: 6825.576
    update_time_ms: 30.98
  timestamp: 1602750140
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:22:21,161	WARNING util.py:136 -- The `process_trial` operation took 0.7310857772827148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    266 |          7042.15 | 43036672 |  282.814 |              305.323 |              128.354 |            790.453 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3065.3705103276225
    time_step_min: 2950
  date: 2020-10-15_08-22-47
  done: false
  episode_len_mean: 790.5187996113586
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.85364562414554
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 190
  episodes_total: 54549
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.185764808371489e-31
        cur_lr: 5.0e-05
        entropy: 0.05614055724193653
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035723550439191363
        model: {}
        policy_loss: -0.006201486530092855
        total_loss: 0.37542610615491867
        vf_explained_var: 0.9993416666984558
        vf_loss: 0.3816556657354037
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.221875
    gpu_util_percent0: 0.30375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464123225693223
    mean_env_wait_ms: 1.2053634921619123
    mean_inference_ms: 4.30401338167159
    mean_raw_obs_processing_ms: 0.3788325564013828
  time_since_restore: 7068.581395864487
  time_this_iter_s: 26.431786060333252
  time_total_s: 7068.581395864487
  timers:
    learn_throughput: 8389.211
    learn_time_ms: 19285.724
    sample_throughput: 23752.678
    sample_time_ms: 6811.527
    update_time_ms: 29.13
  timestamp: 1602750167
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: c9144_00000
  
2020-10-15 08:22:48,580	WARNING util.py:136 -- The `process_trial` operation took 0.7348101139068604 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    267 |          7068.58 | 43198464 |  282.854 |              305.323 |              128.354 |            790.519 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3065.065315891402
    time_step_min: 2950
  date: 2020-10-15_08-23-15
  done: false
  episode_len_mean: 790.5975825740838
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.9003814183021
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 220
  episodes_total: 54769
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.928824041857444e-32
        cur_lr: 5.0e-05
        entropy: 0.05595778984328111
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00678874876696985
        total_loss: .inf
        vf_explained_var: 0.9992280602455139
        vf_loss: 0.5232536171873411
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.415625000000002
    gpu_util_percent0: 0.29093749999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641074146048907
    mean_env_wait_ms: 1.2052466991612976
    mean_inference_ms: 4.303904749538124
    mean_raw_obs_processing_ms: 0.3788243545591994
  time_since_restore: 7095.160092830658
  time_this_iter_s: 26.578696966171265
  time_total_s: 7095.160092830658
  timers:
    learn_throughput: 8394.646
    learn_time_ms: 19273.237
    sample_throughput: 23789.0
    sample_time_ms: 6801.127
    update_time_ms: 30.465
  timestamp: 1602750195
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:23:16,204	WARNING util.py:136 -- The `process_trial` operation took 0.7237548828125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    268 |          7095.16 | 43360256 |    282.9 |              305.323 |              128.354 |            790.598 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3064.7731591016636
    time_step_min: 2950
  date: 2020-10-15_08-23-42
  done: false
  episode_len_mean: 790.6702860988341
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.9441845750571
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 54981
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.893236062786167e-32
        cur_lr: 5.0e-05
        entropy: 0.06139248237013817
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007563914288766682
        total_loss: .inf
        vf_explained_var: 0.9990431666374207
        vf_loss: 0.5552367344498634
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.42903225806452
    gpu_util_percent0: 0.3616129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640858080870656
    mean_env_wait_ms: 1.2051302742876389
    mean_inference_ms: 4.303786719044433
    mean_raw_obs_processing_ms: 0.37881479746615865
  time_since_restore: 7121.602687835693
  time_this_iter_s: 26.4425950050354
  time_total_s: 7121.602687835693
  timers:
    learn_throughput: 8391.773
    learn_time_ms: 19279.835
    sample_throughput: 23842.465
    sample_time_ms: 6785.875
    update_time_ms: 30.743
  timestamp: 1602750222
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:23:43,780	WARNING util.py:136 -- The `process_trial` operation took 0.7901492118835449 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    269 |           7121.6 | 43522048 |  282.944 |              305.323 |              128.354 |             790.67 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3064.534100186875
    time_step_min: 2950
  date: 2020-10-15_08-24-10
  done: false
  episode_len_mean: 790.7318682912678
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 282.98109357280373
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 171
  episodes_total: 55152
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3339854094179248e-31
        cur_lr: 5.0e-05
        entropy: 0.05388890237857898
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006291932329380264
        total_loss: .inf
        vf_explained_var: 0.9996812343597412
        vf_loss: 0.1719930705924829
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.00625
    gpu_util_percent0: 0.3115625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640722988939084
    mean_env_wait_ms: 1.2050395094541588
    mean_inference_ms: 4.3037070358272125
    mean_raw_obs_processing_ms: 0.37880852773073903
  time_since_restore: 7148.32377243042
  time_this_iter_s: 26.721084594726562
  time_total_s: 7148.32377243042
  timers:
    learn_throughput: 8380.843
    learn_time_ms: 19304.979
    sample_throughput: 23820.185
    sample_time_ms: 6792.223
    update_time_ms: 31.242
  timestamp: 1602750250
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:24:11,730	WARNING util.py:136 -- The `process_trial` operation took 0.7426116466522217 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    270 |          7148.32 | 43683840 |  282.981 |              305.323 |              128.354 |            790.732 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3064.2573396485645
    time_step_min: 2950
  date: 2020-10-15_08-24-38
  done: false
  episode_len_mean: 790.8034001192391
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.0233724208901
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 55351
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0009781141268867e-31
        cur_lr: 5.0e-05
        entropy: 0.061276911137004696
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005225998305832036
        total_loss: .inf
        vf_explained_var: 0.9994627833366394
        vf_loss: 0.3152945066491763
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.215625
    gpu_util_percent0: 0.35125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464061252819161
    mean_env_wait_ms: 1.2049353785185242
    mean_inference_ms: 4.30360874107394
    mean_raw_obs_processing_ms: 0.3788018048338523
  time_since_restore: 7174.78715634346
  time_this_iter_s: 26.46338391304016
  time_total_s: 7174.78715634346
  timers:
    learn_throughput: 8378.542
    learn_time_ms: 19310.281
    sample_throughput: 23806.236
    sample_time_ms: 6796.203
    update_time_ms: 31.782
  timestamp: 1602750278
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:24:39,239	WARNING util.py:136 -- The `process_trial` operation took 0.7658119201660156 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    271 |          7174.79 | 43845632 |  283.023 |              305.323 |              128.354 |            790.803 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3063.958046742771
    time_step_min: 2950
  date: 2020-10-15_08-25-05
  done: false
  episode_len_mean: 790.876180879204
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.0698925991784
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 222
  episodes_total: 55573
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0014671711903302e-31
        cur_lr: 5.0e-05
        entropy: 0.06399224201838176
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00710665659668545
        total_loss: .inf
        vf_explained_var: 0.9991635680198669
        vf_loss: 0.5627347081899643
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.884375
    gpu_util_percent0: 0.3175
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640462506146149
    mean_env_wait_ms: 1.2048177870097723
    mean_inference_ms: 4.303506222902923
    mean_raw_obs_processing_ms: 0.3787940614854922
  time_since_restore: 7201.2988493442535
  time_this_iter_s: 26.511693000793457
  time_total_s: 7201.2988493442535
  timers:
    learn_throughput: 8371.444
    learn_time_ms: 19326.655
    sample_throughput: 23825.137
    sample_time_ms: 6790.811
    update_time_ms: 33.81
  timestamp: 1602750305
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:25:06,817	WARNING util.py:136 -- The `process_trial` operation took 0.8032279014587402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    272 |           7201.3 | 44007424 |   283.07 |              305.323 |              128.354 |            790.876 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3063.683417355891
    time_step_min: 2950
  date: 2020-10-15_08-25-33
  done: false
  episode_len_mean: 790.9396492989565
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.1099875290359
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 201
  episodes_total: 55774
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.502200756785495e-31
        cur_lr: 5.0e-05
        entropy: 0.05877332823971907
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008890150726074353
        total_loss: .inf
        vf_explained_var: 0.9990248680114746
        vf_loss: 0.55226352562507
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4741935483871
    gpu_util_percent0: 0.3390322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464025666241497
    mean_env_wait_ms: 1.2047083462830979
    mean_inference_ms: 4.3033995671988405
    mean_raw_obs_processing_ms: 0.3787853327434757
  time_since_restore: 7227.838322877884
  time_this_iter_s: 26.53947353363037
  time_total_s: 7227.838322877884
  timers:
    learn_throughput: 8361.152
    learn_time_ms: 19350.444
    sample_throughput: 23848.893
    sample_time_ms: 6784.047
    update_time_ms: 35.885
  timestamp: 1602750333
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:25:34,443	WARNING util.py:136 -- The `process_trial` operation took 0.7321505546569824 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    273 |          7227.84 | 44169216 |   283.11 |              305.323 |              128.354 |             790.94 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3063.4460124125844
    time_step_min: 2950
  date: 2020-10-15_08-26-00
  done: false
  episode_len_mean: 790.9956386515569
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.14431304067745
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 172
  episodes_total: 55946
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.7533011351782425e-31
        cur_lr: 5.0e-05
        entropy: 0.05928107723593712
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00651249293393145
        total_loss: .inf
        vf_explained_var: 0.9990026354789734
        vf_loss: 0.5685645987590154
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7
    gpu_util_percent0: 0.293125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464012789757513
    mean_env_wait_ms: 1.2046185739088815
    mean_inference_ms: 4.30332204492262
    mean_raw_obs_processing_ms: 0.37877991460030935
  time_since_restore: 7254.252259731293
  time_this_iter_s: 26.413936853408813
  time_total_s: 7254.252259731293
  timers:
    learn_throughput: 8364.935
    learn_time_ms: 19341.693
    sample_throughput: 23851.873
    sample_time_ms: 6783.199
    update_time_ms: 34.365
  timestamp: 1602750360
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:26:01,841	WARNING util.py:136 -- The `process_trial` operation took 0.72464919090271 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    274 |          7254.25 | 44331008 |  283.144 |              305.323 |              128.354 |            790.996 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3063.17585531005
    time_step_min: 2950
  date: 2020-10-15_08-26-28
  done: false
  episode_len_mean: 791.0633247262042
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.1862089868501
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 209
  episodes_total: 56155
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0129951702767362e-30
        cur_lr: 5.0e-05
        entropy: 0.0562315263474981
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006505868597135607
        total_loss: .inf
        vf_explained_var: 0.9996445775032043
        vf_loss: 0.21504292264580727
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.335483870967746
    gpu_util_percent0: 0.3735483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640006464151661
    mean_env_wait_ms: 1.2045091141744289
    mean_inference_ms: 4.303216954897798
    mean_raw_obs_processing_ms: 0.378772250504444
  time_since_restore: 7280.508576154709
  time_this_iter_s: 26.256316423416138
  time_total_s: 7280.508576154709
  timers:
    learn_throughput: 8369.542
    learn_time_ms: 19331.047
    sample_throughput: 23863.797
    sample_time_ms: 6779.81
    update_time_ms: 39.645
  timestamp: 1602750388
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:26:29,124	WARNING util.py:136 -- The `process_trial` operation took 0.767902135848999 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    275 |          7280.51 | 44492800 |  283.186 |              305.323 |              128.354 |            791.063 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3062.8709156416944
    time_step_min: 2950
  date: 2020-10-15_08-26-55
  done: false
  episode_len_mean: 791.1398063074249
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.2325581187017
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 223
  episodes_total: 56378
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5194927554151043e-30
        cur_lr: 5.0e-05
        entropy: 0.05826452032973369
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006881799727428491
        total_loss: .inf
        vf_explained_var: 0.9997605681419373
        vf_loss: 0.15945785120129585
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.106451612903225
    gpu_util_percent0: 0.3551612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639841411346197
    mean_env_wait_ms: 1.2043907097685311
    mean_inference_ms: 4.303116441910162
    mean_raw_obs_processing_ms: 0.37876412902727463
  time_since_restore: 7306.8647129535675
  time_this_iter_s: 26.356136798858643
  time_total_s: 7306.8647129535675
  timers:
    learn_throughput: 8376.249
    learn_time_ms: 19315.568
    sample_throughput: 23863.512
    sample_time_ms: 6779.89
    update_time_ms: 39.404
  timestamp: 1602750415
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:26:56,467	WARNING util.py:136 -- The `process_trial` operation took 0.7283377647399902 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    276 |          7306.86 | 44654592 |  283.233 |              305.323 |              128.354 |             791.14 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3062.624672751716
    time_step_min: 2950
  date: 2020-10-15_08-27-23
  done: false
  episode_len_mean: 791.2039528346916
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.2684766593937
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 189
  episodes_total: 56567
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2792391331226568e-30
        cur_lr: 5.0e-05
        entropy: 0.058297370870908104
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007047434866156739
        total_loss: .inf
        vf_explained_var: 0.9994247555732727
        vf_loss: 0.3170190751552582
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.478125
    gpu_util_percent0: 0.261875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639670447176337
    mean_env_wait_ms: 1.2042898692731039
    mean_inference_ms: 4.303025833470124
    mean_raw_obs_processing_ms: 0.37875692207284145
  time_since_restore: 7333.435975790024
  time_this_iter_s: 26.5712628364563
  time_total_s: 7333.435975790024
  timers:
    learn_throughput: 8379.319
    learn_time_ms: 19308.491
    sample_throughput: 23817.058
    sample_time_ms: 6793.114
    update_time_ms: 39.882
  timestamp: 1602750443
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:27:24,055	WARNING util.py:136 -- The `process_trial` operation took 0.7200422286987305 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    277 |          7333.44 | 44816384 |  283.268 |              305.323 |              128.354 |            791.204 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3062.4124554783652
    time_step_min: 2950
  date: 2020-10-15_08-27-50
  done: false
  episode_len_mean: 791.2589649156813
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.3001156430292
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 182
  episodes_total: 56749
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4188586996839844e-30
        cur_lr: 5.0e-05
        entropy: 0.05914326384663582
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00800890566703553
        total_loss: .inf
        vf_explained_var: 0.9987141489982605
        vf_loss: 0.8057625045379003
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.787096774193543
    gpu_util_percent0: 0.34741935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639552760613966
    mean_env_wait_ms: 1.2041955054700548
    mean_inference_ms: 4.302943463396652
    mean_raw_obs_processing_ms: 0.37875135487293865
  time_since_restore: 7359.721635103226
  time_this_iter_s: 26.285659313201904
  time_total_s: 7359.721635103226
  timers:
    learn_throughput: 8385.742
    learn_time_ms: 19293.702
    sample_throughput: 23827.455
    sample_time_ms: 6790.15
    update_time_ms: 38.62
  timestamp: 1602750470
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:27:51,338	WARNING util.py:136 -- The `process_trial` operation took 0.733396053314209 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    278 |          7359.72 | 44978176 |    283.3 |              305.323 |              128.354 |            791.259 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3062.1400765906615
    time_step_min: 2950
  date: 2020-10-15_08-28-17
  done: false
  episode_len_mean: 791.3318937518653
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.3427491324474
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 56961
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.1282880495259784e-30
        cur_lr: 5.0e-05
        entropy: 0.05831400242944559
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008745437914816042
        total_loss: .inf
        vf_explained_var: 0.999671995639801
        vf_loss: 0.20135855178038278
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.896874999999998
    gpu_util_percent0: 0.3315625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146394498435989
    mean_env_wait_ms: 1.2040850426654834
    mean_inference_ms: 4.302845952176292
    mean_raw_obs_processing_ms: 0.378743799351411
  time_since_restore: 7386.239054441452
  time_this_iter_s: 26.51741933822632
  time_total_s: 7386.239054441452
  timers:
    learn_throughput: 8393.619
    learn_time_ms: 19275.596
    sample_throughput: 23748.304
    sample_time_ms: 6812.781
    update_time_ms: 36.551
  timestamp: 1602750497
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:28:18,886	WARNING util.py:136 -- The `process_trial` operation took 0.7683670520782471 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    279 |          7386.24 | 45139968 |  283.343 |              305.323 |              128.354 |            791.332 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3061.924760721597
    time_step_min: 2950
  date: 2020-10-15_08-28-45
  done: false
  episode_len_mean: 791.3822264190536
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.3745936969104
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 225
  episodes_total: 57186
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.692432074288965e-30
        cur_lr: 5.0e-05
        entropy: 0.07794622331857681
        entropy_coeff: 0.0005000000000000001
        kl: 0.005261012158977489
        model: {}
        policy_loss: -0.009755658858921379
        total_loss: 1.808697134256363
        vf_explained_var: 0.9969837069511414
        vf_loss: 1.8184917569160461
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.68064516129032
    gpu_util_percent0: 0.33064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639247646045409
    mean_env_wait_ms: 1.2039665361353906
    mean_inference_ms: 4.302745257760883
    mean_raw_obs_processing_ms: 0.3787353196892659
  time_since_restore: 7412.559723615646
  time_this_iter_s: 26.320669174194336
  time_total_s: 7412.559723615646
  timers:
    learn_throughput: 8409.06
    learn_time_ms: 19240.2
    sample_throughput: 23760.154
    sample_time_ms: 6809.383
    update_time_ms: 35.809
  timestamp: 1602750525
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: c9144_00000
  
2020-10-15 08:28:46,209	WARNING util.py:136 -- The `process_trial` operation took 0.7324073314666748 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    280 |          7412.56 | 45301760 |  283.375 |              305.323 |              128.354 |            791.382 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3061.7778805058874
    time_step_min: 2950
  date: 2020-10-15_08-29-12
  done: false
  episode_len_mean: 791.4212517433751
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.395989180508
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 57360
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.692432074288965e-30
        cur_lr: 5.0e-05
        entropy: 0.06978352492054303
        entropy_coeff: 0.0005000000000000001
        kl: 0.004672088815520207
        model: {}
        policy_loss: -0.010470011077510813
        total_loss: 1.565340906381607
        vf_explained_var: 0.9970064759254456
        vf_loss: 1.5758457779884338
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.978125
    gpu_util_percent0: 0.258125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146391096047153
    mean_env_wait_ms: 1.2038757264158102
    mean_inference_ms: 4.30266951159033
    mean_raw_obs_processing_ms: 0.37872976394359803
  time_since_restore: 7439.0403299331665
  time_this_iter_s: 26.48060631752014
  time_total_s: 7439.0403299331665
  timers:
    learn_throughput: 8409.071
    learn_time_ms: 19240.176
    sample_throughput: 23785.49
    sample_time_ms: 6802.13
    update_time_ms: 35.07
  timestamp: 1602750552
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: c9144_00000
  
2020-10-15 08:29:13,751	WARNING util.py:136 -- The `process_trial` operation took 0.7919440269470215 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    281 |          7439.04 | 45463552 |  283.396 |              305.323 |              128.354 |            791.421 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3061.590368567455
    time_step_min: 2950
  date: 2020-10-15_08-29-40
  done: false
  episode_len_mean: 791.4680913908436
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.4261299468493
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 195
  episodes_total: 57555
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8462160371444824e-30
        cur_lr: 5.0e-05
        entropy: 0.06483646482229233
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008516339320825258
        total_loss: .inf
        vf_explained_var: 0.9985041618347168
        vf_loss: 0.8522999584674835
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.674193548387098
    gpu_util_percent0: 0.35064516129032264
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639014554788773
    mean_env_wait_ms: 1.2037758264147258
    mean_inference_ms: 4.302584501667696
    mean_raw_obs_processing_ms: 0.378723717400167
  time_since_restore: 7465.309596300125
  time_this_iter_s: 26.269266366958618
  time_total_s: 7465.309596300125
  timers:
    learn_throughput: 8418.326
    learn_time_ms: 19219.023
    sample_throughput: 23764.796
    sample_time_ms: 6808.053
    update_time_ms: 32.826
  timestamp: 1602750580
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:29:41,057	WARNING util.py:136 -- The `process_trial` operation took 0.7748565673828125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    282 |          7465.31 | 45625344 |  283.426 |              305.323 |              128.354 |            791.468 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3061.325787231094
    time_step_min: 2950
  date: 2020-10-15_08-30-07
  done: false
  episode_len_mean: 791.5312884072773
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.4658557392723
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 214
  episodes_total: 57769
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.7693240557167254e-30
        cur_lr: 5.0e-05
        entropy: 0.05511517015596231
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035379873394655683
        model: {}
        policy_loss: -0.006514888916475077
        total_loss: 0.332026203473409
        vf_explained_var: 0.9994643330574036
        vf_loss: 0.33856865515311557
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.709375
    gpu_util_percent0: 0.32875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638881104437626
    mean_env_wait_ms: 1.203666259009288
    mean_inference_ms: 4.302493728940848
    mean_raw_obs_processing_ms: 0.3787167533064658
  time_since_restore: 7491.956072568893
  time_this_iter_s: 26.64647626876831
  time_total_s: 7491.956072568893
  timers:
    learn_throughput: 8418.915
    learn_time_ms: 19217.678
    sample_throughput: 23766.388
    sample_time_ms: 6807.597
    update_time_ms: 32.686
  timestamp: 1602750607
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: c9144_00000
  
2020-10-15 08:30:08,716	WARNING util.py:136 -- The `process_trial` operation took 0.7482943534851074 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    283 |          7491.96 | 45787136 |  283.466 |              305.323 |              128.354 |            791.531 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3061.0553599779114
    time_step_min: 2950
  date: 2020-10-15_08-30-35
  done: false
  episode_len_mean: 791.5972095269303
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.50697635688067
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 214
  episodes_total: 57983
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8846620278583627e-30
        cur_lr: 5.0e-05
        entropy: 0.052015907441576324
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008078026009267584
        total_loss: .inf
        vf_explained_var: 0.9994730353355408
        vf_loss: 0.32399272670348483
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.816129032258058
    gpu_util_percent0: 0.3151612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638684655709305
    mean_env_wait_ms: 1.2035532082361027
    mean_inference_ms: 4.302392415389999
    mean_raw_obs_processing_ms: 0.37870853430508955
  time_since_restore: 7518.337292671204
  time_this_iter_s: 26.38122010231018
  time_total_s: 7518.337292671204
  timers:
    learn_throughput: 8417.082
    learn_time_ms: 19221.864
    sample_throughput: 23767.86
    sample_time_ms: 6807.176
    update_time_ms: 34.198
  timestamp: 1602750635
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:30:36,120	WARNING util.py:136 -- The `process_trial` operation took 0.755486249923706 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    284 |          7518.34 | 45948928 |  283.507 |              305.323 |              128.354 |            791.597 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3060.8520818995185
    time_step_min: 2950
  date: 2020-10-15_08-31-02
  done: false
  episode_len_mean: 791.651534691772
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.53776610573107
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 172
  episodes_total: 58155
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.326993041787544e-30
        cur_lr: 5.0e-05
        entropy: 0.04876280017197132
        entropy_coeff: 0.0005000000000000001
        kl: 0.004069912266762306
        model: {}
        policy_loss: -0.008843616601855805
        total_loss: 0.23716983323295912
        vf_explained_var: 0.9995538592338562
        vf_loss: 0.24603783215085664
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.8875
    gpu_util_percent0: 0.2525
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146385688825667
    mean_env_wait_ms: 1.203465015705821
    mean_inference_ms: 4.3023235133227535
    mean_raw_obs_processing_ms: 0.3787032938321845
  time_since_restore: 7544.707633256912
  time_this_iter_s: 26.370340585708618
  time_total_s: 7544.707633256912
  timers:
    learn_throughput: 8409.4
    learn_time_ms: 19239.423
    sample_throughput: 23777.565
    sample_time_ms: 6804.397
    update_time_ms: 27.109
  timestamp: 1602750662
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: c9144_00000
  
2020-10-15 08:31:03,728	WARNING util.py:136 -- The `process_trial` operation took 0.8004825115203857 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    285 |          7544.71 | 46110720 |  283.538 |              305.323 |              128.354 |            791.652 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3060.617768099043
    time_step_min: 2950
  date: 2020-10-15_08-31-30
  done: false
  episode_len_mean: 791.7139307319246
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.5752372317074
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 198
  episodes_total: 58353
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.163496520893772e-30
        cur_lr: 5.0e-05
        entropy: 0.05000998234997193
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006989828427322209
        total_loss: .inf
        vf_explained_var: 0.9995532035827637
        vf_loss: 0.2689122011264165
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.906451612903233
    gpu_util_percent0: 0.3180645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638480638043616
    mean_env_wait_ms: 1.2033643604092021
    mean_inference_ms: 4.302236707906026
    mean_raw_obs_processing_ms: 0.3786968587643492
  time_since_restore: 7571.14600276947
  time_this_iter_s: 26.438369512557983
  time_total_s: 7571.14600276947
  timers:
    learn_throughput: 8410.174
    learn_time_ms: 19237.652
    sample_throughput: 23735.364
    sample_time_ms: 6816.495
    update_time_ms: 25.261
  timestamp: 1602750690
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:31:31,259	WARNING util.py:136 -- The `process_trial` operation took 0.8122525215148926 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    286 |          7571.15 | 46272512 |  283.575 |              305.323 |              128.354 |            791.714 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3060.3550954870007
    time_step_min: 2950
  date: 2020-10-15_08-31-57
  done: false
  episode_len_mean: 791.7825255646414
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.61611902351444
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 224
  episodes_total: 58577
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.245244781340658e-30
        cur_lr: 5.0e-05
        entropy: 0.05170637431244055
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032961233519017696
        model: {}
        policy_loss: -0.007657934111193754
        total_loss: 0.2745351642370224
        vf_explained_var: 0.9996030926704407
        vf_loss: 0.28221895669897396
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.996875
    gpu_util_percent0: 0.3028125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463833479754416
    mean_env_wait_ms: 1.2032502578968487
    mean_inference_ms: 4.302147482645041
    mean_raw_obs_processing_ms: 0.3786900227429669
  time_since_restore: 7597.5530507564545
  time_this_iter_s: 26.407047986984253
  time_total_s: 7597.5530507564545
  timers:
    learn_throughput: 8408.549
    learn_time_ms: 19241.37
    sample_throughput: 23821.709
    sample_time_ms: 6791.788
    update_time_ms: 26.018
  timestamp: 1602750717
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: c9144_00000
  
2020-10-15 08:31:58,777	WARNING util.py:136 -- The `process_trial` operation took 0.7725234031677246 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    287 |          7597.55 | 46434304 |  283.616 |              305.323 |              128.354 |            791.783 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3060.113910689661
    time_step_min: 2950
  date: 2020-10-15_08-32-25
  done: false
  episode_len_mean: 791.8451696328308
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.6524847962389
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 197
  episodes_total: 58774
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.622622390670329e-30
        cur_lr: 5.0e-05
        entropy: 0.05575968914975723
        entropy_coeff: 0.0005000000000000001
        kl: 0.008123069380720457
        model: {}
        policy_loss: -0.0048125776229426265
        total_loss: 0.42682292064030963
        vf_explained_var: 0.9992468953132629
        vf_loss: 0.4316633741060893
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.935483870967747
    gpu_util_percent0: 0.36677419354838714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638158522208566
    mean_env_wait_ms: 1.2031462206550458
    mean_inference_ms: 4.30205702869976
    mean_raw_obs_processing_ms: 0.378682316387037
  time_since_restore: 7623.939150810242
  time_this_iter_s: 26.38610005378723
  time_total_s: 7623.939150810242
  timers:
    learn_throughput: 8404.412
    learn_time_ms: 19250.84
    sample_throughput: 23831.06
    sample_time_ms: 6789.123
    update_time_ms: 25.415
  timestamp: 1602750745
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: c9144_00000
  
2020-10-15 08:32:26,255	WARNING util.py:136 -- The `process_trial` operation took 0.8099150657653809 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    288 |          7623.94 | 46596096 |  283.652 |              305.323 |              128.354 |            791.845 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3059.906561884717
    time_step_min: 2950
  date: 2020-10-15_08-32-52
  done: false
  episode_len_mean: 791.8987973062374
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.6845138806429
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 177
  episodes_total: 58951
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.622622390670329e-30
        cur_lr: 5.0e-05
        entropy: 0.05752720528592666
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008968172182600634
        total_loss: .inf
        vf_explained_var: 0.999032735824585
        vf_loss: 0.5279928793509802
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.18125
    gpu_util_percent0: 0.35625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463804905915366
    mean_env_wait_ms: 1.2030569404734754
    mean_inference_ms: 4.301990780523848
    mean_raw_obs_processing_ms: 0.37867736709284744
  time_since_restore: 7650.308897256851
  time_this_iter_s: 26.369746446609497
  time_total_s: 7650.308897256851
  timers:
    learn_throughput: 8399.534
    learn_time_ms: 19262.022
    sample_throughput: 23888.959
    sample_time_ms: 6772.669
    update_time_ms: 25.999
  timestamp: 1602750772
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:32:53,870	WARNING util.py:136 -- The `process_trial` operation took 0.797605037689209 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    289 |          7650.31 | 46757888 |  283.685 |              305.323 |              128.354 |            791.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3059.656218812903
    time_step_min: 2950
  date: 2020-10-15_08-33-20
  done: false
  episode_len_mean: 791.9599181796666
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.72098661839
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 203
  episodes_total: 59154
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4339335860054928e-30
        cur_lr: 5.0e-05
        entropy: 0.053393072138230004
        entropy_coeff: 0.0005000000000000001
        kl: 0.003664177629010131
        model: {}
        policy_loss: -0.008021566890723383
        total_loss: 0.2955137987931569
        vf_explained_var: 0.9995085597038269
        vf_loss: 0.30356205503145856
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.209677419354843
    gpu_util_percent0: 0.3416129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637946955825276
    mean_env_wait_ms: 1.202954279047651
    mean_inference_ms: 4.301903968205237
    mean_raw_obs_processing_ms: 0.3786709356922021
  time_since_restore: 7676.6619284152985
  time_this_iter_s: 26.353031158447266
  time_total_s: 7676.6619284152985
  timers:
    learn_throughput: 8397.923
    learn_time_ms: 19265.717
    sample_throughput: 23894.453
    sample_time_ms: 6771.111
    update_time_ms: 25.825
  timestamp: 1602750800
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: c9144_00000
  
2020-10-15 08:33:21,280	WARNING util.py:136 -- The `process_trial` operation took 0.7762854099273682 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    290 |          7676.66 | 46919680 |  283.721 |              305.323 |              128.354 |             791.96 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3059.3869837043526
    time_step_min: 2950
  date: 2020-10-15_08-33-47
  done: false
  episode_len_mean: 792.0264248181084
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.75987236961373
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 222
  episodes_total: 59376
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2169667930027464e-30
        cur_lr: 5.0e-05
        entropy: 0.05558956631769737
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007601508436588726
        total_loss: .inf
        vf_explained_var: 0.9992735981941223
        vf_loss: 0.44687777509291965
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.48125
    gpu_util_percent0: 0.274375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637816252819758
    mean_env_wait_ms: 1.2028414487107684
    mean_inference_ms: 4.301819783726077
    mean_raw_obs_processing_ms: 0.37866429610743724
  time_since_restore: 7703.242619514465
  time_this_iter_s: 26.58069109916687
  time_total_s: 7703.242619514465
  timers:
    learn_throughput: 8396.191
    learn_time_ms: 19269.689
    sample_throughput: 23887.101
    sample_time_ms: 6773.195
    update_time_ms: 26.306
  timestamp: 1602750827
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:33:48,937	WARNING util.py:136 -- The `process_trial` operation took 0.7812747955322266 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    291 |          7703.24 | 47081472 |   283.76 |              305.323 |              128.354 |            792.026 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3059.1806927366792
    time_step_min: 2950
  date: 2020-10-15_08-34-15
  done: false
  episode_len_mean: 792.0841573354375
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.79280643661934
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 59567
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8254501895041197e-30
        cur_lr: 5.0e-05
        entropy: 0.04812524250398079
        entropy_coeff: 0.0005000000000000001
        kl: 0.00394429328540961
        model: {}
        policy_loss: -0.008862634383452436
        total_loss: 0.3373514488339424
        vf_explained_var: 0.9993879199028015
        vf_loss: 0.3462381511926651
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.55483870967743
    gpu_util_percent0: 0.3496774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637649089988644
    mean_env_wait_ms: 1.2027416345271558
    mean_inference_ms: 4.301733537972294
    mean_raw_obs_processing_ms: 0.378657078312654
  time_since_restore: 7729.551127195358
  time_this_iter_s: 26.308507680892944
  time_total_s: 7729.551127195358
  timers:
    learn_throughput: 8392.005
    learn_time_ms: 19279.301
    sample_throughput: 23904.82
    sample_time_ms: 6768.175
    update_time_ms: 26.431
  timestamp: 1602750855
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: c9144_00000
  
2020-10-15 08:34:16,404	WARNING util.py:136 -- The `process_trial` operation took 0.7929885387420654 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    292 |          7729.55 | 47243264 |  283.793 |              305.323 |              128.354 |            792.084 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3058.9642288948803
    time_step_min: 2950
  date: 2020-10-15_08-34-42
  done: false
  episode_len_mean: 792.139335207873
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.82637988643216
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 59748
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.127250947520598e-31
        cur_lr: 5.0e-05
        entropy: 0.04721074923872948
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038511637637081244
        model: {}
        policy_loss: -0.006025887414580211
        total_loss: 0.16385839506983757
        vf_explained_var: 0.9997124671936035
        vf_loss: 0.16990788653492928
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.584375
    gpu_util_percent0: 0.31062500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637567133863202
    mean_env_wait_ms: 1.2026513419862244
    mean_inference_ms: 4.301666233159923
    mean_raw_obs_processing_ms: 0.37865251474315675
  time_since_restore: 7756.09529042244
  time_this_iter_s: 26.5441632270813
  time_total_s: 7756.09529042244
  timers:
    learn_throughput: 8399.631
    learn_time_ms: 19261.799
    sample_throughput: 23866.091
    sample_time_ms: 6779.158
    update_time_ms: 26.493
  timestamp: 1602750882
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: c9144_00000
  
2020-10-15 08:34:44,111	WARNING util.py:136 -- The `process_trial` operation took 0.8435490131378174 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    293 |           7756.1 | 47405056 |  283.826 |              305.323 |              128.354 |            792.139 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3058.7088700527265
    time_step_min: 2950
  date: 2020-10-15_08-35-10
  done: false
  episode_len_mean: 792.2006436873614
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.86422027064384
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 59967
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.563625473760299e-31
        cur_lr: 5.0e-05
        entropy: 0.05305363330990076
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0076416116013812525
        total_loss: .inf
        vf_explained_var: 0.9991051554679871
        vf_loss: 0.5636483977238337
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203225806451613
    gpu_util_percent0: 0.3819354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637456041329897
    mean_env_wait_ms: 1.2025407926819234
    mean_inference_ms: 4.3015781221106355
    mean_raw_obs_processing_ms: 0.3786455340952102
  time_since_restore: 7782.482189655304
  time_this_iter_s: 26.38689923286438
  time_total_s: 7782.482189655304
  timers:
    learn_throughput: 8397.253
    learn_time_ms: 19267.253
    sample_throughput: 23884.875
    sample_time_ms: 6773.827
    update_time_ms: 26.008
  timestamp: 1602750910
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:35:11,642	WARNING util.py:136 -- The `process_trial` operation took 0.7980368137359619 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    294 |          7782.48 | 47566848 |  283.864 |              305.323 |              128.354 |            792.201 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3058.4674536536704
    time_step_min: 2950
  date: 2020-10-15_08-35-37
  done: false
  episode_len_mean: 792.2632768361582
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.90080767797616
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 213
  episodes_total: 60180
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.84543821064045e-31
        cur_lr: 5.0e-05
        entropy: 0.0524570836375157
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035573405136043825
        model: {}
        policy_loss: -0.008794036283992076
        total_loss: 0.3284389947851499
        vf_explained_var: 0.9994630217552185
        vf_loss: 0.3372592628002167
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.403125000000003
    gpu_util_percent0: 0.2628125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463728673393496
    mean_env_wait_ms: 1.2024317622708915
    mean_inference_ms: 4.301492746323742
    mean_raw_obs_processing_ms: 0.3786383275946617
  time_since_restore: 7808.793438196182
  time_this_iter_s: 26.311248540878296
  time_total_s: 7808.793438196182
  timers:
    learn_throughput: 8398.868
    learn_time_ms: 19263.548
    sample_throughput: 23890.81
    sample_time_ms: 6772.144
    update_time_ms: 25.657
  timestamp: 1602750937
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: c9144_00000
  
2020-10-15 08:35:39,203	WARNING util.py:136 -- The `process_trial` operation took 0.757023811340332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    295 |          7808.79 | 47728640 |  283.901 |              305.323 |              128.354 |            792.263 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3058.269999668468
    time_step_min: 2950
  date: 2020-10-15_08-36-05
  done: false
  episode_len_mean: 792.3173572339756
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.932290550173
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 60361
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.422719105320225e-31
        cur_lr: 5.0e-05
        entropy: 0.051530327958365284
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008245044072585491
        total_loss: .inf
        vf_explained_var: 0.9992586970329285
        vf_loss: 0.4069696192940076
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.821875
    gpu_util_percent0: 0.3715625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637155822422987
    mean_env_wait_ms: 1.202339740796471
    mean_inference_ms: 4.301422418910971
    mean_raw_obs_processing_ms: 0.37863241614592247
  time_since_restore: 7835.455019235611
  time_this_iter_s: 26.66158103942871
  time_total_s: 7835.455019235611
  timers:
    learn_throughput: 8381.42
    learn_time_ms: 19303.65
    sample_throughput: 23963.564
    sample_time_ms: 6751.583
    update_time_ms: 27.972
  timestamp: 1602750965
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:36:07,147	WARNING util.py:136 -- The `process_trial` operation took 0.8192975521087646 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    296 |          7835.46 | 47890432 |  283.932 |              305.323 |              128.354 |            792.317 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3058.0550469328396
    time_step_min: 2950
  date: 2020-10-15_08-36-33
  done: false
  episode_len_mean: 792.3729169075264
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 283.9654141294025
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 186
  episodes_total: 60547
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.134078657980337e-31
        cur_lr: 5.0e-05
        entropy: 0.054771813874443374
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008037955975548053
        total_loss: .inf
        vf_explained_var: 0.9994413256645203
        vf_loss: 0.3506626859307289
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.293548387096777
    gpu_util_percent0: 0.34741935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463705755676623
    mean_env_wait_ms: 1.202247489881297
    mean_inference_ms: 4.301350468314897
    mean_raw_obs_processing_ms: 0.3786275265521027
  time_since_restore: 7861.939501523972
  time_this_iter_s: 26.484482288360596
  time_total_s: 7861.939501523972
  timers:
    learn_throughput: 8382.094
    learn_time_ms: 19302.097
    sample_throughput: 23907.582
    sample_time_ms: 6767.393
    update_time_ms: 28.688
  timestamp: 1602750993
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:36:34,746	WARNING util.py:136 -- The `process_trial` operation took 0.8279814720153809 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    297 |          7861.94 | 48052224 |  283.965 |              305.323 |              128.354 |            792.373 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3057.8093465946517
    time_step_min: 2950
  date: 2020-10-15_08-37-01
  done: false
  episode_len_mean: 792.4345242993269
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.0026943562977
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 216
  episodes_total: 60763
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.701117986970505e-31
        cur_lr: 5.0e-05
        entropy: 0.057356386445462704
        entropy_coeff: 0.0005000000000000001
        kl: 0.012734139493356148
        model: {}
        policy_loss: -0.007698326374035484
        total_loss: 0.2013660098115603
        vf_explained_var: 0.9996772408485413
        vf_loss: 0.2090930143992106
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.746875
    gpu_util_percent0: 0.29156249999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463695165107817
    mean_env_wait_ms: 1.2021389089153152
    mean_inference_ms: 4.301270434787665
    mean_raw_obs_processing_ms: 0.3786208781067535
  time_since_restore: 7888.540041446686
  time_this_iter_s: 26.600539922714233
  time_total_s: 7888.540041446686
  timers:
    learn_throughput: 8376.384
    learn_time_ms: 19315.257
    sample_throughput: 23909.949
    sample_time_ms: 6766.723
    update_time_ms: 28.558
  timestamp: 1602751021
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: c9144_00000
  
2020-10-15 08:37:02,446	WARNING util.py:136 -- The `process_trial` operation took 0.8075456619262695 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    298 |          7888.54 | 48214016 |  284.003 |              305.323 |              128.354 |            792.435 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3057.5878907211422
    time_step_min: 2950
  date: 2020-10-15_08-37-28
  done: false
  episode_len_mean: 792.4901115119711
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.0344292382666
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 217
  episodes_total: 60980
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.701117986970505e-31
        cur_lr: 5.0e-05
        entropy: 0.06735560794671376
        entropy_coeff: 0.0005000000000000001
        kl: 0.004125631569574277
        model: {}
        policy_loss: -0.007857280772744465
        total_loss: 1.4971765577793121
        vf_explained_var: 0.997490406036377
        vf_loss: 1.505067487557729
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806453
    gpu_util_percent0: 0.3187096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463679094539157
    mean_env_wait_ms: 1.202029504104069
    mean_inference_ms: 4.3011853593196525
    mean_raw_obs_processing_ms: 0.3786137632817528
  time_since_restore: 7914.962983369827
  time_this_iter_s: 26.42294192314148
  time_total_s: 7914.962983369827
  timers:
    learn_throughput: 8377.596
    learn_time_ms: 19312.462
    sample_throughput: 23894.43
    sample_time_ms: 6771.118
    update_time_ms: 29.269
  timestamp: 1602751048
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: c9144_00000
  
2020-10-15 08:37:30,091	WARNING util.py:136 -- The `process_trial` operation took 0.8680160045623779 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    299 |          7914.96 | 48375808 |  284.034 |              305.323 |              128.354 |             792.49 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3057.453844895288
    time_step_min: 2950
  date: 2020-10-15_08-37-56
  done: false
  episode_len_mean: 792.5175210530618
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.05330717030495
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 175
  episodes_total: 61155
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8505589934852526e-31
        cur_lr: 5.0e-05
        entropy: 0.06810538160304229
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039833514601923525
        model: {}
        policy_loss: -0.010054304975104364
        total_loss: 1.5594183405240376
        vf_explained_var: 0.9970259070396423
        vf_loss: 1.569506694873174
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06875
    gpu_util_percent0: 0.344375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636682871301068
    mean_env_wait_ms: 1.2019413769662388
    mean_inference_ms: 4.301121820849093
    mean_raw_obs_processing_ms: 0.3786085546182548
  time_since_restore: 7941.693729639053
  time_this_iter_s: 26.730746269226074
  time_total_s: 7941.693729639053
  timers:
    learn_throughput: 8365.832
    learn_time_ms: 19339.619
    sample_throughput: 23866.868
    sample_time_ms: 6778.937
    update_time_ms: 31.246
  timestamp: 1602751076
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: c9144_00000
  
2020-10-15 08:37:58,010	WARNING util.py:136 -- The `process_trial` operation took 0.8004803657531738 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    300 |          7941.69 | 48537600 |  284.053 |              305.323 |              128.354 |            792.518 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3057.249107128296
    time_step_min: 2950
  date: 2020-10-15_08-38-24
  done: false
  episode_len_mean: 792.5689441601199
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.0856330360357
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 61354
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9252794967426263e-31
        cur_lr: 5.0e-05
        entropy: 0.0593004801000158
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00936958113258394
        total_loss: .inf
        vf_explained_var: 0.9991896152496338
        vf_loss: 0.4693697343269984
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.815625000000004
    gpu_util_percent0: 0.335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636585359985663
    mean_env_wait_ms: 1.2018436676622997
    mean_inference_ms: 4.301046762160812
    mean_raw_obs_processing_ms: 0.37860354912369026
  time_since_restore: 7968.515813827515
  time_this_iter_s: 26.822084188461304
  time_total_s: 7968.515813827515
  timers:
    learn_throughput: 8359.843
    learn_time_ms: 19353.473
    sample_throughput: 23799.56
    sample_time_ms: 6798.109
    update_time_ms: 32.793
  timestamp: 1602751104
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:38:26,044	WARNING util.py:136 -- The `process_trial` operation took 0.8461968898773193 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    301 |          7968.52 | 48699392 |  284.086 |              305.323 |              128.354 |            792.569 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3057.0062884905997
    time_step_min: 2950
  date: 2020-10-15_08-38-52
  done: false
  episode_len_mean: 792.6230999090554
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.1231583077758
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 222
  episodes_total: 61576
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.88791924511394e-31
        cur_lr: 5.0e-05
        entropy: 0.06095996933678786
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007005108074129869
        total_loss: .inf
        vf_explained_var: 0.9993882775306702
        vf_loss: 0.3854147469003995
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7
    gpu_util_percent0: 0.3021875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463647792484656
    mean_env_wait_ms: 1.2017342119984098
    mean_inference_ms: 4.300967059714403
    mean_raw_obs_processing_ms: 0.378596963671531
  time_since_restore: 7995.079034328461
  time_this_iter_s: 26.563220500946045
  time_total_s: 7995.079034328461
  timers:
    learn_throughput: 8354.207
    learn_time_ms: 19366.53
    sample_throughput: 23775.169
    sample_time_ms: 6805.083
    update_time_ms: 35.087
  timestamp: 1602751132
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:38:53,717	WARNING util.py:136 -- The `process_trial` operation took 0.8036057949066162 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    302 |          7995.08 | 48861184 |  284.123 |              305.323 |              128.354 |            792.623 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3056.778781988986
    time_step_min: 2950
  date: 2020-10-15_08-39-20
  done: false
  episode_len_mean: 792.674091460947
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.1559438987202
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 61775
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.331878867670909e-31
        cur_lr: 5.0e-05
        entropy: 0.05385667271912098
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006104132361845889
        total_loss: .inf
        vf_explained_var: 0.9994774460792542
        vf_loss: 0.304517241815726
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.628125
    gpu_util_percent0: 0.2534375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636311205774277
    mean_env_wait_ms: 1.2016327095078159
    mean_inference_ms: 4.300887118311452
    mean_raw_obs_processing_ms: 0.378589983430259
  time_since_restore: 8021.616943359375
  time_this_iter_s: 26.537909030914307
  time_total_s: 8021.616943359375
  timers:
    learn_throughput: 8349.396
    learn_time_ms: 19377.689
    sample_throughput: 23819.922
    sample_time_ms: 6792.298
    update_time_ms: 33.338
  timestamp: 1602751160
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:39:21,418	WARNING util.py:136 -- The `process_trial` operation took 0.8574404716491699 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    303 |          8021.62 | 49022976 |  284.156 |              305.323 |              128.354 |            792.674 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3056.5928223019027
    time_step_min: 2950
  date: 2020-10-15_08-39-47
  done: false
  episode_len_mean: 792.7218034189414
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.18414772920903
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 61949
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.497818301506365e-31
        cur_lr: 5.0e-05
        entropy: 0.05160200378547112
        entropy_coeff: 0.0005000000000000001
        kl: 0.003550137742422521
        model: {}
        policy_loss: -0.010073071709484793
        total_loss: 0.3325796847542127
        vf_explained_var: 0.9993929862976074
        vf_loss: 0.3426785593231519
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.461290322580652
    gpu_util_percent0: 0.2967741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636218183548846
    mean_env_wait_ms: 1.2015471478451099
    mean_inference_ms: 4.300830668708696
    mean_raw_obs_processing_ms: 0.37858531183091576
  time_since_restore: 8048.001480340958
  time_this_iter_s: 26.38453698158264
  time_total_s: 8048.001480340958
  timers:
    learn_throughput: 8354.081
    learn_time_ms: 19366.822
    sample_throughput: 23778.638
    sample_time_ms: 6804.09
    update_time_ms: 31.626
  timestamp: 1602751187
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: c9144_00000
  
2020-10-15 08:39:48,995	WARNING util.py:136 -- The `process_trial` operation took 0.8135342597961426 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    304 |             8048 | 49184768 |  284.184 |              305.323 |              128.354 |            792.722 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3056.371166878612
    time_step_min: 2950
  date: 2020-10-15_08-40-15
  done: false
  episode_len_mean: 792.7751214646546
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.2176065815984
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 209
  episodes_total: 62158
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2489091507531826e-31
        cur_lr: 5.0e-05
        entropy: 0.05487150605767965
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038719280855730176
        model: {}
        policy_loss: -0.007814623114730542
        total_loss: 0.7389538188775381
        vf_explained_var: 0.9988348484039307
        vf_loss: 0.7467958529790243
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.190624999999997
    gpu_util_percent0: 0.3453125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636131236160285
    mean_env_wait_ms: 1.2014451476490693
    mean_inference_ms: 4.300751914147564
    mean_raw_obs_processing_ms: 0.37857959074979197
  time_since_restore: 8074.235990047455
  time_this_iter_s: 26.234509706497192
  time_total_s: 8074.235990047455
  timers:
    learn_throughput: 8358.227
    learn_time_ms: 19357.216
    sample_throughput: 23769.578
    sample_time_ms: 6806.684
    update_time_ms: 31.683
  timestamp: 1602751215
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: c9144_00000
  
2020-10-15 08:40:16,318	WARNING util.py:136 -- The `process_trial` operation took 0.7981407642364502 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    305 |          8074.24 | 49346560 |  284.218 |              305.323 |              128.354 |            792.775 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3056.1360333627395
    time_step_min: 2950
  date: 2020-10-15_08-40-42
  done: false
  episode_len_mean: 792.834947098429
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.25424167937797
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 222
  episodes_total: 62380
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6244545753765913e-31
        cur_lr: 5.0e-05
        entropy: 0.04944989054153363
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008410792076271415
        total_loss: .inf
        vf_explained_var: 0.9995173811912537
        vf_loss: 0.2988480118413766
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.021875
    gpu_util_percent0: 0.2846875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636008307128764
    mean_env_wait_ms: 1.2013348356157907
    mean_inference_ms: 4.300676610529448
    mean_raw_obs_processing_ms: 0.3785730292345352
  time_since_restore: 8100.628481864929
  time_this_iter_s: 26.392491817474365
  time_total_s: 8100.628481864929
  timers:
    learn_throughput: 8374.21
    learn_time_ms: 19320.27
    sample_throughput: 23765.026
    sample_time_ms: 6807.987
    update_time_ms: 29.238
  timestamp: 1602751242
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:40:43,924	WARNING util.py:136 -- The `process_trial` operation took 0.8560254573822021 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    306 |          8100.63 | 49508352 |  284.254 |              305.323 |              128.354 |            792.835 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3055.919883583331
    time_step_min: 2950
  date: 2020-10-15_08-41-10
  done: false
  episode_len_mean: 792.8903610414103
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.2867015017441
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 189
  episodes_total: 62569
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.436681863064887e-31
        cur_lr: 5.0e-05
        entropy: 0.04604520586629709
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034250224901673696
        model: {}
        policy_loss: -0.006762331283728902
        total_loss: 0.12853352290888628
        vf_explained_var: 0.9997624754905701
        vf_loss: 0.13531888027985892
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95483870967742
    gpu_util_percent0: 0.36064516129032265
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635855241749163
    mean_env_wait_ms: 1.2012402176778931
    mean_inference_ms: 4.300602683101431
    mean_raw_obs_processing_ms: 0.3785666942423929
  time_since_restore: 8127.093229293823
  time_this_iter_s: 26.464747428894043
  time_total_s: 8127.093229293823
  timers:
    learn_throughput: 8377.951
    learn_time_ms: 19311.643
    sample_throughput: 23742.92
    sample_time_ms: 6814.326
    update_time_ms: 29.618
  timestamp: 1602751270
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: c9144_00000
  
2020-10-15 08:41:11,577	WARNING util.py:136 -- The `process_trial` operation took 0.8244791030883789 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    307 |          8127.09 | 49670144 |  284.287 |              305.323 |              128.354 |             792.89 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3055.721265467534
    time_step_min: 2950
  date: 2020-10-15_08-41-38
  done: false
  episode_len_mean: 792.9445391811561
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.31646681808445
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 178
  episodes_total: 62747
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2183409315324434e-31
        cur_lr: 5.0e-05
        entropy: 0.04934921643386284
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035247301566414535
        model: {}
        policy_loss: -0.006453697045799345
        total_loss: 0.21407870948314667
        vf_explained_var: 0.9996106028556824
        vf_loss: 0.22055708120266596
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.86875
    gpu_util_percent0: 0.29625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635769745159255
    mean_env_wait_ms: 1.2011544331728197
    mean_inference_ms: 4.300547480720021
    mean_raw_obs_processing_ms: 0.3785627693017702
  time_since_restore: 8153.58690571785
  time_this_iter_s: 26.49367642402649
  time_total_s: 8153.58690571785
  timers:
    learn_throughput: 8379.427
    learn_time_ms: 19308.241
    sample_throughput: 23747.853
    sample_time_ms: 6812.911
    update_time_ms: 32.01
  timestamp: 1602751298
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: c9144_00000
  
2020-10-15 08:41:39,354	WARNING util.py:136 -- The `process_trial` operation took 0.8036255836486816 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    308 |          8153.59 | 49831936 |  284.316 |              305.323 |              128.354 |            792.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3055.4826068301363
    time_step_min: 2950
  date: 2020-10-15_08-42-05
  done: false
  episode_len_mean: 793.0055112607605
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.3519610513829
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 215
  episodes_total: 62962
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.091704657662217e-32
        cur_lr: 5.0e-05
        entropy: 0.05291570443660021
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011164446890082521
        total_loss: .inf
        vf_explained_var: 0.999525249004364
        vf_loss: 0.290660560131073
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.048387096774196
    gpu_util_percent0: 0.3351612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635679600118878
    mean_env_wait_ms: 1.2010497319431426
    mean_inference_ms: 4.300469249556863
    mean_raw_obs_processing_ms: 0.3785567691187124
  time_since_restore: 8179.9735271930695
  time_this_iter_s: 26.386621475219727
  time_total_s: 8179.9735271930695
  timers:
    learn_throughput: 8379.148
    learn_time_ms: 19308.885
    sample_throughput: 23763.388
    sample_time_ms: 6808.457
    update_time_ms: 30.959
  timestamp: 1602751325
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:42:06,844	WARNING util.py:136 -- The `process_trial` operation took 0.811631441116333 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    309 |          8179.97 | 49993728 |  284.352 |              305.323 |              128.354 |            793.006 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3055.2521973236203
    time_step_min: 2950
  date: 2020-10-15_08-42-33
  done: false
  episode_len_mean: 793.0707818930041
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.38536120943536
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 218
  episodes_total: 63180
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.137556986493324e-32
        cur_lr: 5.0e-05
        entropy: 0.05044925337036451
        entropy_coeff: 0.0005000000000000001
        kl: 0.005792221985757351
        model: {}
        policy_loss: -0.006372326786125389
        total_loss: 0.3492010533809662
        vf_explained_var: 0.9994266033172607
        vf_loss: 0.3555986012021701
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.418750000000003
    gpu_util_percent0: 0.375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635549702026363
    mean_env_wait_ms: 1.2009422520907223
    mean_inference_ms: 4.300397617378307
    mean_raw_obs_processing_ms: 0.37855042326072497
  time_since_restore: 8206.325494527817
  time_this_iter_s: 26.351967334747314
  time_total_s: 8206.325494527817
  timers:
    learn_throughput: 8398.467
    learn_time_ms: 19264.467
    sample_throughput: 23760.417
    sample_time_ms: 6809.308
    update_time_ms: 28.678
  timestamp: 1602751353
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: c9144_00000
  
2020-10-15 08:42:34,373	WARNING util.py:136 -- The `process_trial` operation took 0.8810818195343018 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    310 |          8206.33 | 50155520 |  284.385 |              305.323 |              128.354 |            793.071 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3055.067744686227
    time_step_min: 2950
  date: 2020-10-15_08-43-00
  done: false
  episode_len_mean: 793.1212575559098
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.4124985911258
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 63361
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.137556986493324e-32
        cur_lr: 5.0e-05
        entropy: 0.05989063158631325
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00767438265029341
        total_loss: .inf
        vf_explained_var: 0.9988167881965637
        vf_loss: 0.6597956717014313
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.70625
    gpu_util_percent0: 0.34125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635415487380948
    mean_env_wait_ms: 1.2008526385893346
    mean_inference_ms: 4.3003304444527215
    mean_raw_obs_processing_ms: 0.378544550964283
  time_since_restore: 8232.928651809692
  time_this_iter_s: 26.60315728187561
  time_total_s: 8232.928651809692
  timers:
    learn_throughput: 8404.643
    learn_time_ms: 19250.312
    sample_throughput: 23823.759
    sample_time_ms: 6791.204
    update_time_ms: 28.25
  timestamp: 1602751380
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:43:02,132	WARNING util.py:136 -- The `process_trial` operation took 0.8493406772613525 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    311 |          8232.93 | 50317312 |  284.412 |              305.323 |              128.354 |            793.121 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3054.898822269807
    time_step_min: 2950
  date: 2020-10-15_08-43-28
  done: false
  episode_len_mean: 793.1686783010999
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.43934585599817
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 186
  episodes_total: 63547
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3706335479739984e-31
        cur_lr: 5.0e-05
        entropy: 0.06736864522099495
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010796925091805557
        total_loss: .inf
        vf_explained_var: 0.9990097880363464
        vf_loss: 0.5541118333737055
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91875
    gpu_util_percent0: 0.285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635330128333846
    mean_env_wait_ms: 1.2007634840537111
    mean_inference_ms: 4.3002692283376875
    mean_raw_obs_processing_ms: 0.3785403333454739
  time_since_restore: 8259.69475364685
  time_this_iter_s: 26.766101837158203
  time_total_s: 8259.69475364685
  timers:
    learn_throughput: 8403.667
    learn_time_ms: 19252.548
    sample_throughput: 23778.747
    sample_time_ms: 6804.059
    update_time_ms: 25.93
  timestamp: 1602751408
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:43:30,094	WARNING util.py:136 -- The `process_trial` operation took 0.8943555355072021 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    312 |          8259.69 | 50479104 |  284.439 |              305.323 |              128.354 |            793.169 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3054.8362749250873
    time_step_min: 2950
  date: 2020-10-15_08-43-56
  done: false
  episode_len_mean: 793.1720553186152
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.4413935199968
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 229
  episodes_total: 63776
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.055950321960998e-31
        cur_lr: 5.0e-05
        entropy: 0.12151526597638924
        entropy_coeff: 0.0005000000000000001
        kl: 0.008377472016339501
        model: {}
        policy_loss: -0.013024621817748994
        total_loss: 5.747676173845927
        vf_explained_var: 0.9897949695587158
        vf_loss: 5.760761539141337
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26451612903226
    gpu_util_percent0: 0.2896774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635247921525266
    mean_env_wait_ms: 1.2006544564661294
    mean_inference_ms: 4.300197789353389
    mean_raw_obs_processing_ms: 0.37853416243505883
  time_since_restore: 8286.187527656555
  time_this_iter_s: 26.49277400970459
  time_total_s: 8286.187527656555
  timers:
    learn_throughput: 8406.174
    learn_time_ms: 19246.806
    sample_throughput: 23751.895
    sample_time_ms: 6811.751
    update_time_ms: 27.798
  timestamp: 1602751436
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: c9144_00000
  
2020-10-15 08:43:57,796	WARNING util.py:136 -- The `process_trial` operation took 0.8184974193572998 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    313 |          8286.19 | 50640896 |  284.441 |              305.323 |              128.354 |            793.172 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3055.0792469352014
    time_step_min: 2950
  date: 2020-10-15_08-44-24
  done: false
  episode_len_mean: 793.1036773094535
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.40063014693806
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 63987
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.055950321960998e-31
        cur_lr: 5.0e-05
        entropy: 0.1342778243124485
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013603170174368037
        total_loss: .inf
        vf_explained_var: 0.985877513885498
        vf_loss: 7.37297264734904
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10625
    gpu_util_percent0: 0.30124999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146350678514972
    mean_env_wait_ms: 1.2005515059987442
    mean_inference_ms: 4.300116921416539
    mean_raw_obs_processing_ms: 0.37852726774833845
  time_since_restore: 8312.795251369476
  time_this_iter_s: 26.607723712921143
  time_total_s: 8312.795251369476
  timers:
    learn_throughput: 8402.843
    learn_time_ms: 19254.435
    sample_throughput: 23713.128
    sample_time_ms: 6822.887
    update_time_ms: 30.18
  timestamp: 1602751464
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:44:25,666	WARNING util.py:136 -- The `process_trial` operation took 0.870025634765625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    314 |           8312.8 | 50802688 |  284.401 |              305.323 |              128.354 |            793.104 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3055.168493599239
    time_step_min: 2950
  date: 2020-10-15_08-44-52
  done: false
  episode_len_mean: 793.0760347836928
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.39076055405064
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 64168
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0839254829414973e-31
        cur_lr: 5.0e-05
        entropy: 0.10580665742357571
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011753561013999084
        total_loss: .inf
        vf_explained_var: 0.9917561411857605
        vf_loss: 4.085450867811839
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.115625
    gpu_util_percent0: 0.31999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634985378448198
    mean_env_wait_ms: 1.2004670233702628
    mean_inference_ms: 4.30006309481458
    mean_raw_obs_processing_ms: 0.3785233431127986
  time_since_restore: 8339.602484226227
  time_this_iter_s: 26.80723285675049
  time_total_s: 8339.602484226227
  timers:
    learn_throughput: 8390.905
    learn_time_ms: 19281.83
    sample_throughput: 23629.395
    sample_time_ms: 6847.065
    update_time_ms: 32.243
  timestamp: 1602751492
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:44:53,735	WARNING util.py:136 -- The `process_trial` operation took 0.8661563396453857 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    315 |           8339.6 | 50964480 |  284.391 |              305.323 |              128.354 |            793.076 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3055.0708646733387
    time_step_min: 2950
  date: 2020-10-15_08-45-20
  done: false
  episode_len_mean: 793.1013310967181
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.40524343194306
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 215
  episodes_total: 64383
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.625888224412247e-31
        cur_lr: 5.0e-05
        entropy: 0.07624432568748792
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010519789250376258
        total_loss: .inf
        vf_explained_var: 0.9960030913352966
        vf_loss: 2.445711632569631
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.871875000000003
    gpu_util_percent0: 0.3228125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634915111003097
    mean_env_wait_ms: 1.2003664828365914
    mean_inference_ms: 4.2999929382245226
    mean_raw_obs_processing_ms: 0.37851774899318913
  time_since_restore: 8366.124415159225
  time_this_iter_s: 26.521930932998657
  time_total_s: 8366.124415159225
  timers:
    learn_throughput: 8388.178
    learn_time_ms: 19288.098
    sample_throughput: 23576.099
    sample_time_ms: 6862.543
    update_time_ms: 32.824
  timestamp: 1602751520
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:45:21,404	WARNING util.py:136 -- The `process_trial` operation took 0.8380284309387207 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    316 |          8366.12 | 51126272 |  284.405 |              305.323 |              128.354 |            793.101 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3054.900241613233
    time_step_min: 2950
  date: 2020-10-15_08-45-48
  done: false
  episode_len_mean: 793.1513908453429
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.4335707033963
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 218
  episodes_total: 64601
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.938832336618371e-31
        cur_lr: 5.0e-05
        entropy: 0.05885642860084772
        entropy_coeff: 0.0005000000000000001
        kl: 0.003690120744674156
        model: {}
        policy_loss: -0.010469462625527134
        total_loss: 0.9125289668639501
        vf_explained_var: 0.998457133769989
        vf_loss: 0.9230278233687083
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03125
    gpu_util_percent0: 0.3096875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463477262834581
    mean_env_wait_ms: 1.2002638162007686
    mean_inference_ms: 4.299921723217395
    mean_raw_obs_processing_ms: 0.378511576868926
  time_since_restore: 8392.994436740875
  time_this_iter_s: 26.87002158164978
  time_total_s: 8392.994436740875
  timers:
    learn_throughput: 8379.363
    learn_time_ms: 19308.389
    sample_throughput: 23507.892
    sample_time_ms: 6882.455
    update_time_ms: 32.918
  timestamp: 1602751548
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: c9144_00000
  
2020-10-15 08:45:49,514	WARNING util.py:136 -- The `process_trial` operation took 0.8299956321716309 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    317 |          8392.99 | 51288064 |  284.434 |              305.323 |              128.354 |            793.151 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3054.709839838139
    time_step_min: 2950
  date: 2020-10-15_08-46-15
  done: false
  episode_len_mean: 793.1965978203822
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.46227019664093
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 64782
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4694161683091853e-31
        cur_lr: 5.0e-05
        entropy: 0.055008962439994015
        entropy_coeff: 0.0005000000000000001
        kl: 0.003630008800731351
        model: {}
        policy_loss: -0.008412258714088239
        total_loss: 0.37023836374282837
        vf_explained_var: 0.9992997050285339
        vf_loss: 0.37867813060681027
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.075
    gpu_util_percent0: 0.36124999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463464535251167
    mean_env_wait_ms: 1.2001777380017666
    mean_inference_ms: 4.299856410049535
    mean_raw_obs_processing_ms: 0.37850605951077565
  time_since_restore: 8419.424326658249
  time_this_iter_s: 26.429889917373657
  time_total_s: 8419.424326658249
  timers:
    learn_throughput: 8390.781
    learn_time_ms: 19282.115
    sample_throughput: 23466.284
    sample_time_ms: 6894.658
    update_time_ms: 38.388
  timestamp: 1602751575
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: c9144_00000
  
2020-10-15 08:46:17,118	WARNING util.py:136 -- The `process_trial` operation took 0.866410493850708 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    318 |          8419.42 | 51449856 |  284.462 |              305.323 |              128.354 |            793.197 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3054.517279083377
    time_step_min: 2950
  date: 2020-10-15_08-46-43
  done: false
  episode_len_mean: 793.239283350521
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.4890815837421
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 64969
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347080841545926e-31
        cur_lr: 5.0e-05
        entropy: 0.05678565365572771
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00952487597533036
        total_loss: .inf
        vf_explained_var: 0.9989297986030579
        vf_loss: 0.6176527837912241
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.678125
    gpu_util_percent0: 0.3015625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634568513747095
    mean_env_wait_ms: 1.2000919650985595
    mean_inference_ms: 4.299801605954786
    mean_raw_obs_processing_ms: 0.3785021124812403
  time_since_restore: 8446.07014131546
  time_this_iter_s: 26.645814657211304
  time_total_s: 8446.07014131546
  timers:
    learn_throughput: 8383.962
    learn_time_ms: 19297.798
    sample_throughput: 23476.786
    sample_time_ms: 6891.574
    update_time_ms: 40.252
  timestamp: 1602751603
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:46:44,915	WARNING util.py:136 -- The `process_trial` operation took 0.8429625034332275 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    319 |          8446.07 | 51611648 |  284.489 |              305.323 |              128.354 |            793.239 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3054.306710049727
    time_step_min: 2950
  date: 2020-10-15_08-47-11
  done: false
  episode_len_mean: 793.2942430703624
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.52119049091027
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 222
  episodes_total: 65191
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.602062126231888e-31
        cur_lr: 5.0e-05
        entropy: 0.05505368082473675
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008722013318523144
        total_loss: .inf
        vf_explained_var: 0.9987556338310242
        vf_loss: 0.8039508511622747
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.470967741935485
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463446849741277
    mean_env_wait_ms: 1.1999890257991994
    mean_inference_ms: 4.299731588399593
    mean_raw_obs_processing_ms: 0.37849646096169587
  time_since_restore: 8472.52234530449
  time_this_iter_s: 26.45220398902893
  time_total_s: 8472.52234530449
  timers:
    learn_throughput: 8371.43
    learn_time_ms: 19326.686
    sample_throughput: 23527.237
    sample_time_ms: 6876.796
    update_time_ms: 40.989
  timestamp: 1602751631
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:47:12,596	WARNING util.py:136 -- The `process_trial` operation took 0.8316261768341064 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    320 |          8472.52 | 51773440 |  284.521 |              305.323 |              128.354 |            793.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3054.0982252141985
    time_step_min: 2950
  date: 2020-10-15_08-47-39
  done: false
  episode_len_mean: 793.3511736371282
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.55116884882165
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 204
  episodes_total: 65395
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9030931893478324e-31
        cur_lr: 5.0e-05
        entropy: 0.04913583553085724
        entropy_coeff: 0.0005000000000000001
        kl: 0.006080222898162901
        model: {}
        policy_loss: -0.0076519473805092275
        total_loss: 0.43425243347883224
        vf_explained_var: 0.9992749691009521
        vf_loss: 0.4419289479653041
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.759375
    gpu_util_percent0: 0.2984375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634354626071738
    mean_env_wait_ms: 1.1998942767420133
    mean_inference_ms: 4.299667896890491
    mean_raw_obs_processing_ms: 0.37849100565609467
  time_since_restore: 8499.016109704971
  time_this_iter_s: 26.493764400482178
  time_total_s: 8499.016109704971
  timers:
    learn_throughput: 8373.075
    learn_time_ms: 19322.889
    sample_throughput: 23523.196
    sample_time_ms: 6877.977
    update_time_ms: 40.466
  timestamp: 1602751659
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: c9144_00000
  
2020-10-15 08:47:40,262	WARNING util.py:136 -- The `process_trial` operation took 0.8630266189575195 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    321 |          8499.02 | 51935232 |  284.551 |              305.323 |              128.354 |            793.351 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3053.916736599582
    time_step_min: 2950
  date: 2020-10-15_08-48-06
  done: false
  episode_len_mean: 793.4044133345533
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.5780875827541
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 65574
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9030931893478324e-31
        cur_lr: 5.0e-05
        entropy: 0.046952096124490104
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007908354915950136
        total_loss: .inf
        vf_explained_var: 0.9993335604667664
        vf_loss: 0.3771243095397949
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.384375000000002
    gpu_util_percent0: 0.3325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634231881747922
    mean_env_wait_ms: 1.1998095677090965
    mean_inference_ms: 4.299605748629973
    mean_raw_obs_processing_ms: 0.37848568605527133
  time_since_restore: 8525.530243873596
  time_this_iter_s: 26.514134168624878
  time_total_s: 8525.530243873596
  timers:
    learn_throughput: 8376.455
    learn_time_ms: 19315.093
    sample_throughput: 23593.1
    sample_time_ms: 6857.598
    update_time_ms: 42.397
  timestamp: 1602751686
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:48:07,951	WARNING util.py:136 -- The `process_trial` operation took 0.8609488010406494 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    322 |          8525.53 | 52097024 |  284.578 |              305.323 |              128.354 |            793.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3053.728640540376
    time_step_min: 2950
  date: 2020-10-15_08-48-34
  done: false
  episode_len_mean: 793.4580564720909
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.60581210096933
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 193
  episodes_total: 65767
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.854639784021749e-31
        cur_lr: 5.0e-05
        entropy: 0.05303802558531364
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008823930601162525
        total_loss: .inf
        vf_explained_var: 0.9990519881248474
        vf_loss: 0.543364129960537
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.828125
    gpu_util_percent0: 0.36187499999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463416179559304
    mean_env_wait_ms: 1.1997222782472543
    mean_inference_ms: 4.299550674029524
    mean_raw_obs_processing_ms: 0.3784820605159814
  time_since_restore: 8552.0159034729
  time_this_iter_s: 26.4856595993042
  time_total_s: 8552.0159034729
  timers:
    learn_throughput: 8375.062
    learn_time_ms: 19318.304
    sample_throughput: 23619.001
    sample_time_ms: 6850.078
    update_time_ms: 42.106
  timestamp: 1602751714
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:48:35,788	WARNING util.py:136 -- The `process_trial` operation took 0.8602344989776611 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    323 |          8552.02 | 52258816 |  284.606 |              305.323 |              128.354 |            793.458 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3053.5148436789277
    time_step_min: 2950
  date: 2020-10-15_08-49-02
  done: false
  episode_len_mean: 793.5164345572747
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.63782194491864
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 222
  episodes_total: 65989
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.781959676032623e-31
        cur_lr: 5.0e-05
        entropy: 0.05414887424558401
        entropy_coeff: 0.0005000000000000001
        kl: 0.003412830352317542
        model: {}
        policy_loss: -0.009461205119805527
        total_loss: 0.6201387693484625
        vf_explained_var: 0.9990517497062683
        vf_loss: 0.6296270291010538
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.358064516129037
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463406903022845
    mean_env_wait_ms: 1.1996201088649914
    mean_inference_ms: 4.29948457860236
    mean_raw_obs_processing_ms: 0.3784760137102368
  time_since_restore: 8578.233249664307
  time_this_iter_s: 26.21734619140625
  time_total_s: 8578.233249664307
  timers:
    learn_throughput: 8384.513
    learn_time_ms: 19296.53
    sample_throughput: 23672.283
    sample_time_ms: 6834.66
    update_time_ms: 40.067
  timestamp: 1602751742
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: c9144_00000
  
2020-10-15 08:49:03,172	WARNING util.py:136 -- The `process_trial` operation took 0.8534903526306152 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    324 |          8578.23 | 52420608 |  284.638 |              305.323 |              128.354 |            793.516 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3053.321381603809
    time_step_min: 2950
  date: 2020-10-15_08-49-29
  done: false
  episode_len_mean: 793.5750264390391
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.6676746311888
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 201
  episodes_total: 66190
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3909798380163115e-31
        cur_lr: 5.0e-05
        entropy: 0.05007968501498302
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0077398679762457805
        total_loss: .inf
        vf_explained_var: 0.9994685053825378
        vf_loss: 0.3069004913171132
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.50625
    gpu_util_percent0: 0.33656250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633942637065725
    mean_env_wait_ms: 1.1995268811490303
    mean_inference_ms: 4.2994177748915
    mean_raw_obs_processing_ms: 0.3784705542340011
  time_since_restore: 8604.712239980698
  time_this_iter_s: 26.47899031639099
  time_total_s: 8604.712239980698
  timers:
    learn_throughput: 8399.27
    learn_time_ms: 19262.627
    sample_throughput: 23696.31
    sample_time_ms: 6827.73
    update_time_ms: 38.649
  timestamp: 1602751769
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:49:30,878	WARNING util.py:136 -- The `process_trial` operation took 0.9114570617675781 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    325 |          8604.71 | 52582400 |  284.668 |              305.323 |              128.354 |            793.575 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3053.157455981669
    time_step_min: 2950
  date: 2020-10-15_08-49-57
  done: false
  episode_len_mean: 793.6224405237227
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.6920032769576
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 66371
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.586469757024468e-31
        cur_lr: 5.0e-05
        entropy: 0.05289295191566149
        entropy_coeff: 0.0005000000000000001
        kl: 0.003474887227639556
        model: {}
        policy_loss: -0.009795664809644222
        total_loss: 0.4584497610727946
        vf_explained_var: 0.9991438984870911
        vf_loss: 0.4682718689242999
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.45483870967742
    gpu_util_percent0: 0.323225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633842616046638
    mean_env_wait_ms: 1.199443559582547
    mean_inference_ms: 4.299365093703404
    mean_raw_obs_processing_ms: 0.37846612467604596
  time_since_restore: 8631.403078079224
  time_this_iter_s: 26.690838098526
  time_total_s: 8631.403078079224
  timers:
    learn_throughput: 8390.828
    learn_time_ms: 19282.006
    sample_throughput: 23719.796
    sample_time_ms: 6820.969
    update_time_ms: 40.476
  timestamp: 1602751797
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: c9144_00000
  
2020-10-15 08:49:58,843	WARNING util.py:136 -- The `process_trial` operation took 0.857668399810791 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    326 |           8631.4 | 52744192 |  284.692 |              305.323 |              128.354 |            793.622 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3052.9912524611846
    time_step_min: 2950
  date: 2020-10-15_08-50-25
  done: false
  episode_len_mean: 793.674333012859
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.71878546916105
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 197
  episodes_total: 66568
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.293234878512234e-31
        cur_lr: 5.0e-05
        entropy: 0.05672451419134935
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008555819881924739
        total_loss: .inf
        vf_explained_var: 0.99899822473526
        vf_loss: 0.5830741475025812
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.296875
    gpu_util_percent0: 0.338125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633758574972183
    mean_env_wait_ms: 1.1993537000810786
    mean_inference_ms: 4.299303200816277
    mean_raw_obs_processing_ms: 0.37846145342651444
  time_since_restore: 8657.94597196579
  time_this_iter_s: 26.542893886566162
  time_total_s: 8657.94597196579
  timers:
    learn_throughput: 8397.631
    learn_time_ms: 19266.387
    sample_throughput: 23806.053
    sample_time_ms: 6796.255
    update_time_ms: 46.582
  timestamp: 1602751825
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:50:26,634	WARNING util.py:136 -- The `process_trial` operation took 0.929816484451294 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    327 |          8657.95 | 52905984 |  284.719 |              305.323 |              128.354 |            793.674 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3052.7821872659174
    time_step_min: 2950
  date: 2020-10-15_08-50-53
  done: false
  episode_len_mean: 793.736916972374
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.75026298017997
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 217
  episodes_total: 66785
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.939852317768351e-31
        cur_lr: 5.0e-05
        entropy: 0.054616233023504414
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009353951754974332
        total_loss: .inf
        vf_explained_var: 0.9995302557945251
        vf_loss: 0.2955921987692515
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.581249999999997
    gpu_util_percent0: 0.3209375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633672559278318
    mean_env_wait_ms: 1.199255399535438
    mean_inference_ms: 4.299242279026866
    mean_raw_obs_processing_ms: 0.3784559324989202
  time_since_restore: 8684.491067886353
  time_this_iter_s: 26.545095920562744
  time_total_s: 8684.491067886353
  timers:
    learn_throughput: 8391.855
    learn_time_ms: 19279.647
    sample_throughput: 23817.509
    sample_time_ms: 6792.986
    update_time_ms: 38.781
  timestamp: 1602751853
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:50:54,448	WARNING util.py:136 -- The `process_trial` operation took 0.9267361164093018 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    328 |          8684.49 | 53067776 |   284.75 |              305.323 |              128.354 |            793.737 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3052.613694414985
    time_step_min: 2950
  date: 2020-10-15_08-51-21
  done: false
  episode_len_mean: 793.7985428921203
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.7776787009566
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 197
  episodes_total: 66982
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.409778476652526e-31
        cur_lr: 5.0e-05
        entropy: 0.050136443537970386
        entropy_coeff: 0.0005000000000000001
        kl: 0.003061001072637737
        model: {}
        policy_loss: -0.007503679989895318
        total_loss: 0.4648931175470352
        vf_explained_var: 0.9992242455482483
        vf_loss: 0.4724218746026357
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.615625
    gpu_util_percent0: 0.276875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463354202700135
    mean_env_wait_ms: 1.1991651314917
    mean_inference_ms: 4.29917969608293
    mean_raw_obs_processing_ms: 0.37845090052566155
  time_since_restore: 8711.108030796051
  time_this_iter_s: 26.616962909698486
  time_total_s: 8711.108030796051
  timers:
    learn_throughput: 8394.257
    learn_time_ms: 19274.13
    sample_throughput: 23799.755
    sample_time_ms: 6798.053
    update_time_ms: 36.81
  timestamp: 1602751881
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: c9144_00000
  
2020-10-15 08:51:22,273	WARNING util.py:136 -- The `process_trial` operation took 0.8856747150421143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    329 |          8711.11 | 53229568 |  284.778 |              305.323 |              128.354 |            793.799 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3052.4335403911755
    time_step_min: 2950
  date: 2020-10-15_08-51-48
  done: false
  episode_len_mean: 793.8542417294465
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.80437989158173
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 67166
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.704889238326263e-31
        cur_lr: 5.0e-05
        entropy: 0.05362415499985218
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035716647980734706
        model: {}
        policy_loss: -0.008300383034414457
        total_loss: 0.3087688808639844
        vf_explained_var: 0.9994256496429443
        vf_loss: 0.317096084356308
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20625
    gpu_util_percent0: 0.33218749999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633434041150897
    mean_env_wait_ms: 1.1990809220062562
    mean_inference_ms: 4.299125483709119
    mean_raw_obs_processing_ms: 0.3784464145377688
  time_since_restore: 8737.432727575302
  time_this_iter_s: 26.3246967792511
  time_total_s: 8737.432727575302
  timers:
    learn_throughput: 8400.076
    learn_time_ms: 19260.779
    sample_throughput: 23802.098
    sample_time_ms: 6797.384
    update_time_ms: 37.236
  timestamp: 1602751908
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: c9144_00000
  
2020-10-15 08:51:49,945	WARNING util.py:136 -- The `process_trial` operation took 0.857518196105957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    330 |          8737.43 | 53391360 |  284.804 |              305.323 |              128.354 |            793.854 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3052.236021800793
    time_step_min: 2950
  date: 2020-10-15_08-52-16
  done: false
  episode_len_mean: 793.9146826574838
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.834356897959
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 206
  episodes_total: 67372
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8524446191631315e-31
        cur_lr: 5.0e-05
        entropy: 0.06047057484587034
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008514871859612564
        total_loss: .inf
        vf_explained_var: 0.9992840886116028
        vf_loss: 0.43902478863795596
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.703125
    gpu_util_percent0: 0.2925
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463337289424383
    mean_env_wait_ms: 1.1989879515132305
    mean_inference_ms: 4.299066408909677
    mean_raw_obs_processing_ms: 0.3784418144457709
  time_since_restore: 8763.9302546978
  time_this_iter_s: 26.49752712249756
  time_total_s: 8763.9302546978
  timers:
    learn_throughput: 8399.135
    learn_time_ms: 19262.936
    sample_throughput: 23812.296
    sample_time_ms: 6794.473
    update_time_ms: 37.493
  timestamp: 1602751936
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:52:17,859	WARNING util.py:136 -- The `process_trial` operation took 0.930351734161377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    331 |          8763.93 | 53553152 |  284.834 |              305.323 |              128.354 |            793.915 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3052.075159141377
    time_step_min: 2950
  date: 2020-10-15_08-52-44
  done: false
  episode_len_mean: 793.9617222756528
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.8596208440849
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 213
  episodes_total: 67585
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7786669287446973e-31
        cur_lr: 5.0e-05
        entropy: 0.07683922412494819
        entropy_coeff: 0.0005000000000000001
        kl: 0.004696517018601298
        model: {}
        policy_loss: -0.011926432508820048
        total_loss: 1.3140049080053966
        vf_explained_var: 0.9978187084197998
        vf_loss: 1.3259697755177815
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.887096774193548
    gpu_util_percent0: 0.3341935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633275965320483
    mean_env_wait_ms: 1.1988925388956508
    mean_inference_ms: 4.299008814960006
    mean_raw_obs_processing_ms: 0.3784364979568998
  time_since_restore: 8790.46765589714
  time_this_iter_s: 26.53740119934082
  time_total_s: 8790.46765589714
  timers:
    learn_throughput: 8398.661
    learn_time_ms: 19264.022
    sample_throughput: 23782.326
    sample_time_ms: 6803.035
    update_time_ms: 37.726
  timestamp: 1602751964
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: c9144_00000
  
2020-10-15 08:52:45,859	WARNING util.py:136 -- The `process_trial` operation took 0.8970272541046143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    332 |          8790.47 | 53714944 |   284.86 |              305.323 |              128.354 |            793.962 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3051.9268253077853
    time_step_min: 2950
  date: 2020-10-15_08-53-12
  done: false
  episode_len_mean: 793.9997491774495
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.8835216141825
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 192
  episodes_total: 67777
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3893334643723486e-31
        cur_lr: 5.0e-05
        entropy: 0.07376249010364215
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008757999819257142
        total_loss: .inf
        vf_explained_var: 0.9983752369880676
        vf_loss: 0.8722514361143112
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9875
    gpu_util_percent0: 0.2915625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633155392515768
    mean_env_wait_ms: 1.1988054375632364
    mean_inference_ms: 4.298945925565699
    mean_raw_obs_processing_ms: 0.3784314966961198
  time_since_restore: 8816.959034919739
  time_this_iter_s: 26.491379022598267
  time_total_s: 8816.959034919739
  timers:
    learn_throughput: 8400.407
    learn_time_ms: 19260.02
    sample_throughput: 23760.451
    sample_time_ms: 6809.298
    update_time_ms: 37.76
  timestamp: 1602751992
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:53:13,662	WARNING util.py:136 -- The `process_trial` operation took 0.9120817184448242 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    333 |          8816.96 | 53876736 |  284.884 |              305.323 |              128.354 |                794 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3051.7564961354433
    time_step_min: 2950
  date: 2020-10-15_08-53-40
  done: false
  episode_len_mean: 794.0484549735138
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.9087661488339
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 183
  episodes_total: 67960
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0840001965585234e-31
        cur_lr: 5.0e-05
        entropy: 0.0628617163747549
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010837087233085185
        total_loss: .inf
        vf_explained_var: 0.9993962645530701
        vf_loss: 0.3456090837717056
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.56875
    gpu_util_percent0: 0.2959375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463305878929113
    mean_env_wait_ms: 1.1987230219252776
    mean_inference_ms: 4.298894934841667
    mean_raw_obs_processing_ms: 0.3784272063595955
  time_since_restore: 8843.46925520897
  time_this_iter_s: 26.510220289230347
  time_total_s: 8843.46925520897
  timers:
    learn_throughput: 8396.172
    learn_time_ms: 19269.734
    sample_throughput: 23739.515
    sample_time_ms: 6815.303
    update_time_ms: 38.811
  timestamp: 1602752020
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:53:41,414	WARNING util.py:136 -- The `process_trial` operation took 0.9107749462127686 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    334 |          8843.47 | 54038528 |  284.909 |              305.323 |              128.354 |            794.048 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3051.5633961987232
    time_step_min: 2950
  date: 2020-10-15_08-54-08
  done: false
  episode_len_mean: 794.1062344139651
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.93906128914205
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 210
  episodes_total: 68170
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1260002948377844e-31
        cur_lr: 5.0e-05
        entropy: 0.06216672621667385
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007003627989130716
        total_loss: .inf
        vf_explained_var: 0.9994646906852722
        vf_loss: 0.3242342298229535
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.021875
    gpu_util_percent0: 0.2978125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633002169393236
    mean_env_wait_ms: 1.1986292110151677
    mean_inference_ms: 4.298837394241388
    mean_raw_obs_processing_ms: 0.3784225692404537
  time_since_restore: 8870.090977430344
  time_this_iter_s: 26.62172222137451
  time_total_s: 8870.090977430344
  timers:
    learn_throughput: 8385.823
    learn_time_ms: 19293.515
    sample_throughput: 23780.748
    sample_time_ms: 6803.486
    update_time_ms: 40.201
  timestamp: 1602752048
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:54:09,257	WARNING util.py:136 -- The `process_trial` operation took 0.8960158824920654 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    335 |          8870.09 | 54200320 |  284.939 |              305.323 |              128.354 |            794.106 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3051.3637121788493
    time_step_min: 2950
  date: 2020-10-15_08-54-35
  done: false
  episode_len_mean: 794.1643390901247
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.9679428861536
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 213
  episodes_total: 68383
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.689000442256678e-31
        cur_lr: 5.0e-05
        entropy: 0.06198591242233912
        entropy_coeff: 0.0005000000000000001
        kl: 0.003089892580950012
        model: {}
        policy_loss: -0.005854797724168748
        total_loss: 1.2970982193946838
        vf_explained_var: 0.9979084134101868
        vf_loss: 1.3029840290546417
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280645161290323
    gpu_util_percent0: 0.3096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632894791161022
    mean_env_wait_ms: 1.1985345015718931
    mean_inference_ms: 4.298780222027828
    mean_raw_obs_processing_ms: 0.37841763007076773
  time_since_restore: 8896.321419715881
  time_this_iter_s: 26.23044228553772
  time_total_s: 8896.321419715881
  timers:
    learn_throughput: 8399.289
    learn_time_ms: 19262.582
    sample_throughput: 23829.308
    sample_time_ms: 6789.622
    update_time_ms: 37.825
  timestamp: 1602752075
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: c9144_00000
  
2020-10-15 08:54:36,760	WARNING util.py:136 -- The `process_trial` operation took 0.8824906349182129 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    336 |          8896.32 | 54362112 |  284.968 |              305.323 |              128.354 |            794.164 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3051.1951646555876
    time_step_min: 2950
  date: 2020-10-15_08-55-03
  done: false
  episode_len_mean: 794.2162106982441
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 284.9934683414676
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 189
  episodes_total: 68572
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.344500221128339e-31
        cur_lr: 5.0e-05
        entropy: 0.058302622909347214
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036669696564786136
        model: {}
        policy_loss: -0.007456379108286153
        total_loss: 0.6011908426880836
        vf_explained_var: 0.9989409446716309
        vf_loss: 0.6086763838926951
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.046875
    gpu_util_percent0: 0.2946875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463277379265018
    mean_env_wait_ms: 1.1984491384557339
    mean_inference_ms: 4.298719844289456
    mean_raw_obs_processing_ms: 0.3784127594941692
  time_since_restore: 8922.704924821854
  time_this_iter_s: 26.38350510597229
  time_total_s: 8922.704924821854
  timers:
    learn_throughput: 8404.036
    learn_time_ms: 19251.702
    sample_throughput: 23841.858
    sample_time_ms: 6786.048
    update_time_ms: 30.632
  timestamp: 1602752103
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: c9144_00000
  
2020-10-15 08:55:04,386	WARNING util.py:136 -- The `process_trial` operation took 0.9124515056610107 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    337 |           8922.7 | 54523904 |  284.993 |              305.323 |              128.354 |            794.216 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3051.02977429166
    time_step_min: 2950
  date: 2020-10-15_08-55-31
  done: false
  episode_len_mean: 794.2698394228531
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.0187761663646
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 180
  episodes_total: 68752
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1722501105641694e-31
        cur_lr: 5.0e-05
        entropy: 0.055464242895444237
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008621668375174826
        total_loss: .inf
        vf_explained_var: 0.9995174407958984
        vf_loss: 0.27694425731897354
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.603125
    gpu_util_percent0: 0.2834375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632693294649612
    mean_env_wait_ms: 1.1983694776775224
    mean_inference_ms: 4.298672259525389
    mean_raw_obs_processing_ms: 0.37840864560218035
  time_since_restore: 8949.499886989594
  time_this_iter_s: 26.794962167739868
  time_total_s: 8949.499886989594
  timers:
    learn_throughput: 8397.847
    learn_time_ms: 19265.89
    sample_throughput: 23813.369
    sample_time_ms: 6794.167
    update_time_ms: 32.538
  timestamp: 1602752131
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:55:32,495	WARNING util.py:136 -- The `process_trial` operation took 0.9820544719696045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    338 |           8949.5 | 54685696 |  285.019 |              305.323 |              128.354 |             794.27 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3050.8202193402385
    time_step_min: 2950
  date: 2020-10-15_08-55-58
  done: false
  episode_len_mean: 794.3271469790776
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.0494136510753
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 217
  episodes_total: 68969
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7583751658462538e-31
        cur_lr: 5.0e-05
        entropy: 0.05793195931861798
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008259825990535319
        total_loss: .inf
        vf_explained_var: 0.9995633959770203
        vf_loss: 0.2744135508934657
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.425
    gpu_util_percent0: 0.2478125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632639137102835
    mean_env_wait_ms: 1.1982733952033042
    mean_inference_ms: 4.29861671714327
    mean_raw_obs_processing_ms: 0.378404124100667
  time_since_restore: 8975.986958026886
  time_this_iter_s: 26.48707103729248
  time_total_s: 8975.986958026886
  timers:
    learn_throughput: 8399.685
    learn_time_ms: 19261.675
    sample_throughput: 23826.461
    sample_time_ms: 6790.433
    update_time_ms: 33.985
  timestamp: 1602752158
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:56:00,387	WARNING util.py:136 -- The `process_trial` operation took 0.8896381855010986 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    339 |          8975.99 | 54847488 |  285.049 |              305.323 |              128.354 |            794.327 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3050.620997367736
    time_step_min: 2950
  date: 2020-10-15_08-56-26
  done: false
  episode_len_mean: 794.3792156352545
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.0790409844576
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 208
  episodes_total: 69177
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.637562748769381e-31
        cur_lr: 5.0e-05
        entropy: 0.0577400466427207
        entropy_coeff: 0.0005000000000000001
        kl: 0.0062444052115703625
        model: {}
        policy_loss: -0.006984597275732085
        total_loss: 0.29150237143039703
        vf_explained_var: 0.9995083808898926
        vf_loss: 0.2985158289472262
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.861290322580647
    gpu_util_percent0: 0.29903225806451617
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632527727328437
    mean_env_wait_ms: 1.1981819113893368
    mean_inference_ms: 4.298559872600318
    mean_raw_obs_processing_ms: 0.37839932689423794
  time_since_restore: 9002.457711935043
  time_this_iter_s: 26.47075390815735
  time_total_s: 9002.457711935043
  timers:
    learn_throughput: 8397.117
    learn_time_ms: 19267.565
    sample_throughput: 23794.391
    sample_time_ms: 6799.586
    update_time_ms: 34.079
  timestamp: 1602752186
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: c9144_00000
  
2020-10-15 08:56:28,206	WARNING util.py:136 -- The `process_trial` operation took 0.9162924289703369 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    340 |          9002.46 | 55009280 |  285.079 |              305.323 |              128.354 |            794.379 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3050.4326366953687
    time_step_min: 2950
  date: 2020-10-15_08-56-54
  done: false
  episode_len_mean: 794.4290739245762
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.1072978953414
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 69368
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.637562748769381e-31
        cur_lr: 5.0e-05
        entropy: 0.05809598012516896
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008262775198090822
        total_loss: .inf
        vf_explained_var: 0.9996314644813538
        vf_loss: 0.22568664823969206
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.33125
    gpu_util_percent0: 0.318125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632417828454536
    mean_env_wait_ms: 1.1980960912240741
    mean_inference_ms: 4.298503797057087
    mean_raw_obs_processing_ms: 0.37839449884546783
  time_since_restore: 9029.195475816727
  time_this_iter_s: 26.73776388168335
  time_total_s: 9029.195475816727
  timers:
    learn_throughput: 8395.618
    learn_time_ms: 19271.006
    sample_throughput: 23728.887
    sample_time_ms: 6818.356
    update_time_ms: 35.053
  timestamp: 1602752214
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:56:56,273	WARNING util.py:136 -- The `process_trial` operation took 0.9141664505004883 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    341 |           9029.2 | 55171072 |  285.107 |              305.323 |              128.354 |            794.429 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3050.260793255744
    time_step_min: 2950
  date: 2020-10-15_08-57-22
  done: false
  episode_len_mean: 794.4708969602852
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.13345385526395
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 178
  episodes_total: 69546
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.956344123154071e-31
        cur_lr: 5.0e-05
        entropy: 0.06061066500842571
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008782241605028199
        total_loss: .inf
        vf_explained_var: 0.9994367957115173
        vf_loss: 0.3120071291923523
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.762500000000003
    gpu_util_percent0: 0.359375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632334005941305
    mean_env_wait_ms: 1.1980175848952557
    mean_inference_ms: 4.2984548776238825
    mean_raw_obs_processing_ms: 0.3783905049393442
  time_since_restore: 9055.678745508194
  time_this_iter_s: 26.483269691467285
  time_total_s: 9055.678745508194
  timers:
    learn_throughput: 8402.645
    learn_time_ms: 19254.889
    sample_throughput: 23687.563
    sample_time_ms: 6830.251
    update_time_ms: 33.943
  timestamp: 1602752242
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:57:24,043	WARNING util.py:136 -- The `process_trial` operation took 0.9604864120483398 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    342 |          9055.68 | 55332864 |  285.133 |              305.323 |              128.354 |            794.471 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3050.0627070278333
    time_step_min: 2950
  date: 2020-10-15_08-57-50
  done: false
  episode_len_mean: 794.514074413805
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.16553252527575
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 226
  episodes_total: 69772
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.934516184731107e-31
        cur_lr: 5.0e-05
        entropy: 0.06270458394040664
        entropy_coeff: 0.0005000000000000001
        kl: 0.004821313894353807
        model: {}
        policy_loss: -0.00813452719982403
        total_loss: 0.5568353459239006
        vf_explained_var: 0.9991028904914856
        vf_loss: 0.5650012443463007
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5375
    gpu_util_percent0: 0.36874999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463226241713106
    mean_env_wait_ms: 1.1979189234757603
    mean_inference_ms: 4.2983994854103065
    mean_raw_obs_processing_ms: 0.3783858130673568
  time_since_restore: 9082.271469116211
  time_this_iter_s: 26.592723608016968
  time_total_s: 9082.271469116211
  timers:
    learn_throughput: 8402.245
    learn_time_ms: 19255.806
    sample_throughput: 23655.3
    sample_time_ms: 6839.567
    update_time_ms: 32.476
  timestamp: 1602752270
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: c9144_00000
  
2020-10-15 08:57:51,854	WARNING util.py:136 -- The `process_trial` operation took 0.8936338424682617 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    343 |          9082.27 | 55494656 |  285.166 |              305.323 |              128.354 |            794.514 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3049.846770849716
    time_step_min: 2950
  date: 2020-10-15_08-58-18
  done: false
  episode_len_mean: 794.559663313136
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.197165290726
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 204
  episodes_total: 69976
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9672580923655536e-31
        cur_lr: 5.0e-05
        entropy: 0.05824289129426082
        entropy_coeff: 0.0005000000000000001
        kl: 0.004646655792991321
        model: {}
        policy_loss: -0.0067858526890631765
        total_loss: 0.31811687101920444
        vf_explained_var: 0.9994373917579651
        vf_loss: 0.3249318351348241
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93125
    gpu_util_percent0: 0.36812500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632178624854683
    mean_env_wait_ms: 1.197830397930365
    mean_inference_ms: 4.298345628958128
    mean_raw_obs_processing_ms: 0.3783814214399419
  time_since_restore: 9108.777004480362
  time_this_iter_s: 26.505535364151
  time_total_s: 9108.777004480362
  timers:
    learn_throughput: 8396.521
    learn_time_ms: 19268.933
    sample_throughput: 23669.598
    sample_time_ms: 6835.435
    update_time_ms: 32.463
  timestamp: 1602752298
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: c9144_00000
  
2020-10-15 08:58:19,646	WARNING util.py:136 -- The `process_trial` operation took 0.9100618362426758 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    344 |          9108.78 | 55656448 |  285.197 |              305.323 |              128.354 |             794.56 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3049.646350854177
    time_step_min: 2950
  date: 2020-10-15_08-58-46
  done: false
  episode_len_mean: 794.6040535340146
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.22702502858147
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 185
  episodes_total: 70161
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4836290461827768e-31
        cur_lr: 5.0e-05
        entropy: 0.05730218626558781
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008775635690350706
        total_loss: .inf
        vf_explained_var: 0.9995750784873962
        vf_loss: 0.24573416511217752
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.799999999999997
    gpu_util_percent0: 0.2875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632064985331475
    mean_env_wait_ms: 1.1977480412280561
    mean_inference_ms: 4.298293208155909
    mean_raw_obs_processing_ms: 0.37837687288509686
  time_since_restore: 9135.300819396973
  time_this_iter_s: 26.523814916610718
  time_total_s: 9135.300819396973
  timers:
    learn_throughput: 8401.028
    learn_time_ms: 19258.596
    sample_throughput: 23660.235
    sample_time_ms: 6838.14
    update_time_ms: 30.419
  timestamp: 1602752326
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:58:47,409	WARNING util.py:136 -- The `process_trial` operation took 0.8930797576904297 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    345 |           9135.3 | 55818240 |  285.227 |              305.323 |              128.354 |            794.604 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3049.446015131691
    time_step_min: 2950
  date: 2020-10-15_08-59-14
  done: false
  episode_len_mean: 794.6466148313457
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.25676043745443
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 190
  episodes_total: 70351
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2254435692741655e-31
        cur_lr: 5.0e-05
        entropy: 0.060704924166202545
        entropy_coeff: 0.0005000000000000001
        kl: 0.006066904752515256
        model: {}
        policy_loss: -0.006780258486590658
        total_loss: 0.2725507840514183
        vf_explained_var: 0.9995083808898926
        vf_loss: 0.27936139330267906
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.284374999999997
    gpu_util_percent0: 0.2565625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632014775816757
    mean_env_wait_ms: 1.1976649123502607
    mean_inference_ms: 4.298240512525064
    mean_raw_obs_processing_ms: 0.37837268074101327
  time_since_restore: 9162.036808013916
  time_this_iter_s: 26.73598861694336
  time_total_s: 9162.036808013916
  timers:
    learn_throughput: 8384.189
    learn_time_ms: 19297.276
    sample_throughput: 23656.646
    sample_time_ms: 6839.177
    update_time_ms: 31.768
  timestamp: 1602752354
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: c9144_00000
  
2020-10-15 08:59:15,406	WARNING util.py:136 -- The `process_trial` operation took 0.9299097061157227 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    346 |          9162.04 | 55980032 |  285.257 |              305.323 |              128.354 |            794.647 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3049.217181740856
    time_step_min: 2950
  date: 2020-10-15_08-59-42
  done: false
  episode_len_mean: 794.6965214311017
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.2919846713683
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 224
  episodes_total: 70575
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2254435692741655e-31
        cur_lr: 5.0e-05
        entropy: 0.06451559998095036
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00779983660322614
        total_loss: .inf
        vf_explained_var: 0.9995129108428955
        vf_loss: 0.2944226289788882
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.515625
    gpu_util_percent0: 0.2475
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631923283991574
    mean_env_wait_ms: 1.1975689667323484
    mean_inference_ms: 4.298197790698127
    mean_raw_obs_processing_ms: 0.3783686486677682
  time_since_restore: 9188.7956969738
  time_this_iter_s: 26.758888959884644
  time_total_s: 9188.7956969738
  timers:
    learn_throughput: 8368.845
    learn_time_ms: 19332.656
    sample_throughput: 23677.977
    sample_time_ms: 6833.016
    update_time_ms: 32.242
  timestamp: 1602752382
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 08:59:43,443	WARNING util.py:136 -- The `process_trial` operation took 0.9327888488769531 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    347 |           9188.8 | 56141824 |  285.292 |              305.323 |              128.354 |            794.697 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3049.014631663769
    time_step_min: 2950
  date: 2020-10-15_09-00-10
  done: false
  episode_len_mean: 794.7415220708755
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.32254937894174
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 197
  episodes_total: 70772
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3381653539112474e-31
        cur_lr: 5.0e-05
        entropy: 0.06451648535827796
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007341767360533898
        total_loss: .inf
        vf_explained_var: 0.99908846616745
        vf_loss: 0.5076409503817558
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.74375
    gpu_util_percent0: 0.32875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631831218951832
    mean_env_wait_ms: 1.1974830666708174
    mean_inference_ms: 4.2981382225267035
    mean_raw_obs_processing_ms: 0.37836377079083217
  time_since_restore: 9215.494328260422
  time_this_iter_s: 26.698631286621094
  time_total_s: 9215.494328260422
  timers:
    learn_throughput: 8365.203
    learn_time_ms: 19341.073
    sample_throughput: 23752.952
    sample_time_ms: 6811.448
    update_time_ms: 32.447
  timestamp: 1602752410
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:00:11,401	WARNING util.py:136 -- The `process_trial` operation took 0.9206335544586182 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    348 |          9215.49 | 56303616 |  285.323 |              305.323 |              128.354 |            794.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3048.8357914328417
    time_step_min: 2950
  date: 2020-10-15_09-00-38
  done: false
  episode_len_mean: 794.7791902137915
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.3484545982679
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 185
  episodes_total: 70957
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.007248030866871e-31
        cur_lr: 5.0e-05
        entropy: 0.06019416109969219
        entropy_coeff: 0.0005000000000000001
        kl: 0.005318599675471584
        model: {}
        policy_loss: -0.01057420705910772
        total_loss: 0.28533930455644924
        vf_explained_var: 0.9994571208953857
        vf_loss: 0.29594360291957855
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.065625
    gpu_util_percent0: 0.269375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146317317106626
    mean_env_wait_ms: 1.1974023693014568
    mean_inference_ms: 4.298089318386519
    mean_raw_obs_processing_ms: 0.3783598016254439
  time_since_restore: 9242.138443231583
  time_this_iter_s: 26.64411497116089
  time_total_s: 9242.138443231583
  timers:
    learn_throughput: 8358.105
    learn_time_ms: 19357.497
    sample_throughput: 23753.44
    sample_time_ms: 6811.308
    update_time_ms: 32.189
  timestamp: 1602752438
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: c9144_00000
  
2020-10-15 09:00:39,516	WARNING util.py:136 -- The `process_trial` operation took 0.9594426155090332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    349 |          9242.14 | 56465408 |  285.348 |              305.323 |              128.354 |            794.779 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3048.632641583713
    time_step_min: 2950
  date: 2020-10-15_09-01-06
  done: false
  episode_len_mean: 794.8273865568656
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.3796978483666
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 202
  episodes_total: 71159
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.007248030866871e-31
        cur_lr: 5.0e-05
        entropy: 0.05823983655621608
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006064658349108261
        total_loss: .inf
        vf_explained_var: 0.9994242787361145
        vf_loss: 0.38704265654087067
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.38125
    gpu_util_percent0: 0.2659375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631702898174784
    mean_env_wait_ms: 1.1973157818032685
    mean_inference_ms: 4.2980416937574795
    mean_raw_obs_processing_ms: 0.37835579477517284
  time_since_restore: 9268.880733966827
  time_this_iter_s: 26.74229073524475
  time_total_s: 9268.880733966827
  timers:
    learn_throughput: 8354.333
    learn_time_ms: 19366.238
    sample_throughput: 23738.151
    sample_time_ms: 6815.695
    update_time_ms: 33.204
  timestamp: 1602752466
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:01:07,598	WARNING util.py:136 -- The `process_trial` operation took 1.004058837890625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    350 |          9268.88 | 56627200 |   285.38 |              305.323 |              128.354 |            794.827 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3048.40898832303
    time_step_min: 2950
  date: 2020-10-15_09-01-34
  done: false
  episode_len_mean: 794.8768424592278
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.4138501531596
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 213
  episodes_total: 71372
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.510872046300306e-31
        cur_lr: 5.0e-05
        entropy: 0.05866362899541855
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006957429878336067
        total_loss: .inf
        vf_explained_var: 0.9997208118438721
        vf_loss: 0.16540790597597757
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.896774193548392
    gpu_util_percent0: 0.3225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146315956088108
    mean_env_wait_ms: 1.1972242026773587
    mean_inference_ms: 4.297992800113592
    mean_raw_obs_processing_ms: 0.37835119516726623
  time_since_restore: 9295.389619112015
  time_this_iter_s: 26.508885145187378
  time_total_s: 9295.389619112015
  timers:
    learn_throughput: 8358.004
    learn_time_ms: 19357.731
    sample_throughput: 23788.918
    sample_time_ms: 6801.15
    update_time_ms: 32.928
  timestamp: 1602752494
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:01:35,515	WARNING util.py:136 -- The `process_trial` operation took 0.9756276607513428 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    351 |          9295.39 | 56788992 |  285.414 |              305.323 |              128.354 |            794.877 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3048.2124194427747
    time_step_min: 2950
  date: 2020-10-15_09-02-02
  done: false
  episode_len_mean: 794.9218086295551
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.4440201816964
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 196
  episodes_total: 71568
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1266308069450461e-30
        cur_lr: 5.0e-05
        entropy: 0.05604296984771887
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007376325316727161
        total_loss: .inf
        vf_explained_var: 0.9995672702789307
        vf_loss: 0.2599253294368585
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.31875
    gpu_util_percent0: 0.27718750000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631498790851366
    mean_env_wait_ms: 1.1971400829902574
    mean_inference_ms: 4.2979410390075135
    mean_raw_obs_processing_ms: 0.37834696255297223
  time_since_restore: 9322.053609132767
  time_this_iter_s: 26.663990020751953
  time_total_s: 9322.053609132767
  timers:
    learn_throughput: 8346.935
    learn_time_ms: 19383.402
    sample_throughput: 23827.789
    sample_time_ms: 6790.055
    update_time_ms: 33.669
  timestamp: 1602752522
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:02:03,575	WARNING util.py:136 -- The `process_trial` operation took 0.9579265117645264 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    352 |          9322.05 | 56950784 |  285.444 |              305.323 |              128.354 |            794.922 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3048.020009481845
    time_step_min: 2950
  date: 2020-10-15_09-02-30
  done: false
  episode_len_mean: 794.9650039720701
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.4723887281901
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 183
  episodes_total: 71751
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.689946210417569e-30
        cur_lr: 5.0e-05
        entropy: 0.055931245908141136
        entropy_coeff: 0.0005000000000000001
        kl: 0.004754078070012231
        model: {}
        policy_loss: -0.007416409374854993
        total_loss: 0.29505615184704465
        vf_explained_var: 0.9994496703147888
        vf_loss: 0.30250051617622375
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.80625
    gpu_util_percent0: 0.2753125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631409502510342
    mean_env_wait_ms: 1.1970611811170335
    mean_inference_ms: 4.297893050623558
    mean_raw_obs_processing_ms: 0.37834287386621035
  time_since_restore: 9348.601327180862
  time_this_iter_s: 26.547718048095703
  time_total_s: 9348.601327180862
  timers:
    learn_throughput: 8347.129
    learn_time_ms: 19382.952
    sample_throughput: 23851.486
    sample_time_ms: 6783.309
    update_time_ms: 35.165
  timestamp: 1602752550
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: c9144_00000
  
2020-10-15 09:02:31,492	WARNING util.py:136 -- The `process_trial` operation took 0.9375615119934082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    353 |           9348.6 | 57112576 |  285.472 |              305.323 |              128.354 |            794.965 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3047.8011399972197
    time_step_min: 2950
  date: 2020-10-15_09-02-57
  done: false
  episode_len_mean: 795.0093378725769
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.5056484388104
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 214
  episodes_total: 71965
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.449731052087845e-31
        cur_lr: 5.0e-05
        entropy: 0.060522353587051235
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005560101504670456
        total_loss: .inf
        vf_explained_var: 0.9994856715202332
        vf_loss: 0.3167388439178467
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35625
    gpu_util_percent0: 0.30562500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631349505934255
    mean_env_wait_ms: 1.1969705878067456
    mean_inference_ms: 4.29784505417737
    mean_raw_obs_processing_ms: 0.3783386968905499
  time_since_restore: 9375.005180120468
  time_this_iter_s: 26.403852939605713
  time_total_s: 9375.005180120468
  timers:
    learn_throughput: 8349.083
    learn_time_ms: 19378.415
    sample_throughput: 23872.095
    sample_time_ms: 6777.453
    update_time_ms: 34.072
  timestamp: 1602752577
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:02:59,308	WARNING util.py:136 -- The `process_trial` operation took 0.9782614707946777 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    354 |          9375.01 | 57274368 |  285.506 |              305.323 |              128.354 |            795.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3047.5972025839365
    time_step_min: 2950
  date: 2020-10-15_09-03-25
  done: false
  episode_len_mean: 795.0547018968313
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.53665680120173
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 208
  episodes_total: 72173
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2674596578131768e-30
        cur_lr: 5.0e-05
        entropy: 0.05886805181701978
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009404661832377315
        total_loss: .inf
        vf_explained_var: 0.9992913603782654
        vf_loss: 0.41058063010374707
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.6625
    gpu_util_percent0: 0.26656250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631280137325464
    mean_env_wait_ms: 1.1968830544597917
    mean_inference_ms: 4.297800257755286
    mean_raw_obs_processing_ms: 0.37833487044236797
  time_since_restore: 9401.602284193039
  time_this_iter_s: 26.5971040725708
  time_total_s: 9401.602284193039
  timers:
    learn_throughput: 8341.374
    learn_time_ms: 19396.326
    sample_throughput: 23886.082
    sample_time_ms: 6773.484
    update_time_ms: 34.432
  timestamp: 1602752605
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:03:27,234	WARNING util.py:136 -- The `process_trial` operation took 0.9278013706207275 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    355 |           9401.6 | 57436160 |  285.537 |              305.323 |              128.354 |            795.055 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3047.411387329591
    time_step_min: 2950
  date: 2020-10-15_09-03-54
  done: false
  episode_len_mean: 795.0955210679786
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.56352374088453
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 188
  episodes_total: 72361
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9011894867197648e-30
        cur_lr: 5.0e-05
        entropy: 0.056831132931013904
        entropy_coeff: 0.0005000000000000001
        kl: 0.004505029142213364
        model: {}
        policy_loss: -0.007111530969268642
        total_loss: 0.3168407181898753
        vf_explained_var: 0.9994067549705505
        vf_loss: 0.323980654279391
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.628124999999997
    gpu_util_percent0: 0.28312499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463118290081157
    mean_env_wait_ms: 1.1968018258598052
    mean_inference_ms: 4.2977468221325275
    mean_raw_obs_processing_ms: 0.378330576354098
  time_since_restore: 9428.48489356041
  time_this_iter_s: 26.882609367370605
  time_total_s: 9428.48489356041
  timers:
    learn_throughput: 8345.058
    learn_time_ms: 19387.762
    sample_throughput: 23773.567
    sample_time_ms: 6805.542
    update_time_ms: 35.422
  timestamp: 1602752634
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: c9144_00000
  
2020-10-15 09:03:55,481	WARNING util.py:136 -- The `process_trial` operation took 0.9262866973876953 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    356 |          9428.48 | 57597952 |  285.564 |              305.323 |              128.354 |            795.096 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3047.2358197840385
    time_step_min: 2950
  date: 2020-10-15_09-04-22
  done: false
  episode_len_mean: 795.1346694602195
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.5913975171019
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 72548
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.505947433598824e-31
        cur_lr: 5.0e-05
        entropy: 0.05710577250768741
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009498603641986847
        total_loss: .inf
        vf_explained_var: 0.9992907047271729
        vf_loss: 0.4022391860683759
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.440625
    gpu_util_percent0: 0.324375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631101880471767
    mean_env_wait_ms: 1.1967222765145769
    mean_inference_ms: 4.297698876893402
    mean_raw_obs_processing_ms: 0.3783267010845672
  time_since_restore: 9455.120105981827
  time_this_iter_s: 26.635212421417236
  time_total_s: 9455.120105981827
  timers:
    learn_throughput: 8352.767
    learn_time_ms: 19369.869
    sample_throughput: 23720.858
    sample_time_ms: 6820.664
    update_time_ms: 35.229
  timestamp: 1602752662
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:04:23,495	WARNING util.py:136 -- The `process_trial` operation took 1.02950119972229 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    357 |          9455.12 | 57759744 |  285.591 |              305.323 |              128.354 |            795.135 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3047.02556099439
    time_step_min: 2950
  date: 2020-10-15_09-04-49
  done: false
  episode_len_mean: 795.1811909899262
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.6234764949497
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 215
  episodes_total: 72763
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4258921150398235e-30
        cur_lr: 5.0e-05
        entropy: 0.06096358628322681
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007705827413398462
        total_loss: .inf
        vf_explained_var: 0.9994783401489258
        vf_loss: 0.30613718430201214
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925
    gpu_util_percent0: 0.29625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631032514359246
    mean_env_wait_ms: 1.1966313770553962
    mean_inference_ms: 4.297652743892259
    mean_raw_obs_processing_ms: 0.3783221923534209
  time_since_restore: 9481.563266515732
  time_this_iter_s: 26.44316053390503
  time_total_s: 9481.563266515732
  timers:
    learn_throughput: 8366.598
    learn_time_ms: 19337.847
    sample_throughput: 23690.471
    sample_time_ms: 6829.413
    update_time_ms: 33.899
  timestamp: 1602752689
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:04:51,305	WARNING util.py:136 -- The `process_trial` operation took 1.023775577545166 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    358 |          9481.56 | 57921536 |  285.623 |              305.323 |              128.354 |            795.181 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3046.852514977448
    time_step_min: 2950
  date: 2020-10-15_09-05-17
  done: false
  episode_len_mean: 795.2068157526926
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.6446866095802
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 215
  episodes_total: 72978
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1388381725597357e-30
        cur_lr: 5.0e-05
        entropy: 0.07819981127977371
        entropy_coeff: 0.0005000000000000001
        kl: 0.006483176529097061
        model: {}
        policy_loss: -0.012013678890070878
        total_loss: 1.1700223485628765
        vf_explained_var: 0.9978719353675842
        vf_loss: 1.1820751527945201
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.159375
    gpu_util_percent0: 0.274375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146309536020072
    mean_env_wait_ms: 1.1965420186426448
    mean_inference_ms: 4.297604376236301
    mean_raw_obs_processing_ms: 0.3783183221236986
  time_since_restore: 9508.168692350388
  time_this_iter_s: 26.60542583465576
  time_total_s: 9508.168692350388
  timers:
    learn_throughput: 8373.058
    learn_time_ms: 19322.929
    sample_throughput: 23695.165
    sample_time_ms: 6828.06
    update_time_ms: 34.67
  timestamp: 1602752717
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: c9144_00000
  
2020-10-15 09:05:19,334	WARNING util.py:136 -- The `process_trial` operation took 1.0121405124664307 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    359 |          9508.17 | 58083328 |  285.645 |              305.323 |              128.354 |            795.207 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3046.793495801537
    time_step_min: 2950
  date: 2020-10-15_09-05-45
  done: false
  episode_len_mean: 795.2092075946252
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.6529880457735
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 73157
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1388381725597357e-30
        cur_lr: 5.0e-05
        entropy: 0.07392538463075955
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010469864665841063
        total_loss: .inf
        vf_explained_var: 0.9969245791435242
        vf_loss: 1.541671911875407
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.575000000000003
    gpu_util_percent0: 0.258125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630868101089367
    mean_env_wait_ms: 1.1964660314909341
    mean_inference_ms: 4.297558441699707
    mean_raw_obs_processing_ms: 0.3783144940790021
  time_since_restore: 9534.791857481003
  time_this_iter_s: 26.623165130615234
  time_total_s: 9534.791857481003
  timers:
    learn_throughput: 8373.531
    learn_time_ms: 19321.837
    sample_throughput: 23733.464
    sample_time_ms: 6817.041
    update_time_ms: 35.337
  timestamp: 1602752745
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:05:47,258	WARNING util.py:136 -- The `process_trial` operation took 0.9462742805480957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    360 |          9534.79 | 58245120 |  285.653 |              305.323 |              128.354 |            795.209 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3046.631867532326
    time_step_min: 2950
  date: 2020-10-15_09-06-13
  done: false
  episode_len_mean: 795.2431050701422
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.6793965200395
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 194
  episodes_total: 73351
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.208257258839603e-30
        cur_lr: 5.0e-05
        entropy: 0.05859146142999331
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010489778670792779
        total_loss: .inf
        vf_explained_var: 0.9993400573730469
        vf_loss: 0.3720027133822441
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0
    gpu_util_percent0: 0.27
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463079649699375
    mean_env_wait_ms: 1.1963854170207373
    mean_inference_ms: 4.297514699999826
    mean_raw_obs_processing_ms: 0.3783109014165592
  time_since_restore: 9561.523374319077
  time_this_iter_s: 26.73151683807373
  time_total_s: 9561.523374319077
  timers:
    learn_throughput: 8366.537
    learn_time_ms: 19337.989
    sample_throughput: 23745.151
    sample_time_ms: 6813.686
    update_time_ms: 35.429
  timestamp: 1602752773
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:06:15,280	WARNING util.py:136 -- The `process_trial` operation took 0.9436583518981934 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    361 |          9561.52 | 58406912 |  285.679 |              305.323 |              128.354 |            795.243 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3046.4281634595773
    time_step_min: 2950
  date: 2020-10-15_09-06-42
  done: false
  episode_len_mean: 795.288826967514
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.7114731658025
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 73570
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.8123858882594064e-30
        cur_lr: 5.0e-05
        entropy: 0.05725443456321955
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006797509132108341
        total_loss: .inf
        vf_explained_var: 0.9993212819099426
        vf_loss: 0.41160694013039273
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.525
    gpu_util_percent0: 0.23031249999999998
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630728602706264
    mean_env_wait_ms: 1.1962938948778714
    mean_inference_ms: 4.297467908840305
    mean_raw_obs_processing_ms: 0.37830654485761034
  time_since_restore: 9588.322655200958
  time_this_iter_s: 26.799280881881714
  time_total_s: 9588.322655200958
  timers:
    learn_throughput: 8363.182
    learn_time_ms: 19345.747
    sample_throughput: 23760.109
    sample_time_ms: 6809.396
    update_time_ms: 34.901
  timestamp: 1602752802
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:06:43,383	WARNING util.py:136 -- The `process_trial` operation took 0.949603796005249 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    362 |          9588.32 | 58568704 |  285.711 |              305.323 |              128.354 |            795.289 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3046.2270724000923
    time_step_min: 2950
  date: 2020-10-15_09-07-09
  done: false
  episode_len_mean: 795.3321044213723
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.74204404094075
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 208
  episodes_total: 73778
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.218578832389109e-30
        cur_lr: 5.0e-05
        entropy: 0.055173227873941265
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006072201062731135
        total_loss: .inf
        vf_explained_var: 0.999443531036377
        vf_loss: 0.309781089425087
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.016129032258068
    gpu_util_percent0: 0.29903225806451605
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630643332516594
    mean_env_wait_ms: 1.1962084259237447
    mean_inference_ms: 4.297420028585874
    mean_raw_obs_processing_ms: 0.3783028370858325
  time_since_restore: 9614.64022564888
  time_this_iter_s: 26.317570447921753
  time_total_s: 9614.64022564888
  timers:
    learn_throughput: 8370.961
    learn_time_ms: 19327.77
    sample_throughput: 23781.697
    sample_time_ms: 6803.215
    update_time_ms: 33.697
  timestamp: 1602752829
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:07:11,185	WARNING util.py:136 -- The `process_trial` operation took 1.0455329418182373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    363 |          9614.64 | 58730496 |  285.742 |              305.323 |              128.354 |            795.332 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3046.071675956491
    time_step_min: 2950
  date: 2020-10-15_09-07-37
  done: false
  episode_len_mean: 795.367838163108
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.7666217420244
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 173
  episodes_total: 73951
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0827868248583664e-29
        cur_lr: 5.0e-05
        entropy: 0.054126776133974396
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0077480891898934106
        total_loss: .inf
        vf_explained_var: 0.9995095729827881
        vf_loss: 0.26348911970853806
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0625
    gpu_util_percent0: 0.3059375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463056428556771
    mean_env_wait_ms: 1.1961363960228664
    mean_inference_ms: 4.297377762705525
    mean_raw_obs_processing_ms: 0.3782993949807825
  time_since_restore: 9641.071371078491
  time_this_iter_s: 26.431145429611206
  time_total_s: 9641.071371078491
  timers:
    learn_throughput: 8369.532
    learn_time_ms: 19331.07
    sample_throughput: 23784.777
    sample_time_ms: 6802.334
    update_time_ms: 33.679
  timestamp: 1602752857
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:07:39,111	WARNING util.py:136 -- The `process_trial` operation took 0.9368948936462402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    364 |          9641.07 | 58892288 |  285.767 |              305.323 |              128.354 |            795.368 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3045.8803529507004
    time_step_min: 2950
  date: 2020-10-15_09-08-05
  done: false
  episode_len_mean: 795.4065782908311
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.7958485233983
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 202
  episodes_total: 74153
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6241802372875493e-29
        cur_lr: 5.0e-05
        entropy: 0.05871520626048247
        entropy_coeff: 0.0005000000000000001
        kl: 0.004153803184938927
        model: {}
        policy_loss: -0.008936486933786606
        total_loss: 0.2683264762163162
        vf_explained_var: 0.9995038509368896
        vf_loss: 0.2772923211256663
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.775
    gpu_util_percent0: 0.3559375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146304937162212
    mean_env_wait_ms: 1.1960520547711055
    mean_inference_ms: 4.297330825415354
    mean_raw_obs_processing_ms: 0.3782951660062144
  time_since_restore: 9667.654112100601
  time_this_iter_s: 26.582741022109985
  time_total_s: 9667.654112100601
  timers:
    learn_throughput: 8371.754
    learn_time_ms: 19325.937
    sample_throughput: 23775.728
    sample_time_ms: 6804.923
    update_time_ms: 34.23
  timestamp: 1602752885
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: c9144_00000
  
2020-10-15 09:08:07,099	WARNING util.py:136 -- The `process_trial` operation took 0.970017671585083 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    365 |          9667.65 | 59054080 |  285.796 |              305.323 |              128.354 |            795.407 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3045.6598157239896
    time_step_min: 2950
  date: 2020-10-15_09-08-33
  done: false
  episode_len_mean: 795.4471632159183
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.8276710367998
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 227
  episodes_total: 74380
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.120901186437747e-30
        cur_lr: 5.0e-05
        entropy: 0.057747880617777504
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006995808100327849
        total_loss: .inf
        vf_explained_var: 0.9992648959159851
        vf_loss: 0.4540047546227773
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.284375
    gpu_util_percent0: 0.3509375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630402180593002
    mean_env_wait_ms: 1.195959307415096
    mean_inference_ms: 4.297284698698356
    mean_raw_obs_processing_ms: 0.3782911692747998
  time_since_restore: 9694.317707538605
  time_this_iter_s: 26.66359543800354
  time_total_s: 9694.317707538605
  timers:
    learn_throughput: 8379.027
    learn_time_ms: 19309.162
    sample_throughput: 23802.609
    sample_time_ms: 6797.238
    update_time_ms: 33.896
  timestamp: 1602752913
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:08:35,095	WARNING util.py:136 -- The `process_trial` operation took 0.9866604804992676 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    366 |          9694.32 | 59215872 |  285.828 |              305.323 |              128.354 |            795.447 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3045.470993882151
    time_step_min: 2950
  date: 2020-10-15_09-09-01
  done: false
  episode_len_mean: 795.4836732778158
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.85600503567275
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 74571
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2181351779656622e-29
        cur_lr: 5.0e-05
        entropy: 0.05375989309201638
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00802262497988219
        total_loss: .inf
        vf_explained_var: 0.9996970295906067
        vf_loss: 0.16496643299857774
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.546875
    gpu_util_percent0: 0.31437499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630353846258634
    mean_env_wait_ms: 1.1958813656934222
    mean_inference_ms: 4.297242892350094
    mean_raw_obs_processing_ms: 0.37828754630728845
  time_since_restore: 9720.76355934143
  time_this_iter_s: 26.445851802825928
  time_total_s: 9720.76355934143
  timers:
    learn_throughput: 8378.846
    learn_time_ms: 19309.58
    sample_throughput: 23865.048
    sample_time_ms: 6779.454
    update_time_ms: 32.726
  timestamp: 1602752941
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:09:02,939	WARNING util.py:136 -- The `process_trial` operation took 0.9898877143859863 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    367 |          9720.76 | 59377664 |  285.856 |              305.323 |              128.354 |            795.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3045.2906472509235
    time_step_min: 2950
  date: 2020-10-15_09-09-29
  done: false
  episode_len_mean: 795.5191101122393
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.8828964688018
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 180
  episodes_total: 74751
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8272027669484933e-29
        cur_lr: 5.0e-05
        entropy: 0.05063031396518151
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007613798603415489
        total_loss: .inf
        vf_explained_var: 0.9996190071105957
        vf_loss: 0.20748275145888329
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.6375
    gpu_util_percent0: 0.333125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630266228739774
    mean_env_wait_ms: 1.1958067624460467
    mean_inference_ms: 4.297199066688185
    mean_raw_obs_processing_ms: 0.37828404831816553
  time_since_restore: 9747.402920484543
  time_this_iter_s: 26.639361143112183
  time_total_s: 9747.402920484543
  timers:
    learn_throughput: 8371.861
    learn_time_ms: 19325.692
    sample_throughput: 23833.983
    sample_time_ms: 6788.29
    update_time_ms: 34.217
  timestamp: 1602752969
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:09:30,943	WARNING util.py:136 -- The `process_trial` operation took 1.008208990097046 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    368 |           9747.4 | 59539456 |  285.883 |              305.323 |              128.354 |            795.519 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3045.091126298086
    time_step_min: 2950
  date: 2020-10-15_09-09-57
  done: false
  episode_len_mean: 795.5634597681213
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.9123651495004
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 202
  episodes_total: 74953
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7408041504227395e-29
        cur_lr: 5.0e-05
        entropy: 0.05336814001202583
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007893296045949683
        total_loss: .inf
        vf_explained_var: 0.9997265934944153
        vf_loss: 0.1678737352291743
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.771875
    gpu_util_percent0: 0.3384375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463022672328276
    mean_env_wait_ms: 1.195723792475686
    mean_inference_ms: 4.297155876736463
    mean_raw_obs_processing_ms: 0.37828004069294263
  time_since_restore: 9773.919913291931
  time_this_iter_s: 26.516992807388306
  time_total_s: 9773.919913291931
  timers:
    learn_throughput: 8379.315
    learn_time_ms: 19308.499
    sample_throughput: 23803.191
    sample_time_ms: 6797.072
    update_time_ms: 34.529
  timestamp: 1602752997
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:09:58,766	WARNING util.py:136 -- The `process_trial` operation took 0.9517803192138672 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    369 |          9773.92 | 59701248 |  285.912 |              305.323 |              128.354 |            795.563 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3044.884494597328
    time_step_min: 2950
  date: 2020-10-15_09-10-25
  done: false
  episode_len_mean: 795.5993376162164
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.94142091277087
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 230
  episodes_total: 75183
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.111206225634109e-29
        cur_lr: 5.0e-05
        entropy: 0.06935017928481102
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010105720944314575
        total_loss: .inf
        vf_explained_var: 0.9985065460205078
        vf_loss: 0.883213664094607
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.646874999999998
    gpu_util_percent0: 0.3253125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630138673744025
    mean_env_wait_ms: 1.1956318000107797
    mean_inference_ms: 4.29711179626293
    mean_raw_obs_processing_ms: 0.37827629075739355
  time_since_restore: 9800.563580274582
  time_this_iter_s: 26.643666982650757
  time_total_s: 9800.563580274582
  timers:
    learn_throughput: 8381.476
    learn_time_ms: 19303.522
    sample_throughput: 23776.5
    sample_time_ms: 6804.702
    update_time_ms: 33.938
  timestamp: 1602753025
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:10:26,757	WARNING util.py:136 -- The `process_trial` operation took 0.9886457920074463 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    370 |          9800.56 | 59863040 |  285.941 |              305.323 |              128.354 |            795.599 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3044.755615441801
    time_step_min: 2950
  date: 2020-10-15_09-10-53
  done: false
  episode_len_mean: 795.6117192786911
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.96103224032055
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 180
  episodes_total: 75363
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.166809338451164e-29
        cur_lr: 5.0e-05
        entropy: 0.07338455691933632
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009060739626875147
        total_loss: .inf
        vf_explained_var: 0.9980280995368958
        vf_loss: 0.9819404532512029
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.51875
    gpu_util_percent0: 0.34562499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630067491657583
    mean_env_wait_ms: 1.1955577938563273
    mean_inference_ms: 4.297069625840294
    mean_raw_obs_processing_ms: 0.3782727252290039
  time_since_restore: 9827.267493486404
  time_this_iter_s: 26.70391321182251
  time_total_s: 9827.267493486404
  timers:
    learn_throughput: 8386.861
    learn_time_ms: 19291.126
    sample_throughput: 23752.072
    sample_time_ms: 6811.701
    update_time_ms: 40.872
  timestamp: 1602753053
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:10:54,775	WARNING util.py:136 -- The `process_trial` operation took 0.965442419052124 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    371 |          9827.27 | 60024832 |  285.961 |              305.323 |              128.354 |            795.612 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3044.620424810636
    time_step_min: 2950
  date: 2020-10-15_09-11-21
  done: false
  episode_len_mean: 795.6347368003071
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 285.98213100816645
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 188
  episodes_total: 75551
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.250214007676747e-29
        cur_lr: 5.0e-05
        entropy: 0.063323059429725
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0109587214004326
        total_loss: .inf
        vf_explained_var: 0.9987154006958008
        vf_loss: 0.6710806091626486
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7125
    gpu_util_percent0: 0.29781250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629969804389867
    mean_env_wait_ms: 1.1954807491818231
    mean_inference_ms: 4.297021621031585
    mean_raw_obs_processing_ms: 0.3782690739761901
  time_since_restore: 9853.663573503494
  time_this_iter_s: 26.396080017089844
  time_total_s: 9853.663573503494
  timers:
    learn_throughput: 8402.472
    learn_time_ms: 19255.285
    sample_throughput: 23759.723
    sample_time_ms: 6809.507
    update_time_ms: 40.137
  timestamp: 1602753081
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:11:22,515	WARNING util.py:136 -- The `process_trial` operation took 0.9823360443115234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    372 |          9853.66 | 60186624 |  285.982 |              305.323 |              128.354 |            795.635 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3044.4511805408833
    time_step_min: 2950
  date: 2020-10-15_09-11-49
  done: false
  episode_len_mean: 795.6734817787046
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.00916921015124
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 75763
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3875321011515119e-28
        cur_lr: 5.0e-05
        entropy: 0.05379839427769184
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00600633932723819
        total_loss: .inf
        vf_explained_var: 0.9995629787445068
        vf_loss: 0.26521918798486394
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4875
    gpu_util_percent0: 0.269375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629946325035337
    mean_env_wait_ms: 1.195396522750257
    mean_inference_ms: 4.296989271932294
    mean_raw_obs_processing_ms: 0.3782653884483913
  time_since_restore: 9880.243381977081
  time_this_iter_s: 26.579808473587036
  time_total_s: 9880.243381977081
  timers:
    learn_throughput: 8393.749
    learn_time_ms: 19275.297
    sample_throughput: 23743.608
    sample_time_ms: 6814.129
    update_time_ms: 39.592
  timestamp: 1602753109
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:11:50,645	WARNING util.py:136 -- The `process_trial` operation took 1.0062377452850342 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    373 |          9880.24 | 60348416 |  286.009 |              305.323 |              128.354 |            795.673 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3044.251471459609
    time_step_min: 2950
  date: 2020-10-15_09-12-17
  done: false
  episode_len_mean: 795.7219268228481
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.0396529123826
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 217
  episodes_total: 75980
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0812981517272683e-28
        cur_lr: 5.0e-05
        entropy: 0.05125367144743601
        entropy_coeff: 0.0005000000000000001
        kl: 0.004264845745638013
        model: {}
        policy_loss: -0.007123733346816152
        total_loss: 0.2707364509503047
        vf_explained_var: 0.9995397925376892
        vf_loss: 0.2778858070572217
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.134375000000002
    gpu_util_percent0: 0.3325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462986395509264
    mean_env_wait_ms: 1.1953102210068811
    mean_inference_ms: 4.296943134087409
    mean_raw_obs_processing_ms: 0.3782618990388199
  time_since_restore: 9906.866194725037
  time_this_iter_s: 26.622812747955322
  time_total_s: 9906.866194725037
  timers:
    learn_throughput: 8386.455
    learn_time_ms: 19292.061
    sample_throughput: 23743.777
    sample_time_ms: 6814.08
    update_time_ms: 41.087
  timestamp: 1602753137
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: c9144_00000
  
2020-10-15 09:12:18,804	WARNING util.py:136 -- The `process_trial` operation took 0.9954895973205566 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    374 |          9906.87 | 60510208 |   286.04 |              305.323 |              128.354 |            795.722 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3044.08683315161
    time_step_min: 2950
  date: 2020-10-15_09-12-45
  done: false
  episode_len_mean: 795.7522125055805
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.0622481810145
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 178
  episodes_total: 76158
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0406490758636342e-28
        cur_lr: 5.0e-05
        entropy: 0.05491684439281622
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008501467750951027
        total_loss: .inf
        vf_explained_var: 0.9991014003753662
        vf_loss: 0.4742605487505595
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.359375
    gpu_util_percent0: 0.2590625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629786579967585
    mean_env_wait_ms: 1.1952376710979493
    mean_inference_ms: 4.296900699679387
    mean_raw_obs_processing_ms: 0.3782583594475794
  time_since_restore: 9933.654623270035
  time_this_iter_s: 26.78842854499817
  time_total_s: 9933.654623270035
  timers:
    learn_throughput: 8380.407
    learn_time_ms: 19305.984
    sample_throughput: 23773.572
    sample_time_ms: 6805.54
    update_time_ms: 42.835
  timestamp: 1602753165
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:12:46,938	WARNING util.py:136 -- The `process_trial` operation took 0.9880859851837158 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    375 |          9933.65 | 60672000 |  286.062 |              305.323 |              128.354 |            795.752 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3043.920174013313
    time_step_min: 2950
  date: 2020-10-15_09-13-13
  done: false
  episode_len_mean: 795.7924978061845
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.0885911147467
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 193
  episodes_total: 76351
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5609736137954513e-28
        cur_lr: 5.0e-05
        entropy: 0.053296695773800216
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006527112001397957
        total_loss: .inf
        vf_explained_var: 0.9994176030158997
        vf_loss: 0.3458407570918401
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.09375
    gpu_util_percent0: 0.29312499999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629705595349446
    mean_env_wait_ms: 1.1951595307791805
    mean_inference_ms: 4.2968556594989025
    mean_raw_obs_processing_ms: 0.37825465432585964
  time_since_restore: 9960.382496356964
  time_this_iter_s: 26.72787308692932
  time_total_s: 9960.382496356964
  timers:
    learn_throughput: 8376.609
    learn_time_ms: 19314.738
    sample_throughput: 23808.611
    sample_time_ms: 6795.524
    update_time_ms: 49.319
  timestamp: 1602753193
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:13:15,125	WARNING util.py:136 -- The `process_trial` operation took 1.1023070812225342 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    376 |          9960.38 | 60833792 |  286.089 |              305.323 |              128.354 |            795.792 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3043.7199022656596
    time_step_min: 2950
  date: 2020-10-15_09-13-41
  done: false
  episode_len_mean: 795.8396870796275
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.1192800156089
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 218
  episodes_total: 76569
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.341460420693177e-28
        cur_lr: 5.0e-05
        entropy: 0.05418003040055434
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008054223532478014
        total_loss: .inf
        vf_explained_var: 0.9996553063392639
        vf_loss: 0.22956122333804765
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91875
    gpu_util_percent0: 0.3109375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629652158681333
    mean_env_wait_ms: 1.1950733282441977
    mean_inference_ms: 4.29681876081169
    mean_raw_obs_processing_ms: 0.3782510068503834
  time_since_restore: 9986.89592719078
  time_this_iter_s: 26.51343083381653
  time_total_s: 9986.89592719078
  timers:
    learn_throughput: 8375.158
    learn_time_ms: 19318.082
    sample_throughput: 23802.784
    sample_time_ms: 6797.188
    update_time_ms: 50.586
  timestamp: 1602753221
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:13:43,101	WARNING util.py:136 -- The `process_trial` operation took 1.055410623550415 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    377 |           9986.9 | 60995584 |  286.119 |              305.323 |              128.354 |             795.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3043.5357440708885
    time_step_min: 2950
  date: 2020-10-15_09-14-09
  done: false
  episode_len_mean: 795.8781764897427
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.14743725105177
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 206
  episodes_total: 76775
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5121906310397655e-28
        cur_lr: 5.0e-05
        entropy: 0.05890552389125029
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009795324391840646
        total_loss: .inf
        vf_explained_var: 0.9995060563087463
        vf_loss: 0.2734965905547142
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.248484848484846
    gpu_util_percent0: 0.35515151515151516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8757575757575755
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629582041893271
    mean_env_wait_ms: 1.1949919127933333
    mean_inference_ms: 4.296775829147113
    mean_raw_obs_processing_ms: 0.3782476233236836
  time_since_restore: 10013.722333908081
  time_this_iter_s: 26.826406717300415
  time_total_s: 10013.722333908081
  timers:
    learn_throughput: 8362.388
    learn_time_ms: 19347.584
    sample_throughput: 23834.877
    sample_time_ms: 6788.036
    update_time_ms: 50.285
  timestamp: 1602753249
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:14:11,496	WARNING util.py:136 -- The `process_trial` operation took 1.031553030014038 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    378 |          10013.7 | 61157376 |  286.147 |              305.323 |              128.354 |            795.878 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3043.373064472093
    time_step_min: 2950
  date: 2020-10-15_09-14-37
  done: false
  episode_len_mean: 795.912841771494
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.17155729243797
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 177
  episodes_total: 76952
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.268285946559648e-28
        cur_lr: 5.0e-05
        entropy: 0.05912355240434408
        entropy_coeff: 0.0005000000000000001
        kl: 0.004856265654476981
        model: {}
        policy_loss: -0.007605629158206284
        total_loss: 0.5649874607721964
        vf_explained_var: 0.9989112019538879
        vf_loss: 0.5726226270198822
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.2625
    gpu_util_percent0: 0.31875000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462950763969132
    mean_env_wait_ms: 1.194921006499368
    mean_inference_ms: 4.296736546889318
    mean_raw_obs_processing_ms: 0.3782443439595217
  time_since_restore: 10040.18303346634
  time_this_iter_s: 26.460699558258057
  time_total_s: 10040.18303346634
  timers:
    learn_throughput: 8354.658
    learn_time_ms: 19365.484
    sample_throughput: 23880.013
    sample_time_ms: 6775.206
    update_time_ms: 48.439
  timestamp: 1602753277
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: c9144_00000
  
2020-10-15 09:14:39,514	WARNING util.py:136 -- The `process_trial` operation took 1.0261304378509521 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    379 |          10040.2 | 61319168 |  286.172 |              305.323 |              128.354 |            795.913 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3043.2014367965144
    time_step_min: 2950
  date: 2020-10-15_09-15-05
  done: false
  episode_len_mean: 795.9489183549144
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.1986603995392
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 77151
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.634142973279824e-28
        cur_lr: 5.0e-05
        entropy: 0.06184683615962664
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007751050613781747
        total_loss: .inf
        vf_explained_var: 0.9993483424186707
        vf_loss: 0.39793066928784054
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5875
    gpu_util_percent0: 0.2478125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629421588004507
    mean_env_wait_ms: 1.1948407090806465
    mean_inference_ms: 4.296692589516677
    mean_raw_obs_processing_ms: 0.37824041776771783
  time_since_restore: 10066.637110948563
  time_this_iter_s: 26.45407748222351
  time_total_s: 10066.637110948563
  timers:
    learn_throughput: 8358.031
    learn_time_ms: 19357.67
    sample_throughput: 23905.682
    sample_time_ms: 6767.931
    update_time_ms: 48.46
  timestamp: 1602753305
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:15:07,582	WARNING util.py:136 -- The `process_trial` operation took 1.0496609210968018 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    380 |          10066.6 | 61480960 |  286.199 |              305.323 |              128.354 |            795.949 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3043.0619577725197
    time_step_min: 2950
  date: 2020-10-15_09-15-34
  done: false
  episode_len_mean: 795.9686603427331
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.2170225086817
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 227
  episodes_total: 77378
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9512144599197364e-28
        cur_lr: 5.0e-05
        entropy: 0.07995615402857463
        entropy_coeff: 0.0005000000000000001
        kl: 0.005059907445684075
        model: {}
        policy_loss: -0.010912375185095394
        total_loss: 1.8536803126335144
        vf_explained_var: 0.9967953562736511
        vf_loss: 1.8646326859792073
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79375
    gpu_util_percent0: 0.33343749999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462938057640678
    mean_env_wait_ms: 1.194752140870534
    mean_inference_ms: 4.296656759391277
    mean_raw_obs_processing_ms: 0.3782369142091886
  time_since_restore: 10093.453175783157
  time_this_iter_s: 26.816064834594727
  time_total_s: 10093.453175783157
  timers:
    learn_throughput: 8364.837
    learn_time_ms: 19341.919
    sample_throughput: 23807.49
    sample_time_ms: 6795.845
    update_time_ms: 48.194
  timestamp: 1602753334
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: c9144_00000
  
2020-10-15 09:15:35,766	WARNING util.py:136 -- The `process_trial` operation took 1.0071699619293213 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    381 |          10093.5 | 61642752 |  286.217 |              305.323 |              128.354 |            795.969 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3042.9744122161032
    time_step_min: 2950
  date: 2020-10-15_09-16-02
  done: false
  episode_len_mean: 795.9837828082298
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.23078214205174
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 194
  episodes_total: 77572
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9512144599197364e-28
        cur_lr: 5.0e-05
        entropy: 0.07641768765946229
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008993548806756735
        total_loss: .inf
        vf_explained_var: 0.9961767792701721
        vf_loss: 1.9615405301253002
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.646875
    gpu_util_percent0: 0.325625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629311661750322
    mean_env_wait_ms: 1.1946759860009357
    mean_inference_ms: 4.296615579744007
    mean_raw_obs_processing_ms: 0.3782335557765118
  time_since_restore: 10120.200484514236
  time_this_iter_s: 26.7473087310791
  time_total_s: 10120.200484514236
  timers:
    learn_throughput: 8350.398
    learn_time_ms: 19375.364
    sample_throughput: 23778.137
    sample_time_ms: 6804.234
    update_time_ms: 47.532
  timestamp: 1602753362
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:16:03,999	WARNING util.py:136 -- The `process_trial` operation took 1.0131425857543945 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    382 |          10120.2 | 61804544 |  286.231 |              305.323 |              128.354 |            795.984 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3042.86215148669
    time_step_min: 2950
  date: 2020-10-15_09-16-30
  done: false
  episode_len_mean: 796.0102240283958
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.24986405634064
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 186
  episodes_total: 77758
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.9268216898796025e-28
        cur_lr: 5.0e-05
        entropy: 0.06195176454881827
        entropy_coeff: 0.0005000000000000001
        kl: 0.003602850115081916
        model: {}
        policy_loss: -0.0074584581452654675
        total_loss: 1.2884855369726818
        vf_explained_var: 0.9976356625556946
        vf_loss: 1.2959749698638916
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.34375
    gpu_util_percent0: 0.3203125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629222215802226
    mean_env_wait_ms: 1.1946024250330738
    mean_inference_ms: 4.296572829978347
    mean_raw_obs_processing_ms: 0.3782300191742557
  time_since_restore: 10146.894356489182
  time_this_iter_s: 26.69387197494507
  time_total_s: 10146.894356489182
  timers:
    learn_throughput: 8345.03
    learn_time_ms: 19387.828
    sample_throughput: 23781.737
    sample_time_ms: 6803.204
    update_time_ms: 49.306
  timestamp: 1602753390
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: c9144_00000
  
2020-10-15 09:16:32,215	WARNING util.py:136 -- The `process_trial` operation took 1.0581142902374268 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    383 |          10146.9 | 61966336 |   286.25 |              305.323 |              128.354 |             796.01 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3042.702765479628
    time_step_min: 2950
  date: 2020-10-15_09-16-58
  done: false
  episode_len_mean: 796.0492560287327
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.27575070872916
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 202
  episodes_total: 77960
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9634108449398013e-28
        cur_lr: 5.0e-05
        entropy: 0.055009735748171806
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037951863487251103
        model: {}
        policy_loss: -0.0063179101174076395
        total_loss: 0.2619515967865785
        vf_explained_var: 0.999538004398346
        vf_loss: 0.2682970066865285
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.471875
    gpu_util_percent0: 0.35468750000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629187342528516
    mean_env_wait_ms: 1.1945226525553292
    mean_inference_ms: 4.296535523108334
    mean_raw_obs_processing_ms: 0.37822642831369857
  time_since_restore: 10173.27608370781
  time_this_iter_s: 26.38172721862793
  time_total_s: 10173.27608370781
  timers:
    learn_throughput: 8354.758
    learn_time_ms: 19365.253
    sample_throughput: 23789.512
    sample_time_ms: 6800.98
    update_time_ms: 49.454
  timestamp: 1602753418
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: c9144_00000
  
2020-10-15 09:17:00,080	WARNING util.py:136 -- The `process_trial` operation took 1.0120429992675781 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    384 |          10173.3 | 62128128 |  286.276 |              305.323 |              128.354 |            796.049 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3042.508285878815
    time_step_min: 2950
  date: 2020-10-15_09-17-26
  done: false
  episode_len_mean: 796.096968534152
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.3061563447213
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 220
  episodes_total: 78180
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4817054224699006e-28
        cur_lr: 5.0e-05
        entropy: 0.051512157544493675
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006184767558200595
        total_loss: .inf
        vf_explained_var: 0.9997029304504395
        vf_loss: 0.18124771739045778
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.0875
    gpu_util_percent0: 0.27718750000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462910775928864
    mean_env_wait_ms: 1.1944382840701415
    mean_inference_ms: 4.296498190184152
    mean_raw_obs_processing_ms: 0.37822325641640303
  time_since_restore: 10200.052679777145
  time_this_iter_s: 26.776596069335938
  time_total_s: 10200.052679777145
  timers:
    learn_throughput: 8355.164
    learn_time_ms: 19364.312
    sample_throughput: 23750.736
    sample_time_ms: 6812.084
    update_time_ms: 48.781
  timestamp: 1602753446
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:17:28,383	WARNING util.py:136 -- The `process_trial` operation took 1.057474136352539 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    385 |          10200.1 | 62289920 |  286.306 |              305.323 |              128.354 |            796.097 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3042.3441214396057
    time_step_min: 2950
  date: 2020-10-15_09-17-54
  done: false
  episode_len_mean: 796.1327811949669
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.33141269513493
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 182
  episodes_total: 78362
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2225581337048506e-28
        cur_lr: 5.0e-05
        entropy: 0.05449232428024212
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007067206849266465
        total_loss: .inf
        vf_explained_var: 0.9996634125709534
        vf_loss: 0.1829212854305903
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.509375000000002
    gpu_util_percent0: 0.28
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629050139615593
    mean_env_wait_ms: 1.194366979612689
    mean_inference_ms: 4.296460223467108
    mean_raw_obs_processing_ms: 0.37821991510935454
  time_since_restore: 10226.633856773376
  time_this_iter_s: 26.58117699623108
  time_total_s: 10226.633856773376
  timers:
    learn_throughput: 8353.893
    learn_time_ms: 19367.258
    sample_throughput: 23793.426
    sample_time_ms: 6799.862
    update_time_ms: 40.766
  timestamp: 1602753474
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:17:56,493	WARNING util.py:136 -- The `process_trial` operation took 1.0699851512908936 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    386 |          10226.6 | 62451712 |  286.331 |              305.323 |              128.354 |            796.133 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3042.170748481259
    time_step_min: 2950
  date: 2020-10-15_09-18-23
  done: false
  episode_len_mean: 796.1693230134684
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.35694509059334
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 192
  episodes_total: 78554
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.333837200557276e-28
        cur_lr: 5.0e-05
        entropy: 0.056690232207377754
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00906424326240085
        total_loss: .inf
        vf_explained_var: 0.9994351863861084
        vf_loss: 0.30883313715457916
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.85151515151515
    gpu_util_percent0: 0.306969696969697
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8757575757575755
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462895687814455
    mean_env_wait_ms: 1.1942911770424776
    mean_inference_ms: 4.296415288595493
    mean_raw_obs_processing_ms: 0.3782161092476026
  time_since_restore: 10253.315398454666
  time_this_iter_s: 26.681541681289673
  time_total_s: 10253.315398454666
  timers:
    learn_throughput: 8346.601
    learn_time_ms: 19384.178
    sample_throughput: 23798.947
    sample_time_ms: 6798.284
    update_time_ms: 41.006
  timestamp: 1602753503
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:18:24,760	WARNING util.py:136 -- The `process_trial` operation took 1.0396380424499512 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    387 |          10253.3 | 62613504 |  286.357 |              305.323 |              128.354 |            796.169 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3041.9935852651633
    time_step_min: 2950
  date: 2020-10-15_09-18-51
  done: false
  episode_len_mean: 796.2060563737938
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.38331704551877
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 206
  episodes_total: 78760
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.000755800835914e-28
        cur_lr: 5.0e-05
        entropy: 0.05919215610871712
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009774273600972569
        total_loss: .inf
        vf_explained_var: 0.9993622303009033
        vf_loss: 0.391473725438118
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4375
    gpu_util_percent0: 0.3309375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462893347094707
    mean_env_wait_ms: 1.1942117203924023
    mean_inference_ms: 4.296382312775589
    mean_raw_obs_processing_ms: 0.3782127241174858
  time_since_restore: 10280.136185884476
  time_this_iter_s: 26.82078742980957
  time_total_s: 10280.136185884476
  timers:
    learn_throughput: 8350.337
    learn_time_ms: 19375.506
    sample_throughput: 23814.204
    sample_time_ms: 6793.928
    update_time_ms: 39.861
  timestamp: 1602753531
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:18:53,071	WARNING util.py:136 -- The `process_trial` operation took 1.1112985610961914 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    388 |          10280.1 | 62775296 |  286.383 |              305.323 |              128.354 |            796.206 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3041.8124295359826
    time_step_min: 2950
  date: 2020-10-15_09-19-19
  done: false
  episode_len_mean: 796.2493162479741
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.41094865285766
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 216
  episodes_total: 78976
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.501133701253871e-28
        cur_lr: 5.0e-05
        entropy: 0.054247939648727574
        entropy_coeff: 0.0005000000000000001
        kl: 0.00421986806516846
        model: {}
        policy_loss: -0.008446158230071887
        total_loss: 0.282777967552344
        vf_explained_var: 0.9995179772377014
        vf_loss: 0.29125124340256053
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.84375
    gpu_util_percent0: 0.3265625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628845621899716
    mean_env_wait_ms: 1.1941291283305902
    mean_inference_ms: 4.296345479553494
    mean_raw_obs_processing_ms: 0.3782097300478049
  time_since_restore: 10306.70927453041
  time_this_iter_s: 26.57308864593506
  time_total_s: 10306.70927453041
  timers:
    learn_throughput: 8354.045
    learn_time_ms: 19366.906
    sample_throughput: 23778.102
    sample_time_ms: 6804.244
    update_time_ms: 47.859
  timestamp: 1602753559
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: c9144_00000
  
2020-10-15 09:19:21,025	WARNING util.py:136 -- The `process_trial` operation took 1.0133557319641113 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    389 |          10306.7 | 62937088 |  286.411 |              305.323 |              128.354 |            796.249 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3041.658325118172
    time_step_min: 2950
  date: 2020-10-15_09-19-47
  done: false
  episode_len_mean: 796.2876309107218
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.43407801118434
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 79157
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7505668506269354e-28
        cur_lr: 5.0e-05
        entropy: 0.05343009624630213
        entropy_coeff: 0.0005000000000000001
        kl: 0.00650142253531764
        model: {}
        policy_loss: -0.00686497037531808
        total_loss: 0.25834038729468983
        vf_explained_var: 0.9995208382606506
        vf_loss: 0.26523207624753314
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.98125
    gpu_util_percent0: 0.31656249999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628787877715665
    mean_env_wait_ms: 1.1940580194919679
    mean_inference_ms: 4.296305056429351
    mean_raw_obs_processing_ms: 0.37820624786150303
  time_since_restore: 10333.289306879044
  time_this_iter_s: 26.580032348632812
  time_total_s: 10333.289306879044
  timers:
    learn_throughput: 8354.238
    learn_time_ms: 19366.458
    sample_throughput: 23752.026
    sample_time_ms: 6811.714
    update_time_ms: 46.035
  timestamp: 1602753587
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: c9144_00000
  
2020-10-15 09:19:49,007	WARNING util.py:136 -- The `process_trial` operation took 1.025707483291626 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    390 |          10333.3 | 63098880 |  286.434 |              305.323 |              128.354 |            796.288 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3041.502861825517
    time_step_min: 2950
  date: 2020-10-15_09-20-15
  done: false
  episode_len_mean: 796.3203956902527
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.45841694622493
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 198
  episodes_total: 79355
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7505668506269354e-28
        cur_lr: 5.0e-05
        entropy: 0.057613082230091095
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00942271321643299
        total_loss: .inf
        vf_explained_var: 0.9993463158607483
        vf_loss: 0.36909860372543335
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.859375
    gpu_util_percent0: 0.3053125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462869738856624
    mean_env_wait_ms: 1.1939816325035542
    mean_inference_ms: 4.296264656137742
    mean_raw_obs_processing_ms: 0.37820295370355694
  time_since_restore: 10359.98078083992
  time_this_iter_s: 26.691473960876465
  time_total_s: 10359.98078083992
  timers:
    learn_throughput: 8351.412
    learn_time_ms: 19373.012
    sample_throughput: 23839.144
    sample_time_ms: 6786.821
    update_time_ms: 39.351
  timestamp: 1602753615
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:20:17,279	WARNING util.py:136 -- The `process_trial` operation took 1.1987659931182861 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    391 |            10360 | 63260672 |  286.458 |              305.323 |              128.354 |             796.32 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3041.334808630488
    time_step_min: 2950
  date: 2020-10-15_09-20-43
  done: false
  episode_len_mean: 796.3533374388879
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.48263828984483
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 79567
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.625850275940404e-28
        cur_lr: 5.0e-05
        entropy: 0.05610537280639013
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038192563224583864
        model: {}
        policy_loss: -0.009861814503286345
        total_loss: 0.4012594719727834
        vf_explained_var: 0.999318540096283
        vf_loss: 0.4111493503053983
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.628125
    gpu_util_percent0: 0.35250000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628665487600975
    mean_env_wait_ms: 1.1939004409207927
    mean_inference_ms: 4.296231305793893
    mean_raw_obs_processing_ms: 0.37819939448618695
  time_since_restore: 10386.5932867527
  time_this_iter_s: 26.61250591278076
  time_total_s: 10386.5932867527
  timers:
    learn_throughput: 8356.559
    learn_time_ms: 19361.079
    sample_throughput: 23847.849
    sample_time_ms: 6784.343
    update_time_ms: 40.442
  timestamp: 1602753643
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: c9144_00000
  
2020-10-15 09:20:45,346	WARNING util.py:136 -- The `process_trial` operation took 1.0674710273742676 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    392 |          10386.6 | 63422464 |  286.483 |              305.323 |              128.354 |            796.353 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3041.1862317204705
    time_step_min: 2950
  date: 2020-10-15_09-21-11
  done: false
  episode_len_mean: 796.3883714224825
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.50666273612535
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 202
  episodes_total: 79769
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.812925137970202e-28
        cur_lr: 5.0e-05
        entropy: 0.05494449194520712
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007429428175479795
        total_loss: .inf
        vf_explained_var: 0.9991094470024109
        vf_loss: 0.527049812177817
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.578125
    gpu_util_percent0: 0.339375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628595777040979
    mean_env_wait_ms: 1.193824205911525
    mean_inference_ms: 4.296197798589678
    mean_raw_obs_processing_ms: 0.37819655703473676
  time_since_restore: 10413.177193641663
  time_this_iter_s: 26.583906888961792
  time_total_s: 10413.177193641663
  timers:
    learn_throughput: 8362.4
    learn_time_ms: 19347.555
    sample_throughput: 23843.047
    sample_time_ms: 6785.71
    update_time_ms: 40.238
  timestamp: 1602753671
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:21:13,351	WARNING util.py:136 -- The `process_trial` operation took 1.0444953441619873 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    393 |          10413.2 | 63584256 |  286.507 |              305.323 |              128.354 |            796.388 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3041.0324090294807
    time_step_min: 2950
  date: 2020-10-15_09-21-39
  done: false
  episode_len_mean: 796.4228840164601
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.53028035227135
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 182
  episodes_total: 79951
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.219387706955302e-28
        cur_lr: 5.0e-05
        entropy: 0.051324217269817986
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0073270904055486126
        total_loss: .inf
        vf_explained_var: 0.9996646046638489
        vf_loss: 0.17806110282739004
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.271875
    gpu_util_percent0: 0.3315625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628531793990338
    mean_env_wait_ms: 1.1937534034202282
    mean_inference_ms: 4.296156597062944
    mean_raw_obs_processing_ms: 0.3781933971103263
  time_since_restore: 10439.56392455101
  time_this_iter_s: 26.386730909347534
  time_total_s: 10439.56392455101
  timers:
    learn_throughput: 8366.333
    learn_time_ms: 19338.46
    sample_throughput: 23841.789
    sample_time_ms: 6786.068
    update_time_ms: 47.038
  timestamp: 1602753699
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:21:41,164	WARNING util.py:136 -- The `process_trial` operation took 1.0522534847259521 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    394 |          10439.6 | 63746048 |   286.53 |              305.323 |              128.354 |            796.423 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3040.857519097309
    time_step_min: 2950
  date: 2020-10-15_09-22-07
  done: false
  episode_len_mean: 796.4641364424648
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.55669771790605
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 200
  episodes_total: 80151
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.329081560432954e-28
        cur_lr: 5.0e-05
        entropy: 0.055324703454971313
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006149657606632293
        total_loss: .inf
        vf_explained_var: 0.9995753765106201
        vf_loss: 0.2396284081041813
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.675
    gpu_util_percent0: 0.3678125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462845521926741
    mean_env_wait_ms: 1.1936774143084516
    mean_inference_ms: 4.296121440393387
    mean_raw_obs_processing_ms: 0.3781901493152176
  time_since_restore: 10466.04582452774
  time_this_iter_s: 26.481899976730347
  time_total_s: 10466.04582452774
  timers:
    learn_throughput: 8377.09
    learn_time_ms: 19313.629
    sample_throughput: 23851.311
    sample_time_ms: 6783.359
    update_time_ms: 46.319
  timestamp: 1602753727
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:22:09,128	WARNING util.py:136 -- The `process_trial` operation took 1.107539415359497 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    395 |            10466 | 63907840 |  286.557 |              305.323 |              128.354 |            796.464 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3040.6821774906325
    time_step_min: 2950
  date: 2020-10-15_09-22-35
  done: false
  episode_len_mean: 796.5025259438071
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.58398709741317
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 215
  episodes_total: 80366
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.49362234064943e-28
        cur_lr: 5.0e-05
        entropy: 0.05388410482555628
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047017074733351665
        model: {}
        policy_loss: -0.008878852264994444
        total_loss: 0.29123256355524063
        vf_explained_var: 0.9994843602180481
        vf_loss: 0.30013836671908695
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.06875
    gpu_util_percent0: 0.243125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462842494449194
    mean_env_wait_ms: 1.1935956666775744
    mean_inference_ms: 4.2960860781067804
    mean_raw_obs_processing_ms: 0.3781865322130856
  time_since_restore: 10492.521347045898
  time_this_iter_s: 26.47552251815796
  time_total_s: 10492.521347045898
  timers:
    learn_throughput: 8383.929
    learn_time_ms: 19297.873
    sample_throughput: 23861.775
    sample_time_ms: 6780.384
    update_time_ms: 46.347
  timestamp: 1602753755
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: c9144_00000
  
2020-10-15 09:22:37,066	WARNING util.py:136 -- The `process_trial` operation took 1.0417213439941406 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    396 |          10492.5 | 64069632 |  286.584 |              305.323 |              128.354 |            796.503 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3040.5191108903514
    time_step_min: 2950
  date: 2020-10-15_09-23-04
  done: false
  episode_len_mean: 796.5373425184633
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.60885338208004
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 80565
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.746811170324715e-28
        cur_lr: 5.0e-05
        entropy: 0.055602727768321834
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010400672414107248
        total_loss: .inf
        vf_explained_var: 0.9992994666099548
        vf_loss: 0.4427780931194623
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4625
    gpu_util_percent0: 0.3146875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462834519105925
    mean_env_wait_ms: 1.1935207985216518
    mean_inference_ms: 4.296050895106474
    mean_raw_obs_processing_ms: 0.3781836131207408
  time_since_restore: 10519.544921875
  time_this_iter_s: 27.023574829101562
  time_total_s: 10519.544921875
  timers:
    learn_throughput: 8378.307
    learn_time_ms: 19310.822
    sample_throughput: 23782.661
    sample_time_ms: 6802.939
    update_time_ms: 44.417
  timestamp: 1602753784
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:23:05,668	WARNING util.py:136 -- The `process_trial` operation took 1.1050920486450195 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    397 |          10519.5 | 64231424 |  286.609 |              305.323 |              128.354 |            796.537 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3040.3802827371733
    time_step_min: 2950
  date: 2020-10-15_09-23-32
  done: false
  episode_len_mean: 796.559284670448
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.62962220726075
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 80746
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.120216755487072e-28
        cur_lr: 5.0e-05
        entropy: 0.05835038640846809
        entropy_coeff: 0.0005000000000000001
        kl: 0.003586503560654819
        model: {}
        policy_loss: -0.00894716300535947
        total_loss: 0.498999002079169
        vf_explained_var: 0.9990713596343994
        vf_loss: 0.5079753398895264
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.543750000000003
    gpu_util_percent0: 0.3215625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462828995321714
    mean_env_wait_ms: 1.1934514396989124
    mean_inference_ms: 4.296013991199668
    mean_raw_obs_processing_ms: 0.3781807961476156
  time_since_restore: 10545.936705350876
  time_this_iter_s: 26.391783475875854
  time_total_s: 10545.936705350876
  timers:
    learn_throughput: 8388.994
    learn_time_ms: 19286.222
    sample_throughput: 23811.733
    sample_time_ms: 6794.634
    update_time_ms: 45.42
  timestamp: 1602753812
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: c9144_00000
  
2020-10-15 09:23:33,598	WARNING util.py:136 -- The `process_trial` operation took 1.0898289680480957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    398 |          10545.9 | 64393216 |   286.63 |              305.323 |              128.354 |            796.559 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3040.214933636521
    time_step_min: 2950
  date: 2020-10-15_09-24-00
  done: false
  episode_len_mean: 796.593072523563
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.6554325636263
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 207
  episodes_total: 80953
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.560108377743536e-28
        cur_lr: 5.0e-05
        entropy: 0.05865406213949124
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009303292513019793
        total_loss: .inf
        vf_explained_var: 0.9994961619377136
        vf_loss: 0.2901194716493289
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5
    gpu_util_percent0: 0.3115625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628223299214813
    mean_env_wait_ms: 1.1933725328747868
    mean_inference_ms: 4.295975535108564
    mean_raw_obs_processing_ms: 0.37817706371031334
  time_since_restore: 10572.457386016846
  time_this_iter_s: 26.52068066596985
  time_total_s: 10572.457386016846
  timers:
    learn_throughput: 8388.653
    learn_time_ms: 19287.006
    sample_throughput: 23803.994
    sample_time_ms: 6796.843
    update_time_ms: 36.793
  timestamp: 1602753840
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:24:01,689	WARNING util.py:136 -- The `process_trial` operation took 1.1261534690856934 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    399 |          10572.5 | 64555008 |  286.655 |              305.323 |              128.354 |            796.593 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3040.0698640436835
    time_step_min: 2950
  date: 2020-10-15_09-24-28
  done: false
  episode_len_mean: 796.6259178946331
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.67686089120474
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 81164
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.3401625666153045e-28
        cur_lr: 5.0e-05
        entropy: 0.07811682236691316
        entropy_coeff: 0.0005000000000000001
        kl: 0.006659731424103181
        model: {}
        policy_loss: -0.01070231590832312
        total_loss: 1.0509218126535416
        vf_explained_var: 0.9981359839439392
        vf_loss: 1.0616632054249446
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.00625
    gpu_util_percent0: 0.326875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628168491238502
    mean_env_wait_ms: 1.1932940150678721
    mean_inference_ms: 4.295942076229803
    mean_raw_obs_processing_ms: 0.378174058636742
  time_since_restore: 10598.852359056473
  time_this_iter_s: 26.394973039627075
  time_total_s: 10598.852359056473
  timers:
    learn_throughput: 8390.909
    learn_time_ms: 19281.821
    sample_throughput: 23822.849
    sample_time_ms: 6791.463
    update_time_ms: 38.876
  timestamp: 1602753868
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: c9144_00000
  
2020-10-15 09:24:29,627	WARNING util.py:136 -- The `process_trial` operation took 1.0662872791290283 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    400 |          10598.9 | 64716800 |  286.677 |              305.323 |              128.354 |            796.626 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3039.9796746467964
    time_step_min: 2950
  date: 2020-10-15_09-24-56
  done: false
  episode_len_mean: 796.6484353875273
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.6923604174287
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 198
  episodes_total: 81362
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.3401625666153045e-28
        cur_lr: 5.0e-05
        entropy: 0.07255040605862935
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043387731032756465
        model: {}
        policy_loss: -0.011311115444793055
        total_loss: 1.4826717575391133
        vf_explained_var: 0.9971193671226501
        vf_loss: 1.4940191507339478
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.584375
    gpu_util_percent0: 0.31875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628118827474323
    mean_env_wait_ms: 1.1932202073021951
    mean_inference_ms: 4.295911176894046
    mean_raw_obs_processing_ms: 0.3781713612909438
  time_since_restore: 10625.412170410156
  time_this_iter_s: 26.55981135368347
  time_total_s: 10625.412170410156
  timers:
    learn_throughput: 8386.507
    learn_time_ms: 19291.942
    sample_throughput: 23862.053
    sample_time_ms: 6780.305
    update_time_ms: 38.609
  timestamp: 1602753896
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: c9144_00000
  
2020-10-15 09:24:57,820	WARNING util.py:136 -- The `process_trial` operation took 1.1586852073669434 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    401 |          10625.4 | 64878592 |  286.692 |              305.323 |              128.354 |            796.648 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3039.8534520830267
    time_step_min: 2950
  date: 2020-10-15_09-25-24
  done: false
  episode_len_mean: 796.6850191904452
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.71392184527525
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 189
  episodes_total: 81551
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6700812833076522e-28
        cur_lr: 5.0e-05
        entropy: 0.05069357311973969
        entropy_coeff: 0.0005000000000000001
        kl: 0.004761882203941544
        model: {}
        policy_loss: -0.00793981992561991
        total_loss: 0.36611369252204895
        vf_explained_var: 0.9993459582328796
        vf_loss: 0.37407885243495304
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.58125
    gpu_util_percent0: 0.36625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462804830314148
    mean_env_wait_ms: 1.193148077108173
    mean_inference_ms: 4.295871192058351
    mean_raw_obs_processing_ms: 0.37816816261838476
  time_since_restore: 10651.775486946106
  time_this_iter_s: 26.363316535949707
  time_total_s: 10651.775486946106
  timers:
    learn_throughput: 8397.851
    learn_time_ms: 19265.881
    sample_throughput: 23888.647
    sample_time_ms: 6772.757
    update_time_ms: 37.497
  timestamp: 1602753924
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: c9144_00000
  
2020-10-15 09:25:25,678	WARNING util.py:136 -- The `process_trial` operation took 1.1150615215301514 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    402 |          10651.8 | 65040384 |  286.714 |              305.323 |              128.354 |            796.685 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3039.685728621078
    time_step_min: 2950
  date: 2020-10-15_09-25-52
  done: false
  episode_len_mean: 796.7282235079693
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.7388984461192
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 200
  episodes_total: 81751
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3350406416538261e-28
        cur_lr: 5.0e-05
        entropy: 0.04874570978184541
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006674645827539886
        total_loss: .inf
        vf_explained_var: 0.9997149109840393
        vf_loss: 0.20151047656933466
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.612500000000004
    gpu_util_percent0: 0.32406250000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627997509264334
    mean_env_wait_ms: 1.1930726883261384
    mean_inference_ms: 4.29583378953434
    mean_raw_obs_processing_ms: 0.3781644642414639
  time_since_restore: 10678.428584098816
  time_this_iter_s: 26.65309715270996
  time_total_s: 10678.428584098816
  timers:
    learn_throughput: 8395.188
    learn_time_ms: 19271.993
    sample_throughput: 23888.214
    sample_time_ms: 6772.88
    update_time_ms: 37.626
  timestamp: 1602753952
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:25:54,004	WARNING util.py:136 -- The `process_trial` operation took 1.1854164600372314 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    403 |          10678.4 | 65202176 |  286.739 |              305.323 |              128.354 |            796.728 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3039.5072502685284
    time_step_min: 2950
  date: 2020-10-15_09-26-20
  done: false
  episode_len_mean: 796.7780827934556
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.76608218268484
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 81963
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0025609624807395e-28
        cur_lr: 5.0e-05
        entropy: 0.047138320902983345
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005914819465639691
        total_loss: .inf
        vf_explained_var: 0.9998095631599426
        vf_loss: 0.11474593294163545
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.359375
    gpu_util_percent0: 0.381875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462792663659544
    mean_env_wait_ms: 1.1929946875352824
    mean_inference_ms: 4.295801902976307
    mean_raw_obs_processing_ms: 0.37816156858337097
  time_since_restore: 10704.87440443039
  time_this_iter_s: 26.445820331573486
  time_total_s: 10704.87440443039
  timers:
    learn_throughput: 8390.095
    learn_time_ms: 19283.692
    sample_throughput: 23881.576
    sample_time_ms: 6774.762
    update_time_ms: 29.134
  timestamp: 1602753980
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:26:22,019	WARNING util.py:136 -- The `process_trial` operation took 1.1067798137664795 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    404 |          10704.9 | 65363968 |  286.766 |              305.323 |              128.354 |            796.778 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3039.3463753820674
    time_step_min: 2950
  date: 2020-10-15_09-26-48
  done: false
  episode_len_mean: 796.8215911580689
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.7903587325405
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 82154
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0038414437211096e-28
        cur_lr: 5.0e-05
        entropy: 0.0495624424268802
        entropy_coeff: 0.0005000000000000001
        kl: 0.004515229995983343
        model: {}
        policy_loss: -0.009157540567684919
        total_loss: 0.16435818125804266
        vf_explained_var: 0.9996821284294128
        vf_loss: 0.1735405127207438
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.478125
    gpu_util_percent0: 0.315625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627883296092928
    mean_env_wait_ms: 1.1929239113249201
    mean_inference_ms: 4.295772856929665
    mean_raw_obs_processing_ms: 0.3781589550858908
  time_since_restore: 10731.391845226288
  time_this_iter_s: 26.517440795898438
  time_total_s: 10731.391845226288
  timers:
    learn_throughput: 8388.488
    learn_time_ms: 19287.385
    sample_throughput: 23901.666
    sample_time_ms: 6769.068
    update_time_ms: 29.528
  timestamp: 1602754008
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: c9144_00000
  
2020-10-15 09:26:50,112	WARNING util.py:136 -- The `process_trial` operation took 1.0975732803344727 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    405 |          10731.4 | 65525760 |   286.79 |              305.323 |              128.354 |            796.822 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3039.1892965617785
    time_step_min: 2950
  date: 2020-10-15_09-27-16
  done: false
  episode_len_mean: 796.8642419090412
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.8138639414977
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 82345
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5019207218605548e-28
        cur_lr: 5.0e-05
        entropy: 0.049797244680424534
        entropy_coeff: 0.0005000000000000001
        kl: 0.00419203929292659
        model: {}
        policy_loss: -0.009126825102915367
        total_loss: 0.17567472284038863
        vf_explained_var: 0.9996667504310608
        vf_loss: 0.18482644855976105
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.271875
    gpu_util_percent0: 0.3159375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627801066333398
    mean_env_wait_ms: 1.1928515115012392
    mean_inference_ms: 4.2957319370693545
    mean_raw_obs_processing_ms: 0.3781555543225771
  time_since_restore: 10757.99667263031
  time_this_iter_s: 26.604827404022217
  time_total_s: 10757.99667263031
  timers:
    learn_throughput: 8385.872
    learn_time_ms: 19293.403
    sample_throughput: 23851.723
    sample_time_ms: 6783.242
    update_time_ms: 30.611
  timestamp: 1602754036
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: c9144_00000
  
2020-10-15 09:27:18,337	WARNING util.py:136 -- The `process_trial` operation took 1.1231050491333008 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    406 |            10758 | 65687552 |  286.814 |              305.323 |              128.354 |            796.864 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3039.020142768843
    time_step_min: 2950
  date: 2020-10-15_09-27-44
  done: false
  episode_len_mean: 796.9140236958787
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.8392942582122
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 201
  episodes_total: 82546
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.509603609302774e-29
        cur_lr: 5.0e-05
        entropy: 0.05055579667290052
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007874037672687942
        total_loss: .inf
        vf_explained_var: 0.9998040199279785
        vf_loss: 0.11438884275654952
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.081249999999997
    gpu_util_percent0: 0.32499999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627766569895279
    mean_env_wait_ms: 1.1927767986587037
    mean_inference_ms: 4.295696399345332
    mean_raw_obs_processing_ms: 0.37815235398325286
  time_since_restore: 10784.430273771286
  time_this_iter_s: 26.433601140975952
  time_total_s: 10784.430273771286
  timers:
    learn_throughput: 8402.996
    learn_time_ms: 19254.085
    sample_throughput: 23928.708
    sample_time_ms: 6761.418
    update_time_ms: 31.293
  timestamp: 1602754064
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:27:46,247	WARNING util.py:136 -- The `process_trial` operation took 1.092472791671753 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | RUNNING  | 172.17.0.4:43545 |    407 |          10784.4 | 65849344 |  286.839 |              305.323 |              128.354 |            796.914 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c9144_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3038.8478862682996
    time_step_min: 2950
  date: 2020-10-15_09-28-12
  done: true
  episode_len_mean: 796.9619483783653
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 286.8651785631462
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 210
  episodes_total: 82756
  experiment_id: 9e681967ed1146948bf1a6b1417c2b9a
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1264405413954163e-28
        cur_lr: 5.0e-05
        entropy: 0.05021186172962189
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005776597884202299
        total_loss: .inf
        vf_explained_var: 0.9995726943016052
        vf_loss: 0.25211724018057186
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.549999999999997
    gpu_util_percent0: 0.343125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 43545
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462768654605247
    mean_env_wait_ms: 1.1927003614761742
    mean_inference_ms: 4.295665381958323
    mean_raw_obs_processing_ms: 0.3781495602272409
  time_since_restore: 10810.929861545563
  time_this_iter_s: 26.499587774276733
  time_total_s: 10810.929861545563
  timers:
    learn_throughput: 8408.048
    learn_time_ms: 19242.515
    sample_throughput: 23882.059
    sample_time_ms: 6774.625
    update_time_ms: 29.118
  timestamp: 1602754092
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: c9144_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 09:28:14,413	WARNING util.py:136 -- The `process_trial` operation took 1.2794914245605469 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 27.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | TERMINATED |       |    408 |          10810.9 | 66011136 |  286.865 |              305.323 |              128.354 |            796.962 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.86 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c9144_00000 | TERMINATED |       |    408 |          10810.9 | 66011136 |  286.865 |              305.323 |              128.354 |            796.962 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


