2020-10-15 09:28:32,192	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_cb791_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=53595)[0m 2020-10-15 09:28:34,992	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=53505)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53505)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53499)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53499)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53536)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53536)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53483)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53483)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53500)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53500)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53542)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53542)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53477)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53477)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53479)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53479)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53550)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53550)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53585)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53585)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53564)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53564)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53547)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53547)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53540)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53540)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53560)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53560)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53566)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53566)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53567)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53567)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53596)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53596)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53472)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53472)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53537)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53537)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53488)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53488)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53538)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53538)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53548)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53548)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53562)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53562)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53544)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53544)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53577)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53577)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53561)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53561)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53573)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53573)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53571)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53571)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53603)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53603)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53497)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53497)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53551)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53551)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53470)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53470)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53597)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53597)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53539)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53539)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53586)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53586)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53555)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53555)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53609)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53609)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53558)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53558)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53583)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53583)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53535)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53535)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53508)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53508)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53490)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53490)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53533)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53533)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53492)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53492)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53510)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53510)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53474)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53474)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53557)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53557)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53572)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53572)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53494)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53494)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53487)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53487)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53471)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53471)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53602)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53602)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53552)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53552)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53480)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53480)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53491)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53491)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53546)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53546)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53588)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53588)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53473)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53473)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53545)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53545)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53605)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53605)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53484)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53484)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53599)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53599)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53489)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53489)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53553)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53553)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53549)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53549)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53582)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53582)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53563)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53563)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53554)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53554)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53481)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53481)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53476)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53476)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53478)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53478)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53485)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53485)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53512)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53512)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53579)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53579)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53575)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53575)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53543)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53543)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53541)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53541)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53506)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53506)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=53475)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=53475)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 3941
    time_step_mean: 3510.563025210084
    time_step_min: 3241
  date: 2020-10-15_09-29-08
  done: false
  episode_len_mean: 895.2911392405064
  episode_reward_max: 255.28282828282772
  episode_reward_mean: 213.63438179260947
  episode_reward_min: 147.70707070707059
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1733678877353668
        entropy_coeff: 0.0005000000000000001
        kl: 0.004998006935541828
        model: {}
        policy_loss: -0.00916472086282738
        total_loss: 415.6246159871419
        vf_explained_var: 0.5660186409950256
        vf_loss: 415.63336181640625
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.32727272727272
    gpu_util_percent0: 0.3336363636363636
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5484848484848484
    vram_util_percent0: 0.08582297226114873
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17037321587115312
    mean_env_wait_ms: 1.1675067417560516
    mean_inference_ms: 6.011595871758409
    mean_raw_obs_processing_ms: 0.46208013903767137
  time_since_restore: 28.569518327713013
  time_this_iter_s: 28.569518327713013
  time_total_s: 28.569518327713013
  timers:
    learn_throughput: 8457.686
    learn_time_ms: 19129.582
    sample_throughput: 17274.449
    sample_time_ms: 9365.972
    update_time_ms: 42.283
  timestamp: 1602754148
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      1 |          28.5695 | 161792 |  213.634 |              255.283 |              147.707 |            895.291 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3516.126353790614
    time_step_min: 3207
  date: 2020-10-15_09-29-35
  done: false
  episode_len_mean: 894.6867088607595
  episode_reward_max: 258.9191919191918
  episode_reward_mean: 212.48123641478057
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1481354931990306
        entropy_coeff: 0.0005000000000000001
        kl: 0.007063919988771279
        model: {}
        policy_loss: -0.010035032425851872
        total_loss: 99.03858502705891
        vf_explained_var: 0.832775890827179
        vf_loss: 99.04848734537761
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.56333333333334
    gpu_util_percent0: 0.36233333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7533333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16544866036670558
    mean_env_wait_ms: 1.1641762876300874
    mean_inference_ms: 5.707015684327681
    mean_raw_obs_processing_ms: 0.44670147365787954
  time_since_restore: 54.848758697509766
  time_this_iter_s: 26.279240369796753
  time_total_s: 54.848758697509766
  timers:
    learn_throughput: 8570.726
    learn_time_ms: 18877.281
    sample_throughput: 19091.805
    sample_time_ms: 8474.421
    update_time_ms: 31.708
  timestamp: 1602754175
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      2 |          54.8488 | 323584 |  212.481 |              258.919 |              138.768 |            894.687 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3508.733333333333
    time_step_min: 3207
  date: 2020-10-15_09-30-01
  done: false
  episode_len_mean: 886.3544303797469
  episode_reward_max: 258.9191919191918
  episode_reward_mean: 212.8691343817925
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1320596237977345
        entropy_coeff: 0.0005000000000000001
        kl: 0.009627530816942453
        model: {}
        policy_loss: -0.011608108102033535
        total_loss: 40.9521697362264
        vf_explained_var: 0.9179801344871521
        vf_loss: 40.96338144938151
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.566666666666666
    gpu_util_percent0: 0.3506666666666668
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16230613490693604
    mean_env_wait_ms: 1.1650818339664102
    mean_inference_ms: 5.490602305583973
    mean_raw_obs_processing_ms: 0.4363895834946706
  time_since_restore: 80.76092958450317
  time_this_iter_s: 25.912170886993408
  time_total_s: 80.76092958450317
  timers:
    learn_throughput: 8590.771
    learn_time_ms: 18833.233
    sample_throughput: 20192.168
    sample_time_ms: 8012.612
    update_time_ms: 30.924
  timestamp: 1602754201
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      3 |          80.7609 | 485376 |  212.869 |              258.919 |              138.768 |            886.354 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3507.1973018549747
    time_step_min: 3207
  date: 2020-10-15_09-30-27
  done: false
  episode_len_mean: 878.8560126582279
  episode_reward_max: 258.9191919191918
  episode_reward_mean: 213.623593530239
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1127084891001384
        entropy_coeff: 0.0005000000000000001
        kl: 0.007927578369465968
        model: {}
        policy_loss: -0.01219729493216922
        total_loss: 31.16670513153076
        vf_explained_var: 0.9360077381134033
        vf_loss: 31.17866579691569
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.29666666666667
    gpu_util_percent0: 0.40099999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16018542896141452
    mean_env_wait_ms: 1.1672772221313867
    mean_inference_ms: 5.3350246362472005
    mean_raw_obs_processing_ms: 0.428916440162576
  time_since_restore: 106.69234371185303
  time_this_iter_s: 25.931414127349854
  time_total_s: 106.69234371185303
  timers:
    learn_throughput: 8611.484
    learn_time_ms: 18787.935
    sample_throughput: 20719.206
    sample_time_ms: 7808.794
    update_time_ms: 31.406
  timestamp: 1602754227
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      4 |          106.692 | 647168 |  213.624 |              258.919 |              138.768 |            878.856 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3500.901464713715
    time_step_min: 3207
  date: 2020-10-15_09-30-53
  done: false
  episode_len_mean: 871.9417721518987
  episode_reward_max: 258.9191919191918
  episode_reward_mean: 214.66871244086423
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0736438035964966
        entropy_coeff: 0.0005000000000000001
        kl: 0.008050509185219804
        model: {}
        policy_loss: -0.011381465864057342
        total_loss: 25.45706097284953
        vf_explained_var: 0.9563339352607727
        vf_loss: 25.468174775441486
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.690000000000005
    gpu_util_percent0: 0.30133333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15861370410170691
    mean_env_wait_ms: 1.1702382317219056
    mean_inference_ms: 5.217972568208166
    mean_raw_obs_processing_ms: 0.42314532746971273
  time_since_restore: 132.7169268131256
  time_this_iter_s: 26.024583101272583
  time_total_s: 132.7169268131256
  timers:
    learn_throughput: 8612.437
    learn_time_ms: 18785.857
    sample_throughput: 21075.943
    sample_time_ms: 7676.62
    update_time_ms: 33.583
  timestamp: 1602754253
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      5 |          132.717 | 808960 |  214.669 |              258.919 |              138.768 |            871.942 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3485.6166822867854
    time_step_min: 3203
  date: 2020-10-15_09-31-19
  done: false
  episode_len_mean: 861.7206148282098
  episode_reward_max: 259.5252525252525
  episode_reward_mean: 216.85017443878195
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 316
  episodes_total: 1106
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0657447179158528
        entropy_coeff: 0.0005000000000000001
        kl: 0.008472384458097318
        model: {}
        policy_loss: -0.011950941659354916
        total_loss: 27.248703002929688
        vf_explained_var: 0.9630305171012878
        vf_loss: 27.260340054829914
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.52333333333333
    gpu_util_percent0: 0.35633333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15649246157230187
    mean_env_wait_ms: 1.1750681214811716
    mean_inference_ms: 5.060540646755697
    mean_raw_obs_processing_ms: 0.41568440392320904
  time_since_restore: 158.65502166748047
  time_this_iter_s: 25.93809485435486
  time_total_s: 158.65502166748047
  timers:
    learn_throughput: 8606.71
    learn_time_ms: 18798.356
    sample_throughput: 21390.169
    sample_time_ms: 7563.848
    update_time_ms: 32.975
  timestamp: 1602754279
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      6 |          158.655 | 970752 |   216.85 |              259.525 |              138.768 |            861.721 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3476.8171428571427
    time_step_min: 3167
  date: 2020-10-15_09-31-45
  done: false
  episode_len_mean: 857.3180379746835
  episode_reward_max: 265.7373737373735
  episode_reward_mean: 218.20381025444308
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0459044277668
        entropy_coeff: 0.0005000000000000001
        kl: 0.008025348264103135
        model: {}
        policy_loss: -0.012926413328386843
        total_loss: 15.976893107096354
        vf_explained_var: 0.9690950512886047
        vf_loss: 15.989539941151937
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.790000000000003
    gpu_util_percent0: 0.4123333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1557537270411306
    mean_env_wait_ms: 1.1769768405878724
    mean_inference_ms: 5.004227624825222
    mean_raw_obs_processing_ms: 0.4130283077184474
  time_since_restore: 184.72053122520447
  time_this_iter_s: 26.065509557724
  time_total_s: 184.72053122520447
  timers:
    learn_throughput: 8595.45
    learn_time_ms: 18822.981
    sample_throughput: 21645.984
    sample_time_ms: 7474.458
    update_time_ms: 42.852
  timestamp: 1602754305
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      7 |          184.721 | 1132544 |  218.204 |              265.737 |              138.768 |            857.318 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3467.940708604483
    time_step_min: 3167
  date: 2020-10-15_09-32-11
  done: false
  episode_len_mean: 854.1075949367089
  episode_reward_max: 265.7373737373735
  episode_reward_mean: 219.32312577249283
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0292503933111827
        entropy_coeff: 0.0005000000000000001
        kl: 0.007071365291873614
        model: {}
        policy_loss: -0.011622646396669248
        total_loss: 13.951191822687784
        vf_explained_var: 0.9717023372650146
        vf_loss: 13.962621847788492
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.58275862068966
    gpu_util_percent0: 0.2941379310344827
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15511530047623381
    mean_env_wait_ms: 1.1785606819761996
    mean_inference_ms: 4.955537252830591
    mean_raw_obs_processing_ms: 0.4106581434856589
  time_since_restore: 210.32416415214539
  time_this_iter_s: 25.603632926940918
  time_total_s: 210.32416415214539
  timers:
    learn_throughput: 8607.445
    learn_time_ms: 18796.751
    sample_throughput: 21848.755
    sample_time_ms: 7405.09
    update_time_ms: 40.041
  timestamp: 1602754331
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      8 |          210.324 | 1294336 |  219.323 |              265.737 |              138.768 |            854.108 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3458.6658014276445
    time_step_min: 3148
  date: 2020-10-15_09-32-36
  done: false
  episode_len_mean: 850.2822784810127
  episode_reward_max: 267.8585858585858
  episode_reward_mean: 220.54769211098318
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9820772459109625
        entropy_coeff: 0.0005000000000000001
        kl: 0.007025937355744342
        model: {}
        policy_loss: -0.011879481782671064
        total_loss: 16.12793469429016
        vf_explained_var: 0.9705111384391785
        vf_loss: 16.139602581659954
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.14137931034483
    gpu_util_percent0: 0.3575862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15455806340989636
    mean_env_wait_ms: 1.1802079925843068
    mean_inference_ms: 4.912981493576734
    mean_raw_obs_processing_ms: 0.4085079984155278
  time_since_restore: 235.85832858085632
  time_this_iter_s: 25.534164428710938
  time_total_s: 235.85832858085632
  timers:
    learn_throughput: 8614.043
    learn_time_ms: 18782.352
    sample_throughput: 22053.456
    sample_time_ms: 7336.356
    update_time_ms: 37.985
  timestamp: 1602754356
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |      9 |          235.858 | 1456128 |  220.548 |              267.859 |              138.768 |            850.282 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3443.0635433494886
    time_step_min: 3108
  date: 2020-10-15_09-33-02
  done: false
  episode_len_mean: 842.329641350211
  episode_reward_max: 273.9191919191924
  episode_reward_mean: 223.18886651323356
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 316
  episodes_total: 1896
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9527567028999329
        entropy_coeff: 0.0005000000000000001
        kl: 0.0066283421668534475
        model: {}
        policy_loss: -0.010255232860799879
        total_loss: 16.848692893981934
        vf_explained_var: 0.9762053489685059
        vf_loss: 16.85876162846883
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94
    gpu_util_percent0: 0.35100000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15365657855181947
    mean_env_wait_ms: 1.1835484841513022
    mean_inference_ms: 4.843968808297927
    mean_raw_obs_processing_ms: 0.40511609161434825
  time_since_restore: 261.66285276412964
  time_this_iter_s: 25.804524183273315
  time_total_s: 261.66285276412964
  timers:
    learn_throughput: 8612.495
    learn_time_ms: 18785.728
    sample_throughput: 22189.35
    sample_time_ms: 7291.426
    update_time_ms: 37.459
  timestamp: 1602754382
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     10 |          261.663 | 1617920 |  223.189 |              273.919 |              138.768 |             842.33 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3436.9563275434243
    time_step_min: 3108
  date: 2020-10-15_09-33-28
  done: false
  episode_len_mean: 838.5628042843233
  episode_reward_max: 273.9191919191924
  episode_reward_mean: 224.2034856844983
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9340264697869619
        entropy_coeff: 0.0005000000000000001
        kl: 0.006637935526669025
        model: {}
        policy_loss: -0.010802200093166903
        total_loss: 11.707856257756552
        vf_explained_var: 0.9762506484985352
        vf_loss: 11.718461910883585
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.334482758620688
    gpu_util_percent0: 0.36931034482758623
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689655
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15328939263604605
    mean_env_wait_ms: 1.1850262977783848
    mean_inference_ms: 4.815813363537691
    mean_raw_obs_processing_ms: 0.4037050753418081
  time_since_restore: 287.4609260559082
  time_this_iter_s: 25.798073291778564
  time_total_s: 287.4609260559082
  timers:
    learn_throughput: 8630.444
    learn_time_ms: 18746.659
    sample_throughput: 22941.95
    sample_time_ms: 7052.234
    update_time_ms: 36.408
  timestamp: 1602754408
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     11 |          287.461 | 1779712 |  224.203 |              273.919 |              138.768 |            838.563 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3429.441785549931
    time_step_min: 3108
  date: 2020-10-15_09-33-54
  done: false
  episode_len_mean: 835.3019891500904
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 225.35105119915244
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9185261279344559
        entropy_coeff: 0.0005000000000000001
        kl: 0.006282192383271952
        model: {}
        policy_loss: -0.010594809878966771
        total_loss: 11.78148102760315
        vf_explained_var: 0.9733021855354309
        vf_loss: 11.791906754175821
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.943333333333335
    gpu_util_percent0: 0.35900000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15295429771254251
    mean_env_wait_ms: 1.1864529231068812
    mean_inference_ms: 4.790259575188983
    mean_raw_obs_processing_ms: 0.4023774398072177
  time_since_restore: 313.3229925632477
  time_this_iter_s: 25.862066507339478
  time_total_s: 313.3229925632477
  timers:
    learn_throughput: 8625.165
    learn_time_ms: 18758.134
    sample_throughput: 23127.283
    sample_time_ms: 6995.72
    update_time_ms: 38.676
  timestamp: 1602754434
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     12 |          313.323 | 1941504 |  225.351 |              283.919 |              138.768 |            835.302 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3416.010212418301
    time_step_min: 3107
  date: 2020-10-15_09-34-20
  done: false
  episode_len_mean: 830.3470044229996
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 227.07201081989982
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 275
  episodes_total: 2487
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8739954978227615
        entropy_coeff: 0.0005000000000000001
        kl: 0.006836044291655223
        model: {}
        policy_loss: -0.011842325504403561
        total_loss: 15.203980127970377
        vf_explained_var: 0.9774308800697327
        vf_loss: 15.215575774510702
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.50689655172414
    gpu_util_percent0: 0.3127586206896551
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1524491196287426
    mean_env_wait_ms: 1.1890457606337757
    mean_inference_ms: 4.75154689375602
    mean_raw_obs_processing_ms: 0.4003820774925392
  time_since_restore: 338.9223418235779
  time_this_iter_s: 25.5993492603302
  time_total_s: 338.9223418235779
  timers:
    learn_throughput: 8629.292
    learn_time_ms: 18749.163
    sample_throughput: 23203.15
    sample_time_ms: 6972.846
    update_time_ms: 38.726
  timestamp: 1602754460
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     13 |          338.922 | 2103296 |  227.072 |              283.919 |              138.768 |            830.347 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3407.6898375519454
    time_step_min: 3107
  date: 2020-10-15_09-34-45
  done: false
  episode_len_mean: 827.4717051377513
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 228.25514264010167
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 2686
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8569933970769247
        entropy_coeff: 0.0005000000000000001
        kl: 0.006308569146009783
        model: {}
        policy_loss: -0.010416376687013932
        total_loss: 10.241653362909952
        vf_explained_var: 0.979211151599884
        vf_loss: 10.251867532730103
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.293103448275865
    gpu_util_percent0: 0.3231034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.152128140544226
    mean_env_wait_ms: 1.1905953942533942
    mean_inference_ms: 4.727137887109817
    mean_raw_obs_processing_ms: 0.39913776505210624
  time_since_restore: 364.40479493141174
  time_this_iter_s: 25.482453107833862
  time_total_s: 364.40479493141174
  timers:
    learn_throughput: 8629.906
    learn_time_ms: 18747.829
    sample_throughput: 23350.627
    sample_time_ms: 6928.808
    update_time_ms: 38.955
  timestamp: 1602754485
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     14 |          364.405 | 2265088 |  228.255 |              283.919 |              138.768 |            827.472 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3403.4976827094474
    time_step_min: 3107
  date: 2020-10-15_09-35-11
  done: false
  episode_len_mean: 825.5502812939521
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 228.9589353450113
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.849844848116239
        entropy_coeff: 0.0005000000000000001
        kl: 0.006415587888720135
        model: {}
        policy_loss: -0.01132429470696176
        total_loss: 10.057613849639893
        vf_explained_var: 0.9778110980987549
        vf_loss: 10.068721373875936
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45
    gpu_util_percent0: 0.2866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15189884850724797
    mean_env_wait_ms: 1.1917668464581908
    mean_inference_ms: 4.709533589781011
    mean_raw_obs_processing_ms: 0.39821911491036815
  time_since_restore: 390.1335895061493
  time_this_iter_s: 25.72879457473755
  time_total_s: 390.1335895061493
  timers:
    learn_throughput: 8624.697
    learn_time_ms: 18759.152
    sample_throughput: 23481.742
    sample_time_ms: 6890.119
    update_time_ms: 36.641
  timestamp: 1602754511
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     15 |          390.134 | 2426880 |  228.959 |              283.919 |              138.768 |             825.55 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3397.2335669002337
    time_step_min: 3107
  date: 2020-10-15_09-35-37
  done: false
  episode_len_mean: 823.1696310935441
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 229.98873118537148
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 192
  episodes_total: 3036
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8184456924597422
        entropy_coeff: 0.0005000000000000001
        kl: 0.006053957312057416
        model: {}
        policy_loss: -0.00969617662485689
        total_loss: 11.97330617904663
        vf_explained_var: 0.977895200252533
        vf_loss: 11.982805728912354
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25172413793104
    gpu_util_percent0: 0.34586206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15164228905211363
    mean_env_wait_ms: 1.1931731614977272
    mean_inference_ms: 4.689849610951607
    mean_raw_obs_processing_ms: 0.3971530949397893
  time_since_restore: 415.7142012119293
  time_this_iter_s: 25.58061170578003
  time_total_s: 415.7142012119293
  timers:
    learn_throughput: 8632.782
    learn_time_ms: 18741.583
    sample_throughput: 23550.228
    sample_time_ms: 6870.082
    update_time_ms: 37.744
  timestamp: 1602754537
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     16 |          415.714 | 2588672 |  229.989 |              283.919 |              138.768 |             823.17 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3386.8481244281793
    time_step_min: 3103
  date: 2020-10-15_09-36-03
  done: false
  episode_len_mean: 820.1711874623267
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 231.59162145871008
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 282
  episodes_total: 3318
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7818726052840551
        entropy_coeff: 0.0005000000000000001
        kl: 0.006080446105139951
        model: {}
        policy_loss: -0.010272806757711805
        total_loss: 11.227606932322184
        vf_explained_var: 0.981134831905365
        vf_loss: 11.237662474314371
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.659999999999993
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15131101560666252
    mean_env_wait_ms: 1.1950624206729947
    mean_inference_ms: 4.6642111023500785
    mean_raw_obs_processing_ms: 0.3958322514511132
  time_since_restore: 441.5794458389282
  time_this_iter_s: 25.8652446269989
  time_total_s: 441.5794458389282
  timers:
    learn_throughput: 8632.326
    learn_time_ms: 18742.573
    sample_throughput: 23597.032
    sample_time_ms: 6856.456
    update_time_ms: 29.788
  timestamp: 1602754563
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     17 |          441.579 | 2750464 |  231.592 |              283.919 |              138.768 |            820.171 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3381.3046261274367
    time_step_min: 3094
  date: 2020-10-15_09-36-28
  done: false
  episode_len_mean: 818.4976985040277
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 232.3974584742709
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7945885012547175
        entropy_coeff: 0.0005000000000000001
        kl: 0.006646261123629908
        model: {}
        policy_loss: -0.008898436247060696
        total_loss: 9.387492338816324
        vf_explained_var: 0.978131115436554
        vf_loss: 9.396123170852661
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.413793103448278
    gpu_util_percent0: 0.3744827586206896
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15114222075495348
    mean_env_wait_ms: 1.1960153275092937
    mean_inference_ms: 4.651255473324579
    mean_raw_obs_processing_ms: 0.39515066148400624
  time_since_restore: 467.0190875530243
  time_this_iter_s: 25.43964171409607
  time_total_s: 467.0190875530243
  timers:
    learn_throughput: 8632.866
    learn_time_ms: 18741.401
    sample_throughput: 23653.087
    sample_time_ms: 6840.207
    update_time_ms: 29.916
  timestamp: 1602754588
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     18 |          467.019 | 2912256 |  232.397 |              283.919 |              138.768 |            818.498 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3375.6286111111112
    time_step_min: 3094
  date: 2020-10-15_09-36-54
  done: false
  episode_len_mean: 816.8419895575707
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 233.2244039737857
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 163
  episodes_total: 3639
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7640985002120336
        entropy_coeff: 0.0005000000000000001
        kl: 0.005924703553318977
        model: {}
        policy_loss: -0.01154794641964448
        total_loss: 9.984641949335733
        vf_explained_var: 0.9783595204353333
        vf_loss: 9.995979229609171
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.503333333333334
    gpu_util_percent0: 0.303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15097950344089361
    mean_env_wait_ms: 1.1969938715695279
    mean_inference_ms: 4.638722420659977
    mean_raw_obs_processing_ms: 0.39447967618141266
  time_since_restore: 492.80052280426025
  time_this_iter_s: 25.781435251235962
  time_total_s: 492.80052280426025
  timers:
    learn_throughput: 8619.445
    learn_time_ms: 18770.581
    sample_throughput: 23672.916
    sample_time_ms: 6834.477
    update_time_ms: 31.087
  timestamp: 1602754614
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     19 |          492.801 | 3074048 |  233.224 |              283.919 |              138.768 |            816.842 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3366.6026100307063
    time_step_min: 3068
  date: 2020-10-15_09-37-20
  done: false
  episode_len_mean: 814.0339498353179
  episode_reward_max: 286.1919191919192
  episode_reward_mean: 234.70736245147194
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 308
  episodes_total: 3947
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7212709039449692
        entropy_coeff: 0.0005000000000000001
        kl: 0.005585941020399332
        model: {}
        policy_loss: -0.009148939342897696
        total_loss: 11.774536848068237
        vf_explained_var: 0.9819743633270264
        vf_loss: 11.783488035202026
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.476666666666663
    gpu_util_percent0: 0.32866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1506989077010467
    mean_env_wait_ms: 1.1986951957921863
    mean_inference_ms: 4.617191777834063
    mean_raw_obs_processing_ms: 0.3933573239608248
  time_since_restore: 518.7033152580261
  time_this_iter_s: 25.90279245376587
  time_total_s: 518.7033152580261
  timers:
    learn_throughput: 8614.037
    learn_time_ms: 18782.366
    sample_throughput: 23704.143
    sample_time_ms: 6825.474
    update_time_ms: 30.202
  timestamp: 1602754640
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     20 |          518.703 | 3235840 |  234.707 |              286.192 |              138.768 |            814.034 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3361.2777095109363
    time_step_min: 3068
  date: 2020-10-15_09-37-46
  done: false
  episode_len_mean: 812.6032132424538
  episode_reward_max: 286.1919191919192
  episode_reward_mean: 235.5290219625663
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 161
  episodes_total: 4108
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7334958960612615
        entropy_coeff: 0.0005000000000000001
        kl: 0.00623501290101558
        model: {}
        policy_loss: -0.012696098769083619
        total_loss: 8.210275928179422
        vf_explained_var: 0.9805936217308044
        vf_loss: 8.222715377807617
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.082758620689653
    gpu_util_percent0: 0.356551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789655172413793
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1505649475184156
    mean_env_wait_ms: 1.1995012803356315
    mean_inference_ms: 4.60695843153469
    mean_raw_obs_processing_ms: 0.39281594319618296
  time_since_restore: 544.1341621875763
  time_this_iter_s: 25.43084692955017
  time_total_s: 544.1341621875763
  timers:
    learn_throughput: 8624.579
    learn_time_ms: 18759.408
    sample_throughput: 23751.965
    sample_time_ms: 6811.731
    update_time_ms: 29.01
  timestamp: 1602754666
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     21 |          544.134 | 3397632 |  235.529 |              286.192 |              138.768 |            812.603 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3355.935888336882
    time_step_min: 2992
  date: 2020-10-15_09-38-11
  done: false
  episode_len_mean: 811.4566338490389
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 236.30717157510412
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 4266
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7313190599282583
        entropy_coeff: 0.0005000000000000001
        kl: 0.0059486522028843565
        model: {}
        policy_loss: -0.010567928196905996
        total_loss: 8.188967108726501
        vf_explained_var: 0.9800074100494385
        vf_loss: 8.199306011199951
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.200000000000003
    gpu_util_percent0: 0.37275862068965515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.793103448275862
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15044093260482375
    mean_env_wait_ms: 1.200251145824553
    mean_inference_ms: 4.597462300765394
    mean_raw_obs_processing_ms: 0.3923095921744388
  time_since_restore: 569.2792632579803
  time_this_iter_s: 25.145101070404053
  time_total_s: 569.2792632579803
  timers:
    learn_throughput: 8645.919
    learn_time_ms: 18713.107
    sample_throughput: 23834.005
    sample_time_ms: 6788.284
    update_time_ms: 26.808
  timestamp: 1602754691
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     22 |          569.279 | 3559424 |  236.307 |              291.495 |              138.768 |            811.457 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3346.508729281768
    time_step_min: 2992
  date: 2020-10-15_09-38-37
  done: false
  episode_len_mean: 809.3422436459247
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 237.67161536486694
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 298
  episodes_total: 4564
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6868849446376165
        entropy_coeff: 0.0005000000000000001
        kl: 0.005426900926977396
        model: {}
        policy_loss: -0.012831605854444206
        total_loss: 10.865561644236246
        vf_explained_var: 0.9829086661338806
        vf_loss: 10.878193855285645
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.813793103448276
    gpu_util_percent0: 0.4317241379310346
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15022353611789072
    mean_env_wait_ms: 1.2016075691213242
    mean_inference_ms: 4.580996412839131
    mean_raw_obs_processing_ms: 0.3914451142955876
  time_since_restore: 594.5345947742462
  time_this_iter_s: 25.25533151626587
  time_total_s: 594.5345947742462
  timers:
    learn_throughput: 8660.704
    learn_time_ms: 18681.161
    sample_throughput: 23849.001
    sample_time_ms: 6784.016
    update_time_ms: 26.978
  timestamp: 1602754717
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     23 |          594.535 | 3721216 |  237.672 |              291.495 |              138.768 |            809.342 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3341.208040842374
    time_step_min: 2992
  date: 2020-10-15_09-39-02
  done: false
  episode_len_mean: 808.3808016877637
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 238.4455632272088
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 176
  episodes_total: 4740
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6830945213635763
        entropy_coeff: 0.0005000000000000001
        kl: 0.005740340022991101
        model: {}
        policy_loss: -0.010633966104554323
        total_loss: 7.890702724456787
        vf_explained_var: 0.9825429320335388
        vf_loss: 7.901104132334392
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.186206896551724
    gpu_util_percent0: 0.30275862068965514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7862068965517235
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15010847748860268
    mean_env_wait_ms: 1.202330473762237
    mean_inference_ms: 4.572016215174834
    mean_raw_obs_processing_ms: 0.3909792213715556
  time_since_restore: 620.0717613697052
  time_this_iter_s: 25.537166595458984
  time_total_s: 620.0717613697052
  timers:
    learn_throughput: 8661.351
    learn_time_ms: 18679.764
    sample_throughput: 23827.081
    sample_time_ms: 6790.257
    update_time_ms: 25.858
  timestamp: 1602754742
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     24 |          620.072 | 3883008 |  238.446 |              291.495 |              138.768 |            808.381 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3336.7478905124512
    time_step_min: 2992
  date: 2020-10-15_09-39-28
  done: false
  episode_len_mean: 807.5679869334422
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 239.13053359235477
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 4898
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6929802199204763
        entropy_coeff: 0.0005000000000000001
        kl: 0.005470787757076323
        model: {}
        policy_loss: -0.010624278977047652
        total_loss: 7.717013875643413
        vf_explained_var: 0.9808815121650696
        vf_loss: 7.727437655131022
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.51034482758621
    gpu_util_percent0: 0.30310344827586205
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7896551724137932
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15000907610415204
    mean_env_wait_ms: 1.2029310258908237
    mean_inference_ms: 4.564386183066052
    mean_raw_obs_processing_ms: 0.39057511161456654
  time_since_restore: 645.5239987373352
  time_this_iter_s: 25.452237367630005
  time_total_s: 645.5239987373352
  timers:
    learn_throughput: 8678.877
    learn_time_ms: 18642.043
    sample_throughput: 23799.781
    sample_time_ms: 6798.046
    update_time_ms: 27.13
  timestamp: 1602754768
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     25 |          645.524 | 4044800 |  239.131 |              291.495 |              138.768 |            807.568 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3330.2917155138725
    time_step_min: 2992
  date: 2020-10-15_09-39-53
  done: false
  episode_len_mean: 806.1334108978089
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 240.13724798890595
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 259
  episodes_total: 5157
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6603399912516276
        entropy_coeff: 0.0005000000000000001
        kl: 0.005722672794945538
        model: {}
        policy_loss: -0.010399677489961809
        total_loss: 11.333871920903524
        vf_explained_var: 0.9813134074211121
        vf_loss: 11.344029506047567
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.893103448275866
    gpu_util_percent0: 0.32448275862068965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14985454133688986
    mean_env_wait_ms: 1.2038963128718474
    mean_inference_ms: 4.552729451661486
    mean_raw_obs_processing_ms: 0.3899520161578701
  time_since_restore: 670.6112186908722
  time_this_iter_s: 25.087219953536987
  time_total_s: 670.6112186908722
  timers:
    learn_throughput: 8701.274
    learn_time_ms: 18594.058
    sample_throughput: 23802.259
    sample_time_ms: 6797.338
    update_time_ms: 24.953
  timestamp: 1602754793
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     26 |          670.611 | 4206592 |  240.137 |              291.495 |              138.768 |            806.133 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3324.14869679355
    time_step_min: 2992
  date: 2020-10-15_09-40-19
  done: false
  episode_len_mean: 805.1243484735667
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 241.03947328835642
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 215
  episodes_total: 5372
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6385591675837835
        entropy_coeff: 0.0005000000000000001
        kl: 0.004849217327622076
        model: {}
        policy_loss: -0.010857531859073788
        total_loss: 7.4299672444661455
        vf_explained_var: 0.9844462871551514
        vf_loss: 7.440659125645955
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.589999999999996
    gpu_util_percent0: 0.31233333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.846666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14974265533828268
    mean_env_wait_ms: 1.2046397149327823
    mean_inference_ms: 4.5437713006802944
    mean_raw_obs_processing_ms: 0.38950729862331335
  time_since_restore: 695.9468140602112
  time_this_iter_s: 25.33559536933899
  time_total_s: 695.9468140602112
  timers:
    learn_throughput: 8738.089
    learn_time_ms: 18515.719
    sample_throughput: 23739.624
    sample_time_ms: 6815.272
    update_time_ms: 24.857
  timestamp: 1602754819
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     27 |          695.947 | 4368384 |  241.039 |              291.495 |              138.768 |            805.124 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3319.8796211983245
    time_step_min: 2992
  date: 2020-10-15_09-40-44
  done: false
  episode_len_mean: 804.3748643761302
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 241.64884833872176
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 5530
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6570624609788259
        entropy_coeff: 0.0005000000000000001
        kl: 0.006124866000997524
        model: {}
        policy_loss: -0.01042374266156306
        total_loss: 7.550791382789612
        vf_explained_var: 0.980835497379303
        vf_loss: 7.561237573623657
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.731034482758627
    gpu_util_percent0: 0.34620689655172415
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8379310344827595
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496653517154595
    mean_env_wait_ms: 1.2051641410251355
    mean_inference_ms: 4.537626858182996
    mean_raw_obs_processing_ms: 0.3891925223741288
  time_since_restore: 721.3634538650513
  time_this_iter_s: 25.416639804840088
  time_total_s: 721.3634538650513
  timers:
    learn_throughput: 8749.744
    learn_time_ms: 18491.056
    sample_throughput: 23667.552
    sample_time_ms: 6836.026
    update_time_ms: 25.757
  timestamp: 1602754844
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     28 |          721.363 | 4530176 |  241.649 |              291.495 |              138.768 |            804.375 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3314.7637285764254
    time_step_min: 2992
  date: 2020-10-15_09-41-09
  done: false
  episode_len_mean: 803.18273406288
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 242.43614887804574
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 227
  episodes_total: 5757
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6203485031922659
        entropy_coeff: 0.0005000000000000001
        kl: 0.005696180664623777
        model: {}
        policy_loss: -0.009003105558804236
        total_loss: 8.377814809481302
        vf_explained_var: 0.9846324920654297
        vf_loss: 8.38684352238973
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.231034482758623
    gpu_util_percent0: 0.353448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8482758620689657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14956685597730335
    mean_env_wait_ms: 1.205941406797925
    mean_inference_ms: 4.529427579627007
    mean_raw_obs_processing_ms: 0.3887745253838261
  time_since_restore: 746.4022307395935
  time_this_iter_s: 25.038776874542236
  time_total_s: 746.4022307395935
  timers:
    learn_throughput: 8786.616
    learn_time_ms: 18413.46
    sample_throughput: 23654.628
    sample_time_ms: 6839.761
    update_time_ms: 24.314
  timestamp: 1602754869
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     29 |          746.402 | 4691968 |  242.436 |              291.495 |              138.768 |            803.183 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3308.6626990779546
    time_step_min: 2967
  date: 2020-10-15_09-41-35
  done: false
  episode_len_mean: 802.0041638907395
  episode_reward_max: 295.2828282828282
  episode_reward_mean: 243.3372818794205
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 247
  episodes_total: 6004
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5812346041202545
        entropy_coeff: 0.0005000000000000001
        kl: 0.005257382406853139
        model: {}
        policy_loss: -0.010139185052442675
        total_loss: 8.125032424926758
        vf_explained_var: 0.9838623404502869
        vf_loss: 8.135199348131815
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.5896551724138
    gpu_util_percent0: 0.3741379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.858620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14946061732675547
    mean_env_wait_ms: 1.2067398868099624
    mean_inference_ms: 4.520874607332682
    mean_raw_obs_processing_ms: 0.38834888652097294
  time_since_restore: 771.405237197876
  time_this_iter_s: 25.00300645828247
  time_total_s: 771.405237197876
  timers:
    learn_throughput: 8827.586
    learn_time_ms: 18328.001
    sample_throughput: 23643.231
    sample_time_ms: 6843.058
    update_time_ms: 24.067
  timestamp: 1602754895
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     30 |          771.405 | 4853760 |  243.337 |              295.283 |              138.768 |            802.004 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3304.9157275845173
    time_step_min: 2967
  date: 2020-10-15_09-42-00
  done: false
  episode_len_mean: 801.1801363193769
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 243.90335684006575
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 6162
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6109522531429926
        entropy_coeff: 0.0005000000000000001
        kl: 0.005762041818040113
        model: {}
        policy_loss: -0.010459269299947968
        total_loss: 6.1143150726954145
        vf_explained_var: 0.9838274121284485
        vf_loss: 6.124791661898295
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.048275862068962
    gpu_util_percent0: 0.38
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.844827586206896
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1493998489078335
    mean_env_wait_ms: 1.207236971685695
    mean_inference_ms: 4.515795999266553
    mean_raw_obs_processing_ms: 0.3880964313811511
  time_since_restore: 796.7033309936523
  time_this_iter_s: 25.298093795776367
  time_total_s: 796.7033309936523
  timers:
    learn_throughput: 8835.97
    learn_time_ms: 18310.61
    sample_throughput: 23633.693
    sample_time_ms: 6845.82
    update_time_ms: 24.574
  timestamp: 1602754920
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     31 |          796.703 | 5015552 |  243.903 |              295.889 |              138.768 |             801.18 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3299.7715456121
    time_step_min: 2967
  date: 2020-10-15_09-42-26
  done: false
  episode_len_mean: 800.0822110867523
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 244.67128693765088
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 224
  episodes_total: 6386
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.586687371134758
        entropy_coeff: 0.0005000000000000001
        kl: 0.005857182511438926
        model: {}
        policy_loss: -0.008998592941982983
        total_loss: 9.322317441304525
        vf_explained_var: 0.9821708798408508
        vf_loss: 9.331316312154135
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.076666666666664
    gpu_util_percent0: 0.32200000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14931783336710988
    mean_env_wait_ms: 1.2079295703699668
    mean_inference_ms: 4.508943200750969
    mean_raw_obs_processing_ms: 0.3877543425308813
  time_since_restore: 822.2755188941956
  time_this_iter_s: 25.572187900543213
  time_total_s: 822.2755188941956
  timers:
    learn_throughput: 8825.627
    learn_time_ms: 18332.068
    sample_throughput: 23569.72
    sample_time_ms: 6864.401
    update_time_ms: 26.208
  timestamp: 1602754946
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     32 |          822.276 | 5177344 |  244.671 |              295.889 |              138.768 |            800.082 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3294.336213430347
    time_step_min: 2967
  date: 2020-10-15_09-42-51
  done: false
  episode_len_mean: 798.8270042194093
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 245.54492026960386
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 6636
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5415168702602386
        entropy_coeff: 0.0005000000000000001
        kl: 0.005145472784837087
        model: {}
        policy_loss: -0.009888297997046417
        total_loss: 7.5816722710927325
        vf_explained_var: 0.9850125908851624
        vf_loss: 7.5915742715199785
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.75862068965517
    gpu_util_percent0: 0.39344827586206893
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275867
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14923089182228808
    mean_env_wait_ms: 1.208648802861343
    mean_inference_ms: 4.501710742156145
    mean_raw_obs_processing_ms: 0.3873970458936143
  time_since_restore: 847.6820237636566
  time_this_iter_s: 25.40650486946106
  time_total_s: 847.6820237636566
  timers:
    learn_throughput: 8823.914
    learn_time_ms: 18335.627
    sample_throughput: 23528.774
    sample_time_ms: 6876.346
    update_time_ms: 25.32
  timestamp: 1602754971
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     33 |          847.682 | 5339136 |  245.545 |              295.889 |              138.768 |            798.827 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3291.0977054034047
    time_step_min: 2967
  date: 2020-10-15_09-43-17
  done: false
  episode_len_mean: 798.0628495731528
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 246.05275897033334
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 6794
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5788290202617645
        entropy_coeff: 0.0005000000000000001
        kl: 0.00525987334549427
        model: {}
        policy_loss: -0.011562899654866973
        total_loss: 7.844460328420003
        vf_explained_var: 0.9792365431785583
        vf_loss: 7.856049656867981
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.858620689655176
    gpu_util_percent0: 0.326551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14917845097818067
    mean_env_wait_ms: 1.2090764108629084
    mean_inference_ms: 4.497376296203897
    mean_raw_obs_processing_ms: 0.3871801868591249
  time_since_restore: 873.0166478157043
  time_this_iter_s: 25.33462405204773
  time_total_s: 873.0166478157043
  timers:
    learn_throughput: 8835.751
    learn_time_ms: 18311.064
    sample_throughput: 23513.86
    sample_time_ms: 6880.708
    update_time_ms: 24.686
  timestamp: 1602754997
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     34 |          873.017 | 5500928 |  246.053 |              295.889 |              138.768 |            798.063 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3286.5707540420663
    time_step_min: 2967
  date: 2020-10-15_09-43-42
  done: false
  episode_len_mean: 796.9588787706317
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 246.76130830214498
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 7028
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5510778526465098
        entropy_coeff: 0.0005000000000000001
        kl: 0.005085315167283018
        model: {}
        policy_loss: -0.011924359165277565
        total_loss: 9.076769431432089
        vf_explained_var: 0.9827437996864319
        vf_loss: 9.088714996973673
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.393103448275863
    gpu_util_percent0: 0.29586206896551726
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14910353294037842
    mean_env_wait_ms: 1.2097022144031815
    mean_inference_ms: 4.491256977544629
    mean_raw_obs_processing_ms: 0.3868705242934376
  time_since_restore: 898.3698682785034
  time_this_iter_s: 25.353220462799072
  time_total_s: 898.3698682785034
  timers:
    learn_throughput: 8837.471
    learn_time_ms: 18307.5
    sample_throughput: 23534.796
    sample_time_ms: 6874.587
    update_time_ms: 23.753
  timestamp: 1602755022
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     35 |           898.37 | 5662720 |  246.761 |              295.889 |              138.768 |            796.959 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3281.62373772306
    time_step_min: 2960
  date: 2020-10-15_09-44-08
  done: false
  episode_len_mean: 795.8569069895432
  episode_reward_max: 296.3434343434343
  episode_reward_mean: 247.52184614443837
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 240
  episodes_total: 7268
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5111012508471807
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051787659370650845
        model: {}
        policy_loss: -0.00917195157671813
        total_loss: 6.548242966334025
        vf_explained_var: 0.9863994717597961
        vf_loss: 6.557411591211955
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.799999999999997
    gpu_util_percent0: 0.3946666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14903094036047287
    mean_env_wait_ms: 1.210306600043729
    mean_inference_ms: 4.485234145762319
    mean_raw_obs_processing_ms: 0.3865772934909757
  time_since_restore: 923.649831533432
  time_this_iter_s: 25.27996325492859
  time_total_s: 923.649831533432
  timers:
    learn_throughput: 8833.053
    learn_time_ms: 18316.657
    sample_throughput: 23502.503
    sample_time_ms: 6884.033
    update_time_ms: 23.851
  timestamp: 1602755048
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     36 |           923.65 | 5824512 |  247.522 |              296.343 |              138.768 |            795.857 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3278.666170299174
    time_step_min: 2947
  date: 2020-10-15_09-44-34
  done: false
  episode_len_mean: 795.1316994344196
  episode_reward_max: 298.3131313131316
  episode_reward_mean: 247.9472179375223
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 7426
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.545738473534584
        entropy_coeff: 0.0005000000000000001
        kl: 0.005450511390032868
        model: {}
        policy_loss: -0.012406253100683292
        total_loss: 5.776481469472249
        vf_explained_var: 0.9846447110176086
        vf_loss: 5.788888335227966
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.903448275862072
    gpu_util_percent0: 0.393103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789655172413793
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148985303664036
    mean_env_wait_ms: 1.2106831419669413
    mean_inference_ms: 4.481457356983387
    mean_raw_obs_processing_ms: 0.3863889755269919
  time_since_restore: 949.1942126750946
  time_this_iter_s: 25.544381141662598
  time_total_s: 949.1942126750946
  timers:
    learn_throughput: 8818.384
    learn_time_ms: 18347.127
    sample_throughput: 23515.245
    sample_time_ms: 6880.303
    update_time_ms: 24.013
  timestamp: 1602755074
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     37 |          949.194 | 5986304 |  247.947 |              298.313 |              138.768 |            795.132 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3273.961251472706
    time_step_min: 2947
  date: 2020-10-15_09-44-59
  done: false
  episode_len_mean: 794.016671008075
  episode_reward_max: 298.61616161616115
  episode_reward_mean: 248.6355992853779
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 252
  episodes_total: 7678
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.519211878379186
        entropy_coeff: 0.0005000000000000001
        kl: 0.00521182909142226
        model: {}
        policy_loss: -0.009023654982835675
        total_loss: 8.441483894983927
        vf_explained_var: 0.9842881560325623
        vf_loss: 8.450506289800009
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.79310344827586
    gpu_util_percent0: 0.3182758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689655
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1489144442223095
    mean_env_wait_ms: 1.2112931809094059
    mean_inference_ms: 4.475737080194631
    mean_raw_obs_processing_ms: 0.3861045942091553
  time_since_restore: 974.4429366588593
  time_this_iter_s: 25.24872398376465
  time_total_s: 974.4429366588593
  timers:
    learn_throughput: 8819.48
    learn_time_ms: 18344.847
    sample_throughput: 23565.374
    sample_time_ms: 6865.666
    update_time_ms: 23.076
  timestamp: 1602755099
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     38 |          974.443 | 6148096 |  248.636 |              298.616 |              138.768 |            794.017 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3270.134206843913
    time_step_min: 2947
  date: 2020-10-15_09-45-25
  done: false
  episode_len_mean: 793.15
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 249.21516430124026
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 222
  episodes_total: 7900
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4856214125951131
        entropy_coeff: 0.0005000000000000001
        kl: 0.005182099722636242
        model: {}
        policy_loss: -0.00891703084926121
        total_loss: 6.5547778606414795
        vf_explained_var: 0.9860953688621521
        vf_loss: 6.563678741455078
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.237931034482763
    gpu_util_percent0: 0.3258620689655172
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7862068965517235
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14885693550440063
    mean_env_wait_ms: 1.211778243806471
    mean_inference_ms: 4.470836848044071
    mean_raw_obs_processing_ms: 0.385864774718793
  time_since_restore: 999.8051853179932
  time_this_iter_s: 25.36224865913391
  time_total_s: 999.8051853179932
  timers:
    learn_throughput: 8805.735
    learn_time_ms: 18373.48
    sample_throughput: 23557.633
    sample_time_ms: 6867.923
    update_time_ms: 23.62
  timestamp: 1602755125
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     39 |          999.805 | 6309888 |  249.215 |              303.616 |              138.768 |             793.15 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3267.4345928420003
    time_step_min: 2947
  date: 2020-10-15_09-45-51
  done: false
  episode_len_mean: 792.5824025812857
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 249.58562918838425
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 8058
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.526516318321228
        entropy_coeff: 0.0005000000000000001
        kl: 0.005193507065996528
        model: {}
        policy_loss: -0.011457257942917446
        total_loss: 6.553280353546143
        vf_explained_var: 0.9829640984535217
        vf_loss: 6.564741214116414
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35333333333334
    gpu_util_percent0: 0.3950000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14881653626058752
    mean_env_wait_ms: 1.2121147093800466
    mean_inference_ms: 4.467526822703954
    mean_raw_obs_processing_ms: 0.385700627485691
  time_since_restore: 1025.2843720912933
  time_this_iter_s: 25.47918677330017
  time_total_s: 1025.2843720912933
  timers:
    learn_throughput: 8789.35
    learn_time_ms: 18407.731
    sample_throughput: 23547.875
    sample_time_ms: 6870.768
    update_time_ms: 25.262
  timestamp: 1602755151
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     40 |          1025.28 | 6471680 |  249.586 |              303.616 |              138.768 |            792.582 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3263.2761617380806
    time_step_min: 2947
  date: 2020-10-15_09-46-16
  done: false
  episode_len_mean: 791.6511292647765
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 250.2041219499173
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 266
  episodes_total: 8324
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49336690952380496
        entropy_coeff: 0.0005000000000000001
        kl: 0.005102361280781527
        model: {}
        policy_loss: -0.009011513475949565
        total_loss: 9.669517993927002
        vf_explained_var: 0.9830517768859863
        vf_loss: 9.678521235783895
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.813793103448276
    gpu_util_percent0: 0.3882758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1487499291020892
    mean_env_wait_ms: 1.212677779836052
    mean_inference_ms: 4.462225090737758
    mean_raw_obs_processing_ms: 0.3854342897020459
  time_since_restore: 1050.7636091709137
  time_this_iter_s: 25.47923707962036
  time_total_s: 1050.7636091709137
  timers:
    learn_throughput: 8785.037
    learn_time_ms: 18416.768
    sample_throughput: 23518.854
    sample_time_ms: 6879.247
    update_time_ms: 24.772
  timestamp: 1602755176
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     41 |          1050.76 | 6633472 |  250.204 |              303.616 |              138.768 |            791.651 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3259.8172612739904
    time_step_min: 2947
  date: 2020-10-15_09-46-42
  done: false
  episode_len_mean: 790.9975386779184
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 250.71389705777898
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 208
  episodes_total: 8532
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4733814299106598
        entropy_coeff: 0.0005000000000000001
        kl: 0.004729776371580859
        model: {}
        policy_loss: -0.010568847976780186
        total_loss: 6.403569380442302
        vf_explained_var: 0.9858106970787048
        vf_loss: 6.414138555526733
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.982758620689655
    gpu_util_percent0: 0.3075862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789655172413793
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1487025390909323
    mean_env_wait_ms: 1.2130857830012112
    mean_inference_ms: 4.458203489235618
    mean_raw_obs_processing_ms: 0.3852425785196856
  time_since_restore: 1076.1438705921173
  time_this_iter_s: 25.380261421203613
  time_total_s: 1076.1438705921173
  timers:
    learn_throughput: 8791.007
    learn_time_ms: 18404.263
    sample_throughput: 23541.239
    sample_time_ms: 6872.705
    update_time_ms: 22.959
  timestamp: 1602755202
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     42 |          1076.14 | 6795264 |  250.714 |              303.616 |              138.768 |            790.998 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3257.5926482487575
    time_step_min: 2947
  date: 2020-10-15_09-47-07
  done: false
  episode_len_mean: 790.5673187571922
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 251.05876370145648
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 8690
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5028343424201012
        entropy_coeff: 0.0005000000000000001
        kl: 0.005660295176009337
        model: {}
        policy_loss: -0.011312471678441701
        total_loss: 5.480809609095256
        vf_explained_var: 0.9858322143554688
        vf_loss: 5.492232163747151
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.117241379310347
    gpu_util_percent0: 0.33620689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486669119557743
    mean_env_wait_ms: 1.2133848559813398
    mean_inference_ms: 4.455287585707035
    mean_raw_obs_processing_ms: 0.38509862362338443
  time_since_restore: 1101.6251499652863
  time_this_iter_s: 25.481279373168945
  time_total_s: 1101.6251499652863
  timers:
    learn_throughput: 8787.839
    learn_time_ms: 18410.897
    sample_throughput: 23569.845
    sample_time_ms: 6864.364
    update_time_ms: 30.717
  timestamp: 1602755227
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     43 |          1101.63 | 6957056 |  251.059 |              303.616 |              138.768 |            790.567 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3253.6264303343055
    time_step_min: 2947
  date: 2020-10-15_09-47-33
  done: false
  episode_len_mean: 789.8943370937116
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 251.6527601492418
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 263
  episodes_total: 8953
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.47802816579739255
        entropy_coeff: 0.0005000000000000001
        kl: 0.005564751607986788
        model: {}
        policy_loss: -0.01014368303124987
        total_loss: 7.744199713071187
        vf_explained_var: 0.9861556887626648
        vf_loss: 7.754443327585856
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.227586206896557
    gpu_util_percent0: 0.3120689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486093507899377
    mean_env_wait_ms: 1.2138679896901612
    mean_inference_ms: 4.45063897428115
    mean_raw_obs_processing_ms: 0.3848663084593683
  time_since_restore: 1127.1951382160187
  time_this_iter_s: 25.569988250732422
  time_total_s: 1127.1951382160187
  timers:
    learn_throughput: 8780.813
    learn_time_ms: 18425.629
    sample_throughput: 23545.007
    sample_time_ms: 6871.605
    update_time_ms: 30.926
  timestamp: 1602755253
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     44 |           1127.2 | 7118848 |  251.653 |              303.616 |              138.768 |            789.894 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3250.506301369863
    time_step_min: 2940
  date: 2020-10-15_09-47-59
  done: false
  episode_len_mean: 789.403644696639
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 252.1213465955937
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 211
  episodes_total: 9164
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4490925893187523
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058276798808947206
        model: {}
        policy_loss: -0.011005985550582409
        total_loss: 5.937054077784221
        vf_explained_var: 0.9868550300598145
        vf_loss: 5.948138991991679
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.127586206896552
    gpu_util_percent0: 0.3513793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689654
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14856577596355064
    mean_env_wait_ms: 1.2142338626310112
    mean_inference_ms: 4.447041063797639
    mean_raw_obs_processing_ms: 0.3846952491402056
  time_since_restore: 1152.7496552467346
  time_this_iter_s: 25.554517030715942
  time_total_s: 1152.7496552467346
  timers:
    learn_throughput: 8779.928
    learn_time_ms: 18427.486
    sample_throughput: 23487.208
    sample_time_ms: 6888.516
    update_time_ms: 31.707
  timestamp: 1602755279
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     45 |          1152.75 | 7280640 |  252.121 |              303.616 |              138.768 |            789.404 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3248.310029085425
    time_step_min: 2940
  date: 2020-10-15_09-48-25
  done: false
  episode_len_mean: 788.9847672173354
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 252.43690498635792
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 9322
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4758431166410446
        entropy_coeff: 0.0005000000000000001
        kl: 0.005838179378770292
        model: {}
        policy_loss: -0.011624956270679832
        total_loss: 5.875628630320231
        vf_explained_var: 0.9845533967018127
        vf_loss: 5.8873454332351685
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.613333333333337
    gpu_util_percent0: 0.4096666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7966666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14853375821310807
    mean_env_wait_ms: 1.2144962464267635
    mean_inference_ms: 4.444449136237662
    mean_raw_obs_processing_ms: 0.3845677931821678
  time_since_restore: 1178.4103214740753
  time_this_iter_s: 25.6606662273407
  time_total_s: 1178.4103214740753
  timers:
    learn_throughput: 8765.715
    learn_time_ms: 18457.365
    sample_throughput: 23463.328
    sample_time_ms: 6895.527
    update_time_ms: 31.679
  timestamp: 1602755305
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     46 |          1178.41 | 7442432 |  252.437 |              303.616 |              138.768 |            788.985 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3244.7780452071997
    time_step_min: 2940
  date: 2020-10-15_09-48-50
  done: false
  episode_len_mean: 788.2295987493486
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 252.98352466825636
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 273
  episodes_total: 9595
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4465416222810745
        entropy_coeff: 0.0005000000000000001
        kl: 0.005149371650380393
        model: {}
        policy_loss: -0.009117977499651412
        total_loss: 7.547347227732341
        vf_explained_var: 0.9863469004631042
        vf_loss: 7.556559681892395
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.665517241379316
    gpu_util_percent0: 0.3993103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148480012204535
    mean_env_wait_ms: 1.2149442753547917
    mean_inference_ms: 4.4401230321990415
    mean_raw_obs_processing_ms: 0.3843522536714458
  time_since_restore: 1203.7887575626373
  time_this_iter_s: 25.37843608856201
  time_total_s: 1203.7887575626373
  timers:
    learn_throughput: 8766.139
    learn_time_ms: 18456.472
    sample_throughput: 23520.429
    sample_time_ms: 6878.786
    update_time_ms: 31.366
  timestamp: 1602755330
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     47 |          1203.79 | 7604224 |  252.984 |              303.616 |              138.768 |             788.23 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3242.339858563083
    time_step_min: 2938
  date: 2020-10-15_09-49-16
  done: false
  episode_len_mean: 787.7311147407105
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 253.35953244160675
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 9796
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42620716243982315
        entropy_coeff: 0.0005000000000000001
        kl: 0.005318921951887508
        model: {}
        policy_loss: -0.01032576325815171
        total_loss: 5.459895412127177
        vf_explained_var: 0.9876511096954346
        vf_loss: 5.470301270484924
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.53103448275862
    gpu_util_percent0: 0.35655172413793107
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.793103448275862
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14844286465416862
    mean_env_wait_ms: 1.2152583753073374
    mean_inference_ms: 4.43707174287909
    mean_raw_obs_processing_ms: 0.3842098850624854
  time_since_restore: 1229.012372493744
  time_this_iter_s: 25.223614931106567
  time_total_s: 1229.012372493744
  timers:
    learn_throughput: 8767.979
    learn_time_ms: 18452.599
    sample_throughput: 23520.838
    sample_time_ms: 6878.666
    update_time_ms: 31.425
  timestamp: 1602755356
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     48 |          1229.01 | 7766016 |   253.36 |              303.616 |              138.768 |            787.731 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3240.424682395644
    time_step_min: 2938
  date: 2020-10-15_09-49-42
  done: false
  episode_len_mean: 787.3573365471527
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 253.64726505793095
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 161
  episodes_total: 9957
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.45852066328128177
        entropy_coeff: 0.0005000000000000001
        kl: 0.005530392401851714
        model: {}
        policy_loss: -0.010260472452500835
        total_loss: 6.348827560742696
        vf_explained_var: 0.9838187098503113
        vf_loss: 6.3591790199279785
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.563333333333336
    gpu_util_percent0: 0.3773333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14841371661765432
    mean_env_wait_ms: 1.2154975457707187
    mean_inference_ms: 4.434692602559898
    mean_raw_obs_processing_ms: 0.3840926186361674
  time_since_restore: 1254.5409824848175
  time_this_iter_s: 25.52860999107361
  time_total_s: 1254.5409824848175
  timers:
    learn_throughput: 8767.785
    learn_time_ms: 18453.007
    sample_throughput: 23465.721
    sample_time_ms: 6894.823
    update_time_ms: 30.827
  timestamp: 1602755382
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     49 |          1254.54 | 7927808 |  253.647 |              303.616 |              138.768 |            787.357 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3237.025402118478
    time_step_min: 2938
  date: 2020-10-15_09-50-07
  done: false
  episode_len_mean: 786.685979482169
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 254.1688699402427
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 278
  episodes_total: 10235
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42400458206733066
        entropy_coeff: 0.0005000000000000001
        kl: 0.005758004845120013
        model: {}
        policy_loss: -0.009732729863571876
        total_loss: 9.089882930119833
        vf_explained_var: 0.9840230941772461
        vf_loss: 9.09968360265096
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.817241379310346
    gpu_util_percent0: 0.373448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275867
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14836411038656214
    mean_env_wait_ms: 1.2159030729909135
    mean_inference_ms: 4.430689461386763
    mean_raw_obs_processing_ms: 0.3838943111443688
  time_since_restore: 1280.0171992778778
  time_this_iter_s: 25.476216793060303
  time_total_s: 1280.0171992778778
  timers:
    learn_throughput: 8763.776
    learn_time_ms: 18461.449
    sample_throughput: 23465.351
    sample_time_ms: 6894.932
    update_time_ms: 28.933
  timestamp: 1602755407
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     50 |          1280.02 | 8089600 |  254.169 |              303.616 |              138.768 |            786.686 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3234.5002406391377
    time_step_min: 2938
  date: 2020-10-15_09-50-33
  done: false
  episode_len_mean: 786.243670886076
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 254.5625937162186
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 10428
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4049416979153951
        entropy_coeff: 0.0005000000000000001
        kl: 0.005929892572263877
        model: {}
        policy_loss: -0.011005657501906777
        total_loss: 4.866203625996907
        vf_explained_var: 0.9882550835609436
        vf_loss: 4.87726350625356
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.268965517241377
    gpu_util_percent0: 0.3510344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.793103448275862
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14833159314114258
    mean_env_wait_ms: 1.216174935454952
    mean_inference_ms: 4.428082441567676
    mean_raw_obs_processing_ms: 0.3837743190916421
  time_since_restore: 1305.344899892807
  time_this_iter_s: 25.3277006149292
  time_total_s: 1305.344899892807
  timers:
    learn_throughput: 8763.478
    learn_time_ms: 18462.076
    sample_throughput: 23524.144
    sample_time_ms: 6877.7
    update_time_ms: 28.954
  timestamp: 1602755433
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     51 |          1305.34 | 8251392 |  254.563 |              303.616 |              138.768 |            786.244 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3232.345588932057
    time_step_min: 2919
  date: 2020-10-15_09-50-59
  done: false
  episode_len_mean: 785.8859516616315
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 254.876103367512
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 164
  episodes_total: 10592
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4286295870939891
        entropy_coeff: 0.0005000000000000001
        kl: 0.005036363222946723
        model: {}
        policy_loss: -0.009228799298095206
        total_loss: 5.845330556233724
        vf_explained_var: 0.9849271178245544
        vf_loss: 5.8546479145685835
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.02333333333333
    gpu_util_percent0: 0.3476666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14830496855825573
    mean_env_wait_ms: 1.216392577903492
    mean_inference_ms: 4.425912646920895
    mean_raw_obs_processing_ms: 0.38366667287708983
  time_since_restore: 1330.9797713756561
  time_this_iter_s: 25.63487148284912
  time_total_s: 1330.9797713756561
  timers:
    learn_throughput: 8754.186
    learn_time_ms: 18481.672
    sample_throughput: 23505.638
    sample_time_ms: 6883.115
    update_time_ms: 28.986
  timestamp: 1602755459
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     52 |          1330.98 | 8413184 |  254.876 |              303.616 |              138.768 |            785.886 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3228.687338501292
    time_step_min: 2918
  date: 2020-10-15_09-51-25
  done: false
  episode_len_mean: 785.3101609195402
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 255.41159177986768
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 283
  episodes_total: 10875
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.39254015187422436
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054301081302886205
        model: {}
        policy_loss: -0.007928162871394306
        total_loss: 7.315639098485311
        vf_explained_var: 0.9870920777320862
        vf_loss: 7.323627670605977
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.041379310344826
    gpu_util_percent0: 0.41344827586206906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14825900726741614
    mean_env_wait_ms: 1.2167640681979082
    mean_inference_ms: 4.422223474314901
    mean_raw_obs_processing_ms: 0.38348620045999554
  time_since_restore: 1356.684547662735
  time_this_iter_s: 25.704776287078857
  time_total_s: 1356.684547662735
  timers:
    learn_throughput: 8741.867
    learn_time_ms: 18507.716
    sample_throughput: 23498.929
    sample_time_ms: 6885.08
    update_time_ms: 23.04
  timestamp: 1602755485
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     53 |          1356.68 | 8574976 |  255.412 |              303.616 |              138.768 |             785.31 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3226.419199709645
    time_step_min: 2918
  date: 2020-10-15_09-51-51
  done: false
  episode_len_mean: 784.951356238698
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 255.77582789924566
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 185
  episodes_total: 11060
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.38370103389024734
        entropy_coeff: 0.0005000000000000001
        kl: 0.005197393669125934
        model: {}
        policy_loss: -0.009420125027342388
        total_loss: 5.069755752881368
        vf_explained_var: 0.9875216484069824
        vf_loss: 5.079237739245097
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.68
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7966666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14822986692029236
    mean_env_wait_ms: 1.2169926777432136
    mean_inference_ms: 4.419954542858773
    mean_raw_obs_processing_ms: 0.3833811280955723
  time_since_restore: 1382.3803510665894
  time_this_iter_s: 25.69580340385437
  time_total_s: 1382.3803510665894
  timers:
    learn_throughput: 8736.171
    learn_time_ms: 18519.784
    sample_throughput: 23503.135
    sample_time_ms: 6883.848
    update_time_ms: 24.048
  timestamp: 1602755511
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     54 |          1382.38 | 8736768 |  255.776 |              303.616 |              138.768 |            784.951 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3224.2455528738715
    time_step_min: 2909
  date: 2020-10-15_09-52-17
  done: false
  episode_len_mean: 784.6326385177267
  episode_reward_max: 304.0707070707074
  episode_reward_mean: 256.1064979025963
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 166
  episodes_total: 11226
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4106709385911624
        entropy_coeff: 0.0005000000000000001
        kl: 0.005636657549378772
        model: {}
        policy_loss: -0.01151395154496034
        total_loss: 5.502703309059143
        vf_explained_var: 0.9857354164123535
        vf_loss: 5.514281590779622
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.656666666666673
    gpu_util_percent0: 0.32566666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.796666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482050936860099
    mean_env_wait_ms: 1.2171861560883543
    mean_inference_ms: 4.417961368899147
    mean_raw_obs_processing_ms: 0.38328247844893243
  time_since_restore: 1408.1777501106262
  time_this_iter_s: 25.797399044036865
  time_total_s: 1408.1777501106262
  timers:
    learn_throughput: 8720.534
    learn_time_ms: 18552.992
    sample_throughput: 23541.202
    sample_time_ms: 6872.716
    update_time_ms: 24.671
  timestamp: 1602755537
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     55 |          1408.18 | 8898560 |  256.106 |              304.071 |              138.768 |            784.633 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3220.7535297193654
    time_step_min: 2909
  date: 2020-10-15_09-52-42
  done: false
  episode_len_mean: 784.0581082254843
  episode_reward_max: 306.3434343434339
  episode_reward_mean: 256.6566770808932
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 287
  episodes_total: 11513
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3653937478860219
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045549396115044756
        model: {}
        policy_loss: -0.005883191052513818
        total_loss: 8.344082752863565
        vf_explained_var: 0.9852698445320129
        vf_loss: 8.350034753481546
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.641379310344824
    gpu_util_percent0: 0.3237931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275867
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481619991768215
    mean_env_wait_ms: 1.217521780171274
    mean_inference_ms: 4.414568947018353
    mean_raw_obs_processing_ms: 0.3831184909848838
  time_since_restore: 1433.5305213928223
  time_this_iter_s: 25.352771282196045
  time_total_s: 1433.5305213928223
  timers:
    learn_throughput: 8727.379
    learn_time_ms: 18538.44
    sample_throughput: 23602.233
    sample_time_ms: 6854.945
    update_time_ms: 25.002
  timestamp: 1602755562
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     56 |          1433.53 | 9060352 |  256.657 |              306.343 |              138.768 |            784.058 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3218.6061100145885
    time_step_min: 2909
  date: 2020-10-15_09-53-08
  done: false
  episode_len_mean: 783.7340916866233
  episode_reward_max: 306.3434343434339
  episode_reward_mean: 256.99917927133123
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 179
  episodes_total: 11692
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3698219656944275
        entropy_coeff: 0.0005000000000000001
        kl: 0.005350918586676319
        model: {}
        policy_loss: -0.009632504275941756
        total_loss: 4.928918838500977
        vf_explained_var: 0.987260639667511
        vf_loss: 4.938669403394063
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.76666666666667
    gpu_util_percent0: 0.32633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481367305595268
    mean_env_wait_ms: 1.2177156626259042
    mean_inference_ms: 4.412592695081641
    mean_raw_obs_processing_ms: 0.3830260151887144
  time_since_restore: 1459.2168862819672
  time_this_iter_s: 25.686364889144897
  time_total_s: 1459.2168862819672
  timers:
    learn_throughput: 8726.858
    learn_time_ms: 18539.548
    sample_throughput: 23509.907
    sample_time_ms: 6881.865
    update_time_ms: 26.973
  timestamp: 1602755588
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     57 |          1459.22 | 9222144 |  256.999 |              306.343 |              138.768 |            783.734 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3216.5854690010997
    time_step_min: 2909
  date: 2020-10-15_09-53-34
  done: false
  episode_len_mean: 783.4070982970832
  episode_reward_max: 306.3434343434339
  episode_reward_mean: 257.3064918277362
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 170
  episodes_total: 11862
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.39362389345963794
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050841827954476075
        model: {}
        policy_loss: -0.012481133700930513
        total_loss: 5.161785840988159
        vf_explained_var: 0.9871451258659363
        vf_loss: 5.174400289853414
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89333333333333
    gpu_util_percent0: 0.4073333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481131791588241
    mean_env_wait_ms: 1.217893398571362
    mean_inference_ms: 4.410732018457688
    mean_raw_obs_processing_ms: 0.38293390796992816
  time_since_restore: 1484.9283435344696
  time_this_iter_s: 25.71145725250244
  time_total_s: 1484.9283435344696
  timers:
    learn_throughput: 8714.131
    learn_time_ms: 18566.626
    sample_throughput: 23467.448
    sample_time_ms: 6894.316
    update_time_ms: 26.859
  timestamp: 1602755614
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     58 |          1484.93 | 9383936 |  257.306 |              306.343 |              138.768 |            783.407 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3213.302039468252
    time_step_min: 2909
  date: 2020-10-15_09-54-00
  done: false
  episode_len_mean: 782.942304526749
  episode_reward_max: 307.10101010100993
  episode_reward_mean: 257.8080184561666
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 288
  episodes_total: 12150
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3501675600806872
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049746248017375665
        model: {}
        policy_loss: -0.010083653352921829
        total_loss: 6.506309151649475
        vf_explained_var: 0.9882670044898987
        vf_loss: 6.5165055592854815
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.944827586206898
    gpu_util_percent0: 0.373448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14807383807859442
    mean_env_wait_ms: 1.2181906258319926
    mean_inference_ms: 4.407652872472788
    mean_raw_obs_processing_ms: 0.3827868161513801
  time_since_restore: 1510.590265750885
  time_this_iter_s: 25.661922216415405
  time_total_s: 1510.590265750885
  timers:
    learn_throughput: 8708.014
    learn_time_ms: 18579.668
    sample_throughput: 23478.788
    sample_time_ms: 6890.986
    update_time_ms: 28.997
  timestamp: 1602755640
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     59 |          1510.59 | 9545728 |  257.808 |              307.101 |              138.768 |            782.942 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3211.332600732601
    time_step_min: 2909
  date: 2020-10-15_09-54-26
  done: false
  episode_len_mean: 782.6514930217462
  episode_reward_max: 307.10101010100993
  episode_reward_mean: 258.11093899068584
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 174
  episodes_total: 12324
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.35738612711429596
        entropy_coeff: 0.0005000000000000001
        kl: 0.004641391996604701
        model: {}
        policy_loss: -0.00926777491501222
        total_loss: 4.795818567276001
        vf_explained_var: 0.9875019192695618
        vf_loss: 4.8052358229955034
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.353333333333335
    gpu_util_percent0: 0.32866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148050704000257
    mean_env_wait_ms: 1.2183557533754723
    mean_inference_ms: 4.405886551475431
    mean_raw_obs_processing_ms: 0.3827031713255567
  time_since_restore: 1536.3341145515442
  time_this_iter_s: 25.74384880065918
  time_total_s: 1536.3341145515442
  timers:
    learn_throughput: 8695.486
    learn_time_ms: 18606.435
    sample_throughput: 23491.504
    sample_time_ms: 6887.256
    update_time_ms: 31.658
  timestamp: 1602755666
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     60 |          1536.33 | 9707520 |  258.111 |              307.101 |              138.768 |            782.651 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3209.3472679130227
    time_step_min: 2876
  date: 2020-10-15_09-54-52
  done: false
  episode_len_mean: 782.3579427291634
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 258.4143151237216
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 12502
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.37262995292743045
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047727123989413185
        model: {}
        policy_loss: -0.009100308564181129
        total_loss: 5.967288215955098
        vf_explained_var: 0.9856808185577393
        vf_loss: 5.97655991713206
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.326666666666668
    gpu_util_percent0: 0.383
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333326
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14802742567212468
    mean_env_wait_ms: 1.2185194215920687
    mean_inference_ms: 4.40408981823857
    mean_raw_obs_processing_ms: 0.3826145228928232
  time_since_restore: 1562.1822981834412
  time_this_iter_s: 25.848183631896973
  time_total_s: 1562.1822981834412
  timers:
    learn_throughput: 8679.757
    learn_time_ms: 18640.154
    sample_throughput: 23459.416
    sample_time_ms: 6896.676
    update_time_ms: 39.873
  timestamp: 1602755692
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     61 |          1562.18 | 9869312 |  258.414 |              309.071 |              138.768 |            782.358 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3206.071540633825
    time_step_min: 2876
  date: 2020-10-15_09-55-19
  done: false
  episode_len_mean: 781.8988034722765
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 258.8943323909305
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 285
  episodes_total: 12787
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3330593133966128
        entropy_coeff: 0.0005000000000000001
        kl: 0.00444906127328674
        model: {}
        policy_loss: -0.00821452463666598
        total_loss: 7.139691313107808
        vf_explained_var: 0.9868228435516357
        vf_loss: 7.148065447807312
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.82
    gpu_util_percent0: 0.29933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479912153472001
    mean_env_wait_ms: 1.218774197272222
    mean_inference_ms: 4.401311425666439
    mean_raw_obs_processing_ms: 0.38248098079193643
  time_since_restore: 1588.1013956069946
  time_this_iter_s: 25.919097423553467
  time_total_s: 1588.1013956069946
  timers:
    learn_throughput: 8671.273
    learn_time_ms: 18658.391
    sample_throughput: 23440.943
    sample_time_ms: 6902.111
    update_time_ms: 42.875
  timestamp: 1602755719
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     62 |           1588.1 | 10031104 |  258.894 |              309.071 |              138.768 |            781.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3204.3154757296584
    time_step_min: 2876
  date: 2020-10-15_09-55-44
  done: false
  episode_len_mean: 781.6361531336833
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 259.16756169287817
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 169
  episodes_total: 12956
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3515119304259618
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048386941974361735
        model: {}
        policy_loss: -0.009805110188002194
        total_loss: 4.415205558141072
        vf_explained_var: 0.988283634185791
        vf_loss: 4.425182620684306
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.882758620689657
    gpu_util_percent0: 0.4141379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.793103448275862
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14797100258508047
    mean_env_wait_ms: 1.2189165669885387
    mean_inference_ms: 4.399754257689588
    mean_raw_obs_processing_ms: 0.3824047927373949
  time_since_restore: 1613.6176793575287
  time_this_iter_s: 25.516283750534058
  time_total_s: 1613.6176793575287
  timers:
    learn_throughput: 8675.775
    learn_time_ms: 18648.708
    sample_throughput: 23475.272
    sample_time_ms: 6892.018
    update_time_ms: 40.933
  timestamp: 1602755744
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     63 |          1613.62 | 10192896 |  259.168 |              309.071 |              138.768 |            781.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3202.2302822273073
    time_step_min: 2876
  date: 2020-10-15_09-56-10
  done: false
  episode_len_mean: 781.3836793672523
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 259.4798751835028
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 13149
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.36590735614299774
        entropy_coeff: 0.0005000000000000001
        kl: 0.005199664505198598
        model: {}
        policy_loss: -0.009823094194871373
        total_loss: 6.801222324371338
        vf_explained_var: 0.9842493534088135
        vf_loss: 6.81122624874115
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.050000000000008
    gpu_util_percent0: 0.33066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333326
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14794664544634398
    mean_env_wait_ms: 1.2190753036899755
    mean_inference_ms: 4.397969468508504
    mean_raw_obs_processing_ms: 0.3823167838169247
  time_since_restore: 1639.0970180034637
  time_this_iter_s: 25.47933864593506
  time_total_s: 1639.0970180034637
  timers:
    learn_throughput: 8684.709
    learn_time_ms: 18629.524
    sample_throughput: 23493.269
    sample_time_ms: 6886.739
    update_time_ms: 41.477
  timestamp: 1602755770
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     64 |           1639.1 | 10354688 |   259.48 |              309.071 |              138.768 |            781.384 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3199.4806427503736
    time_step_min: 2876
  date: 2020-10-15_09-56-36
  done: false
  episode_len_mean: 781.0432222967435
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 259.9165791607106
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 270
  episodes_total: 13419
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.3239520415663719
        entropy_coeff: 0.0005000000000000001
        kl: 0.004762139287777245
        model: {}
        policy_loss: -0.007904218912396269
        total_loss: 6.6040810743967695
        vf_explained_var: 0.987603485584259
        vf_loss: 6.612145344416301
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.227586206896554
    gpu_util_percent0: 0.38241379310344825
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14791577296223943
    mean_env_wait_ms: 1.2192839988106812
    mean_inference_ms: 4.395560105926969
    mean_raw_obs_processing_ms: 0.382198847777824
  time_since_restore: 1664.660623550415
  time_this_iter_s: 25.563605546951294
  time_total_s: 1664.660623550415
  timers:
    learn_throughput: 8692.88
    learn_time_ms: 18612.013
    sample_throughput: 23513.308
    sample_time_ms: 6880.869
    update_time_ms: 40.307
  timestamp: 1602755796
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     65 |          1664.66 | 10516480 |  259.917 |              309.071 |              138.768 |            781.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3197.6880950623663
    time_step_min: 2876
  date: 2020-10-15_09-57-02
  done: false
  episode_len_mean: 780.8112304974978
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 260.20380356404786
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 169
  episodes_total: 13588
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.34279561787843704
        entropy_coeff: 0.0005000000000000001
        kl: 0.005283504142425954
        model: {}
        policy_loss: -0.011030456070632985
        total_loss: 4.626668572425842
        vf_explained_var: 0.9871037602424622
        vf_loss: 4.6378694375356035
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58
    gpu_util_percent0: 0.3606666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7966666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14789670708936478
    mean_env_wait_ms: 1.219406525984315
    mean_inference_ms: 4.394113687162331
    mean_raw_obs_processing_ms: 0.38212732850285347
  time_since_restore: 1690.2191231250763
  time_this_iter_s: 25.558499574661255
  time_total_s: 1690.2191231250763
  timers:
    learn_throughput: 8688.002
    learn_time_ms: 18622.463
    sample_throughput: 23482.3
    sample_time_ms: 6889.955
    update_time_ms: 40.228
  timestamp: 1602755822
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     66 |          1690.22 | 10678272 |  260.204 |              309.071 |              138.768 |            780.811 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3195.457063913328
    time_step_min: 2873
  date: 2020-10-15_09-57-28
  done: false
  episode_len_mean: 780.5256670533643
  episode_reward_max: 309.5252525252523
  episode_reward_mean: 260.53601634090325
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 204
  episodes_total: 13792
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.3453119471669197
        entropy_coeff: 0.0005000000000000001
        kl: 0.006284578509318332
        model: {}
        policy_loss: -0.010803500733648738
        total_loss: 6.535979986190796
        vf_explained_var: 0.9849061965942383
        vf_loss: 6.546955108642578
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.520689655172415
    gpu_util_percent0: 0.34206896551724136
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689655
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787349336129887
    mean_env_wait_ms: 1.21954913669658
    mean_inference_ms: 4.3923647878696945
    mean_raw_obs_processing_ms: 0.3820385202892465
  time_since_restore: 1715.656985282898
  time_this_iter_s: 25.437862157821655
  time_total_s: 1715.656985282898
  timers:
    learn_throughput: 8689.92
    learn_time_ms: 18618.352
    sample_throughput: 23550.878
    sample_time_ms: 6869.892
    update_time_ms: 38.604
  timestamp: 1602755848
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     67 |          1715.66 | 10840064 |  260.536 |              309.525 |              138.768 |            780.526 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3192.8586002710995
    time_step_min: 2873
  date: 2020-10-15_09-57-53
  done: false
  episode_len_mean: 780.1807768924302
  episode_reward_max: 312.55555555555554
  episode_reward_mean: 260.92363087333206
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 264
  episodes_total: 14056
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.3043691962957382
        entropy_coeff: 0.0005000000000000001
        kl: 0.004952103287602465
        model: {}
        policy_loss: -0.008787163727295896
        total_loss: 5.887337724367778
        vf_explained_var: 0.9885470867156982
        vf_loss: 5.896276235580444
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.860000000000003
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478447597880773
    mean_env_wait_ms: 1.2197289393091708
    mean_inference_ms: 4.390208966543792
    mean_raw_obs_processing_ms: 0.3819319113509814
  time_since_restore: 1740.8756909370422
  time_this_iter_s: 25.218705654144287
  time_total_s: 1740.8756909370422
  timers:
    learn_throughput: 8705.649
    learn_time_ms: 18584.714
    sample_throughput: 23576.681
    sample_time_ms: 6862.374
    update_time_ms: 38.592
  timestamp: 1602755873
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     68 |          1740.88 | 11001856 |  260.924 |              312.556 |              138.768 |            780.181 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3191.4019884360455
    time_step_min: 2849
  date: 2020-10-15_09-58-19
  done: false
  episode_len_mean: 779.978060614584
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 261.1654005777485
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 165
  episodes_total: 14221
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.33161305139462155
        entropy_coeff: 0.0005000000000000001
        kl: 0.005548228432113926
        model: {}
        policy_loss: -0.008611835034874579
        total_loss: 4.937494079271953
        vf_explained_var: 0.9861431121826172
        vf_loss: 4.946271141370137
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.317241379310342
    gpu_util_percent0: 0.37758620689655176
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7896551724137932
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782714450743692
    mean_env_wait_ms: 1.2198405463873816
    mean_inference_ms: 4.388903848979828
    mean_raw_obs_processing_ms: 0.38186791415307864
  time_since_restore: 1766.2738993167877
  time_this_iter_s: 25.398208379745483
  time_total_s: 1766.2738993167877
  timers:
    learn_throughput: 8710.468
    learn_time_ms: 18574.432
    sample_throughput: 23629.883
    sample_time_ms: 6846.924
    update_time_ms: 36.542
  timestamp: 1602755899
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     69 |          1766.27 | 11163648 |  261.165 |              313.162 |              138.768 |            779.978 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3189.56353783089
    time_step_min: 2849
  date: 2020-10-15_09-58-45
  done: false
  episode_len_mean: 779.7287971175166
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 261.44417848104104
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 211
  episodes_total: 14432
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3377840941150983
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046046188023562236
        model: {}
        policy_loss: -0.009337110163566345
        total_loss: 6.7480814854304
        vf_explained_var: 0.9852480888366699
        vf_loss: 6.7575870752334595
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.86
    gpu_util_percent0: 0.3433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8566666666666682
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14780439902479964
    mean_env_wait_ms: 1.219973225720225
    mean_inference_ms: 4.387232285668559
    mean_raw_obs_processing_ms: 0.38178177345209313
  time_since_restore: 1791.9016456604004
  time_this_iter_s: 25.62774634361267
  time_total_s: 1791.9016456604004
  timers:
    learn_throughput: 8715.4
    learn_time_ms: 18563.922
    sample_throughput: 23636.711
    sample_time_ms: 6844.946
    update_time_ms: 36.505
  timestamp: 1602755925
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     70 |           1791.9 | 11325440 |  261.444 |              313.162 |              138.768 |            779.729 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3187.2540440925536
    time_step_min: 2849
  date: 2020-10-15_09-59-11
  done: false
  episode_len_mean: 779.4633764465623
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 261.81610179397785
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 258
  episodes_total: 14690
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2946927572290103
        entropy_coeff: 0.0005000000000000001
        kl: 0.004823330091312528
        model: {}
        policy_loss: -0.009921685205578493
        total_loss: 5.918912967046102
        vf_explained_var: 0.9878664016723633
        vf_loss: 5.928981900215149
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.056666666666672
    gpu_util_percent0: 0.32433333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.856666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477783124091239
    mean_env_wait_ms: 1.2201310312378097
    mean_inference_ms: 4.385289072477147
    mean_raw_obs_processing_ms: 0.3816849718317349
  time_since_restore: 1817.684998512268
  time_this_iter_s: 25.783352851867676
  time_total_s: 1817.684998512268
  timers:
    learn_throughput: 8723.525
    learn_time_ms: 18546.63
    sample_throughput: 23602.66
    sample_time_ms: 6854.821
    update_time_ms: 28.173
  timestamp: 1602755951
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     71 |          1817.68 | 11487232 |  261.816 |              313.162 |              138.768 |            779.463 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3185.827055488052
    time_step_min: 2849
  date: 2020-10-15_09-59-37
  done: false
  episode_len_mean: 779.2945532888979
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 262.03675412986667
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 163
  episodes_total: 14853
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.31879138201475143
        entropy_coeff: 0.0005000000000000001
        kl: 0.004923491855151951
        model: {}
        policy_loss: -0.00857447545422474
        total_loss: 4.748826503753662
        vf_explained_var: 0.9867516160011292
        vf_loss: 4.75756021340688
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.296551724137927
    gpu_util_percent0: 0.38517241379310346
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14776209204157312
    mean_env_wait_ms: 1.2202279454256242
    mean_inference_ms: 4.384102478776068
    mean_raw_obs_processing_ms: 0.38162576944536086
  time_since_restore: 1843.0404241085052
  time_this_iter_s: 25.355425596237183
  time_total_s: 1843.0404241085052
  timers:
    learn_throughput: 8741.152
    learn_time_ms: 18509.23
    sample_throughput: 23659.025
    sample_time_ms: 6838.49
    update_time_ms: 25.053
  timestamp: 1602755977
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     72 |          1843.04 | 11649024 |  262.037 |              313.162 |              138.768 |            779.295 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3183.870568673096
    time_step_min: 2849
  date: 2020-10-15_10-00-02
  done: false
  episode_len_mean: 779.0706514528327
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 262.34537024751967
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 221
  episodes_total: 15074
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3208874662717183
        entropy_coeff: 0.0005000000000000001
        kl: 0.005192111323898037
        model: {}
        policy_loss: -0.010388918977696449
        total_loss: 6.205375750859578
        vf_explained_var: 0.9863477349281311
        vf_loss: 6.215924978256226
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.06206896551725
    gpu_util_percent0: 0.3693103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477391920602832
    mean_env_wait_ms: 1.2203549399409903
    mean_inference_ms: 4.3824953944508795
    mean_raw_obs_processing_ms: 0.3815422429563495
  time_since_restore: 1868.355391740799
  time_this_iter_s: 25.3149676322937
  time_total_s: 1868.355391740799
  timers:
    learn_throughput: 8750.199
    learn_time_ms: 18490.093
    sample_throughput: 23658.035
    sample_time_ms: 6838.776
    update_time_ms: 25.048
  timestamp: 1602756002
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     73 |          1868.36 | 11810816 |  262.345 |              313.162 |              138.768 |            779.071 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3181.8615636244685
    time_step_min: 2849
  date: 2020-10-15_10-00-28
  done: false
  episode_len_mean: 778.799008091882
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 262.6656225528583
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 15324
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.28383104751507443
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046672336369132
        model: {}
        policy_loss: -0.010195812821621075
        total_loss: 6.37613062063853
        vf_explained_var: 0.9868685603141785
        vf_loss: 6.386468291282654
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.790000000000003
    gpu_util_percent0: 0.3186666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771640232288222
    mean_env_wait_ms: 1.2204899458776421
    mean_inference_ms: 4.380744016777237
    mean_raw_obs_processing_ms: 0.38145342176618113
  time_since_restore: 1893.7468667030334
  time_this_iter_s: 25.391474962234497
  time_total_s: 1893.7468667030334
  timers:
    learn_throughput: 8753.153
    learn_time_ms: 18483.853
    sample_throughput: 23661.349
    sample_time_ms: 6837.818
    update_time_ms: 23.847
  timestamp: 1602756028
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     74 |          1893.75 | 11972608 |  262.666 |              313.162 |              138.768 |            778.799 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3180.5587207043895
    time_step_min: 2849
  date: 2020-10-15_10-00-54
  done: false
  episode_len_mean: 778.62092347433
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 262.87419235950074
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 161
  episodes_total: 15485
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.30613749474287033
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050003100574637456
        model: {}
        policy_loss: -0.010951006136262246
        total_loss: 4.720480720202128
        vf_explained_var: 0.9867067337036133
        vf_loss: 4.731584827105205
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.080000000000002
    gpu_util_percent0: 0.28966666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14770144295252022
    mean_env_wait_ms: 1.2205737329399018
    mean_inference_ms: 4.379642077620266
    mean_raw_obs_processing_ms: 0.38139666098931513
  time_since_restore: 1919.6488111019135
  time_this_iter_s: 25.901944398880005
  time_total_s: 1919.6488111019135
  timers:
    learn_throughput: 8749.137
    learn_time_ms: 18492.339
    sample_throughput: 23580.659
    sample_time_ms: 6861.216
    update_time_ms: 25.15
  timestamp: 1602756054
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     75 |          1919.65 | 12134400 |  262.874 |              313.162 |              138.768 |            778.621 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3178.5969254321617
    time_step_min: 2849
  date: 2020-10-15_10-01-20
  done: false
  episode_len_mean: 778.3596334945279
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 263.1888611233228
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 15716
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.3035415733853976
        entropy_coeff: 0.0005000000000000001
        kl: 0.005273402590925495
        model: {}
        policy_loss: -0.01085835959383985
        total_loss: 6.099811514218648
        vf_explained_var: 0.9868482947349548
        vf_loss: 6.110821644465129
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.78620689655173
    gpu_util_percent0: 0.3082758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14768001323092744
    mean_env_wait_ms: 1.2206915271602194
    mean_inference_ms: 4.378077928243574
    mean_raw_obs_processing_ms: 0.3813141170157503
  time_since_restore: 1945.191931962967
  time_this_iter_s: 25.543120861053467
  time_total_s: 1945.191931962967
  timers:
    learn_throughput: 8749.353
    learn_time_ms: 18491.882
    sample_throughput: 23591.908
    sample_time_ms: 6857.945
    update_time_ms: 27.211
  timestamp: 1602756080
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     76 |          1945.19 | 12296192 |  263.189 |              313.162 |              138.768 |             778.36 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3176.516868756675
    time_step_min: 2849
  date: 2020-10-15_10-01-46
  done: false
  episode_len_mean: 778.1283529706693
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 263.49384608177536
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 240
  episodes_total: 15956
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.2723412364721298
        entropy_coeff: 0.0005000000000000001
        kl: 0.00461164415658762
        model: {}
        policy_loss: -0.008690265705808997
        total_loss: 6.037911971410115
        vf_explained_var: 0.9869084358215332
        vf_loss: 6.046738187472026
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.983333333333334
    gpu_util_percent0: 0.366
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14765878530033794
    mean_env_wait_ms: 1.220810696111765
    mean_inference_ms: 4.376509698195474
    mean_raw_obs_processing_ms: 0.3812335380947641
  time_since_restore: 1970.628675699234
  time_this_iter_s: 25.43674373626709
  time_total_s: 1970.628675699234
  timers:
    learn_throughput: 8749.785
    learn_time_ms: 18490.968
    sample_throughput: 23597.335
    sample_time_ms: 6856.367
    update_time_ms: 28.401
  timestamp: 1602756106
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     77 |          1970.63 | 12457984 |  263.494 |              313.162 |              138.768 |            778.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3175.1828358208954
    time_step_min: 2849
  date: 2020-10-15_10-02-12
  done: false
  episode_len_mean: 777.9526025187666
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 263.7082513202
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 163
  episodes_total: 16119
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.3022257635990779
        entropy_coeff: 0.0005000000000000001
        kl: 0.005352613128100832
        model: {}
        policy_loss: -0.012025649872763703
        total_loss: 4.878254731496175
        vf_explained_var: 0.9861853718757629
        vf_loss: 4.890431523323059
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74137931034483
    gpu_util_percent0: 0.3368965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476447734337104
    mean_env_wait_ms: 1.2208887997065432
    mean_inference_ms: 4.3754744818580535
    mean_raw_obs_processing_ms: 0.381179790268461
  time_since_restore: 1996.2014780044556
  time_this_iter_s: 25.572802305221558
  time_total_s: 1996.2014780044556
  timers:
    learn_throughput: 8730.621
    learn_time_ms: 18531.557
    sample_throughput: 23622.302
    sample_time_ms: 6849.121
    update_time_ms: 28.84
  timestamp: 1602756132
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     78 |           1996.2 | 12619776 |  263.708 |              313.162 |              138.768 |            777.953 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3173.099877526026
    time_step_min: 2849
  date: 2020-10-15_10-02-38
  done: false
  episode_len_mean: 777.6837925346692
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 264.0238613145938
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 16369
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2879169334967931
        entropy_coeff: 0.0005000000000000001
        kl: 0.005028028429175417
        model: {}
        policy_loss: -0.00984137509658467
        total_loss: 6.26339606444041
        vf_explained_var: 0.9869596362113953
        vf_loss: 6.273381392161052
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.880000000000003
    gpu_util_percent0: 0.3226666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762251865807713
    mean_env_wait_ms: 1.2210043808622137
    mean_inference_ms: 4.373892376032169
    mean_raw_obs_processing_ms: 0.38109413122023045
  time_since_restore: 2021.7245254516602
  time_this_iter_s: 25.52304744720459
  time_total_s: 2021.7245254516602
  timers:
    learn_throughput: 8729.635
    learn_time_ms: 18533.65
    sample_throughput: 23592.475
    sample_time_ms: 6857.78
    update_time_ms: 29.704
  timestamp: 1602756158
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     79 |          2021.72 | 12781568 |  264.024 |              313.162 |              138.768 |            777.684 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3171.303262839879
    time_step_min: 2849
  date: 2020-10-15_10-03-03
  done: false
  episode_len_mean: 777.4493338959552
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 264.2888874275335
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 220
  episodes_total: 16589
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.26022494087616604
        entropy_coeff: 0.0005000000000000001
        kl: 0.004336869033674399
        model: {}
        policy_loss: -0.008254327617275218
        total_loss: 5.896654645601909
        vf_explained_var: 0.986591100692749
        vf_loss: 5.905039032300313
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.058620689655175
    gpu_util_percent0: 0.3358620689655173
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760473807604746
    mean_env_wait_ms: 1.2211016561466843
    mean_inference_ms: 4.372543098527841
    mean_raw_obs_processing_ms: 0.38102533509917563
  time_since_restore: 2047.0648062229156
  time_this_iter_s: 25.340280771255493
  time_total_s: 2047.0648062229156
  timers:
    learn_throughput: 8740.623
    learn_time_ms: 18510.35
    sample_throughput: 23609.953
    sample_time_ms: 6852.703
    update_time_ms: 27.019
  timestamp: 1602756183
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     80 |          2047.06 | 12943360 |  264.289 |              313.162 |              138.768 |            777.449 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3170.0904032547564
    time_step_min: 2849
  date: 2020-10-15_10-03-29
  done: false
  episode_len_mean: 777.2681310810004
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 264.5057806622302
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 164
  episodes_total: 16753
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2832312931617101
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045790177925179405
        model: {}
        policy_loss: -0.009791253217069121
        total_loss: 4.2394417723019915
        vf_explained_var: 0.9878109097480774
        vf_loss: 4.249374568462372
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.763333333333332
    gpu_util_percent0: 0.3096666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475914883391859
    mean_env_wait_ms: 1.2211721421724946
    mean_inference_ms: 4.3715575793187345
    mean_raw_obs_processing_ms: 0.3809737747971725
  time_since_restore: 2072.7726242542267
  time_this_iter_s: 25.707818031311035
  time_total_s: 2072.7726242542267
  timers:
    learn_throughput: 8735.057
    learn_time_ms: 18522.145
    sample_throughput: 23681.659
    sample_time_ms: 6831.954
    update_time_ms: 35.683
  timestamp: 1602756209
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     81 |          2072.77 | 13105152 |  264.506 |              313.162 |              138.768 |            777.268 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3167.917015136345
    time_step_min: 2849
  date: 2020-10-15_10-03-55
  done: false
  episode_len_mean: 776.9709131507815
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 264.82390243960344
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 265
  episodes_total: 17018
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.26808729271094006
        entropy_coeff: 0.0005000000000000001
        kl: 0.004614657140336931
        model: {}
        policy_loss: -0.00847732342663221
        total_loss: 6.035964131355286
        vf_explained_var: 0.9881357550621033
        vf_loss: 6.044575452804565
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73793103448276
    gpu_util_percent0: 0.37034482758620696
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756996015122245
    mean_env_wait_ms: 1.221283873554102
    mean_inference_ms: 4.36999305520524
    mean_raw_obs_processing_ms: 0.3808901409624041
  time_since_restore: 2098.3405623435974
  time_this_iter_s: 25.567938089370728
  time_total_s: 2098.3405623435974
  timers:
    learn_throughput: 8723.749
    learn_time_ms: 18546.156
    sample_throughput: 23695.301
    sample_time_ms: 6828.02
    update_time_ms: 36.383
  timestamp: 1602756235
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     82 |          2098.34 | 13266944 |  264.824 |              313.162 |              138.768 |            776.971 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3166.2549612989583
    time_step_min: 2849
  date: 2020-10-15_10-04-21
  done: false
  episode_len_mean: 776.7420160260133
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 265.08169372273426
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 204
  episodes_total: 17222
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.24432009706894556
        entropy_coeff: 0.0005000000000000001
        kl: 0.004341511405073106
        model: {}
        policy_loss: -0.007233019404035683
        total_loss: 4.631472627321879
        vf_explained_var: 0.9885926246643066
        vf_loss: 4.638827800750732
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84333333333334
    gpu_util_percent0: 0.3393333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14755403298902636
    mean_env_wait_ms: 1.221366886601356
    mean_inference_ms: 4.368821128117286
    mean_raw_obs_processing_ms: 0.3808297016938496
  time_since_restore: 2123.914649248123
  time_this_iter_s: 25.574086904525757
  time_total_s: 2123.914649248123
  timers:
    learn_throughput: 8714.626
    learn_time_ms: 18565.57
    sample_throughput: 23679.78
    sample_time_ms: 6832.496
    update_time_ms: 36.806
  timestamp: 1602756261
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     83 |          2123.91 | 13428736 |  265.082 |              313.162 |              138.768 |            776.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3164.96455535704
    time_step_min: 2849
  date: 2020-10-15_10-04-47
  done: false
  episode_len_mean: 776.5568717653824
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 265.27914277914283
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 168
  episodes_total: 17390
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2776962071657181
        entropy_coeff: 0.0005000000000000001
        kl: 0.004789162892848253
        model: {}
        policy_loss: -0.01052733378795286
        total_loss: 4.960049589474996
        vf_explained_var: 0.9871343970298767
        vf_loss: 4.970715920130412
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.062068965517245
    gpu_util_percent0: 0.3341379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754117287102025
    mean_env_wait_ms: 1.2214324972797654
    mean_inference_ms: 4.367883808497643
    mean_raw_obs_processing_ms: 0.38077974716026924
  time_since_restore: 2149.253698348999
  time_this_iter_s: 25.339049100875854
  time_total_s: 2149.253698348999
  timers:
    learn_throughput: 8711.918
    learn_time_ms: 18571.342
    sample_throughput: 23719.784
    sample_time_ms: 6820.973
    update_time_ms: 36.527
  timestamp: 1602756287
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     84 |          2149.25 | 13590528 |  265.279 |              313.162 |              138.768 |            776.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3162.7364539007094
    time_step_min: 2849
  date: 2020-10-15_10-05-13
  done: false
  episode_len_mean: 776.2942708333334
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 265.6203320569829
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 274
  episodes_total: 17664
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.25675638020038605
        entropy_coeff: 0.0005000000000000001
        kl: 0.005409693229012191
        model: {}
        policy_loss: -0.008175827337254304
        total_loss: 6.087591091791789
        vf_explained_var: 0.9879869818687439
        vf_loss: 6.095895171165466
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89
    gpu_util_percent0: 0.3616666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752037926362876
    mean_env_wait_ms: 1.221539678628137
    mean_inference_ms: 4.366339427108225
    mean_raw_obs_processing_ms: 0.38069786230560937
  time_since_restore: 2174.9077999591827
  time_this_iter_s: 25.654101610183716
  time_total_s: 2174.9077999591827
  timers:
    learn_throughput: 8724.82
    learn_time_ms: 18543.879
    sample_throughput: 23712.23
    sample_time_ms: 6823.146
    update_time_ms: 36.335
  timestamp: 1602756313
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     85 |          2174.91 | 13752320 |   265.62 |              313.162 |              138.768 |            776.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3161.1331462250914
    time_step_min: 2849
  date: 2020-10-15_10-05-39
  done: false
  episode_len_mean: 776.1326873529741
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 265.8626915508847
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 17854
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.24634397650758424
        entropy_coeff: 0.0005000000000000001
        kl: 0.005001329622852306
        model: {}
        policy_loss: -0.009673356369603425
        total_loss: 3.7913096944491067
        vf_explained_var: 0.9898946285247803
        vf_loss: 3.8011061549186707
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.46206896551724
    gpu_util_percent0: 0.2972413793103449
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475062676596759
    mean_env_wait_ms: 1.2216083901368158
    mean_inference_ms: 4.365330949460814
    mean_raw_obs_processing_ms: 0.3806457346006009
  time_since_restore: 2200.3785288333893
  time_this_iter_s: 25.470728874206543
  time_total_s: 2200.3785288333893
  timers:
    learn_throughput: 8732.296
    learn_time_ms: 18528.001
    sample_throughput: 23677.907
    sample_time_ms: 6833.036
    update_time_ms: 34.121
  timestamp: 1602756339
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     86 |          2200.38 | 13914112 |  265.863 |              313.162 |              138.768 |            776.133 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3159.6469182459846
    time_step_min: 2849
  date: 2020-10-15_10-06-05
  done: false
  episode_len_mean: 775.9814219165927
  episode_reward_max: 314.82828282828297
  episode_reward_mean: 266.09162611025965
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 18032
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.26942487557729083
        entropy_coeff: 0.0005000000000000001
        kl: 0.004710895552610357
        model: {}
        policy_loss: -0.008555350175204998
        total_loss: 4.417007406552632
        vf_explained_var: 0.9881947636604309
        vf_loss: 4.425697326660156
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.156666666666673
    gpu_util_percent0: 0.4056666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749321475784305
    mean_env_wait_ms: 1.2216697973414192
    mean_inference_ms: 4.364384453179265
    mean_raw_obs_processing_ms: 0.3805952189059535
  time_since_restore: 2225.811149597168
  time_this_iter_s: 25.432620763778687
  time_total_s: 2225.811149597168
  timers:
    learn_throughput: 8737.75
    learn_time_ms: 18516.437
    sample_throughput: 23634.301
    sample_time_ms: 6845.644
    update_time_ms: 32.479
  timestamp: 1602756365
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     87 |          2225.81 | 14075904 |  266.092 |              314.828 |              138.768 |            775.981 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3157.477937150991
    time_step_min: 2849
  date: 2020-10-15_10-06-31
  done: false
  episode_len_mean: 775.7841027041792
  episode_reward_max: 314.82828282828297
  episode_reward_mean: 266.4329031919854
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 273
  episodes_total: 18305
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.24425077935059866
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047048449438686175
        model: {}
        policy_loss: -0.007794409650765981
        total_loss: 4.551439007123311
        vf_explained_var: 0.9909267425537109
        vf_loss: 4.55935553709666
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.020000000000007
    gpu_util_percent0: 0.3423333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474743681256076
    mean_env_wait_ms: 1.2217611198380793
    mean_inference_ms: 4.362957410583848
    mean_raw_obs_processing_ms: 0.3805194964697468
  time_since_restore: 2251.614339828491
  time_this_iter_s: 25.803190231323242
  time_total_s: 2251.614339828491
  timers:
    learn_throughput: 8738.453
    learn_time_ms: 18514.947
    sample_throughput: 23554.981
    sample_time_ms: 6868.696
    update_time_ms: 33.315
  timestamp: 1602756391
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     88 |          2251.61 | 14237696 |  266.433 |              314.828 |              138.768 |            775.784 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3156.0455900688457
    time_step_min: 2849
  date: 2020-10-15_10-06-57
  done: false
  episode_len_mean: 775.6608785026507
  episode_reward_max: 314.82828282828297
  episode_reward_mean: 266.6613407689357
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 18486
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.23832658181587854
        entropy_coeff: 0.0005000000000000001
        kl: 0.004391344225344558
        model: {}
        policy_loss: -0.010078011987692056
        total_loss: 4.442067782084147
        vf_explained_var: 0.9879171252250671
        vf_loss: 4.452265024185181
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.743333333333336
    gpu_util_percent0: 0.37133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746140976680616
    mean_env_wait_ms: 1.2218201455749036
    mean_inference_ms: 4.362057159850421
    mean_raw_obs_processing_ms: 0.3804721447131352
  time_since_restore: 2277.3185155391693
  time_this_iter_s: 25.7041757106781
  time_total_s: 2277.3185155391693
  timers:
    learn_throughput: 8740.818
    learn_time_ms: 18509.938
    sample_throughput: 23475.226
    sample_time_ms: 6892.032
    update_time_ms: 32.851
  timestamp: 1602756417
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     89 |          2277.32 | 14399488 |  266.661 |              314.828 |              138.768 |            775.661 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3154.493719132489
    time_step_min: 2844
  date: 2020-10-15_10-07-23
  done: false
  episode_len_mean: 775.5111694434028
  episode_reward_max: 314.82828282828297
  episode_reward_mean: 266.8942361959987
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 18667
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.26231640080610913
        entropy_coeff: 0.0005000000000000001
        kl: 0.004421331454068422
        model: {}
        policy_loss: -0.01087857196883609
        total_loss: 4.68465252717336
        vf_explained_var: 0.987766444683075
        vf_loss: 4.695662339528401
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.28275862068966
    gpu_util_percent0: 0.29586206896551726
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474486462807718
    mean_env_wait_ms: 1.2218753978522923
    mean_inference_ms: 4.361162352197118
    mean_raw_obs_processing_ms: 0.38042421032823487
  time_since_restore: 2302.6270654201508
  time_this_iter_s: 25.308549880981445
  time_total_s: 2302.6270654201508
  timers:
    learn_throughput: 8743.545
    learn_time_ms: 18504.166
    sample_throughput: 23465.412
    sample_time_ms: 6894.914
    update_time_ms: 33.092
  timestamp: 1602756443
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     90 |          2302.63 | 14561280 |  266.894 |              314.828 |              138.768 |            775.511 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3152.272914793463
    time_step_min: 2838
  date: 2020-10-15_10-07-48
  done: false
  episode_len_mean: 775.2899292726697
  episode_reward_max: 314.97979797979775
  episode_reward_mean: 267.24438409216197
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 279
  episodes_total: 18946
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.22865770384669304
        entropy_coeff: 0.0005000000000000001
        kl: 0.00466413233273973
        model: {}
        policy_loss: -0.010199747819569893
        total_loss: 4.387278318405151
        vf_explained_var: 0.99107426404953
        vf_loss: 4.397592385609944
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84
    gpu_util_percent0: 0.3233333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743078151807873
    mean_env_wait_ms: 1.2219583644706555
    mean_inference_ms: 4.359780502391076
    mean_raw_obs_processing_ms: 0.3803503730349641
  time_since_restore: 2327.872563123703
  time_this_iter_s: 25.245497703552246
  time_total_s: 2327.872563123703
  timers:
    learn_throughput: 8764.08
    learn_time_ms: 18460.809
    sample_throughput: 23446.1
    sample_time_ms: 6900.593
    update_time_ms: 24.322
  timestamp: 1602756468
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     91 |          2327.87 | 14723072 |  267.244 |               314.98 |              138.768 |             775.29 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3150.9178153991297
    time_step_min: 2838
  date: 2020-10-15_10-08-15
  done: false
  episode_len_mean: 775.1534679359766
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 267.45693360004486
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 172
  episodes_total: 19118
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.23448821902275085
        entropy_coeff: 0.0005000000000000001
        kl: 0.004760275594890118
        model: {}
        policy_loss: -0.011740240413928404
        total_loss: 3.7931633392969766
        vf_explained_var: 0.9892194271087646
        vf_loss: 3.8050206700960794
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.309999999999995
    gpu_util_percent0: 0.37899999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14741895299407984
    mean_env_wait_ms: 1.222006273028721
    mean_inference_ms: 4.3589716638278615
    mean_raw_obs_processing_ms: 0.38030736928133185
  time_since_restore: 2353.627506017685
  time_this_iter_s: 25.754942893981934
  time_total_s: 2353.627506017685
  timers:
    learn_throughput: 8754.082
    learn_time_ms: 18481.892
    sample_throughput: 23463.527
    sample_time_ms: 6895.468
    update_time_ms: 24.846
  timestamp: 1602756495
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     92 |          2353.63 | 14884864 |  267.457 |              318.768 |              138.768 |            775.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3149.4271708683473
    time_step_min: 2838
  date: 2020-10-15_10-08-41
  done: false
  episode_len_mean: 774.982502458974
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 267.6832967036415
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 19317
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.24960773438215256
        entropy_coeff: 0.0005000000000000001
        kl: 0.005254663371791442
        model: {}
        policy_loss: -0.009448278980319932
        total_loss: 5.701354543368022
        vf_explained_var: 0.9859640598297119
        vf_loss: 5.710927724838257
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793103448275865
    gpu_util_percent0: 0.3244827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740629043302067
    mean_env_wait_ms: 1.2220593866042948
    mean_inference_ms: 4.358041687085245
    mean_raw_obs_processing_ms: 0.38025710741880786
  time_since_restore: 2379.2233424186707
  time_this_iter_s: 25.595836400985718
  time_total_s: 2379.2233424186707
  timers:
    learn_throughput: 8751.61
    learn_time_ms: 18487.112
    sample_throughput: 23482.902
    sample_time_ms: 6889.779
    update_time_ms: 26.432
  timestamp: 1602756521
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     93 |          2379.22 | 15046656 |  267.683 |              318.768 |              138.768 |            774.983 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3147.3726463364715
    time_step_min: 2838
  date: 2020-10-15_10-09-07
  done: false
  episode_len_mean: 774.7720471837819
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 267.9979538014058
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 266
  episodes_total: 19583
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.21555884927511215
        entropy_coeff: 0.0005000000000000001
        kl: 0.004697610507719219
        model: {}
        policy_loss: -0.010029192916893711
        total_loss: 5.045426448186238
        vf_explained_var: 0.9896988868713379
        vf_loss: 5.05556333065033
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333337
    gpu_util_percent0: 0.3626666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473889045911478
    mean_env_wait_ms: 1.2221237187800345
    mean_inference_ms: 4.356789219457671
    mean_raw_obs_processing_ms: 0.3801880360509776
  time_since_restore: 2404.6709744930267
  time_this_iter_s: 25.44763207435608
  time_total_s: 2404.6709744930267
  timers:
    learn_throughput: 8752.882
    learn_time_ms: 18484.426
    sample_throughput: 23443.536
    sample_time_ms: 6901.348
    update_time_ms: 27.249
  timestamp: 1602756547
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     94 |          2404.67 | 15208448 |  267.998 |              318.768 |              138.768 |            774.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3146.0323169803664
    time_step_min: 2838
  date: 2020-10-15_10-09-33
  done: false
  episode_len_mean: 774.6390886075949
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 268.20130929548645
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 167
  episodes_total: 19750
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.2268433632949988
        entropy_coeff: 0.0005000000000000001
        kl: 0.004843841656111181
        model: {}
        policy_loss: -0.007885286041224996
        total_loss: 3.483263889948527
        vf_explained_var: 0.9896848201751709
        vf_loss: 3.4912625551223755
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490000000000002
    gpu_util_percent0: 0.3816666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737860800407657
    mean_env_wait_ms: 1.2221676850598364
    mean_inference_ms: 4.356050679097234
    mean_raw_obs_processing_ms: 0.380148959176313
  time_since_restore: 2430.244598865509
  time_this_iter_s: 25.5736243724823
  time_total_s: 2430.244598865509
  timers:
    learn_throughput: 8743.723
    learn_time_ms: 18503.788
    sample_throughput: 23538.021
    sample_time_ms: 6873.645
    update_time_ms: 26.049
  timestamp: 1602756573
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     95 |          2430.24 | 15370240 |  268.201 |              318.768 |              138.768 |            774.639 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3144.332212575902
    time_step_min: 2838
  date: 2020-10-15_10-09-59
  done: false
  episode_len_mean: 774.4661925272964
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 268.4488772327097
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 19966
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.2383594090739886
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050382125191390514
        model: {}
        policy_loss: -0.009273823350667953
        total_loss: 4.70072074731191
        vf_explained_var: 0.9889609217643738
        vf_loss: 4.710113684336345
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.96206896551724
    gpu_util_percent0: 0.35724137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736508022236805
    mean_env_wait_ms: 1.2222199349433798
    mean_inference_ms: 4.3550939718139015
    mean_raw_obs_processing_ms: 0.38009596552391745
  time_since_restore: 2455.7981884479523
  time_this_iter_s: 25.553589582443237
  time_total_s: 2455.7981884479523
  timers:
    learn_throughput: 8732.483
    learn_time_ms: 18527.606
    sample_throughput: 23597.679
    sample_time_ms: 6856.268
    update_time_ms: 26.942
  timestamp: 1602756599
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     96 |           2455.8 | 15532032 |  268.449 |              318.768 |              138.768 |            774.466 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3142.3177403369673
    time_step_min: 2838
  date: 2020-10-15_10-10-24
  done: false
  episode_len_mean: 774.2631188486077
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 268.7547276514089
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 253
  episodes_total: 20219
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.19948154812057814
        entropy_coeff: 0.0005000000000000001
        kl: 0.004395933899407585
        model: {}
        policy_loss: -0.008341047330759466
        total_loss: 4.934412956237793
        vf_explained_var: 0.9891325831413269
        vf_loss: 4.942853768666585
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.117241379310343
    gpu_util_percent0: 0.27999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734980119686802
    mean_env_wait_ms: 1.222274284255453
    mean_inference_ms: 4.3539778251058205
    mean_raw_obs_processing_ms: 0.38003499638505794
  time_since_restore: 2481.042339324951
  time_this_iter_s: 25.2441508769989
  time_total_s: 2481.042339324951
  timers:
    learn_throughput: 8736.599
    learn_time_ms: 18518.877
    sample_throughput: 23637.816
    sample_time_ms: 6844.626
    update_time_ms: 26.813
  timestamp: 1602756624
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     97 |          2481.04 | 15693824 |  268.755 |              318.768 |              138.768 |            774.263 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3140.9621510027528
    time_step_min: 2838
  date: 2020-10-15_10-10-50
  done: false
  episode_len_mean: 774.1383505862728
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 268.9533484281068
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 164
  episodes_total: 20383
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.22233347470561662
        entropy_coeff: 0.0005000000000000001
        kl: 0.004189171517888705
        model: {}
        policy_loss: -0.008752714784350246
        total_loss: 4.05763824780782
        vf_explained_var: 0.9878931641578674
        vf_loss: 4.066502054532369
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.216666666666665
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734022702395072
    mean_env_wait_ms: 1.2223081713117003
    mean_inference_ms: 4.353277375720085
    mean_raw_obs_processing_ms: 0.3799970026580805
  time_since_restore: 2506.417622089386
  time_this_iter_s: 25.375282764434814
  time_total_s: 2506.417622089386
  timers:
    learn_throughput: 8753.065
    learn_time_ms: 18484.04
    sample_throughput: 23698.798
    sample_time_ms: 6827.013
    update_time_ms: 27.075
  timestamp: 1602756650
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     98 |          2506.42 | 15855616 |  268.953 |              318.768 |              138.768 |            774.138 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3139.0598211682377
    time_step_min: 2838
  date: 2020-10-15_10-11-16
  done: false
  episode_len_mean: 773.9408255323277
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 269.2442178980473
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 20617
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.22517057011524835
        entropy_coeff: 0.0005000000000000001
        kl: 0.004362859607984622
        model: {}
        policy_loss: -0.010387252984704295
        total_loss: 4.173368493715922
        vf_explained_var: 0.9902940392494202
        vf_loss: 4.183868249257405
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.217241379310348
    gpu_util_percent0: 0.3406896551724138
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732658671704113
    mean_env_wait_ms: 1.2223571633773542
    mean_inference_ms: 4.352304331792924
    mean_raw_obs_processing_ms: 0.3799433704524469
  time_since_restore: 2531.754466533661
  time_this_iter_s: 25.336844444274902
  time_total_s: 2531.754466533661
  timers:
    learn_throughput: 8755.193
    learn_time_ms: 18479.546
    sample_throughput: 23810.713
    sample_time_ms: 6794.925
    update_time_ms: 26.58
  timestamp: 1602756676
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |     99 |          2531.75 | 16017408 |  269.244 |              318.768 |              138.768 |            773.941 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3137.207936965504
    time_step_min: 2837
  date: 2020-10-15_10-11-42
  done: false
  episode_len_mean: 773.7462235649547
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 269.51988304858395
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 236
  episodes_total: 20853
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.1935069573422273
        entropy_coeff: 0.0005000000000000001
        kl: 0.004311881765412788
        model: {}
        policy_loss: -0.008507055084919557
        total_loss: 3.6038972536722818
        vf_explained_var: 0.9915869832038879
        vf_loss: 3.612501045068105
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4
    gpu_util_percent0: 0.33433333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473126583324068
    mean_env_wait_ms: 1.222399706960773
    mean_inference_ms: 4.351305467471239
    mean_raw_obs_processing_ms: 0.379887848871453
  time_since_restore: 2557.1577649116516
  time_this_iter_s: 25.403298377990723
  time_total_s: 2557.1577649116516
  timers:
    learn_throughput: 8755.992
    learn_time_ms: 18477.861
    sample_throughput: 23779.285
    sample_time_ms: 6803.905
    update_time_ms: 28.501
  timestamp: 1602756702
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    100 |          2557.16 | 16179200 |   269.52 |              318.768 |              138.768 |            773.746 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3135.9166785833454
    time_step_min: 2837
  date: 2020-10-15_10-12-08
  done: false
  episode_len_mean: 773.6080026643829
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 269.7113412169079
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 165
  episodes_total: 21018
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.21861098458369574
        entropy_coeff: 0.0005000000000000001
        kl: 0.004833029815927148
        model: {}
        policy_loss: -0.01079400188367193
        total_loss: 3.533422271410624
        vf_explained_var: 0.9896146655082703
        vf_loss: 3.544325590133667
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.193333333333335
    gpu_util_percent0: 0.388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730348688141723
    mean_env_wait_ms: 1.2224292781743087
    mean_inference_ms: 4.350637326864681
    mean_raw_obs_processing_ms: 0.37985121658732646
  time_since_restore: 2582.810686826706
  time_this_iter_s: 25.65292191505432
  time_total_s: 2582.810686826706
  timers:
    learn_throughput: 8736.34
    learn_time_ms: 18519.425
    sample_throughput: 23789.473
    sample_time_ms: 6800.991
    update_time_ms: 29.349
  timestamp: 1602756728
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    101 |          2582.81 | 16340992 |  269.711 |              318.768 |              138.768 |            773.608 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3133.961427965902
    time_step_min: 2833
  date: 2020-10-15_10-12-34
  done: false
  episode_len_mean: 773.4100695750282
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 270.0156111699925
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 254
  episodes_total: 21272
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.21355180690685907
        entropy_coeff: 0.0005000000000000001
        kl: 0.005046109048028787
        model: {}
        policy_loss: -0.010105241177370772
        total_loss: 5.040608127911885
        vf_explained_var: 0.9889450669288635
        vf_loss: 5.0508198738098145
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.820689655172416
    gpu_util_percent0: 0.3368965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728921318094065
    mean_env_wait_ms: 1.2224750220804428
    mean_inference_ms: 4.349650684224254
    mean_raw_obs_processing_ms: 0.37979451114885304
  time_since_restore: 2608.3679428100586
  time_this_iter_s: 25.55725598335266
  time_total_s: 2608.3679428100586
  timers:
    learn_throughput: 8745.742
    learn_time_ms: 18499.516
    sample_throughput: 23785.963
    sample_time_ms: 6801.995
    update_time_ms: 28.529
  timestamp: 1602756754
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    102 |          2608.37 | 16502784 |  270.016 |              319.374 |              138.768 |             773.41 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3132.2652461768
    time_step_min: 2833
  date: 2020-10-15_10-13-00
  done: false
  episode_len_mean: 773.248801600968
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 270.27392226354385
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 215
  episodes_total: 21487
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.18353131289283434
        entropy_coeff: 0.0005000000000000001
        kl: 0.004030068579595536
        model: {}
        policy_loss: -0.006411568048254897
        total_loss: 3.7287562092145285
        vf_explained_var: 0.9905366897583008
        vf_loss: 3.735259552796682
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.476666666666663
    gpu_util_percent0: 0.34566666666666657
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472775296562607
    mean_env_wait_ms: 1.2225074721568745
    mean_inference_ms: 4.348775152693172
    mean_raw_obs_processing_ms: 0.37974779319071167
  time_since_restore: 2633.876520395279
  time_this_iter_s: 25.508577585220337
  time_total_s: 2633.876520395279
  timers:
    learn_throughput: 8749.914
    learn_time_ms: 18490.697
    sample_throughput: 23784.871
    sample_time_ms: 6802.307
    update_time_ms: 27.445
  timestamp: 1602756780
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    103 |          2633.88 | 16664576 |  270.274 |              319.374 |              138.768 |            773.249 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3130.8655411655873
    time_step_min: 2823
  date: 2020-10-15_10-13-26
  done: false
  episode_len_mean: 773.1197654554688
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 270.47427364741185
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 172
  episodes_total: 21659
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.21097562834620476
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045883768470957875
        model: {}
        policy_loss: -0.010255051990194866
        total_loss: 3.559800664583842
        vf_explained_var: 0.9896824359893799
        vf_loss: 3.570161203543345
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.389655172413796
    gpu_util_percent0: 0.4110344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726860538053171
    mean_env_wait_ms: 1.2225316547958833
    mean_inference_ms: 4.348121384512181
    mean_raw_obs_processing_ms: 0.37971099805064323
  time_since_restore: 2659.5069949626923
  time_this_iter_s: 25.63047456741333
  time_total_s: 2659.5069949626923
  timers:
    learn_throughput: 8736.214
    learn_time_ms: 18519.692
    sample_throughput: 23822.552
    sample_time_ms: 6791.548
    update_time_ms: 27.328
  timestamp: 1602756806
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    104 |          2659.51 | 16826368 |  270.474 |              319.374 |              138.768 |             773.12 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3128.907271148485
    time_step_min: 2812
  date: 2020-10-15_10-13-52
  done: false
  episode_len_mean: 772.9262773722628
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 270.77272957678974
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 261
  episodes_total: 21920
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.1978634881476561
        entropy_coeff: 0.0005000000000000001
        kl: 0.004231312137562782
        model: {}
        policy_loss: -0.006590141090176378
        total_loss: 5.831996877988179
        vf_explained_var: 0.9878156185150146
        vf_loss: 5.838685909907023
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.373333333333335
    gpu_util_percent0: 0.3683333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472547040926938
    mean_env_wait_ms: 1.2225711161199222
    mean_inference_ms: 4.347118378646979
    mean_raw_obs_processing_ms: 0.37965441806409816
  time_since_restore: 2684.9459085464478
  time_this_iter_s: 25.438913583755493
  time_total_s: 2684.9459085464478
  timers:
    learn_throughput: 8748.256
    learn_time_ms: 18494.201
    sample_throughput: 23788.753
    sample_time_ms: 6801.197
    update_time_ms: 28.84
  timestamp: 1602756832
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    105 |          2684.95 | 16988160 |  270.773 |              319.374 |              138.768 |            772.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3127.2984601449275
    time_step_min: 2810
  date: 2020-10-15_10-14-18
  done: false
  episode_len_mean: 772.7865183778651
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 271.01216834012166
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 22119
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.17703540747364363
        entropy_coeff: 0.0005000000000000001
        kl: 0.004091060778591782
        model: {}
        policy_loss: -0.010252801740231613
        total_loss: 4.546480933825175
        vf_explained_var: 0.9877767562866211
        vf_loss: 4.5568223396937055
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.693103448275863
    gpu_util_percent0: 0.3410344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724405218366465
    mean_env_wait_ms: 1.2225955455451216
    mean_inference_ms: 4.3463715362847655
    mean_raw_obs_processing_ms: 0.3796135364071832
  time_since_restore: 2710.2519648075104
  time_this_iter_s: 25.306056261062622
  time_total_s: 2710.2519648075104
  timers:
    learn_throughput: 8761.615
    learn_time_ms: 18466.003
    sample_throughput: 23786.406
    sample_time_ms: 6801.868
    update_time_ms: 30.068
  timestamp: 1602756858
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    106 |          2710.25 | 17149952 |  271.012 |              319.374 |              138.768 |            772.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3125.9276859504134
    time_step_min: 2810
  date: 2020-10-15_10-14-44
  done: false
  episode_len_mean: 772.6478052279963
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 271.22524804155074
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 184
  episodes_total: 22303
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.20374412337938944
        entropy_coeff: 0.0005000000000000001
        kl: 0.004298495904852946
        model: {}
        policy_loss: -0.00846323888496651
        total_loss: 5.233591516812642
        vf_explained_var: 0.9859102368354797
        vf_loss: 5.242156624794006
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.423333333333336
    gpu_util_percent0: 0.33633333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147233908315672
    mean_env_wait_ms: 1.222614541079154
    mean_inference_ms: 4.34567826824334
    mean_raw_obs_processing_ms: 0.37957449904633545
  time_since_restore: 2735.9696106910706
  time_this_iter_s: 25.71764588356018
  time_total_s: 2735.9696106910706
  timers:
    learn_throughput: 8742.834
    learn_time_ms: 18505.67
    sample_throughput: 23768.765
    sample_time_ms: 6806.917
    update_time_ms: 32.111
  timestamp: 1602756884
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    107 |          2735.97 | 17311744 |  271.225 |              319.374 |              138.768 |            772.648 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3123.8324605006214
    time_step_min: 2804
  date: 2020-10-15_10-15-10
  done: false
  episode_len_mean: 772.4519073146959
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 271.53983143651294
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 268
  episodes_total: 22571
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.18263191108902296
        entropy_coeff: 0.0005000000000000001
        kl: 0.004251166867713134
        model: {}
        policy_loss: -0.006676604227929299
        total_loss: 5.817436297734578
        vf_explained_var: 0.9875534176826477
        vf_loss: 5.824204405148824
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05333333333334
    gpu_util_percent0: 0.3693333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722132102140667
    mean_env_wait_ms: 1.2226476045827166
    mean_inference_ms: 4.3447116092714975
    mean_raw_obs_processing_ms: 0.37951967716293367
  time_since_restore: 2761.7587950229645
  time_this_iter_s: 25.78918433189392
  time_total_s: 2761.7587950229645
  timers:
    learn_throughput: 8720.598
    learn_time_ms: 18552.857
    sample_throughput: 23792.667
    sample_time_ms: 6800.078
    update_time_ms: 32.379
  timestamp: 1602756910
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    108 |          2761.76 | 17473536 |   271.54 |               319.98 |              138.768 |            772.452 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3122.5004622903184
    time_step_min: 2804
  date: 2020-10-15_10-15-36
  done: false
  episode_len_mean: 772.3194883966245
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 271.7522162553808
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 22752
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.17062795783082643
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039137817414787906
        model: {}
        policy_loss: -0.010251026911040148
        total_loss: 2.7435816526412964
        vf_explained_var: 0.9920342564582825
        vf_loss: 2.7539179921150208
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.69666666666667
    gpu_util_percent0: 0.3473333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721209694417786
    mean_env_wait_ms: 1.2226662174985323
    mean_inference_ms: 4.344081357110574
    mean_raw_obs_processing_ms: 0.3794848091539561
  time_since_restore: 2787.4489347934723
  time_this_iter_s: 25.690139770507812
  time_total_s: 2787.4489347934723
  timers:
    learn_throughput: 8704.92
    learn_time_ms: 18586.27
    sample_throughput: 23801.045
    sample_time_ms: 6797.685
    update_time_ms: 34.632
  timestamp: 1602756936
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    109 |          2787.45 | 17635328 |  271.752 |               319.98 |              138.768 |            772.319 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3120.9862529457973
    time_step_min: 2804
  date: 2020-10-15_10-16-02
  done: false
  episode_len_mean: 772.1804992811398
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 271.97904149322255
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 22953
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.19461134945352873
        entropy_coeff: 0.0005000000000000001
        kl: 0.004375814266192417
        model: {}
        policy_loss: -0.008800105351838283
        total_loss: 4.470666786034902
        vf_explained_var: 0.9886862635612488
        vf_loss: 4.4795641501744585
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.213793103448275
    gpu_util_percent0: 0.3182758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14720207385045941
    mean_env_wait_ms: 1.222685360805622
    mean_inference_ms: 4.3433898671131645
    mean_raw_obs_processing_ms: 0.37944563892292754
  time_since_restore: 2812.898244857788
  time_this_iter_s: 25.449310064315796
  time_total_s: 2812.898244857788
  timers:
    learn_throughput: 8697.514
    learn_time_ms: 18602.097
    sample_throughput: 23841.987
    sample_time_ms: 6786.012
    update_time_ms: 33.731
  timestamp: 1602756962
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    110 |           2812.9 | 17797120 |  271.979 |               319.98 |              138.768 |             772.18 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3118.906972730411
    time_step_min: 2804
  date: 2020-10-15_10-16-28
  done: false
  episode_len_mean: 772.0018953262977
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 272.2870248902986
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 262
  episodes_total: 23215
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.1695228231449922
        entropy_coeff: 0.0005000000000000001
        kl: 0.004864651942625642
        model: {}
        policy_loss: -0.008603686078762015
        total_loss: 3.592876652876536
        vf_explained_var: 0.9920200705528259
        vf_loss: 3.601564943790436
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32666666666667
    gpu_util_percent0: 0.30566666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718964357300757
    mean_env_wait_ms: 1.222704820691166
    mean_inference_ms: 4.342457532607666
    mean_raw_obs_processing_ms: 0.3793921978097015
  time_since_restore: 2838.345952987671
  time_this_iter_s: 25.447708129882812
  time_total_s: 2838.345952987671
  timers:
    learn_throughput: 8708.623
    learn_time_ms: 18578.367
    sample_throughput: 23850.797
    sample_time_ms: 6783.505
    update_time_ms: 35.116
  timestamp: 1602756988
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    111 |          2838.35 | 17958912 |  272.287 |               319.98 |              138.768 |            772.002 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3117.6138953139725
    time_step_min: 2804
  date: 2020-10-15_10-16-54
  done: false
  episode_len_mean: 771.8905708787685
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 272.47965867786263
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 170
  episodes_total: 23385
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.1722496673464775
        entropy_coeff: 0.0005000000000000001
        kl: 0.004023686614042769
        model: {}
        policy_loss: -0.010478878810924167
        total_loss: 2.8431402444839478
        vf_explained_var: 0.9914758205413818
        vf_loss: 2.853705187638601
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.83448275862069
    gpu_util_percent0: 0.3396551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471815360618639
    mean_env_wait_ms: 1.2227190697899544
    mean_inference_ms: 4.341897593257222
    mean_raw_obs_processing_ms: 0.37936132556236524
  time_since_restore: 2863.7904195785522
  time_this_iter_s: 25.444466590881348
  time_total_s: 2863.7904195785522
  timers:
    learn_throughput: 8716.119
    learn_time_ms: 18562.391
    sample_throughput: 23842.447
    sample_time_ms: 6785.881
    update_time_ms: 36.924
  timestamp: 1602757014
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    112 |          2863.79 | 18120704 |   272.48 |               319.98 |              138.768 |            771.891 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3116.1105169340462
    time_step_min: 2804
  date: 2020-10-15_10-17-20
  done: false
  episode_len_mean: 771.7591627473412
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 272.7119369620958
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 23601
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.1865410308043162
        entropy_coeff: 0.0005000000000000001
        kl: 0.004090183802569906
        model: {}
        policy_loss: -0.00852372680674307
        total_loss: 4.244714816411336
        vf_explained_var: 0.9897566437721252
        vf_loss: 4.253331899642944
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.743333333333336
    gpu_util_percent0: 0.319
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717093081891966
    mean_env_wait_ms: 1.222733883394757
    mean_inference_ms: 4.341186741657921
    mean_raw_obs_processing_ms: 0.3793201090159577
  time_since_restore: 2889.3455896377563
  time_this_iter_s: 25.5551700592041
  time_total_s: 2889.3455896377563
  timers:
    learn_throughput: 8721.615
    learn_time_ms: 18550.694
    sample_throughput: 23788.013
    sample_time_ms: 6801.409
    update_time_ms: 37.235
  timestamp: 1602757040
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    113 |          2889.35 | 18282496 |  272.712 |               319.98 |              138.768 |            771.759 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3114.2817788602865
    time_step_min: 2804
  date: 2020-10-15_10-17-47
  done: false
  episode_len_mean: 771.5973503270166
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 272.9948046624216
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 251
  episodes_total: 23852
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.15399332592884699
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035449591426489255
        model: {}
        policy_loss: -0.008280562309664674
        total_loss: 4.150957663853963
        vf_explained_var: 0.9905694127082825
        vf_loss: 4.159315228462219
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12666666666667
    gpu_util_percent0: 0.2800000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715976838305378
    mean_env_wait_ms: 1.222745997106846
    mean_inference_ms: 4.340349621439479
    mean_raw_obs_processing_ms: 0.3792734346336906
  time_since_restore: 2915.2566225528717
  time_this_iter_s: 25.911032915115356
  time_total_s: 2915.2566225528717
  timers:
    learn_throughput: 8715.143
    learn_time_ms: 18564.469
    sample_throughput: 23747.211
    sample_time_ms: 6813.095
    update_time_ms: 38.287
  timestamp: 1602757067
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    114 |          2915.26 | 18444288 |  272.995 |               319.98 |              138.768 |            771.597 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3113.0023771790807
    time_step_min: 2804
  date: 2020-10-15_10-18-13
  done: false
  episode_len_mean: 771.4951076320939
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 273.1791290092076
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 165
  episodes_total: 24017
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.16551268845796585
        entropy_coeff: 0.0005000000000000001
        kl: 0.004297597178568442
        model: {}
        policy_loss: -0.011308001344635462
        total_loss: 2.8338273962338767
        vf_explained_var: 0.9913229942321777
        vf_loss: 2.8452181617418923
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.96206896551724
    gpu_util_percent0: 0.2837931034482758
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715234209581637
    mean_env_wait_ms: 1.2227547180796376
    mean_inference_ms: 4.339822556549552
    mean_raw_obs_processing_ms: 0.3792441043031767
  time_since_restore: 2940.7420477867126
  time_this_iter_s: 25.485425233840942
  time_total_s: 2940.7420477867126
  timers:
    learn_throughput: 8710.049
    learn_time_ms: 18575.327
    sample_throughput: 23768.144
    sample_time_ms: 6807.094
    update_time_ms: 37.317
  timestamp: 1602757093
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    115 |          2940.74 | 18606080 |  273.179 |               319.98 |              138.768 |            771.495 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3111.1840138722596
    time_step_min: 2804
  date: 2020-10-15_10-18-39
  done: false
  episode_len_mean: 771.3350783182193
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 273.45406663502285
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 243
  episodes_total: 24260
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17203684026996294
        entropy_coeff: 0.0005000000000000001
        kl: 0.004602224294406672
        model: {}
        policy_loss: -0.00808883226030351
        total_loss: 3.334753175576528
        vf_explained_var: 0.9921055436134338
        vf_loss: 3.342927932739258
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11666666666667
    gpu_util_percent0: 0.37799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471409025125738
    mean_env_wait_ms: 1.222763968053341
    mean_inference_ms: 4.33905963533936
    mean_raw_obs_processing_ms: 0.3791988385665933
  time_since_restore: 2966.5697691440582
  time_this_iter_s: 25.82772135734558
  time_total_s: 2966.5697691440582
  timers:
    learn_throughput: 8689.349
    learn_time_ms: 18619.577
    sample_throughput: 23737.288
    sample_time_ms: 6815.943
    update_time_ms: 35.652
  timestamp: 1602757119
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    116 |          2966.57 | 18767872 |  273.454 |               319.98 |              138.768 |            771.335 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3109.5765769451036
    time_step_min: 2804
  date: 2020-10-15_10-19-05
  done: false
  episode_len_mean: 771.1866857259547
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 273.70095069543703
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 225
  episodes_total: 24485
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.1411927379667759
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031797124538570642
        model: {}
        policy_loss: -0.00962009271218752
        total_loss: 2.685124695301056
        vf_explained_var: 0.9932982921600342
        vf_loss: 2.6948153177897134
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73666666666667
    gpu_util_percent0: 0.3633333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713142130075305
    mean_env_wait_ms: 1.222771583096842
    mean_inference_ms: 4.338344078549653
    mean_raw_obs_processing_ms: 0.379160471161334
  time_since_restore: 2992.135176897049
  time_this_iter_s: 25.565407752990723
  time_total_s: 2992.135176897049
  timers:
    learn_throughput: 8693.092
    learn_time_ms: 18611.559
    sample_throughput: 23761.181
    sample_time_ms: 6809.089
    update_time_ms: 33.858
  timestamp: 1602757145
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    117 |          2992.14 | 18929664 |  273.701 |               319.98 |              138.768 |            771.187 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3108.3202925045703
    time_step_min: 2804
  date: 2020-10-15_10-19-31
  done: false
  episode_len_mean: 771.0825829480003
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 273.89194819944385
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 169
  episodes_total: 24654
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.16122792412837347
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038139744816968837
        model: {}
        policy_loss: -0.009541969523221875
        total_loss: 3.4975943168004355
        vf_explained_var: 0.9894265532493591
        vf_loss: 3.5072169502576194
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.706666666666667
    gpu_util_percent0: 0.43200000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712447353950167
    mean_env_wait_ms: 1.2227747030187808
    mean_inference_ms: 4.337820985193984
    mean_raw_obs_processing_ms: 0.3791309118761407
  time_since_restore: 3017.7604863643646
  time_this_iter_s: 25.625309467315674
  time_total_s: 3017.7604863643646
  timers:
    learn_throughput: 8696.331
    learn_time_ms: 18604.628
    sample_throughput: 23764.467
    sample_time_ms: 6808.148
    update_time_ms: 32.287
  timestamp: 1602757171
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    118 |          3017.76 | 19091456 |  273.892 |               319.98 |              138.768 |            771.083 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3106.541953121859
    time_step_min: 2801
  date: 2020-10-15_10-19-57
  done: false
  episode_len_mean: 770.9273442517663
  episode_reward_max: 321.0404040404039
  episode_reward_mean: 274.1637254043323
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 258
  episodes_total: 24912
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.16436305890480676
        entropy_coeff: 0.0005000000000000001
        kl: 0.004030725064997871
        model: {}
        policy_loss: -0.008017327233877344
        total_loss: 4.7343523899714155
        vf_explained_var: 0.9895167350769043
        vf_loss: 4.742451747258504
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.6
    gpu_util_percent0: 0.35551724137931034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711304580973975
    mean_env_wait_ms: 1.2227817507905399
    mean_inference_ms: 4.337053695442514
    mean_raw_obs_processing_ms: 0.379085883933737
  time_since_restore: 3043.303305864334
  time_this_iter_s: 25.542819499969482
  time_total_s: 3043.303305864334
  timers:
    learn_throughput: 8703.548
    learn_time_ms: 18589.2
    sample_throughput: 23763.776
    sample_time_ms: 6808.346
    update_time_ms: 32.233
  timestamp: 1602757197
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    119 |           3043.3 | 19253248 |  274.164 |               321.04 |              138.768 |            770.927 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3105.0814593301434
    time_step_min: 2801
  date: 2020-10-15_10-20-23
  done: false
  episode_len_mean: 770.8063219077193
  episode_reward_max: 321.0404040404039
  episode_reward_mean: 274.3921165554988
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 207
  episodes_total: 25119
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.13733991732199988
        entropy_coeff: 0.0005000000000000001
        kl: 0.00398658006452024
        model: {}
        policy_loss: -0.008998860430438071
        total_loss: 3.066338857014974
        vf_explained_var: 0.9917945265769958
        vf_loss: 3.0754063924153647
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.430000000000003
    gpu_util_percent0: 0.3266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710410242666047
    mean_env_wait_ms: 1.222782533241702
    mean_inference_ms: 4.336416705850478
    mean_raw_obs_processing_ms: 0.37905153340442743
  time_since_restore: 3068.7049956321716
  time_this_iter_s: 25.401689767837524
  time_total_s: 3068.7049956321716
  timers:
    learn_throughput: 8708.992
    learn_time_ms: 18577.58
    sample_throughput: 23740.801
    sample_time_ms: 6814.934
    update_time_ms: 31.461
  timestamp: 1602757223
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    120 |           3068.7 | 19415040 |  274.392 |               321.04 |              138.768 |            770.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3103.7490300102936
    time_step_min: 2801
  date: 2020-10-15_10-20-49
  done: false
  episode_len_mean: 770.6989761631814
  episode_reward_max: 321.0404040404039
  episode_reward_mean: 274.58665638078213
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 25297
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.15856833880146345
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043513024575077
        model: {}
        policy_loss: -0.010370379663072526
        total_loss: 3.0508464574813843
        vf_explained_var: 0.9910359978675842
        vf_loss: 3.0612961451212564
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.089999999999996
    gpu_util_percent0: 0.351
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709699848310726
    mean_env_wait_ms: 1.2227813366437605
    mean_inference_ms: 4.335892628169443
    mean_raw_obs_processing_ms: 0.3790222740095184
  time_since_restore: 3094.3413486480713
  time_this_iter_s: 25.636353015899658
  time_total_s: 3094.3413486480713
  timers:
    learn_throughput: 8699.755
    learn_time_ms: 18597.305
    sample_throughput: 23729.836
    sample_time_ms: 6818.084
    update_time_ms: 29.836
  timestamp: 1602757249
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    121 |          3094.34 | 19576832 |  274.587 |               321.04 |              138.768 |            770.699 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3101.7473750195895
    time_step_min: 2792
  date: 2020-10-15_10-21-15
  done: false
  episode_len_mean: 770.5444979071314
  episode_reward_max: 321.7979797979798
  episode_reward_mean: 274.8853492085507
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 266
  episodes_total: 25563
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.15092444668213525
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040728193513738615
        model: {}
        policy_loss: -0.009207504287284488
        total_loss: 3.7745024959246316
        vf_explained_var: 0.9915598034858704
        vf_loss: 3.7837854425112405
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.417241379310347
    gpu_util_percent0: 0.3058620689655172
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708558022210932
    mean_env_wait_ms: 1.22278415605768
    mean_inference_ms: 4.335122454988719
    mean_raw_obs_processing_ms: 0.37897911320002
  time_since_restore: 3119.8114609718323
  time_this_iter_s: 25.470112323760986
  time_total_s: 3119.8114609718323
  timers:
    learn_throughput: 8699.94
    learn_time_ms: 18596.909
    sample_throughput: 23715.101
    sample_time_ms: 6822.32
    update_time_ms: 27.732
  timestamp: 1602757275
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    122 |          3119.81 | 19738624 |  274.885 |              321.798 |              138.768 |            770.544 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3100.3683064359325
    time_step_min: 2792
  date: 2020-10-15_10-21-42
  done: false
  episode_len_mean: 770.4423778830472
  episode_reward_max: 321.7979797979798
  episode_reward_mean: 275.09578466971493
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 191
  episodes_total: 25754
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.13362999260425568
        entropy_coeff: 0.0005000000000000001
        kl: 0.003501818204919497
        model: {}
        policy_loss: -0.008884420075143376
        total_loss: 2.953603208065033
        vf_explained_var: 0.9916164875030518
        vf_loss: 2.962554454803467
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46333333333333
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707821745863678
    mean_env_wait_ms: 1.2227806982801384
    mean_inference_ms: 4.3345813677716825
    mean_raw_obs_processing_ms: 0.37894882567706295
  time_since_restore: 3145.5634911060333
  time_this_iter_s: 25.75203013420105
  time_total_s: 3145.5634911060333
  timers:
    learn_throughput: 8688.89
    learn_time_ms: 18620.561
    sample_throughput: 23737.239
    sample_time_ms: 6815.957
    update_time_ms: 28.578
  timestamp: 1602757302
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    123 |          3145.56 | 19900416 |  275.096 |              321.798 |              138.768 |            770.442 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3098.9696583671107
    time_step_min: 2792
  date: 2020-10-15_10-22-08
  done: false
  episode_len_mean: 770.3282839962998
  episode_reward_max: 321.7979797979798
  episode_reward_mean: 275.30651566544253
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 25944
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.1557726909716924
        entropy_coeff: 0.0005000000000000001
        kl: 0.004061012587044388
        model: {}
        policy_loss: -0.008608610582693169
        total_loss: 2.6421515742937722
        vf_explained_var: 0.9926905632019043
        vf_loss: 2.6508379379908242
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.30666666666667
    gpu_util_percent0: 0.35866666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707057874014154
    mean_env_wait_ms: 1.2227758530099588
    mean_inference_ms: 4.33404679373747
    mean_raw_obs_processing_ms: 0.3789185393449611
  time_since_restore: 3171.2850358486176
  time_this_iter_s: 25.72154474258423
  time_total_s: 3171.2850358486176
  timers:
    learn_throughput: 8699.772
    learn_time_ms: 18597.269
    sample_throughput: 23721.181
    sample_time_ms: 6820.571
    update_time_ms: 26.894
  timestamp: 1602757328
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    124 |          3171.29 | 20062208 |  275.307 |              321.798 |              138.768 |            770.328 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3097.0838746656477
    time_step_min: 2792
  date: 2020-10-15_10-22-34
  done: false
  episode_len_mean: 770.1792514021901
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 275.59222350561197
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 265
  episodes_total: 26209
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.13781721393267313
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036383712431415915
        model: {}
        policy_loss: -0.008307600607319424
        total_loss: 3.5946669379870095
        vf_explained_var: 0.9922294616699219
        vf_loss: 3.6030433972676597
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62333333333333
    gpu_util_percent0: 0.4333333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706023634095483
    mean_env_wait_ms: 1.222768762231495
    mean_inference_ms: 4.333303503131556
    mean_raw_obs_processing_ms: 0.37887539102772905
  time_since_restore: 3196.7872455120087
  time_this_iter_s: 25.502209663391113
  time_total_s: 3196.7872455120087
  timers:
    learn_throughput: 8706.219
    learn_time_ms: 18583.497
    sample_throughput: 23671.117
    sample_time_ms: 6834.996
    update_time_ms: 27.057
  timestamp: 1602757354
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    125 |          3196.79 | 20224000 |  275.592 |              323.616 |              138.768 |            770.179 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3095.8419614391983
    time_step_min: 2792
  date: 2020-10-15_10-23-00
  done: false
  episode_len_mean: 770.0779929510744
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 275.7806342501836
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 26387
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.1308093493183454
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037285079791521034
        model: {}
        policy_loss: -0.007945271533875106
        total_loss: 2.71152796347936
        vf_explained_var: 0.9917721748352051
        vf_loss: 2.719538609186808
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.234482758620697
    gpu_util_percent0: 0.2968965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705319359357352
    mean_env_wait_ms: 1.222763102435168
    mean_inference_ms: 4.3328288278271865
    mean_raw_obs_processing_ms: 0.3788487968511624
  time_since_restore: 3222.0676822662354
  time_this_iter_s: 25.280436754226685
  time_total_s: 3222.0676822662354
  timers:
    learn_throughput: 8726.954
    learn_time_ms: 18539.344
    sample_throughput: 23721.312
    sample_time_ms: 6820.533
    update_time_ms: 27.972
  timestamp: 1602757380
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    126 |          3222.07 | 20385792 |  275.781 |              323.616 |              138.768 |            770.078 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3094.3565404021388
    time_step_min: 2792
  date: 2020-10-15_10-23-26
  done: false
  episode_len_mean: 769.9681919013423
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 275.9993194341428
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 210
  episodes_total: 26597
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.15123373518387476
        entropy_coeff: 0.0005000000000000001
        kl: 0.00409364519873634
        model: {}
        policy_loss: -0.00796691418994063
        total_loss: 3.045759856700897
        vf_explained_var: 0.9921451210975647
        vf_loss: 3.0538024504979453
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.733333333333334
    gpu_util_percent0: 0.333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704505491474498
    mean_env_wait_ms: 1.2227548984751653
    mean_inference_ms: 4.332280148219297
    mean_raw_obs_processing_ms: 0.3788159970886826
  time_since_restore: 3247.6514542102814
  time_this_iter_s: 25.58377194404602
  time_total_s: 3247.6514542102814
  timers:
    learn_throughput: 8735.6
    learn_time_ms: 18520.995
    sample_throughput: 23657.777
    sample_time_ms: 6838.85
    update_time_ms: 29.437
  timestamp: 1602757406
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    127 |          3247.65 | 20547584 |  275.999 |              323.616 |              138.768 |            769.968 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3092.535434539351
    time_step_min: 2792
  date: 2020-10-15_10-23-52
  done: false
  episode_len_mean: 769.8398823047413
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 276.2735711241055
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 252
  episodes_total: 26849
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.13039244463046393
        entropy_coeff: 0.0005000000000000001
        kl: 0.004497993310603003
        model: {}
        policy_loss: -0.007785009889630601
        total_loss: 2.6508864561716714
        vf_explained_var: 0.9937828183174133
        vf_loss: 2.6587366461753845
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77
    gpu_util_percent0: 0.3296666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470355170074786
    mean_env_wait_ms: 1.2227395291077086
    mean_inference_ms: 4.331584071847649
    mean_raw_obs_processing_ms: 0.37877689491657374
  time_since_restore: 3273.5269737243652
  time_this_iter_s: 25.875519514083862
  time_total_s: 3273.5269737243652
  timers:
    learn_throughput: 8732.188
    learn_time_ms: 18528.231
    sample_throughput: 23604.594
    sample_time_ms: 6854.259
    update_time_ms: 30.85
  timestamp: 1602757432
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    128 |          3273.53 | 20709376 |  276.274 |              323.616 |              138.768 |             769.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3091.328799110452
    time_step_min: 2792
  date: 2020-10-15_10-24-18
  done: false
  episode_len_mean: 769.7559865279989
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 276.45577392040985
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 170
  episodes_total: 27019
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.12892911210656166
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038845023179116347
        model: {}
        policy_loss: -0.009248940439041084
        total_loss: 1.9274434347947438
        vf_explained_var: 0.9939970970153809
        vf_loss: 1.936756859223048
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.266666666666666
    gpu_util_percent0: 0.3093333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702914471238096
    mean_env_wait_ms: 1.2227312734593596
    mean_inference_ms: 4.331155198575033
    mean_raw_obs_processing_ms: 0.3787525073297177
  time_since_restore: 3298.9624996185303
  time_this_iter_s: 25.43552589416504
  time_total_s: 3298.9624996185303
  timers:
    learn_throughput: 8740.888
    learn_time_ms: 18509.79
    sample_throughput: 23574.718
    sample_time_ms: 6862.945
    update_time_ms: 29.081
  timestamp: 1602757458
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    129 |          3298.96 | 20871168 |  276.456 |              323.616 |              138.768 |            769.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3089.693528828134
    time_step_min: 2792
  date: 2020-10-15_10-24-44
  done: false
  episode_len_mean: 769.6424849552326
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 276.69739780010576
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 27252
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.14697920406858125
        entropy_coeff: 0.0005000000000000001
        kl: 0.004123337586255123
        model: {}
        policy_loss: -0.007596952326518173
        total_loss: 3.160941501458486
        vf_explained_var: 0.9924089908599854
        vf_loss: 3.168611983458201
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.139999999999997
    gpu_util_percent0: 0.3426666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702057474963334
    mean_env_wait_ms: 1.222718373126148
    mean_inference_ms: 4.330572374027836
    mean_raw_obs_processing_ms: 0.3787180464615177
  time_since_restore: 3324.6542358398438
  time_this_iter_s: 25.691736221313477
  time_total_s: 3324.6542358398438
  timers:
    learn_throughput: 8732.839
    learn_time_ms: 18526.851
    sample_throughput: 23536.561
    sample_time_ms: 6874.071
    update_time_ms: 28.754
  timestamp: 1602757484
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    130 |          3324.65 | 21032960 |  276.697 |              323.616 |              138.768 |            769.642 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3088.0417167632163
    time_step_min: 2789
  date: 2020-10-15_10-25-11
  done: false
  episode_len_mean: 769.526486211162
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 276.9462977295328
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 27486
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.12067394703626633
        entropy_coeff: 0.0005000000000000001
        kl: 0.003435036613761137
        model: {}
        policy_loss: -0.008166467906751981
        total_loss: 2.307787001132965
        vf_explained_var: 0.9942525029182434
        vf_loss: 2.3160137931505838
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5
    gpu_util_percent0: 0.32448275862068965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701182792382778
    mean_env_wait_ms: 1.2226966248131275
    mean_inference_ms: 4.32995588248422
    mean_raw_obs_processing_ms: 0.3786821812807862
  time_since_restore: 3350.2303438186646
  time_this_iter_s: 25.5761079788208
  time_total_s: 3350.2303438186646
  timers:
    learn_throughput: 8740.061
    learn_time_ms: 18511.541
    sample_throughput: 23506.278
    sample_time_ms: 6882.927
    update_time_ms: 28.322
  timestamp: 1602757511
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    131 |          3350.23 | 21194752 |  276.946 |              323.616 |              138.768 |            769.526 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3086.855171664494
    time_step_min: 2789
  date: 2020-10-15_10-25-37
  done: false
  episode_len_mean: 769.4495316625076
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 277.1201008676326
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 165
  episodes_total: 27651
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.12907345096270242
        entropy_coeff: 0.0005000000000000001
        kl: 0.004641744385783871
        model: {}
        policy_loss: -0.008278223656816408
        total_loss: 1.972252627213796
        vf_explained_var: 0.9937462210655212
        vf_loss: 1.9805953900019329
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.319999999999997
    gpu_util_percent0: 0.37999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700590460331286
    mean_env_wait_ms: 1.2226853977842427
    mean_inference_ms: 4.329555596270841
    mean_raw_obs_processing_ms: 0.378659186759408
  time_since_restore: 3375.7077145576477
  time_this_iter_s: 25.477370738983154
  time_total_s: 3375.7077145576477
  timers:
    learn_throughput: 8740.957
    learn_time_ms: 18509.643
    sample_throughput: 23509.528
    sample_time_ms: 6881.976
    update_time_ms: 30.097
  timestamp: 1602757537
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    132 |          3375.71 | 21356544 |   277.12 |              323.616 |              138.768 |             769.45 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3085.203969564281
    time_step_min: 2789
  date: 2020-10-15_10-26-03
  done: false
  episode_len_mean: 769.3404537471775
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 277.3689650890467
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 27901
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.1399788794418176
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037671815565166375
        model: {}
        policy_loss: -0.010458155030695101
        total_loss: 3.28119424978892
        vf_explained_var: 0.9924675822257996
        vf_loss: 3.2917223374048867
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14333333333334
    gpu_util_percent0: 0.3066666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699703039307208
    mean_env_wait_ms: 1.2226640362416643
    mean_inference_ms: 4.328947615380945
    mean_raw_obs_processing_ms: 0.378621526827814
  time_since_restore: 3401.257212162018
  time_this_iter_s: 25.549497604370117
  time_total_s: 3401.257212162018
  timers:
    learn_throughput: 8746.967
    learn_time_ms: 18496.927
    sample_throughput: 23541.414
    sample_time_ms: 6872.654
    update_time_ms: 29.706
  timestamp: 1602757563
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    133 |          3401.26 | 21518336 |  277.369 |              323.616 |              138.768 |             769.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3083.7306363733487
    time_step_min: 2782
  date: 2020-10-15_10-26-29
  done: false
  episode_len_mean: 769.2385490753912
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 277.59121980832504
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 219
  episodes_total: 28120
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.11294228211045265
        entropy_coeff: 0.0005000000000000001
        kl: 0.003609710819243143
        model: {}
        policy_loss: -0.00830613449215889
        total_loss: 2.3648006518681846
        vf_explained_var: 0.993797242641449
        vf_loss: 2.3731632828712463
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56896551724138
    gpu_util_percent0: 0.3472413793103448
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.879310344827587
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469890277366124
    mean_env_wait_ms: 1.222641824115056
    mean_inference_ms: 4.328407385176828
    mean_raw_obs_processing_ms: 0.3785914187385426
  time_since_restore: 3427.0112504959106
  time_this_iter_s: 25.754038333892822
  time_total_s: 3427.0112504959106
  timers:
    learn_throughput: 8739.742
    learn_time_ms: 18512.218
    sample_throughput: 23589.066
    sample_time_ms: 6858.771
    update_time_ms: 31.313
  timestamp: 1602757589
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    134 |          3427.01 | 21680128 |  277.591 |              323.616 |              138.768 |            769.239 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3082.608757212134
    time_step_min: 2782
  date: 2020-10-15_10-26-55
  done: false
  episode_len_mean: 769.1729939908095
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 277.7624120312349
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 170
  episodes_total: 28290
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.12997115651766458
        entropy_coeff: 0.0005000000000000001
        kl: 0.00404245105649655
        model: {}
        policy_loss: -0.008053736610842558
        total_loss: 2.1743253270785012
        vf_explained_var: 0.9934215545654297
        vf_loss: 2.1824441154797873
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08
    gpu_util_percent0: 0.317
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469833977407973
    mean_env_wait_ms: 1.2226240101568897
    mean_inference_ms: 4.328006428804203
    mean_raw_obs_processing_ms: 0.3785682130368348
  time_since_restore: 3452.7332067489624
  time_this_iter_s: 25.721956253051758
  time_total_s: 3452.7332067489624
  timers:
    learn_throughput: 8728.522
    learn_time_ms: 18536.013
    sample_throughput: 23598.565
    sample_time_ms: 6856.01
    update_time_ms: 30.509
  timestamp: 1602757615
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    135 |          3452.73 | 21841920 |  277.762 |              323.616 |              138.768 |            769.173 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3080.922353931402
    time_step_min: 2782
  date: 2020-10-15_10-27-22
  done: false
  episode_len_mean: 769.0851399152453
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 278.0212021097041
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 263
  episodes_total: 28553
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.13214701289931932
        entropy_coeff: 0.0005000000000000001
        kl: 0.003503160609398037
        model: {}
        policy_loss: -0.007317108246146138
        total_loss: 2.9581462740898132
        vf_explained_var: 0.9934385418891907
        vf_loss: 2.9655295610427856
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.026666666666667
    gpu_util_percent0: 0.34700000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697367212158077
    mean_env_wait_ms: 1.222595308411283
    mean_inference_ms: 4.327389175678794
    mean_raw_obs_processing_ms: 0.37853010539194
  time_since_restore: 3478.3243219852448
  time_this_iter_s: 25.59111523628235
  time_total_s: 3478.3243219852448
  timers:
    learn_throughput: 8723.096
    learn_time_ms: 18547.543
    sample_throughput: 23531.474
    sample_time_ms: 6875.557
    update_time_ms: 31.273
  timestamp: 1602757642
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    136 |          3478.32 | 22003712 |  278.021 |              323.616 |              138.768 |            769.085 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3079.6354518544313
    time_step_min: 2782
  date: 2020-10-15_10-27-48
  done: false
  episode_len_mean: 769.0164151074633
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 278.2208490272411
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 28754
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.10710334964096546
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034265152062289417
        model: {}
        policy_loss: -0.008674220361475212
        total_loss: 2.136854350566864
        vf_explained_var: 0.9942054748535156
        vf_loss: 2.145582139492035
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.453333333333333
    gpu_util_percent0: 0.3446666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696719208679898
    mean_env_wait_ms: 1.2225714447516098
    mean_inference_ms: 4.326931223036657
    mean_raw_obs_processing_ms: 0.37850407615777487
  time_since_restore: 3504.00213265419
  time_this_iter_s: 25.677810668945312
  time_total_s: 3504.00213265419
  timers:
    learn_throughput: 8711.708
    learn_time_ms: 18571.788
    sample_throughput: 23587.27
    sample_time_ms: 6859.293
    update_time_ms: 30.119
  timestamp: 1602757668
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    137 |             3504 | 22165504 |  278.221 |              323.616 |              138.768 |            769.016 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3078.5140691516976
    time_step_min: 2782
  date: 2020-10-15_10-28-14
  done: false
  episode_len_mean: 768.9601133692797
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 278.3945604950374
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 28932
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.388131789017202e-22
        cur_lr: 5.0e-05
        entropy: 0.12466605628530185
        entropy_coeff: 0.0005000000000000001
        kl: 0.003404979477636516
        model: {}
        policy_loss: -0.007752647623419762
        total_loss: 1.8008587062358856
        vf_explained_var: 0.9947686791419983
        vf_loss: 1.8086736798286438
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.379310344827587
    gpu_util_percent0: 0.32931034482758614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696147043990418
    mean_env_wait_ms: 1.2225481123652049
    mean_inference_ms: 4.326530010074473
    mean_raw_obs_processing_ms: 0.37847992120468155
  time_since_restore: 3529.547334432602
  time_this_iter_s: 25.545201778411865
  time_total_s: 3529.547334432602
  timers:
    learn_throughput: 8719.594
    learn_time_ms: 18554.993
    sample_throughput: 23639.683
    sample_time_ms: 6844.085
    update_time_ms: 28.504
  timestamp: 1602757694
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    138 |          3529.55 | 22327296 |  278.395 |              323.616 |              138.768 |             768.96 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3076.8041435137548
    time_step_min: 2782
  date: 2020-10-15_10-28-40
  done: false
  episode_len_mean: 768.8663035659233
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 278.6590164308795
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 261
  episodes_total: 29193
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.694065894508601e-22
        cur_lr: 5.0e-05
        entropy: 0.12118960606555144
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032144446158781648
        model: {}
        policy_loss: -0.009764510788954794
        total_loss: 2.056720733642578
        vf_explained_var: 0.9952906966209412
        vf_loss: 2.0665458540121713
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.68666666666666
    gpu_util_percent0: 0.3373333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469522700772691
    mean_env_wait_ms: 1.2225154554692377
    mean_inference_ms: 4.325939422878889
    mean_raw_obs_processing_ms: 0.37844424028296175
  time_since_restore: 3554.945763349533
  time_this_iter_s: 25.398428916931152
  time_total_s: 3554.945763349533
  timers:
    learn_throughput: 8718.618
    learn_time_ms: 18557.068
    sample_throughput: 23663.096
    sample_time_ms: 6837.313
    update_time_ms: 28.221
  timestamp: 1602757720
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    139 |          3554.95 | 22489088 |  278.659 |              323.616 |              138.768 |            768.866 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3075.5738380809594
    time_step_min: 2782
  date: 2020-10-15_10-29-06
  done: false
  episode_len_mean: 768.79759757716
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 278.84941771476633
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 194
  episodes_total: 29387
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.470329472543005e-23
        cur_lr: 5.0e-05
        entropy: 0.1030605894823869
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035943142332447073
        model: {}
        policy_loss: -0.0074074975758170085
        total_loss: 1.661388059457143
        vf_explained_var: 0.9952811598777771
        vf_loss: 1.6688470443089802
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.856666666666666
    gpu_util_percent0: 0.32799999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694611647116715
    mean_env_wait_ms: 1.2224878876656167
    mean_inference_ms: 4.325519187363499
    mean_raw_obs_processing_ms: 0.3784197021721675
  time_since_restore: 3580.6903035640717
  time_this_iter_s: 25.744540214538574
  time_total_s: 3580.6903035640717
  timers:
    learn_throughput: 8706.82
    learn_time_ms: 18582.216
    sample_throughput: 23741.89
    sample_time_ms: 6814.622
    update_time_ms: 30.187
  timestamp: 1602757746
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    140 |          3580.69 | 22650880 |  278.849 |              323.616 |              138.768 |            768.798 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3074.3130755687976
    time_step_min: 2775
  date: 2020-10-15_10-29-33
  done: false
  episode_len_mean: 768.7265595942519
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 279.0362031131261
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 188
  episodes_total: 29575
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2351647362715025e-23
        cur_lr: 5.0e-05
        entropy: 0.11879971747597058
        entropy_coeff: 0.0005000000000000001
        kl: 0.003946047897140185
        model: {}
        policy_loss: -0.009734965026533851
        total_loss: 1.9565891822179158
        vf_explained_var: 0.9943842887878418
        vf_loss: 1.9663835366566975
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74666666666667
    gpu_util_percent0: 0.31300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693983682477985
    mean_env_wait_ms: 1.2224586532248893
    mean_inference_ms: 4.325102286480256
    mean_raw_obs_processing_ms: 0.3783939355619004
  time_since_restore: 3606.570489883423
  time_this_iter_s: 25.880186319351196
  time_total_s: 3606.570489883423
  timers:
    learn_throughput: 8693.88
    learn_time_ms: 18609.872
    sample_throughput: 23738.31
    sample_time_ms: 6815.649
    update_time_ms: 30.621
  timestamp: 1602757773
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    141 |          3606.57 | 22812672 |  279.036 |              324.374 |              138.768 |            768.727 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3072.613993288591
    time_step_min: 2775
  date: 2020-10-15_10-29-59
  done: false
  episode_len_mean: 768.6267971446764
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 279.2902946147692
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 264
  episodes_total: 29839
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1175823681357513e-23
        cur_lr: 5.0e-05
        entropy: 0.11921367173393567
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037126412110713622
        model: {}
        policy_loss: -0.009510534519601302
        total_loss: 2.35510516166687
        vf_explained_var: 0.9946184754371643
        vf_loss: 2.3646753231684365
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.54666666666667
    gpu_util_percent0: 0.337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693199309954144
    mean_env_wait_ms: 1.222420283421807
    mean_inference_ms: 4.324543190214798
    mean_raw_obs_processing_ms: 0.37836006495027397
  time_since_restore: 3632.1806721687317
  time_this_iter_s: 25.610182285308838
  time_total_s: 3632.1806721687317
  timers:
    learn_throughput: 8687.913
    learn_time_ms: 18622.654
    sample_throughput: 23732.575
    sample_time_ms: 6817.296
    update_time_ms: 28.994
  timestamp: 1602757799
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    142 |          3632.18 | 22974464 |   279.29 |              324.374 |              138.768 |            768.627 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3071.5098895967444
    time_step_min: 2775
  date: 2020-10-15_10-30-25
  done: false
  episode_len_mean: 768.5690206528981
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 279.45742400689096
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 30020
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0587911840678756e-23
        cur_lr: 5.0e-05
        entropy: 0.10532878028849761
        entropy_coeff: 0.0005000000000000001
        kl: 0.003156912668297688
        model: {}
        policy_loss: -0.008365836290370984
        total_loss: 1.6403030653794606
        vf_explained_var: 0.9950303435325623
        vf_loss: 1.6487215757369995
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.836666666666666
    gpu_util_percent0: 0.2946666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692586990708886
    mean_env_wait_ms: 1.2223931954601974
    mean_inference_ms: 4.32416560304663
    mean_raw_obs_processing_ms: 0.3783381833198019
  time_since_restore: 3657.964794397354
  time_this_iter_s: 25.784122228622437
  time_total_s: 3657.964794397354
  timers:
    learn_throughput: 8677.902
    learn_time_ms: 18644.137
    sample_throughput: 23722.739
    sample_time_ms: 6820.123
    update_time_ms: 28.684
  timestamp: 1602757825
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    143 |          3657.96 | 23136256 |  279.457 |              324.374 |              138.768 |            768.569 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3070.226271326818
    time_step_min: 2775
  date: 2020-10-15_10-30-51
  done: false
  episode_len_mean: 768.4957649550026
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 279.6458169572912
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 204
  episodes_total: 30224
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.293955920339378e-24
        cur_lr: 5.0e-05
        entropy: 0.11878691303233306
        entropy_coeff: 0.0005000000000000001
        kl: 0.003895909544856598
        model: {}
        policy_loss: -0.007602581534835433
        total_loss: 1.9458973010381062
        vf_explained_var: 0.9947932362556458
        vf_loss: 1.9535592794418335
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.003333333333334
    gpu_util_percent0: 0.3766666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691952507113817
    mean_env_wait_ms: 1.2223603938908052
    mean_inference_ms: 4.323750802987287
    mean_raw_obs_processing_ms: 0.3783125028382005
  time_since_restore: 3683.6729946136475
  time_this_iter_s: 25.708200216293335
  time_total_s: 3683.6729946136475
  timers:
    learn_throughput: 8680.526
    learn_time_ms: 18638.501
    sample_throughput: 23719.145
    sample_time_ms: 6821.157
    update_time_ms: 27.403
  timestamp: 1602757851
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    144 |          3683.67 | 23298048 |  279.646 |              324.374 |              138.768 |            768.496 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3068.6326094812575
    time_step_min: 2775
  date: 2020-10-15_10-31-18
  done: false
  episode_len_mean: 768.4101318984185
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 279.8914580545264
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 254
  episodes_total: 30478
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.646977960169689e-24
        cur_lr: 5.0e-05
        entropy: 0.10605212921897571
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033869923014814654
        model: {}
        policy_loss: -0.007927563895160953
        total_loss: 1.597449113925298
        vf_explained_var: 0.9962825775146484
        vf_loss: 1.6054296791553497
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.063333333333333
    gpu_util_percent0: 0.29766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691187925267096
    mean_env_wait_ms: 1.2223175530151353
    mean_inference_ms: 4.323213118905313
    mean_raw_obs_processing_ms: 0.3782800921269715
  time_since_restore: 3709.4072074890137
  time_this_iter_s: 25.73421287536621
  time_total_s: 3709.4072074890137
  timers:
    learn_throughput: 8676.829
    learn_time_ms: 18646.444
    sample_throughput: 23748.951
    sample_time_ms: 6812.596
    update_time_ms: 28.218
  timestamp: 1602757878
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    145 |          3709.41 | 23459840 |  279.891 |              324.374 |              138.768 |             768.41 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3067.5701966420593
    time_step_min: 2775
  date: 2020-10-15_10-31-44
  done: false
  episode_len_mean: 768.3533096271165
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 280.0557705723268
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 175
  episodes_total: 30653
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3234889800848445e-24
        cur_lr: 5.0e-05
        entropy: 0.0977273906270663
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037857512361370027
        model: {}
        policy_loss: -0.00689352405606769
        total_loss: 1.260213981072108
        vf_explained_var: 0.9960773587226868
        vf_loss: 1.2671564221382141
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.813333333333336
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690625428452486
    mean_env_wait_ms: 1.2222878264841506
    mean_inference_ms: 4.322865800234529
    mean_raw_obs_processing_ms: 0.37825939262261377
  time_since_restore: 3735.1394991874695
  time_this_iter_s: 25.73229169845581
  time_total_s: 3735.1394991874695
  timers:
    learn_throughput: 8665.47
    learn_time_ms: 18670.885
    sample_throughput: 23785.535
    sample_time_ms: 6802.117
    update_time_ms: 27.547
  timestamp: 1602757904
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    146 |          3735.14 | 23621632 |  280.056 |              324.374 |              138.768 |            768.353 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3066.2436263379823
    time_step_min: 2775
  date: 2020-10-15_10-32-11
  done: false
  episode_len_mean: 768.2819333311737
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 280.2614917846055
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 30869
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.617444900424223e-25
        cur_lr: 5.0e-05
        entropy: 0.11433778641124566
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032856951002031565
        model: {}
        policy_loss: -0.008618910646570535
        total_loss: 1.538850982983907
        vf_explained_var: 0.9960334300994873
        vf_loss: 1.5475270946820576
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.27741935483871
    gpu_util_percent0: 0.3590322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689993223377917
    mean_env_wait_ms: 1.2222526847999047
    mean_inference_ms: 4.322450128143018
    mean_raw_obs_processing_ms: 0.37823401705389137
  time_since_restore: 3761.0051810741425
  time_this_iter_s: 25.865681886672974
  time_total_s: 3761.0051810741425
  timers:
    learn_throughput: 8661.363
    learn_time_ms: 18679.738
    sample_throughput: 23753.713
    sample_time_ms: 6811.23
    update_time_ms: 28.654
  timestamp: 1602757931
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    147 |          3761.01 | 23783424 |  280.261 |              324.374 |              138.768 |            768.282 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3064.726757843926
    time_step_min: 2775
  date: 2020-10-15_10-32-37
  done: false
  episode_len_mean: 768.2092948511923
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 280.491162833581
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 245
  episodes_total: 31114
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3087224502121113e-25
        cur_lr: 5.0e-05
        entropy: 0.09957902195552985
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008435903175268322
        total_loss: .inf
        vf_explained_var: 0.9963071346282959
        vf_loss: 1.5454525550206502
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.431034482758626
    gpu_util_percent0: 0.3241379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689252360433003
    mean_env_wait_ms: 1.2222038280569154
    mean_inference_ms: 4.321948952225047
    mean_raw_obs_processing_ms: 0.37820339883413584
  time_since_restore: 3786.1298727989197
  time_this_iter_s: 25.12469172477722
  time_total_s: 3786.1298727989197
  timers:
    learn_throughput: 8691.936
    learn_time_ms: 18614.035
    sample_throughput: 23682.75
    sample_time_ms: 6831.639
    update_time_ms: 30.049
  timestamp: 1602757957
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    148 |          3786.13 | 23945216 |  280.491 |              324.374 |              138.768 |            768.209 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3063.7147474876783
    time_step_min: 2775
  date: 2020-10-15_10-33-03
  done: false
  episode_len_mean: 768.164647594694
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 280.6453507425219
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 171
  episodes_total: 31285
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.963083675318166e-25
        cur_lr: 5.0e-05
        entropy: 0.09554123257597287
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008957299881634148
        total_loss: .inf
        vf_explained_var: 0.9951561093330383
        vf_loss: 1.581534852584203
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.329032258064515
    gpu_util_percent0: 0.42903225806451617
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688738028424458
    mean_env_wait_ms: 1.2221725721283585
    mean_inference_ms: 4.321622272337516
    mean_raw_obs_processing_ms: 0.3781839503046471
  time_since_restore: 3812.069991350174
  time_this_iter_s: 25.940118551254272
  time_total_s: 3812.069991350174
  timers:
    learn_throughput: 8673.67
    learn_time_ms: 18653.235
    sample_throughput: 23643.124
    sample_time_ms: 6843.089
    update_time_ms: 31.928
  timestamp: 1602757983
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    149 |          3812.07 | 24107008 |  280.645 |              324.374 |              138.768 |            768.165 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3062.271910040977
    time_step_min: 2775
  date: 2020-10-15_10-33-29
  done: false
  episode_len_mean: 768.0985723350254
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 280.86545499410335
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 235
  episodes_total: 31520
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.444625512977251e-25
        cur_lr: 5.0e-05
        entropy: 0.1134835754831632
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033160342136397958
        model: {}
        policy_loss: -0.009089715439282978
        total_loss: 1.5633023182551067
        vf_explained_var: 0.9962064623832703
        vf_loss: 1.5724487006664276
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.548275862068966
    gpu_util_percent0: 0.3520689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688067031872806
    mean_env_wait_ms: 1.2221298619412404
    mean_inference_ms: 4.321195426676066
    mean_raw_obs_processing_ms: 0.3781562961077185
  time_since_restore: 3837.587008714676
  time_this_iter_s: 25.517017364501953
  time_total_s: 3837.587008714676
  timers:
    learn_throughput: 8686.442
    learn_time_ms: 18625.807
    sample_throughput: 23627.037
    sample_time_ms: 6847.748
    update_time_ms: 30.476
  timestamp: 1602758009
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    150 |          3837.59 | 24268800 |  280.865 |              324.374 |              138.768 |            768.099 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3060.8393302008767
    time_step_min: 2775
  date: 2020-10-15_10-33-55
  done: false
  episode_len_mean: 768.0350866141732
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 281.0877531217688
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 230
  episodes_total: 31750
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7223127564886255e-25
        cur_lr: 5.0e-05
        entropy: 0.09516688312093417
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005806210484782544
        total_loss: .inf
        vf_explained_var: 0.9968576431274414
        vf_loss: 1.240128070116043
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.103333333333335
    gpu_util_percent0: 0.3433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687390631259337
    mean_env_wait_ms: 1.2220788600899455
    mean_inference_ms: 4.320729610760011
    mean_raw_obs_processing_ms: 0.3781290296696429
  time_since_restore: 3863.118944644928
  time_this_iter_s: 25.531935930252075
  time_total_s: 3863.118944644928
  timers:
    learn_throughput: 8700.653
    learn_time_ms: 18595.386
    sample_throughput: 23656.355
    sample_time_ms: 6839.262
    update_time_ms: 33.452
  timestamp: 1602758035
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    151 |          3863.12 | 24430592 |  281.088 |              324.374 |              138.768 |            768.035 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3059.7726303243207
    time_step_min: 2775
  date: 2020-10-15_10-34-22
  done: false
  episode_len_mean: 767.9896619780083
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 281.2545548843909
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 171
  episodes_total: 31921
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.583469134732936e-25
        cur_lr: 5.0e-05
        entropy: 0.09393879460791747
        entropy_coeff: 0.0005000000000000001
        kl: 0.004076915211044252
        model: {}
        policy_loss: -0.007175326851817469
        total_loss: 0.8288830568393072
        vf_explained_var: 0.9972878098487854
        vf_loss: 0.8361053317785263
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.430000000000003
    gpu_util_percent0: 0.2863333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146869166648352
    mean_env_wait_ms: 1.2220450695396532
    mean_inference_ms: 4.320419616722839
    mean_raw_obs_processing_ms: 0.3781106786418013
  time_since_restore: 3888.634596824646
  time_this_iter_s: 25.515652179718018
  time_total_s: 3888.634596824646
  timers:
    learn_throughput: 8702.497
    learn_time_ms: 18591.445
    sample_throughput: 23677.886
    sample_time_ms: 6833.043
    update_time_ms: 33.811
  timestamp: 1602758062
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    152 |          3888.63 | 24592384 |  281.255 |              324.374 |              138.768 |             767.99 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3058.285945525292
    time_step_min: 2775
  date: 2020-10-15_10-34-48
  done: false
  episode_len_mean: 767.9241387887079
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 281.4813779506292
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 243
  episodes_total: 32164
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.791734567366468e-25
        cur_lr: 5.0e-05
        entropy: 0.10896957727770011
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037495537350575128
        model: {}
        policy_loss: -0.008017466655777147
        total_loss: 1.5259252389272053
        vf_explained_var: 0.9962170124053955
        vf_loss: 1.5339971681435902
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.52068965517241
    gpu_util_percent0: 0.32655172413793104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468619667909096
    mean_env_wait_ms: 1.2219976603628444
    mean_inference_ms: 4.319982696711627
    mean_raw_obs_processing_ms: 0.378081875557345
  time_since_restore: 3914.1154890060425
  time_this_iter_s: 25.480892181396484
  time_total_s: 3914.1154890060425
  timers:
    learn_throughput: 8714.208
    learn_time_ms: 18566.46
    sample_throughput: 23699.537
    sample_time_ms: 6826.8
    update_time_ms: 33.778
  timestamp: 1602758088
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    153 |          3914.12 | 24754176 |  281.481 |              324.374 |              138.768 |            767.924 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3056.9329458682414
    time_step_min: 2775
  date: 2020-10-15_10-35-14
  done: false
  episode_len_mean: 767.8632124992281
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 281.68574461966654
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 222
  episodes_total: 32386
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.395867283683234e-25
        cur_lr: 5.0e-05
        entropy: 0.09024138189852238
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006049924171444824
        total_loss: .inf
        vf_explained_var: 0.9967355132102966
        vf_loss: 1.2009389698505402
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.758064516129032
    gpu_util_percent0: 0.34225806451612895
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468557954800184
    mean_env_wait_ms: 1.2219470163280475
    mean_inference_ms: 4.319566108093978
    mean_raw_obs_processing_ms: 0.37805781445341946
  time_since_restore: 3939.996116399765
  time_this_iter_s: 25.880627393722534
  time_total_s: 3939.996116399765
  timers:
    learn_throughput: 8712.613
    learn_time_ms: 18569.86
    sample_throughput: 23654.666
    sample_time_ms: 6839.75
    update_time_ms: 33.342
  timestamp: 1602758114
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    154 |             3940 | 24915968 |  281.686 |              324.374 |              138.768 |            767.863 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3055.897708749808
    time_step_min: 2775
  date: 2020-10-15_10-35-41
  done: false
  episode_len_mean: 767.8210972537937
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 281.84320535328084
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 168
  episodes_total: 32554
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.093800925524851e-25
        cur_lr: 5.0e-05
        entropy: 0.09232568802932899
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031394645726929107
        model: {}
        policy_loss: -0.00657769003980017
        total_loss: 1.0968682318925858
        vf_explained_var: 0.9964626431465149
        vf_loss: 1.1034921010335286
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.390000000000004
    gpu_util_percent0: 0.326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685149145851203
    mean_env_wait_ms: 1.221909723501732
    mean_inference_ms: 4.319268845066565
    mean_raw_obs_processing_ms: 0.3780397794283072
  time_since_restore: 3965.5762593746185
  time_this_iter_s: 25.580142974853516
  time_total_s: 3965.5762593746185
  timers:
    learn_throughput: 8718.258
    learn_time_ms: 18557.835
    sample_throughput: 23671.456
    sample_time_ms: 6834.898
    update_time_ms: 34.706
  timestamp: 1602758141
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    155 |          3965.58 | 25077760 |  281.843 |              324.374 |              138.768 |            767.821 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3054.22707716718
    time_step_min: 2775
  date: 2020-10-15_10-36-07
  done: false
  episode_len_mean: 767.7540533950994
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 282.0883114948091
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 258
  episodes_total: 32812
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0469004627624256e-25
        cur_lr: 5.0e-05
        entropy: 0.10243581794202328
        entropy_coeff: 0.0005000000000000001
        kl: 0.003938167181331664
        model: {}
        policy_loss: -0.004925790805524836
        total_loss: 1.0605674584706624
        vf_explained_var: 0.997377872467041
        vf_loss: 1.065544495979945
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.227586206896557
    gpu_util_percent0: 0.3479310344827587
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684346336826629
    mean_env_wait_ms: 1.2218539433262792
    mean_inference_ms: 4.318806159635721
    mean_raw_obs_processing_ms: 0.37801038782834606
  time_since_restore: 3990.729151248932
  time_this_iter_s: 25.152891874313354
  time_total_s: 3990.729151248932
  timers:
    learn_throughput: 8743.106
    learn_time_ms: 18505.095
    sample_throughput: 23687.506
    sample_time_ms: 6830.267
    update_time_ms: 33.572
  timestamp: 1602758167
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    156 |          3990.73 | 25239552 |  282.088 |              324.374 |              138.768 |            767.754 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3052.956276531231
    time_step_min: 2775
  date: 2020-10-15_10-36-33
  done: false
  episode_len_mean: 767.6970834973803
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 282.28276036968
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 207
  episodes_total: 33019
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.234502313812128e-26
        cur_lr: 5.0e-05
        entropy: 0.08641458923617999
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037357525628370545
        model: {}
        policy_loss: -0.009041970712132752
        total_loss: 1.0939714113871257
        vf_explained_var: 0.996756374835968
        vf_loss: 1.1030566145976384
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48666666666667
    gpu_util_percent0: 0.32133333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468384403139713
    mean_env_wait_ms: 1.2218059358139528
    mean_inference_ms: 4.318450929359456
    mean_raw_obs_processing_ms: 0.37798927814298555
  time_since_restore: 4016.585909128189
  time_this_iter_s: 25.856757879257202
  time_total_s: 4016.585909128189
  timers:
    learn_throughput: 8738.709
    learn_time_ms: 18514.404
    sample_throughput: 23721.889
    sample_time_ms: 6820.367
    update_time_ms: 32.18
  timestamp: 1602758193
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    157 |          4016.59 | 25401344 |  282.283 |              324.374 |              138.768 |            767.697 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3051.917480998914
    time_step_min: 2775
  date: 2020-10-15_10-36-59
  done: false
  episode_len_mean: 767.6494954059347
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 282.44201618535095
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 176
  episodes_total: 33195
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.617251156906064e-26
        cur_lr: 5.0e-05
        entropy: 0.09179580770432949
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00979267479973108
        total_loss: .inf
        vf_explained_var: 0.9964656233787537
        vf_loss: 1.116971602042516
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.55333333333333
    gpu_util_percent0: 0.35666666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683391704779264
    mean_env_wait_ms: 1.2217652052270809
    mean_inference_ms: 4.318147392007787
    mean_raw_obs_processing_ms: 0.37797025394253775
  time_since_restore: 4042.124307155609
  time_this_iter_s: 25.538398027420044
  time_total_s: 4042.124307155609
  timers:
    learn_throughput: 8710.399
    learn_time_ms: 18574.58
    sample_throughput: 23784.015
    sample_time_ms: 6802.552
    update_time_ms: 30.796
  timestamp: 1602758219
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    158 |          4042.12 | 25563136 |  282.442 |              324.374 |              138.768 |            767.649 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3050.340983900892
    time_step_min: 2775
  date: 2020-10-15_10-37-26
  done: false
  episode_len_mean: 767.5870819260543
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 282.6853195855497
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 262
  episodes_total: 33457
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9258767353590975e-26
        cur_lr: 5.0e-05
        entropy: 0.0933098600556453
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0072219056395018315
        total_loss: .inf
        vf_explained_var: 0.9968317151069641
        vf_loss: 1.3164959947268169
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.493333333333336
    gpu_util_percent0: 0.3523333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682648895230016
    mean_env_wait_ms: 1.2217059421542869
    mean_inference_ms: 4.317702708866006
    mean_raw_obs_processing_ms: 0.3779426018557058
  time_since_restore: 4067.916773080826
  time_this_iter_s: 25.792465925216675
  time_total_s: 4067.916773080826
  timers:
    learn_throughput: 8714.405
    learn_time_ms: 18566.042
    sample_throughput: 23802.294
    sample_time_ms: 6797.328
    update_time_ms: 29.346
  timestamp: 1602758246
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    159 |          4067.92 | 25724928 |  282.685 |              324.374 |              138.768 |            767.587 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3049.1358362587016
    time_step_min: 2775
  date: 2020-10-15_10-37-52
  done: false
  episode_len_mean: 767.5397141413841
  episode_reward_max: 324.3737373737368
  episode_reward_mean: 282.8705258990522
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 196
  episodes_total: 33653
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.888815103038644e-26
        cur_lr: 5.0e-05
        entropy: 0.08153647184371948
        entropy_coeff: 0.0005000000000000001
        kl: 0.004223004235730817
        model: {}
        policy_loss: -0.007477518501218583
        total_loss: 0.7372397234042486
        vf_explained_var: 0.9977213740348816
        vf_loss: 0.7447579850753149
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.27
    gpu_util_percent0: 0.30533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682160653457466
    mean_env_wait_ms: 1.2216574307235404
    mean_inference_ms: 4.317376338506869
    mean_raw_obs_processing_ms: 0.3779230620331473
  time_since_restore: 4093.314479112625
  time_this_iter_s: 25.397706031799316
  time_total_s: 4093.314479112625
  timers:
    learn_throughput: 8724.764
    learn_time_ms: 18543.998
    sample_throughput: 23801.142
    sample_time_ms: 6797.657
    update_time_ms: 30.191
  timestamp: 1602758272
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    160 |          4093.31 | 25886720 |  282.871 |              324.374 |              138.768 |             767.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3048.0079881656807
    time_step_min: 2775
  date: 2020-10-15_10-38-18
  done: false
  episode_len_mean: 767.4986553976181
  episode_reward_max: 324.373737373737
  episode_reward_mean: 283.0469958606723
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 186
  episodes_total: 33839
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.944407551519322e-26
        cur_lr: 5.0e-05
        entropy: 0.0922065166135629
        entropy_coeff: 0.0005000000000000001
        kl: 0.004568621205786864
        model: {}
        policy_loss: -0.008462109991038838
        total_loss: 0.9089147051175436
        vf_explained_var: 0.9970846176147461
        vf_loss: 0.9174229204654694
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.310000000000006
    gpu_util_percent0: 0.2986666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681674405321618
    mean_env_wait_ms: 1.2216110403835414
    mean_inference_ms: 4.317064978448065
    mean_raw_obs_processing_ms: 0.3779030787158976
  time_since_restore: 4119.059887647629
  time_this_iter_s: 25.745408535003662
  time_total_s: 4119.059887647629
  timers:
    learn_throughput: 8713.247
    learn_time_ms: 18568.508
    sample_throughput: 23802.441
    sample_time_ms: 6797.286
    update_time_ms: 27.206
  timestamp: 1602758298
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    161 |          4119.06 | 26048512 |  283.047 |              324.374 |              138.768 |            767.499 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3046.3953024075163
    time_step_min: 2775
  date: 2020-10-15_10-38-44
  done: false
  episode_len_mean: 767.4345288718145
  episode_reward_max: 324.8282828282826
  episode_reward_mean: 283.2955310458169
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 260
  episodes_total: 34099
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.472203775759661e-26
        cur_lr: 5.0e-05
        entropy: 0.09418968545893829
        entropy_coeff: 0.0005000000000000001
        kl: 0.00408096140017733
        model: {}
        policy_loss: -0.006181283970363438
        total_loss: 0.9683219989140829
        vf_explained_var: 0.9975235462188721
        vf_loss: 0.9745503564675649
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.160000000000004
    gpu_util_percent0: 0.3643333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680961157715067
    mean_env_wait_ms: 1.2215480348838548
    mean_inference_ms: 4.3166355316012
    mean_raw_obs_processing_ms: 0.3778764997165826
  time_since_restore: 4144.792236804962
  time_this_iter_s: 25.732349157333374
  time_total_s: 4144.792236804962
  timers:
    learn_throughput: 8706.119
    learn_time_ms: 18583.712
    sample_throughput: 23810.775
    sample_time_ms: 6794.907
    update_time_ms: 26.738
  timestamp: 1602758324
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    162 |          4144.79 | 26210304 |  283.296 |              324.828 |              138.768 |            767.435 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3045.2573731238685
    time_step_min: 2763
  date: 2020-10-15_10-39-11
  done: false
  episode_len_mean: 767.3893247775995
  episode_reward_max: 326.1919191919188
  episode_reward_mean: 283.46794766978513
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 186
  episodes_total: 34285
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.361018878798304e-27
        cur_lr: 5.0e-05
        entropy: 0.08309613975385825
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037086777932321033
        model: {}
        policy_loss: -0.009094477068477621
        total_loss: 0.9130694021781286
        vf_explained_var: 0.9969951510429382
        vf_loss: 0.9222054233153661
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.383333333333333
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680535731986685
    mean_env_wait_ms: 1.2215017390387901
    mean_inference_ms: 4.316340901304466
    mean_raw_obs_processing_ms: 0.3778588308941577
  time_since_restore: 4170.58373093605
  time_this_iter_s: 25.791494131088257
  time_total_s: 4170.58373093605
  timers:
    learn_throughput: 8693.532
    learn_time_ms: 18610.617
    sample_throughput: 23796.77
    sample_time_ms: 6798.906
    update_time_ms: 25.476
  timestamp: 1602758351
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    163 |          4170.58 | 26372096 |  283.468 |              326.192 |              138.768 |            767.389 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3044.1312565323424
    time_step_min: 2763
  date: 2020-10-15_10-39-37
  done: false
  episode_len_mean: 767.3366006437955
  episode_reward_max: 326.1919191919188
  episode_reward_mean: 283.64473959793384
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 198
  episodes_total: 34483
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.680509439399152e-27
        cur_lr: 5.0e-05
        entropy: 0.09427649962405364
        entropy_coeff: 0.0005000000000000001
        kl: 0.00411300176832204
        model: {}
        policy_loss: -0.007713900180230364
        total_loss: 1.2599371373653412
        vf_explained_var: 0.9961473941802979
        vf_loss: 1.2676981687545776
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63666666666667
    gpu_util_percent0: 0.2983333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680061914150846
    mean_env_wait_ms: 1.2214525274583292
    mean_inference_ms: 4.316030920630309
    mean_raw_obs_processing_ms: 0.37783904958502823
  time_since_restore: 4196.333810806274
  time_this_iter_s: 25.750079870224
  time_total_s: 4196.333810806274
  timers:
    learn_throughput: 8694.915
    learn_time_ms: 18607.658
    sample_throughput: 23836.131
    sample_time_ms: 6787.679
    update_time_ms: 25.825
  timestamp: 1602758377
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    164 |          4196.33 | 26533888 |  283.645 |              326.192 |              138.768 |            767.337 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3042.542923835048
    time_step_min: 2763
  date: 2020-10-15_10-40-04
  done: false
  episode_len_mean: 767.2651698330454
  episode_reward_max: 326.1919191919188
  episode_reward_mean: 283.8829791873833
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 257
  episodes_total: 34740
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.840254719699576e-27
        cur_lr: 5.0e-05
        entropy: 0.08769398182630539
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031244339770637453
        model: {}
        policy_loss: -0.008168087969731156
        total_loss: 1.089421461025874
        vf_explained_var: 0.9971721172332764
        vf_loss: 1.0976334114869435
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203333333333337
    gpu_util_percent0: 0.34833333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679374542012502
    mean_env_wait_ms: 1.221385201765328
    mean_inference_ms: 4.315614923479877
    mean_raw_obs_processing_ms: 0.3778126639744098
  time_since_restore: 4222.075417041779
  time_this_iter_s: 25.74160623550415
  time_total_s: 4222.075417041779
  timers:
    learn_throughput: 8690.521
    learn_time_ms: 18617.065
    sample_throughput: 23815.107
    sample_time_ms: 6793.671
    update_time_ms: 25.298
  timestamp: 1602758404
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: cb791_00000
  
2020-10-15 10:40:04,895	WARNING util.py:136 -- The `process_trial` operation took 0.518826961517334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    165 |          4222.08 | 26695680 |  283.883 |              326.192 |              138.768 |            767.265 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3041.4528226153275
    time_step_min: 2763
  date: 2020-10-15_10-40-30
  done: false
  episode_len_mean: 767.2175382324302
  episode_reward_max: 326.1919191919188
  episode_reward_mean: 284.0490546104841
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 34918
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.20127359849788e-28
        cur_lr: 5.0e-05
        entropy: 0.07676359700659911
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00859888240423364
        total_loss: .inf
        vf_explained_var: 0.9971551895141602
        vf_loss: 0.8361716171105703
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.163333333333338
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678959407150202
    mean_env_wait_ms: 1.2213398645690599
    mean_inference_ms: 4.315339191445976
    mean_raw_obs_processing_ms: 0.3777962891735157
  time_since_restore: 4247.643748998642
  time_this_iter_s: 25.568331956863403
  time_total_s: 4247.643748998642
  timers:
    learn_throughput: 8670.554
    learn_time_ms: 18659.938
    sample_throughput: 23825.405
    sample_time_ms: 6790.735
    update_time_ms: 25.478
  timestamp: 1602758430
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    166 |          4247.64 | 26857472 |  284.049 |              326.192 |              138.768 |            767.218 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3040.1512396694216
    time_step_min: 2763
  date: 2020-10-15_10-40-56
  done: false
  episode_len_mean: 767.1594693842694
  episode_reward_max: 326.1919191919189
  episode_reward_mean: 284.24726843716843
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 211
  episodes_total: 35129
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.380191039774682e-27
        cur_lr: 5.0e-05
        entropy: 0.08733725796143214
        entropy_coeff: 0.0005000000000000001
        kl: 0.003690407572624584
        model: {}
        policy_loss: -0.006405888282946155
        total_loss: 1.0814073880513508
        vf_explained_var: 0.9967207312583923
        vf_loss: 1.0878569682439168
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.686666666666667
    gpu_util_percent0: 0.3433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678463078893875
    mean_env_wait_ms: 1.2212869677190041
    mean_inference_ms: 4.3150274307433625
    mean_raw_obs_processing_ms: 0.3777766825301033
  time_since_restore: 4273.494248628616
  time_this_iter_s: 25.850499629974365
  time_total_s: 4273.494248628616
  timers:
    learn_throughput: 8670.707
    learn_time_ms: 18659.609
    sample_throughput: 23831.134
    sample_time_ms: 6789.102
    update_time_ms: 25.583
  timestamp: 1602758456
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    167 |          4273.49 | 27019264 |  284.247 |              326.192 |              138.768 |            767.159 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3038.6575648680005
    time_step_min: 2763
  date: 2020-10-15_10-41-23
  done: false
  episode_len_mean: 767.0884963256077
  episode_reward_max: 326.191919191919
  episode_reward_mean: 284.47750255523
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 251
  episodes_total: 35380
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.90095519887341e-28
        cur_lr: 5.0e-05
        entropy: 0.0817490400125583
        entropy_coeff: 0.0005000000000000001
        kl: 0.002988045491899053
        model: {}
        policy_loss: -0.006139235959077875
        total_loss: 0.800287276506424
        vf_explained_var: 0.9978485703468323
        vf_loss: 0.8064673840999603
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32333333333333
    gpu_util_percent0: 0.32399999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677862564545396
    mean_env_wait_ms: 1.2212172922023965
    mean_inference_ms: 4.314628203618744
    mean_raw_obs_processing_ms: 0.3777511606539973
  time_since_restore: 4299.142821073532
  time_this_iter_s: 25.64857244491577
  time_total_s: 4299.142821073532
  timers:
    learn_throughput: 8671.9
    learn_time_ms: 18657.041
    sample_throughput: 23785.286
    sample_time_ms: 6802.189
    update_time_ms: 25.387
  timestamp: 1602758483
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    168 |          4299.14 | 27181056 |  284.478 |              326.192 |              138.768 |            767.088 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3037.5734921439434
    time_step_min: 2763
  date: 2020-10-15_10-41-49
  done: false
  episode_len_mean: 767.0365370010969
  episode_reward_max: 326.1919191919192
  episode_reward_mean: 284.6402494270184
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 173
  episodes_total: 35553
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.450477599436705e-28
        cur_lr: 5.0e-05
        entropy: 0.07290441232422988
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006650259456364438
        total_loss: .inf
        vf_explained_var: 0.997340977191925
        vf_loss: 0.7230565547943115
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.480000000000004
    gpu_util_percent0: 0.35200000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677451117065812
    mean_env_wait_ms: 1.2211726107719907
    mean_inference_ms: 4.314370325071246
    mean_raw_obs_processing_ms: 0.3777356799903274
  time_since_restore: 4324.564983129501
  time_this_iter_s: 25.42216205596924
  time_total_s: 4324.564983129501
  timers:
    learn_throughput: 8687.8
    learn_time_ms: 18622.897
    sample_throughput: 23804.862
    sample_time_ms: 6796.595
    update_time_ms: 26.725
  timestamp: 1602758509
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    169 |          4324.56 | 27342848 |   284.64 |              326.192 |              138.768 |            767.037 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3036.12813697787
    time_step_min: 2756
  date: 2020-10-15_10-42-15
  done: false
  episode_len_mean: 766.9685037169527
  episode_reward_max: 327.252525252525
  episode_reward_mean: 284.86239512107255
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 229
  episodes_total: 35782
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.1757163991550595e-28
        cur_lr: 5.0e-05
        entropy: 0.08711125329136848
        entropy_coeff: 0.0005000000000000001
        kl: 0.004456831530357401
        model: {}
        policy_loss: -0.006215931922876432
        total_loss: 0.7274179210265478
        vf_explained_var: 0.997829020023346
        vf_loss: 0.733677422006925
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.063333333333333
    gpu_util_percent0: 0.36266666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676921156390527
    mean_env_wait_ms: 1.2211149035503328
    mean_inference_ms: 4.314038572519781
    mean_raw_obs_processing_ms: 0.3777133985191278
  time_since_restore: 4350.35617518425
  time_this_iter_s: 25.791192054748535
  time_total_s: 4350.35617518425
  timers:
    learn_throughput: 8667.844
    learn_time_ms: 18665.772
    sample_throughput: 23787.653
    sample_time_ms: 6801.512
    update_time_ms: 25.712
  timestamp: 1602758535
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: cb791_00000
  
2020-10-15 10:42:16,433	WARNING util.py:136 -- The `process_trial` operation took 0.5001382827758789 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    170 |          4350.36 | 27504640 |  284.862 |              327.253 |              138.768 |            766.969 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3034.6238603513452
    time_step_min: 2756
  date: 2020-10-15_10-42-42
  done: false
  episode_len_mean: 766.8994030265168
  episode_reward_max: 327.25252525252506
  episode_reward_mean: 285.08937774243884
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 36015
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5878581995775298e-28
        cur_lr: 5.0e-05
        entropy: 0.07942072302103043
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0062004284603366005
        total_loss: .inf
        vf_explained_var: 0.9977352619171143
        vf_loss: 0.7428334405024847
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.230000000000008
    gpu_util_percent0: 0.36500000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676373753947544
    mean_env_wait_ms: 1.221047691038004
    mean_inference_ms: 4.313684007032322
    mean_raw_obs_processing_ms: 0.37769197040335917
  time_since_restore: 4376.332949399948
  time_this_iter_s: 25.976774215698242
  time_total_s: 4376.332949399948
  timers:
    learn_throughput: 8658.672
    learn_time_ms: 18685.544
    sample_throughput: 23785.987
    sample_time_ms: 6801.988
    update_time_ms: 25.289
  timestamp: 1602758562
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 10:42:43,151	WARNING util.py:136 -- The `process_trial` operation took 0.5130853652954102 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    171 |          4376.33 | 27666432 |  285.089 |              327.253 |              138.768 |            766.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3033.4984646878197
    time_step_min: 2756
  date: 2020-10-15_10-43-08
  done: false
  episode_len_mean: 766.8511661324196
  episode_reward_max: 327.25252525252506
  episode_reward_mean: 285.2594612534095
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 173
  episodes_total: 36188
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8817872993662938e-28
        cur_lr: 5.0e-05
        entropy: 0.07686239356795947
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035498805421714983
        model: {}
        policy_loss: -0.006360175437293947
        total_loss: 0.49532413234313327
        vf_explained_var: 0.9980742335319519
        vf_loss: 0.5017227455973625
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.223333333333336
    gpu_util_percent0: 0.41733333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467598042127421
    mean_env_wait_ms: 1.2210003851312186
    mean_inference_ms: 4.313431521064057
    mean_raw_obs_processing_ms: 0.3776762633175303
  time_since_restore: 4401.745327472687
  time_this_iter_s: 25.412378072738647
  time_total_s: 4401.745327472687
  timers:
    learn_throughput: 8670.432
    learn_time_ms: 18660.201
    sample_throughput: 23800.086
    sample_time_ms: 6797.959
    update_time_ms: 26.966
  timestamp: 1602758588
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: cb791_00000
  
2020-10-15 10:43:09,244	WARNING util.py:136 -- The `process_trial` operation took 0.5113005638122559 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    172 |          4401.75 | 27828224 |  285.259 |              327.253 |              138.768 |            766.851 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3031.8753709198813
    time_step_min: 2756
  date: 2020-10-15_10-43-35
  done: false
  episode_len_mean: 766.7791958281872
  episode_reward_max: 327.2525252525251
  episode_reward_mean: 285.50498396896086
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 247
  episodes_total: 36435
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9408936496831469e-28
        cur_lr: 5.0e-05
        entropy: 0.08353711975117524
        entropy_coeff: 0.0005000000000000001
        kl: 0.004961777168015639
        model: {}
        policy_loss: -0.006370469704658414
        total_loss: 0.8331981599330902
        vf_explained_var: 0.9976368546485901
        vf_loss: 0.8396103779474894
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.213333333333335
    gpu_util_percent0: 0.31299999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675360359201176
    mean_env_wait_ms: 1.220935444200019
    mean_inference_ms: 4.313078196773141
    mean_raw_obs_processing_ms: 0.3776524378948257
  time_since_restore: 4427.582865476608
  time_this_iter_s: 25.83753800392151
  time_total_s: 4427.582865476608
  timers:
    learn_throughput: 8672.886
    learn_time_ms: 18654.921
    sample_throughput: 23780.215
    sample_time_ms: 6803.639
    update_time_ms: 28.733
  timestamp: 1602758615
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: cb791_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    173 |          4427.58 | 27990016 |  285.505 |              327.253 |              138.768 |            766.779 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3030.536871142186
    time_step_min: 2756
  date: 2020-10-15_10-44-01
  done: false
  episode_len_mean: 766.7240062205003
  episode_reward_max: 327.2525252525251
  episode_reward_mean: 285.70334039106024
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 218
  episodes_total: 36653
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.704468248415734e-29
        cur_lr: 5.0e-05
        entropy: 0.08103619515895844
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007676887459335073
        total_loss: .inf
        vf_explained_var: 0.9964308142662048
        vf_loss: 1.1436338027318318
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17666666666667
    gpu_util_percent0: 0.3213333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674904332215535
    mean_env_wait_ms: 1.2208724987183972
    mean_inference_ms: 4.312764933950751
    mean_raw_obs_processing_ms: 0.37763391236661553
  time_since_restore: 4453.163977861404
  time_this_iter_s: 25.581112384796143
  time_total_s: 4453.163977861404
  timers:
    learn_throughput: 8682.306
    learn_time_ms: 18634.68
    sample_throughput: 23782.387
    sample_time_ms: 6803.018
    update_time_ms: 29.812
  timestamp: 1602758641
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 10:44:02,028	WARNING util.py:136 -- The `process_trial` operation took 0.5183637142181396 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    174 |          4453.16 | 28151808 |  285.703 |              327.253 |              138.768 |            766.724 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3029.5737664809026
    time_step_min: 2756
  date: 2020-10-15_10-44-27
  done: false
  episode_len_mean: 766.6752118183794
  episode_reward_max: 327.2525252525251
  episode_reward_mean: 285.85630967506904
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 171
  episodes_total: 36824
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4556702372623603e-28
        cur_lr: 5.0e-05
        entropy: 0.07726180491348107
        entropy_coeff: 0.0005000000000000001
        kl: 0.003173594770487398
        model: {}
        policy_loss: -0.008127809029247146
        total_loss: 0.8771662414073944
        vf_explained_var: 0.9967873096466064
        vf_loss: 0.8853326737880707
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.660000000000007
    gpu_util_percent0: 0.3273333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467451367783616
    mean_env_wait_ms: 1.220825076200739
    mean_inference_ms: 4.312521812907145
    mean_raw_obs_processing_ms: 0.3776182619563191
  time_since_restore: 4478.74822473526
  time_this_iter_s: 25.58424687385559
  time_total_s: 4478.74822473526
  timers:
    learn_throughput: 8689.174
    learn_time_ms: 18619.953
    sample_throughput: 23789.272
    sample_time_ms: 6801.049
    update_time_ms: 29.93
  timestamp: 1602758667
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: cb791_00000
  
2020-10-15 10:44:28,298	WARNING util.py:136 -- The `process_trial` operation took 0.5126104354858398 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    175 |          4478.75 | 28313600 |  285.856 |              327.253 |              138.768 |            766.675 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3027.906065268443
    time_step_min: 2756
  date: 2020-10-15_10-44-53
  done: false
  episode_len_mean: 766.5963975624225
  episode_reward_max: 327.2525252525251
  episode_reward_mean: 286.1080723102239
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 262
  episodes_total: 37086
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.278351186311802e-29
        cur_lr: 5.0e-05
        entropy: 0.08202212676405907
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006639929664136919
        total_loss: .inf
        vf_explained_var: 0.9975475668907166
        vf_loss: 0.8458654334147772
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53666666666667
    gpu_util_percent0: 0.3223333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673903178644487
    mean_env_wait_ms: 1.2207520912282404
    mean_inference_ms: 4.312150591453856
    mean_raw_obs_processing_ms: 0.3775941824971
  time_since_restore: 4504.298641681671
  time_this_iter_s: 25.550416946411133
  time_total_s: 4504.298641681671
  timers:
    learn_throughput: 8693.146
    learn_time_ms: 18611.444
    sample_throughput: 23769.337
    sample_time_ms: 6806.753
    update_time_ms: 30.197
  timestamp: 1602758693
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 10:44:54,558	WARNING util.py:136 -- The `process_trial` operation took 0.5339479446411133 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    176 |           4504.3 | 28475392 |  286.108 |              327.253 |              138.768 |            766.596 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3026.744174183849
    time_step_min: 2756
  date: 2020-10-15_10-45-20
  done: false
  episode_len_mean: 766.5405100973529
  episode_reward_max: 327.2525252525251
  episode_reward_mean: 286.26951549447307
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 37287
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0917526779467704e-28
        cur_lr: 5.0e-05
        entropy: 0.09421898362537225
        entropy_coeff: 0.0005000000000000001
        kl: 0.004396985789450507
        model: {}
        policy_loss: -0.009274958865717053
        total_loss: 1.06871797144413
        vf_explained_var: 0.9962894916534424
        vf_loss: 1.0780400683482487
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0
    gpu_util_percent0: 0.3466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673507532701444
    mean_env_wait_ms: 1.2206954000249117
    mean_inference_ms: 4.311886069584802
    mean_raw_obs_processing_ms: 0.3775781959863831
  time_since_restore: 4529.773644685745
  time_this_iter_s: 25.475003004074097
  time_total_s: 4529.773644685745
  timers:
    learn_throughput: 8710.194
    learn_time_ms: 18575.017
    sample_throughput: 23784.087
    sample_time_ms: 6802.531
    update_time_ms: 32.226
  timestamp: 1602758720
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: cb791_00000
  
2020-10-15 10:45:20,709	WARNING util.py:136 -- The `process_trial` operation took 0.5016088485717773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    177 |          4529.77 | 28637184 |   286.27 |              327.253 |              138.768 |            766.541 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3026.057651207523
    time_step_min: 2756
  date: 2020-10-15_10-45-46
  done: false
  episode_len_mean: 766.5020682661258
  episode_reward_max: 327.2525252525251
  episode_reward_mean: 286.3579587069218
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 184
  episodes_total: 37471
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.458763389733852e-29
        cur_lr: 5.0e-05
        entropy: 0.114405183121562
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00948937328212196
        total_loss: .inf
        vf_explained_var: 0.9917685985565186
        vf_loss: 2.6849155028661094
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60333333333334
    gpu_util_percent0: 0.30366666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467310259521434
    mean_env_wait_ms: 1.2206439199653563
    mean_inference_ms: 4.311632784035764
    mean_raw_obs_processing_ms: 0.3775619627798172
  time_since_restore: 4555.5643658638
  time_this_iter_s: 25.79072117805481
  time_total_s: 4555.5643658638
  timers:
    learn_throughput: 8700.682
    learn_time_ms: 18595.325
    sample_throughput: 23815.066
    sample_time_ms: 6793.683
    update_time_ms: 34.163
  timestamp: 1602758746
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 10:45:47,194	WARNING util.py:136 -- The `process_trial` operation took 0.5136764049530029 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    178 |          4555.56 | 28798976 |  286.358 |              327.253 |              138.768 |            766.502 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3024.852996577614
    time_step_min: 2756
  date: 2020-10-15_10-46-13
  done: false
  episode_len_mean: 766.4294233011767
  episode_reward_max: 327.25252525252523
  episode_reward_mean: 286.5482640997057
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 261
  episodes_total: 37732
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.188145084600776e-29
        cur_lr: 5.0e-05
        entropy: 0.08328358208139737
        entropy_coeff: 0.0005000000000000001
        kl: 0.003505716643606623
        model: {}
        policy_loss: -0.008036039750246951
        total_loss: 1.7896802723407745
        vf_explained_var: 0.9952108263969421
        vf_loss: 1.7977579832077026
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.38709677419355
    gpu_util_percent0: 0.37709677419354837
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672530348402676
    mean_env_wait_ms: 1.2205707688972538
    mean_inference_ms: 4.311279115405864
    mean_raw_obs_processing_ms: 0.3775391878465404
  time_since_restore: 4581.4991106987
  time_this_iter_s: 25.934744834899902
  time_total_s: 4581.4991106987
  timers:
    learn_throughput: 8681.099
    learn_time_ms: 18637.271
    sample_throughput: 23784.367
    sample_time_ms: 6802.451
    update_time_ms: 34.366
  timestamp: 1602758773
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: cb791_00000
  
2020-10-15 10:46:13,984	WARNING util.py:136 -- The `process_trial` operation took 0.519538402557373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    179 |           4581.5 | 28960768 |  286.548 |              327.253 |              138.768 |            766.429 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3023.7330517423443
    time_step_min: 2756
  date: 2020-10-15_10-46-39
  done: false
  episode_len_mean: 766.3728210132124
  episode_reward_max: 327.25252525252523
  episode_reward_mean: 286.7145363815106
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 187
  episodes_total: 37919
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.094072542300388e-29
        cur_lr: 5.0e-05
        entropy: 0.06994740044077237
        entropy_coeff: 0.0005000000000000001
        kl: 0.004487611081761618
        model: {}
        policy_loss: -0.009470570143700266
        total_loss: 0.8419470489025116
        vf_explained_var: 0.9968921542167664
        vf_loss: 0.8514525641997656
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.216666666666672
    gpu_util_percent0: 0.3789999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672155380570423
    mean_env_wait_ms: 1.220517967798765
    mean_inference_ms: 4.31103426366384
    mean_raw_obs_processing_ms: 0.377523812727829
  time_since_restore: 4607.199001550674
  time_this_iter_s: 25.699890851974487
  time_total_s: 4607.199001550674
  timers:
    learn_throughput: 8687.112
    learn_time_ms: 18624.371
    sample_throughput: 23809.499
    sample_time_ms: 6795.271
    update_time_ms: 35.808
  timestamp: 1602758799
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: cb791_00000
  
2020-10-15 10:46:40,493	WARNING util.py:136 -- The `process_trial` operation took 0.5827138423919678 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    180 |           4607.2 | 29122560 |  286.715 |              327.253 |              138.768 |            766.373 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3022.4950372859994
    time_step_min: 2756
  date: 2020-10-15_10-47-06
  done: false
  episode_len_mean: 766.311937675419
  episode_reward_max: 327.25252525252523
  episode_reward_mean: 286.90486270251756
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 204
  episodes_total: 38123
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.047036271150194e-29
        cur_lr: 5.0e-05
        entropy: 0.07407057161132495
        entropy_coeff: 0.0005000000000000001
        kl: 0.003962598954482625
        model: {}
        policy_loss: -0.0069495004912217455
        total_loss: 0.5926105231046677
        vf_explained_var: 0.997894823551178
        vf_loss: 0.5995970616738001
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490000000000002
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671735518472723
    mean_env_wait_ms: 1.220461597257268
    mean_inference_ms: 4.310773589412746
    mean_raw_obs_processing_ms: 0.3775071749642595
  time_since_restore: 4633.072237968445
  time_this_iter_s: 25.873236417770386
  time_total_s: 4633.072237968445
  timers:
    learn_throughput: 8692.409
    learn_time_ms: 18613.022
    sample_throughput: 23805.693
    sample_time_ms: 6796.357
    update_time_ms: 36.368
  timestamp: 1602758826
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: cb791_00000
  
2020-10-15 10:47:07,061	WARNING util.py:136 -- The `process_trial` operation took 0.5092830657958984 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    181 |          4633.07 | 29284352 |  286.905 |              327.253 |              138.768 |            766.312 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3020.9337732797744
    time_step_min: 2756
  date: 2020-10-15_10-47-32
  done: false
  episode_len_mean: 766.2358965005081
  episode_reward_max: 327.2525252525253
  episode_reward_mean: 287.14241352998937
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 254
  episodes_total: 38377
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.023518135575097e-29
        cur_lr: 5.0e-05
        entropy: 0.0708690956234932
        entropy_coeff: 0.0005000000000000001
        kl: 0.0029709960411613188
        model: {}
        policy_loss: -0.007767587613974077
        total_loss: 0.41888130207856494
        vf_explained_var: 0.9987284541130066
        vf_loss: 0.4266843299070994
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.816666666666666
    gpu_util_percent0: 0.31266666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146711844786713
    mean_env_wait_ms: 1.2203875085843792
    mean_inference_ms: 4.31042832193897
    mean_raw_obs_processing_ms: 0.3774842182953784
  time_since_restore: 4658.998619556427
  time_this_iter_s: 25.926381587982178
  time_total_s: 4658.998619556427
  timers:
    learn_throughput: 8678.235
    learn_time_ms: 18643.422
    sample_throughput: 23725.084
    sample_time_ms: 6819.449
    update_time_ms: 35.879
  timestamp: 1602758852
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: cb791_00000
  
2020-10-15 10:47:33,685	WARNING util.py:136 -- The `process_trial` operation took 0.5166358947753906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    182 |             4659 | 29446144 |  287.142 |              327.253 |              138.768 |            766.236 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3019.875188243236
    time_step_min: 2756
  date: 2020-10-15_10-47-59
  done: false
  episode_len_mean: 766.1821907503955
  episode_reward_max: 327.25252525252534
  episode_reward_mean: 287.30742016696405
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 176
  episodes_total: 38553
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.117590677875485e-30
        cur_lr: 5.0e-05
        entropy: 0.06284689375509818
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036459980377306542
        model: {}
        policy_loss: -0.007903100457042456
        total_loss: 0.3889431258042653
        vf_explained_var: 0.9984272122383118
        vf_loss: 0.396877646446228
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.856666666666666
    gpu_util_percent0: 0.369
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670853348910678
    mean_env_wait_ms: 1.2203370717032553
    mean_inference_ms: 4.310206069155239
    mean_raw_obs_processing_ms: 0.3774705820806895
  time_since_restore: 4684.545927762985
  time_this_iter_s: 25.547308206558228
  time_total_s: 4684.545927762985
  timers:
    learn_throughput: 8689.82
    learn_time_ms: 18618.568
    sample_throughput: 23734.843
    sample_time_ms: 6816.645
    update_time_ms: 35.886
  timestamp: 1602758879
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: cb791_00000
  
2020-10-15 10:47:59,929	WARNING util.py:136 -- The `process_trial` operation took 0.514934778213501 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    183 |          4684.55 | 29607936 |  287.307 |              327.253 |              138.768 |            766.182 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3018.4933519221336
    time_step_min: 2756
  date: 2020-10-15_10-48-25
  done: false
  episode_len_mean: 766.1198287423914
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 287.51181108516283
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 219
  episodes_total: 38772
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5587953389377425e-30
        cur_lr: 5.0e-05
        entropy: 0.07026164668301742
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035429392203999064
        model: {}
        policy_loss: -0.006088022967257227
        total_loss: 0.58597465356191
        vf_explained_var: 0.9980202317237854
        vf_loss: 0.5920978089173635
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.689999999999998
    gpu_util_percent0: 0.36466666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467041820655247
    mean_env_wait_ms: 1.2202775221155444
    mean_inference_ms: 4.309942142578253
    mean_raw_obs_processing_ms: 0.3774522309837452
  time_since_restore: 4710.1648478508
  time_this_iter_s: 25.61892008781433
  time_total_s: 4710.1648478508
  timers:
    learn_throughput: 8689.946
    learn_time_ms: 18618.298
    sample_throughput: 23713.027
    sample_time_ms: 6822.916
    update_time_ms: 34.773
  timestamp: 1602758905
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: cb791_00000
  
2020-10-15 10:48:26,251	WARNING util.py:136 -- The `process_trial` operation took 0.5189430713653564 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    184 |          4710.16 | 29769728 |  287.512 |              327.253 |              138.768 |             766.12 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3017.0518011083745
    time_step_min: 2756
  date: 2020-10-15_10-48-52
  done: false
  episode_len_mean: 766.0559015763168
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 287.7276494277647
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 243
  episodes_total: 39015
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2793976694688712e-30
        cur_lr: 5.0e-05
        entropy: 0.06790498271584511
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038964179499695697
        model: {}
        policy_loss: -0.0055027159008507924
        total_loss: 0.6306971063216528
        vf_explained_var: 0.9980533123016357
        vf_loss: 0.6362337718407313
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.463333333333335
    gpu_util_percent0: 0.2923333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669924140980006
    mean_env_wait_ms: 1.2202032854389413
    mean_inference_ms: 4.309617814592994
    mean_raw_obs_processing_ms: 0.37743221401476396
  time_since_restore: 4736.026826620102
  time_this_iter_s: 25.861978769302368
  time_total_s: 4736.026826620102
  timers:
    learn_throughput: 8676.912
    learn_time_ms: 18646.266
    sample_throughput: 23713.256
    sample_time_ms: 6822.85
    update_time_ms: 34.198
  timestamp: 1602758932
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: cb791_00000
  
2020-10-15 10:48:52,952	WARNING util.py:136 -- The `process_trial` operation took 0.5578656196594238 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    185 |          4736.03 | 29931520 |  287.728 |              327.253 |              138.768 |            766.056 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3016.016884211602
    time_step_min: 2756
  date: 2020-10-15_10-49-18
  done: false
  episode_len_mean: 766.0085230172501
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 287.88710804070087
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 173
  episodes_total: 39188
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.396988347344356e-31
        cur_lr: 5.0e-05
        entropy: 0.06338527395079534
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031924977471741536
        model: {}
        policy_loss: -0.00507054395469216
        total_loss: 0.46230100840330124
        vf_explained_var: 0.9981231689453125
        vf_loss: 0.46740324298540753
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.976666666666667
    gpu_util_percent0: 0.36666666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669580866977613
    mean_env_wait_ms: 1.2201544419315733
    mean_inference_ms: 4.309409968199202
    mean_raw_obs_processing_ms: 0.3774192457072136
  time_since_restore: 4761.417981624603
  time_this_iter_s: 25.391155004501343
  time_total_s: 4761.417981624603
  timers:
    learn_throughput: 8684.225
    learn_time_ms: 18630.563
    sample_throughput: 23715.792
    sample_time_ms: 6822.121
    update_time_ms: 33.392
  timestamp: 1602758958
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: cb791_00000
  
2020-10-15 10:49:19,064	WARNING util.py:136 -- The `process_trial` operation took 0.531989574432373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    186 |          4761.42 | 30093312 |  287.887 |              327.253 |              138.768 |            766.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3014.6253078426894
    time_step_min: 2756
  date: 2020-10-15_10-49-44
  done: false
  episode_len_mean: 765.9488915943793
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 288.0950049882479
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 238
  episodes_total: 39426
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.198494173672178e-31
        cur_lr: 5.0e-05
        entropy: 0.07486003264784813
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007429530878046838
        total_loss: .inf
        vf_explained_var: 0.9975438714027405
        vf_loss: 0.7983326663573583
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.379999999999995
    gpu_util_percent0: 0.36766666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669078602918575
    mean_env_wait_ms: 1.2200889791235123
    mean_inference_ms: 4.309115738037159
    mean_raw_obs_processing_ms: 0.37739825365000995
  time_since_restore: 4786.979684591293
  time_this_iter_s: 25.561702966690063
  time_total_s: 4786.979684591293
  timers:
    learn_throughput: 8682.853
    learn_time_ms: 18633.508
    sample_throughput: 23693.679
    sample_time_ms: 6828.488
    update_time_ms: 31.771
  timestamp: 1602758984
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 10:49:45,366	WARNING util.py:136 -- The `process_trial` operation took 0.5535740852355957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    187 |          4786.98 | 30255104 |  288.095 |              327.253 |              138.768 |            765.949 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3013.371815909722
    time_step_min: 2756
  date: 2020-10-15_10-50-11
  done: false
  episode_len_mean: 765.8927616645649
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 288.29083011705956
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 224
  episodes_total: 39650
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.797741260508268e-31
        cur_lr: 5.0e-05
        entropy: 0.06382909572372834
        entropy_coeff: 0.0005000000000000001
        kl: 0.004115981030433129
        model: {}
        policy_loss: -0.0067630681151058525
        total_loss: 0.577077197531859
        vf_explained_var: 0.9980366826057434
        vf_loss: 0.5838721791903178
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.476666666666667
    gpu_util_percent0: 0.36466666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466866504671859
    mean_env_wait_ms: 1.2200192574739126
    mean_inference_ms: 4.308839698311538
    mean_raw_obs_processing_ms: 0.37738184749727655
  time_since_restore: 4812.725609064102
  time_this_iter_s: 25.745924472808838
  time_total_s: 4812.725609064102
  timers:
    learn_throughput: 8685.104
    learn_time_ms: 18628.678
    sample_throughput: 23696.851
    sample_time_ms: 6827.574
    update_time_ms: 31.517
  timestamp: 1602759011
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: cb791_00000
  
2020-10-15 10:50:11,830	WARNING util.py:136 -- The `process_trial` operation took 0.5226137638092041 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    188 |          4812.73 | 30416896 |  288.291 |              327.253 |              138.768 |            765.893 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3012.3577981651374
    time_step_min: 2756
  date: 2020-10-15_10-50-37
  done: false
  episode_len_mean: 765.8480313378867
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 288.4458592554714
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 174
  episodes_total: 39824
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.398870630254134e-31
        cur_lr: 5.0e-05
        entropy: 0.06672611335913341
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031197991920635104
        model: {}
        policy_loss: -0.006127858153225437
        total_loss: 0.49073829501867294
        vf_explained_var: 0.997971773147583
        vf_loss: 0.4968995253245036
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34666666666667
    gpu_util_percent0: 0.36000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668334335399608
    mean_env_wait_ms: 1.2199687834184725
    mean_inference_ms: 4.308630855598303
    mean_raw_obs_processing_ms: 0.377368554609187
  time_since_restore: 4838.42396903038
  time_this_iter_s: 25.698359966278076
  time_total_s: 4838.42396903038
  timers:
    learn_throughput: 8692.161
    learn_time_ms: 18613.554
    sample_throughput: 23724.226
    sample_time_ms: 6819.696
    update_time_ms: 29.757
  timestamp: 1602759037
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: cb791_00000
  
2020-10-15 10:50:38,284	WARNING util.py:136 -- The `process_trial` operation took 0.5593235492706299 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    189 |          4838.42 | 30578688 |  288.446 |              327.253 |              138.768 |            765.848 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3010.9506243756246
    time_step_min: 2756
  date: 2020-10-15_10-51-04
  done: false
  episode_len_mean: 765.7918111729334
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 288.659534288467
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 255
  episodes_total: 40079
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.199435315127067e-31
        cur_lr: 5.0e-05
        entropy: 0.07794370812674363
        entropy_coeff: 0.0005000000000000001
        kl: 0.003035459880872319
        model: {}
        policy_loss: -0.00857597811652037
        total_loss: 0.6496935586134592
        vf_explained_var: 0.9980555176734924
        vf_loss: 0.6583084960778555
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.67
    gpu_util_percent0: 0.32999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667812620376722
    mean_env_wait_ms: 1.2198960116453588
    mean_inference_ms: 4.308318262715163
    mean_raw_obs_processing_ms: 0.37734748911333443
  time_since_restore: 4864.1851851940155
  time_this_iter_s: 25.761216163635254
  time_total_s: 4864.1851851940155
  timers:
    learn_throughput: 8688.631
    learn_time_ms: 18621.115
    sample_throughput: 23697.92
    sample_time_ms: 6827.266
    update_time_ms: 28.232
  timestamp: 1602759064
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: cb791_00000
  
2020-10-15 10:51:04,766	WARNING util.py:136 -- The `process_trial` operation took 0.5228142738342285 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    190 |          4864.19 | 30740480 |   288.66 |              327.253 |              138.768 |            765.792 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3009.8241242236027
    time_step_min: 2756
  date: 2020-10-15_10-51-30
  done: false
  episode_len_mean: 765.7459107945097
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 288.8318043549495
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 210
  episodes_total: 40289
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.997176575635335e-32
        cur_lr: 5.0e-05
        entropy: 0.06393057263145845
        entropy_coeff: 0.0005000000000000001
        kl: 0.003599466484350463
        model: {}
        policy_loss: -0.008970730056413837
        total_loss: 0.5971894413232803
        vf_explained_var: 0.9979524612426758
        vf_loss: 0.6061921343207359
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.561290322580653
    gpu_util_percent0: 0.3116129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667441669213363
    mean_env_wait_ms: 1.2198328312128512
    mean_inference_ms: 4.308081813868668
    mean_raw_obs_processing_ms: 0.3773330845483166
  time_since_restore: 4889.97735786438
  time_this_iter_s: 25.79217267036438
  time_total_s: 4889.97735786438
  timers:
    learn_throughput: 8690.755
    learn_time_ms: 18616.563
    sample_throughput: 23722.767
    sample_time_ms: 6820.115
    update_time_ms: 29.985
  timestamp: 1602759090
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: cb791_00000
  
2020-10-15 10:51:31,452	WARNING util.py:136 -- The `process_trial` operation took 0.525933027267456 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    191 |          4889.98 | 30902272 |  288.832 |              327.253 |              138.768 |            765.746 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3008.8331560590696
    time_step_min: 2756
  date: 2020-10-15_10-51-57
  done: false
  episode_len_mean: 765.7059753867444
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 288.9830464981949
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 177
  episodes_total: 40466
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9985882878176673e-32
        cur_lr: 5.0e-05
        entropy: 0.06738592311739922
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0073330585679893074
        total_loss: .inf
        vf_explained_var: 0.9984531402587891
        vf_loss: 0.40854734430710477
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.433333333333337
    gpu_util_percent0: 0.32533333333333325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466710327117411
    mean_env_wait_ms: 1.2197803716301583
    mean_inference_ms: 4.307871175787574
    mean_raw_obs_processing_ms: 0.37731922647983324
  time_since_restore: 4915.803385734558
  time_this_iter_s: 25.826027870178223
  time_total_s: 4915.803385734558
  timers:
    learn_throughput: 8688.059
    learn_time_ms: 18622.341
    sample_throughput: 23780.465
    sample_time_ms: 6803.568
    update_time_ms: 29.41
  timestamp: 1602759117
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 10:51:58,051	WARNING util.py:136 -- The `process_trial` operation took 0.5709235668182373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    192 |           4915.8 | 31064064 |  288.983 |              327.253 |              138.768 |            765.706 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3007.3746773836738
    time_step_min: 2756
  date: 2020-10-15_10-52-23
  done: false
  episode_len_mean: 765.6509012327489
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 289.20045601141805
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 256
  episodes_total: 40722
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.497882431726502e-32
        cur_lr: 5.0e-05
        entropy: 0.0729480969409148
        entropy_coeff: 0.0005000000000000001
        kl: 0.004264651661893974
        model: {}
        policy_loss: -0.007625874655786902
        total_loss: 0.5937003741661707
        vf_explained_var: 0.9982103705406189
        vf_loss: 0.6013627350330353
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.473333333333333
    gpu_util_percent0: 0.31866666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666605395155305
    mean_env_wait_ms: 1.219705408857717
    mean_inference_ms: 4.3075671389326295
    mean_raw_obs_processing_ms: 0.3772995066625769
  time_since_restore: 4941.685228586197
  time_this_iter_s: 25.881842851638794
  time_total_s: 4941.685228586197
  timers:
    learn_throughput: 8673.843
    learn_time_ms: 18652.862
    sample_throughput: 23768.577
    sample_time_ms: 6806.971
    update_time_ms: 27.535
  timestamp: 1602759143
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: cb791_00000
  
2020-10-15 10:52:24,693	WARNING util.py:136 -- The `process_trial` operation took 0.568993330001831 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    193 |          4941.69 | 31225856 |    289.2 |              327.253 |              138.768 |            765.651 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3006.2446553495424
    time_step_min: 2756
  date: 2020-10-15_10-52-50
  done: false
  episode_len_mean: 765.6107622003373
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 289.37303387482996
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 40921
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.248941215863251e-32
        cur_lr: 5.0e-05
        entropy: 0.06197920193274816
        entropy_coeff: 0.0005000000000000001
        kl: 0.003416899940930307
        model: {}
        policy_loss: -0.00756358050906177
        total_loss: 0.2732717767357826
        vf_explained_var: 0.9989109635353088
        vf_loss: 0.2808663435280323
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.040000000000006
    gpu_util_percent0: 0.3723333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666278318524006
    mean_env_wait_ms: 1.2196451428200914
    mean_inference_ms: 4.30735443443739
    mean_raw_obs_processing_ms: 0.37728571064846883
  time_since_restore: 4967.277866125107
  time_this_iter_s: 25.592637538909912
  time_total_s: 4967.277866125107
  timers:
    learn_throughput: 8671.624
    learn_time_ms: 18657.636
    sample_throughput: 23796.764
    sample_time_ms: 6798.908
    update_time_ms: 27.335
  timestamp: 1602759170
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: cb791_00000
  
2020-10-15 10:52:51,042	WARNING util.py:136 -- The `process_trial` operation took 0.5600833892822266 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    194 |          4967.28 | 31387648 |  289.373 |              327.253 |              138.768 |            765.611 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3005.159086482275
    time_step_min: 2756
  date: 2020-10-15_10-53-16
  done: false
  episode_len_mean: 765.5679015348691
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 289.53398350708056
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 41111
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1244706079316255e-32
        cur_lr: 5.0e-05
        entropy: 0.06991863499085109
        entropy_coeff: 0.0005000000000000001
        kl: 0.003769861359614879
        model: {}
        policy_loss: -0.007915193079194674
        total_loss: 0.3973678300778071
        vf_explained_var: 0.9984781742095947
        vf_loss: 0.4053179870049159
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.577419354838707
    gpu_util_percent0: 0.3648387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466592954741953
    mean_env_wait_ms: 1.2195891011448974
    mean_inference_ms: 4.307144667153826
    mean_raw_obs_processing_ms: 0.3772723400163745
  time_since_restore: 4993.15651845932
  time_this_iter_s: 25.878652334213257
  time_total_s: 4993.15651845932
  timers:
    learn_throughput: 8669.362
    learn_time_ms: 18662.504
    sample_throughput: 23808.671
    sample_time_ms: 6795.507
    update_time_ms: 25.712
  timestamp: 1602759196
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: cb791_00000
  
2020-10-15 10:53:17,867	WARNING util.py:136 -- The `process_trial` operation took 0.5760068893432617 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    195 |          4993.16 | 31549440 |  289.534 |              327.253 |              138.768 |            765.568 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3003.7514938913755
    time_step_min: 2756
  date: 2020-10-15_10-53-43
  done: false
  episode_len_mean: 765.514332672693
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 289.7476620021454
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 263
  episodes_total: 41374
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.622353039658127e-33
        cur_lr: 5.0e-05
        entropy: 0.07158522742489974
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033751919982023537
        model: {}
        policy_loss: -0.007664059424617638
        total_loss: 0.5832248975833257
        vf_explained_var: 0.9982898235321045
        vf_loss: 0.5909247398376465
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.086666666666666
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665454759209545
    mean_env_wait_ms: 1.2195091657376333
    mean_inference_ms: 4.306843532142634
    mean_raw_obs_processing_ms: 0.3772514132551425
  time_since_restore: 5018.71443939209
  time_this_iter_s: 25.557920932769775
  time_total_s: 5018.71443939209
  timers:
    learn_throughput: 8662.512
    learn_time_ms: 18677.261
    sample_throughput: 23805.002
    sample_time_ms: 6796.555
    update_time_ms: 25.623
  timestamp: 1602759223
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: cb791_00000
  
2020-10-15 10:53:44,508	WARNING util.py:136 -- The `process_trial` operation took 0.6091625690460205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    196 |          5018.71 | 31711232 |  289.748 |              327.253 |              138.768 |            765.514 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3002.7938526762055
    time_step_min: 2756
  date: 2020-10-15_10-54-10
  done: false
  episode_len_mean: 765.4779197651193
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 289.89114887230534
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 179
  episodes_total: 41553
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8111765198290637e-33
        cur_lr: 5.0e-05
        entropy: 0.06239682280768951
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030955872498452663
        model: {}
        policy_loss: -0.00801596175491189
        total_loss: 0.5226655105749766
        vf_explained_var: 0.9979700446128845
        vf_loss: 0.5307126715779305
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.776666666666674
    gpu_util_percent0: 0.3463333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466514770140251
    mean_env_wait_ms: 1.219455419017957
    mean_inference_ms: 4.306650299280385
    mean_raw_obs_processing_ms: 0.37723950212570345
  time_since_restore: 5044.237140893936
  time_this_iter_s: 25.522701501846313
  time_total_s: 5044.237140893936
  timers:
    learn_throughput: 8661.509
    learn_time_ms: 18679.425
    sample_throughput: 23826.662
    sample_time_ms: 6790.376
    update_time_ms: 25.135
  timestamp: 1602759250
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: cb791_00000
  
2020-10-15 10:54:10,778	WARNING util.py:136 -- The `process_trial` operation took 0.552863359451294 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    197 |          5044.24 | 31873024 |  289.891 |              327.253 |              138.768 |            765.478 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3001.6849232981785
    time_step_min: 2756
  date: 2020-10-15_10-54-36
  done: false
  episode_len_mean: 765.4350439426231
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 290.06098171300863
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 206
  episodes_total: 41759
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4055882599145318e-33
        cur_lr: 5.0e-05
        entropy: 0.07131153345108032
        entropy_coeff: 0.0005000000000000001
        kl: 0.003249704469150553
        model: {}
        policy_loss: -0.008478549852346381
        total_loss: 0.4926060338815053
        vf_explained_var: 0.998266875743866
        vf_loss: 0.5011202221115431
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.52
    gpu_util_percent0: 0.33566666666666656
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146648080466627
    mean_env_wait_ms: 1.219395043872441
    mean_inference_ms: 4.306438232303118
    mean_raw_obs_processing_ms: 0.3772249792765511
  time_since_restore: 5069.861105442047
  time_this_iter_s: 25.623964548110962
  time_total_s: 5069.861105442047
  timers:
    learn_throughput: 8664.858
    learn_time_ms: 18672.204
    sample_throughput: 23841.799
    sample_time_ms: 6786.065
    update_time_ms: 23.719
  timestamp: 1602759276
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: cb791_00000
  
2020-10-15 10:54:37,155	WARNING util.py:136 -- The `process_trial` operation took 0.5502352714538574 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    198 |          5069.86 | 32034816 |  290.061 |              327.253 |              138.768 |            765.435 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3000.427274676578
    time_step_min: 2756
  date: 2020-10-15_10-55-03
  done: false
  episode_len_mean: 765.3951252023231
  episode_reward_max: 327.2525252525254
  episode_reward_mean: 290.24714583711994
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 253
  episodes_total: 42012
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.027941299572659e-34
        cur_lr: 5.0e-05
        entropy: 0.07513796351850033
        entropy_coeff: 0.0005000000000000001
        kl: 0.004521643548893432
        model: {}
        policy_loss: -0.0095182734997555
        total_loss: 1.1155704607566197
        vf_explained_var: 0.9969211220741272
        vf_loss: 1.1251263121763866
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583333333333332
    gpu_util_percent0: 0.39799999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664372726784364
    mean_env_wait_ms: 1.2193154515846794
    mean_inference_ms: 4.306146224440999
    mean_raw_obs_processing_ms: 0.3772056217707009
  time_since_restore: 5095.705607175827
  time_this_iter_s: 25.844501733779907
  time_total_s: 5095.705607175827
  timers:
    learn_throughput: 8658.5
    learn_time_ms: 18685.916
    sample_throughput: 23842.162
    sample_time_ms: 6785.962
    update_time_ms: 23.79
  timestamp: 1602759303
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: cb791_00000
  
2020-10-15 10:55:03,924	WARNING util.py:136 -- The `process_trial` operation took 0.6242384910583496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    199 |          5095.71 | 32196608 |  290.247 |              327.253 |              138.768 |            765.395 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2999.5487200170824
    time_step_min: 2756
  date: 2020-10-15_10-55-29
  done: false
  episode_len_mean: 765.3627808855599
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 290.38415323233266
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 176
  episodes_total: 42188
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5139706497863296e-34
        cur_lr: 5.0e-05
        entropy: 0.06629481352865696
        entropy_coeff: 0.0005000000000000001
        kl: 0.004307543199198942
        model: {}
        policy_loss: -0.00736634536103035
        total_loss: 0.5667182902495066
        vf_explained_var: 0.9977354407310486
        vf_loss: 0.5741177871823311
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01935483870968
    gpu_util_percent0: 0.31709677419354837
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664065429341955
    mean_env_wait_ms: 1.2192628803155043
    mean_inference_ms: 4.305964764588587
    mean_raw_obs_processing_ms: 0.37719431072625004
  time_since_restore: 5121.282998085022
  time_this_iter_s: 25.577390909194946
  time_total_s: 5121.282998085022
  timers:
    learn_throughput: 8664.528
    learn_time_ms: 18672.915
    sample_throughput: 23869.927
    sample_time_ms: 6778.068
    update_time_ms: 23.389
  timestamp: 1602759329
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: cb791_00000
  
2020-10-15 10:55:30,476	WARNING util.py:136 -- The `process_trial` operation took 0.613353967666626 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    200 |          5121.28 | 32358400 |  290.384 |              327.253 |              138.768 |            765.363 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2998.4452547853384
    time_step_min: 2756
  date: 2020-10-15_10-55-56
  done: false
  episode_len_mean: 765.3154829277495
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 290.5585338386695
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 220
  episodes_total: 42408
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7569853248931648e-34
        cur_lr: 5.0e-05
        entropy: 0.07716074958443642
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041891449557927745
        model: {}
        policy_loss: -0.00797088138582088
        total_loss: 0.44932149598995846
        vf_explained_var: 0.9984795451164246
        vf_loss: 0.45733096202214557
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.240000000000002
    gpu_util_percent0: 0.36533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663691743860835
    mean_env_wait_ms: 1.2191982927969607
    mean_inference_ms: 4.305740283991366
    mean_raw_obs_processing_ms: 0.3771781115322317
  time_since_restore: 5146.964600563049
  time_this_iter_s: 25.681602478027344
  time_total_s: 5146.964600563049
  timers:
    learn_throughput: 8671.69
    learn_time_ms: 18657.494
    sample_throughput: 23875.356
    sample_time_ms: 6776.527
    update_time_ms: 21.088
  timestamp: 1602759356
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: cb791_00000
  
2020-10-15 10:55:56,988	WARNING util.py:136 -- The `process_trial` operation took 0.5889406204223633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    201 |          5146.96 | 32520192 |  290.559 |              327.253 |              138.768 |            765.315 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2997.1909880309786
    time_step_min: 2756
  date: 2020-10-15_10-56-22
  done: false
  episode_len_mean: 765.2645079603274
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 290.74724288063385
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 241
  episodes_total: 42649
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.784926624465824e-35
        cur_lr: 5.0e-05
        entropy: 0.06735494049886863
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036274514665516713
        model: {}
        policy_loss: -0.006760470481822267
        total_loss: 0.408637801806132
        vf_explained_var: 0.9986672401428223
        vf_loss: 0.4154319614171982
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.423333333333336
    gpu_util_percent0: 0.33833333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663293435084726
    mean_env_wait_ms: 1.2191222561710942
    mean_inference_ms: 4.305479403775487
    mean_raw_obs_processing_ms: 0.3771618640713652
  time_since_restore: 5172.654480457306
  time_this_iter_s: 25.689879894256592
  time_total_s: 5172.654480457306
  timers:
    learn_throughput: 8676.141
    learn_time_ms: 18647.923
    sample_throughput: 23891.577
    sample_time_ms: 6771.926
    update_time_ms: 21.314
  timestamp: 1602759382
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: cb791_00000
  
2020-10-15 10:56:23,445	WARNING util.py:136 -- The `process_trial` operation took 0.5713000297546387 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    202 |          5172.65 | 32681984 |  290.747 |              327.253 |              138.768 |            765.265 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2996.2711464298236
    time_step_min: 2756
  date: 2020-10-15_10-56-49
  done: false
  episode_len_mean: 765.2270689333084
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 290.8875479529084
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 175
  episodes_total: 42824
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.392463312232912e-35
        cur_lr: 5.0e-05
        entropy: 0.06816097224752109
        entropy_coeff: 0.0005000000000000001
        kl: 0.015655967639759183
        model: {}
        policy_loss: -0.008134387571772095
        total_loss: 0.23214582105477652
        vf_explained_var: 0.9990302920341492
        vf_loss: 0.2403142862021923
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05
    gpu_util_percent0: 0.38566666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662999889143302
    mean_env_wait_ms: 1.2190686298921887
    mean_inference_ms: 4.305300010691503
    mean_raw_obs_processing_ms: 0.3771499528344917
  time_since_restore: 5198.291017770767
  time_this_iter_s: 25.636537313461304
  time_total_s: 5198.291017770767
  timers:
    learn_throughput: 8686.745
    learn_time_ms: 18625.158
    sample_throughput: 23905.186
    sample_time_ms: 6768.071
    update_time_ms: 21.795
  timestamp: 1602759409
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: cb791_00000
  
2020-10-15 10:56:49,930	WARNING util.py:136 -- The `process_trial` operation took 0.6361539363861084 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    203 |          5198.29 | 32843776 |  290.888 |              327.253 |              138.768 |            765.227 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2995.2705611604447
    time_step_min: 2756
  date: 2020-10-15_10-57-15
  done: false
  episode_len_mean: 765.1907703741551
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 291.02467084388707
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 43057
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.392463312232912e-35
        cur_lr: 5.0e-05
        entropy: 0.10400437812010448
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010433138251149407
        total_loss: .inf
        vf_explained_var: 0.9951229691505432
        vf_loss: 1.7242611348628998
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77
    gpu_util_percent0: 0.3336666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662585698802716
    mean_env_wait_ms: 1.2190011993410523
    mean_inference_ms: 4.305072233016752
    mean_raw_obs_processing_ms: 0.3771332980710548
  time_since_restore: 5223.856410980225
  time_this_iter_s: 25.565393209457397
  time_total_s: 5223.856410980225
  timers:
    learn_throughput: 8691.685
    learn_time_ms: 18614.571
    sample_throughput: 23880.482
    sample_time_ms: 6775.073
    update_time_ms: 21.654
  timestamp: 1602759435
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 10:57:16,282	WARNING util.py:136 -- The `process_trial` operation took 0.5820989608764648 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    204 |          5223.86 | 33005568 |  291.025 |              327.253 |              138.768 |            765.191 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2994.655250560784
    time_step_min: 2756
  date: 2020-10-15_10-57-41
  done: false
  episode_len_mean: 765.1682685642993
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 291.12468920058666
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 225
  episodes_total: 43282
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.588694968349366e-35
        cur_lr: 5.0e-05
        entropy: 0.0899576476464669
        entropy_coeff: 0.0005000000000000001
        kl: 0.004277675354387611
        model: {}
        policy_loss: -0.01140762918900388
        total_loss: 2.378506580988566
        vf_explained_var: 0.9935474991798401
        vf_loss: 2.3899592757225037
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.163333333333334
    gpu_util_percent0: 0.33333333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662244592056625
    mean_env_wait_ms: 1.218928898606917
    mean_inference_ms: 4.304833208742692
    mean_raw_obs_processing_ms: 0.37711845008375516
  time_since_restore: 5249.413504123688
  time_this_iter_s: 25.557093143463135
  time_total_s: 5249.413504123688
  timers:
    learn_throughput: 8710.787
    learn_time_ms: 18573.752
    sample_throughput: 23854.365
    sample_time_ms: 6782.49
    update_time_ms: 22.244
  timestamp: 1602759461
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: cb791_00000
  
2020-10-15 10:57:42,637	WARNING util.py:136 -- The `process_trial` operation took 0.5972206592559814 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    205 |          5249.41 | 33167360 |  291.125 |              327.253 |              138.768 |            765.168 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2994.005481218765
    time_step_min: 2756
  date: 2020-10-15_10-58-08
  done: false
  episode_len_mean: 765.1373907040958
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 291.230575427538
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 43460
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.294347484174683e-35
        cur_lr: 5.0e-05
        entropy: 0.0773713489373525
        entropy_coeff: 0.0005000000000000001
        kl: 0.003245993905390302
        model: {}
        policy_loss: -0.010338431360044828
        total_loss: 1.1410289804140727
        vf_explained_var: 0.995872437953949
        vf_loss: 1.1514061093330383
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.793333333333337
    gpu_util_percent0: 0.36899999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466194265596476
    mean_env_wait_ms: 1.2188758427709203
    mean_inference_ms: 4.304656849064873
    mean_raw_obs_processing_ms: 0.377106679962768
  time_since_restore: 5274.877269983292
  time_this_iter_s: 25.463765859603882
  time_total_s: 5274.877269983292
  timers:
    learn_throughput: 8719.128
    learn_time_ms: 18555.984
    sample_throughput: 23831.284
    sample_time_ms: 6789.059
    update_time_ms: 23.663
  timestamp: 1602759488
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: cb791_00000
  
2020-10-15 10:58:08,903	WARNING util.py:136 -- The `process_trial` operation took 0.5981795787811279 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    206 |          5274.88 | 33329152 |  291.231 |              327.253 |              138.768 |            765.137 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2992.815750303419
    time_step_min: 2756
  date: 2020-10-15_10-58-34
  done: false
  episode_len_mean: 765.0858195296056
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 291.41678961297777
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 248
  episodes_total: 43708
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6471737420873414e-35
        cur_lr: 5.0e-05
        entropy: 0.07426848448812962
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008717285498278216
        total_loss: .inf
        vf_explained_var: 0.9982645511627197
        vf_loss: 0.5556789487600327
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64666666666666
    gpu_util_percent0: 0.31833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466149767011326
    mean_env_wait_ms: 1.2188021608541488
    mean_inference_ms: 4.304408740868051
    mean_raw_obs_processing_ms: 0.3770892867768002
  time_since_restore: 5300.787461280823
  time_this_iter_s: 25.910191297531128
  time_total_s: 5300.787461280823
  timers:
    learn_throughput: 8706.799
    learn_time_ms: 18582.26
    sample_throughput: 23790.442
    sample_time_ms: 6800.714
    update_time_ms: 23.758
  timestamp: 1602759514
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 10:58:35,608	WARNING util.py:136 -- The `process_trial` operation took 0.5833089351654053 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    207 |          5300.79 | 33490944 |  291.417 |              327.253 |              138.768 |            765.086 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2991.761251509696
    time_step_min: 2756
  date: 2020-10-15_10-59-01
  done: false
  episode_len_mean: 765.0445334911889
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 291.57210762513336
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 214
  episodes_total: 43922
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4707606131310126e-35
        cur_lr: 5.0e-05
        entropy: 0.0652862327794234
        entropy_coeff: 0.0005000000000000001
        kl: 0.002864952567809572
        model: {}
        policy_loss: -0.00684005550283473
        total_loss: 0.9091760466496149
        vf_explained_var: 0.9968664646148682
        vf_loss: 0.9160487453142802
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23666666666667
    gpu_util_percent0: 0.36866666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466119643243775
    mean_env_wait_ms: 1.218735122046607
    mean_inference_ms: 4.304201100510026
    mean_raw_obs_processing_ms: 0.3770761904796149
  time_since_restore: 5326.477503061295
  time_this_iter_s: 25.6900417804718
  time_total_s: 5326.477503061295
  timers:
    learn_throughput: 8700.93
    learn_time_ms: 18594.793
    sample_throughput: 23815.742
    sample_time_ms: 6793.49
    update_time_ms: 23.918
  timestamp: 1602759541
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: cb791_00000
  
2020-10-15 10:59:02,199	WARNING util.py:136 -- The `process_trial` operation took 0.6245663166046143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    208 |          5326.48 | 33652736 |  291.572 |              327.253 |              138.768 |            765.045 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2990.884226055379
    time_step_min: 2756
  date: 2020-10-15_10-59-28
  done: false
  episode_len_mean: 765.0078686591532
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 291.7044814914833
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 177
  episodes_total: 44099
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2353803065655063e-35
        cur_lr: 5.0e-05
        entropy: 0.0663008497407039
        entropy_coeff: 0.0005000000000000001
        kl: 0.004795469537687798
        model: {}
        policy_loss: -0.007821273077449101
        total_loss: 0.40883205831050873
        vf_explained_var: 0.9983492493629456
        vf_loss: 0.41668647279342014
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.396774193548385
    gpu_util_percent0: 0.3196774193548388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660901356948425
    mean_env_wait_ms: 1.2186811832397444
    mean_inference_ms: 4.304028921365891
    mean_raw_obs_processing_ms: 0.37706413794271837
  time_since_restore: 5352.383903980255
  time_this_iter_s: 25.90640091896057
  time_total_s: 5352.383903980255
  timers:
    learn_throughput: 8702.846
    learn_time_ms: 18590.7
    sample_throughput: 23813.074
    sample_time_ms: 6794.251
    update_time_ms: 23.87
  timestamp: 1602759568
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: cb791_00000
  
2020-10-15 10:59:29,035	WARNING util.py:136 -- The `process_trial` operation took 0.640756368637085 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    209 |          5352.38 | 33814528 |  291.704 |              327.253 |              138.768 |            765.008 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2989.6557288257995
    time_step_min: 2756
  date: 2020-10-15_10-59-54
  done: false
  episode_len_mean: 764.9553551296505
  episode_reward_max: 327.2525252525256
  episode_reward_mean: 291.88859508273254
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 251
  episodes_total: 44350
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.176901532827531e-36
        cur_lr: 5.0e-05
        entropy: 0.07262366202970345
        entropy_coeff: 0.0005000000000000001
        kl: 0.004380441620014608
        model: {}
        policy_loss: -0.007513424362211178
        total_loss: 0.5699629882971445
        vf_explained_var: 0.9982748627662659
        vf_loss: 0.5775127212206522
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39
    gpu_util_percent0: 0.293
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660496595465183
    mean_env_wait_ms: 1.2186053578363605
    mean_inference_ms: 4.303778440013806
    mean_raw_obs_processing_ms: 0.37704694782089987
  time_since_restore: 5378.307574033737
  time_this_iter_s: 25.923670053482056
  time_total_s: 5378.307574033737
  timers:
    learn_throughput: 8693.689
    learn_time_ms: 18610.282
    sample_throughput: 23765.798
    sample_time_ms: 6807.766
    update_time_ms: 26.271
  timestamp: 1602759594
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: cb791_00000
  
2020-10-15 10:59:55,781	WARNING util.py:136 -- The `process_trial` operation took 0.6009531021118164 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    210 |          5378.31 | 33976320 |  291.889 |              327.253 |              138.768 |            764.955 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2988.6744316650193
    time_step_min: 2756
  date: 2020-10-15_11-00-21
  done: false
  episode_len_mean: 764.914151049265
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 292.03769033619767
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 205
  episodes_total: 44555
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.088450766413766e-36
        cur_lr: 5.0e-05
        entropy: 0.06571343479057153
        entropy_coeff: 0.0005000000000000001
        kl: 0.00408879896470656
        model: {}
        policy_loss: -0.00768096884106247
        total_loss: 0.3464731772740682
        vf_explained_var: 0.9986885190010071
        vf_loss: 0.3541869943340619
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.822580645161295
    gpu_util_percent0: 0.3109677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660200907510262
    mean_env_wait_ms: 1.218542005907343
    mean_inference_ms: 4.303595524978727
    mean_raw_obs_processing_ms: 0.3770351213020986
  time_since_restore: 5404.288407564163
  time_this_iter_s: 25.980833530426025
  time_total_s: 5404.288407564163
  timers:
    learn_throughput: 8687.209
    learn_time_ms: 18624.163
    sample_throughput: 23716.375
    sample_time_ms: 6821.953
    update_time_ms: 27.93
  timestamp: 1602759621
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: cb791_00000
  
2020-10-15 11:00:22,623	WARNING util.py:136 -- The `process_trial` operation took 0.5814828872680664 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    211 |          5404.29 | 34138112 |  292.038 |              327.707 |              138.768 |            764.914 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2987.785679133858
    time_step_min: 2756
  date: 2020-10-15_11-00-48
  done: false
  episode_len_mean: 764.8786402342266
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 292.1715984239506
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 188
  episodes_total: 44743
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.544225383206883e-36
        cur_lr: 5.0e-05
        entropy: 0.07137483544647694
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008187689954259744
        total_loss: .inf
        vf_explained_var: 0.9977461695671082
        vf_loss: 0.6325149933497111
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.146666666666672
    gpu_util_percent0: 0.3056666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659911085428376
    mean_env_wait_ms: 1.2184847299132635
    mean_inference_ms: 4.303419712472965
    mean_raw_obs_processing_ms: 0.3770234175500537
  time_since_restore: 5430.197345972061
  time_this_iter_s: 25.90893840789795
  time_total_s: 5430.197345972061
  timers:
    learn_throughput: 8683.527
    learn_time_ms: 18632.06
    sample_throughput: 23675.516
    sample_time_ms: 6833.726
    update_time_ms: 28.965
  timestamp: 1602759648
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:00:49,410	WARNING util.py:136 -- The `process_trial` operation took 0.663581371307373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    212 |           5430.2 | 34299904 |  292.172 |              327.707 |              138.768 |            764.879 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2986.602117296444
    time_step_min: 2756
  date: 2020-10-15_11-01-15
  done: false
  episode_len_mean: 764.8337407226345
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 292.352584778499
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 259
  episodes_total: 45002
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3163380748103245e-36
        cur_lr: 5.0e-05
        entropy: 0.07470043934881687
        entropy_coeff: 0.0005000000000000001
        kl: 0.006637521126928429
        model: {}
        policy_loss: -0.009294714715603428
        total_loss: 0.39034586151440936
        vf_explained_var: 0.9987933039665222
        vf_loss: 0.39967793971300125
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.409677419354836
    gpu_util_percent0: 0.31354838709677424
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465951159721079
    mean_env_wait_ms: 1.2184062872325883
    mean_inference_ms: 4.303169734168252
    mean_raw_obs_processing_ms: 0.3770062995148941
  time_since_restore: 5455.807767868042
  time_this_iter_s: 25.610421895980835
  time_total_s: 5455.807767868042
  timers:
    learn_throughput: 8683.17
    learn_time_ms: 18632.827
    sample_throughput: 23687.009
    sample_time_ms: 6830.411
    update_time_ms: 28.777
  timestamp: 1602759675
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: cb791_00000
  
2020-10-15 11:01:16,021	WARNING util.py:136 -- The `process_trial` operation took 0.6124136447906494 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    213 |          5455.81 | 34461696 |  292.353 |              327.707 |              138.768 |            764.834 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2985.767210064676
    time_step_min: 2756
  date: 2020-10-15_11-01-41
  done: false
  episode_len_mean: 764.8033062606502
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 292.4765102951527
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 185
  episodes_total: 45187
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3163380748103245e-36
        cur_lr: 5.0e-05
        entropy: 0.07225712264577548
        entropy_coeff: 0.0005000000000000001
        kl: 0.003969482611864805
        model: {}
        policy_loss: -0.009938354380816842
        total_loss: 0.5763102894028028
        vf_explained_var: 0.9978063106536865
        vf_loss: 0.5862847715616226
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116666666666667
    gpu_util_percent0: 0.3416666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659235592138598
    mean_env_wait_ms: 1.218348838245293
    mean_inference_ms: 4.303007341395744
    mean_raw_obs_processing_ms: 0.37699519294287165
  time_since_restore: 5481.501664876938
  time_this_iter_s: 25.693897008895874
  time_total_s: 5481.501664876938
  timers:
    learn_throughput: 8678.319
    learn_time_ms: 18643.241
    sample_throughput: 23718.791
    sample_time_ms: 6821.258
    update_time_ms: 31.095
  timestamp: 1602759701
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: cb791_00000
  
2020-10-15 11:01:42,560	WARNING util.py:136 -- The `process_trial` operation took 0.6361894607543945 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    214 |           5481.5 | 34623488 |  292.477 |              327.707 |              138.768 |            764.803 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2984.929664417692
    time_step_min: 2756
  date: 2020-10-15_11-02-08
  done: false
  episode_len_mean: 764.7664177296059
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 292.61238917494273
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 206
  episodes_total: 45393
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1581690374051622e-36
        cur_lr: 5.0e-05
        entropy: 0.07674248268206914
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030214605115664503
        model: {}
        policy_loss: -0.00821085298472705
        total_loss: 0.8297143230835596
        vf_explained_var: 0.9971761107444763
        vf_loss: 0.8379635463158289
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62333333333334
    gpu_util_percent0: 0.36566666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658945157901004
    mean_env_wait_ms: 1.2182882532310553
    mean_inference_ms: 4.302833267363386
    mean_raw_obs_processing_ms: 0.3769831411163469
  time_since_restore: 5507.19288277626
  time_this_iter_s: 25.69121789932251
  time_total_s: 5507.19288277626
  timers:
    learn_throughput: 8672.149
    learn_time_ms: 18656.507
    sample_throughput: 23723.293
    sample_time_ms: 6819.964
    update_time_ms: 30.846
  timestamp: 1602759728
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: cb791_00000
  
2020-10-15 11:02:09,087	WARNING util.py:136 -- The `process_trial` operation took 0.6234359741210938 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    215 |          5507.19 | 34785280 |  292.612 |              327.707 |              138.768 |            764.766 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2983.7849311463906
    time_step_min: 2756
  date: 2020-10-15_11-02-34
  done: false
  episode_len_mean: 764.7207238787985
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 292.7824415528772
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 45643
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.790845187025811e-37
        cur_lr: 5.0e-05
        entropy: 0.07163495073715846
        entropy_coeff: 0.0005000000000000001
        kl: 0.004217141288487862
        model: {}
        policy_loss: -0.0069398619525600225
        total_loss: 0.38480077932278317
        vf_explained_var: 0.998786449432373
        vf_loss: 0.3917764574289322
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.723333333333336
    gpu_util_percent0: 0.3123333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658556746013385
    mean_env_wait_ms: 1.2182086228934732
    mean_inference_ms: 4.302586029926275
    mean_raw_obs_processing_ms: 0.37696580544028324
  time_since_restore: 5532.9999923706055
  time_this_iter_s: 25.807109594345093
  time_total_s: 5532.9999923706055
  timers:
    learn_throughput: 8656.101
    learn_time_ms: 18691.094
    sample_throughput: 23730.735
    sample_time_ms: 6817.825
    update_time_ms: 31.756
  timestamp: 1602759754
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: cb791_00000
  
2020-10-15 11:02:35,792	WARNING util.py:136 -- The `process_trial` operation took 0.6188323497772217 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    216 |             5533 | 34947072 |  292.782 |              327.707 |              138.768 |            764.721 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2982.973834225183
    time_step_min: 2756
  date: 2020-10-15_11-03-01
  done: false
  episode_len_mean: 764.6885256634079
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 292.90947886688105
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 45824
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8954225935129056e-37
        cur_lr: 5.0e-05
        entropy: 0.06373652908951044
        entropy_coeff: 0.0005000000000000001
        kl: 0.004129771841689944
        model: {}
        policy_loss: -0.00668886643446361
        total_loss: 0.2524422990779082
        vf_explained_var: 0.9989380836486816
        vf_loss: 0.2591630245248477
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.983870967741936
    gpu_util_percent0: 0.30096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658302966716708
    mean_env_wait_ms: 1.2181535952725486
    mean_inference_ms: 4.302432985952302
    mean_raw_obs_processing_ms: 0.37695590455597067
  time_since_restore: 5558.693890094757
  time_this_iter_s: 25.69389772415161
  time_total_s: 5558.693890094757
  timers:
    learn_throughput: 8660.676
    learn_time_ms: 18681.22
    sample_throughput: 23781.409
    sample_time_ms: 6803.297
    update_time_ms: 33.623
  timestamp: 1602759781
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: cb791_00000
  
2020-10-15 11:03:02,486	WARNING util.py:136 -- The `process_trial` operation took 0.6171398162841797 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    217 |          5558.69 | 35108864 |  292.909 |              327.707 |              138.768 |            764.689 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2981.990478881812
    time_step_min: 2756
  date: 2020-10-15_11-03-28
  done: false
  episode_len_mean: 764.6511663263976
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 293.0542668771024
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 218
  episodes_total: 46042
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4477112967564528e-37
        cur_lr: 5.0e-05
        entropy: 0.07557442287604015
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033028496933790543
        model: {}
        policy_loss: -0.008350389369297773
        total_loss: 0.6848899573087692
        vf_explained_var: 0.9977160096168518
        vf_loss: 0.6932781289021174
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583333333333336
    gpu_util_percent0: 0.32699999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465798844540534
    mean_env_wait_ms: 1.218089420332481
    mean_inference_ms: 4.3022544438215595
    mean_raw_obs_processing_ms: 0.37694217317326856
  time_since_restore: 5584.583891630173
  time_this_iter_s: 25.89000153541565
  time_total_s: 5584.583891630173
  timers:
    learn_throughput: 8661.516
    learn_time_ms: 18679.41
    sample_throughput: 23716.116
    sample_time_ms: 6822.028
    update_time_ms: 35.338
  timestamp: 1602759808
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: cb791_00000
  
2020-10-15 11:03:29,213	WARNING util.py:136 -- The `process_trial` operation took 0.6175718307495117 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    218 |          5584.58 | 35270656 |  293.054 |              327.707 |              138.768 |            764.651 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2980.931992647854
    time_step_min: 2756
  date: 2020-10-15_11-03-54
  done: false
  episode_len_mean: 764.6119609368249
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 293.2172771269866
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 242
  episodes_total: 46284
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.238556483782264e-38
        cur_lr: 5.0e-05
        entropy: 0.0713088692476352
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009096704724167163
        total_loss: .inf
        vf_explained_var: 0.9985646605491638
        vf_loss: 0.45027278115351993
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.956666666666667
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657634992186966
    mean_env_wait_ms: 1.2180114788608793
    mean_inference_ms: 4.302022897312689
    mean_raw_obs_processing_ms: 0.37692722072770635
  time_since_restore: 5610.27795791626
  time_this_iter_s: 25.694066286087036
  time_total_s: 5610.27795791626
  timers:
    learn_throughput: 8672.408
    learn_time_ms: 18655.948
    sample_throughput: 23686.736
    sample_time_ms: 6830.489
    update_time_ms: 35.562
  timestamp: 1602759834
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:03:55,729	WARNING util.py:136 -- The `process_trial` operation took 0.6049797534942627 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    219 |          5610.28 | 35432448 |  293.217 |              327.707 |              138.768 |            764.612 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2980.1859838855617
    time_step_min: 2753
  date: 2020-10-15_11-04-21
  done: false
  episode_len_mean: 764.5840454613945
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 293.32992755546934
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 173
  episodes_total: 46457
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0857834725673393e-37
        cur_lr: 5.0e-05
        entropy: 0.07014155263702075
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008608978416305035
        total_loss: .inf
        vf_explained_var: 0.9983547329902649
        vf_loss: 0.41041197379430133
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73225806451613
    gpu_util_percent0: 0.3722580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657389061292173
    mean_env_wait_ms: 1.2179592404830244
    mean_inference_ms: 4.301876583850135
    mean_raw_obs_processing_ms: 0.37691732568329944
  time_since_restore: 5636.093876123428
  time_this_iter_s: 25.81591820716858
  time_total_s: 5636.093876123428
  timers:
    learn_throughput: 8671.629
    learn_time_ms: 18657.624
    sample_throughput: 23728.088
    sample_time_ms: 6818.586
    update_time_ms: 33.549
  timestamp: 1602759861
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:04:22,574	WARNING util.py:136 -- The `process_trial` operation took 0.6182897090911865 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    220 |          5636.09 | 35594240 |   293.33 |              327.707 |              138.768 |            764.584 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2979.1808886888302
    time_step_min: 2753
  date: 2020-10-15_11-04-48
  done: false
  episode_len_mean: 764.549751563437
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 293.4782170198514
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 235
  episodes_total: 46692
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6286752088510095e-37
        cur_lr: 5.0e-05
        entropy: 0.08103212714195251
        entropy_coeff: 0.0005000000000000001
        kl: 0.00375064976590996
        model: {}
        policy_loss: -0.009824815545774376
        total_loss: 0.802077571551005
        vf_explained_var: 0.9975123405456543
        vf_loss: 0.8119428902864456
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35666666666667
    gpu_util_percent0: 0.35100000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465700265774134
    mean_env_wait_ms: 1.217889473952668
    mean_inference_ms: 4.301678975947464
    mean_raw_obs_processing_ms: 0.3769021851773488
  time_since_restore: 5661.9267790317535
  time_this_iter_s: 25.832902908325195
  time_total_s: 5661.9267790317535
  timers:
    learn_throughput: 8675.31
    learn_time_ms: 18649.709
    sample_throughput: 23729.356
    sample_time_ms: 6818.221
    update_time_ms: 33.613
  timestamp: 1602759888
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: cb791_00000
  
2020-10-15 11:04:49,274	WARNING util.py:136 -- The `process_trial` operation took 0.6418681144714355 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    221 |          5661.93 | 35756032 |  293.478 |              327.707 |              138.768 |             764.55 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2978.2925190383753
    time_step_min: 2753
  date: 2020-10-15_11-05-14
  done: false
  episode_len_mean: 764.5213350952725
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 293.6126545302979
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 226
  episodes_total: 46918
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.143376044255048e-38
        cur_lr: 5.0e-05
        entropy: 0.07458447168270747
        entropy_coeff: 0.0005000000000000001
        kl: 0.004376704338937998
        model: {}
        policy_loss: -0.009001576710337153
        total_loss: 0.7739914755026499
        vf_explained_var: 0.9976282119750977
        vf_loss: 0.7830303361018499
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000002
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656734088538606
    mean_env_wait_ms: 1.2178170405695357
    mean_inference_ms: 4.301476209542852
    mean_raw_obs_processing_ms: 0.37688956646570915
  time_since_restore: 5687.415764093399
  time_this_iter_s: 25.488985061645508
  time_total_s: 5687.415764093399
  timers:
    learn_throughput: 8690.544
    learn_time_ms: 18617.016
    sample_throughput: 23763.796
    sample_time_ms: 6808.34
    update_time_ms: 33.091
  timestamp: 1602759914
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: cb791_00000
  
2020-10-15 11:05:15,588	WARNING util.py:136 -- The `process_trial` operation took 0.6003243923187256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    222 |          5687.42 | 35917824 |  293.613 |              327.707 |              138.768 |            764.521 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2977.624962808688
    time_step_min: 2753
  date: 2020-10-15_11-05-41
  done: false
  episode_len_mean: 764.4990338266833
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 293.7213692999902
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 175
  episodes_total: 47093
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.071688022127524e-38
        cur_lr: 5.0e-05
        entropy: 0.07517549395561218
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008632489286052683
        total_loss: .inf
        vf_explained_var: 0.9981219172477722
        vf_loss: 0.4893546576301257
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.259999999999994
    gpu_util_percent0: 0.38066666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656478313530014
    mean_env_wait_ms: 1.2177645985756036
    mean_inference_ms: 4.301332927887428
    mean_raw_obs_processing_ms: 0.3768796156588475
  time_since_restore: 5712.891337394714
  time_this_iter_s: 25.475573301315308
  time_total_s: 5712.891337394714
  timers:
    learn_throughput: 8698.495
    learn_time_ms: 18599.999
    sample_throughput: 23762.633
    sample_time_ms: 6808.673
    update_time_ms: 34.319
  timestamp: 1602759941
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:05:41,938	WARNING util.py:136 -- The `process_trial` operation took 0.6387767791748047 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    223 |          5712.89 | 36079616 |  293.721 |              327.707 |              138.768 |            764.499 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2976.6052091921947
    time_step_min: 2753
  date: 2020-10-15_11-06-07
  done: false
  episode_len_mean: 764.4680397127165
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 293.87465593834395
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 247
  episodes_total: 47340
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.107532033191283e-38
        cur_lr: 5.0e-05
        entropy: 0.08401622623205185
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009630618966184556
        total_loss: .inf
        vf_explained_var: 0.9978122115135193
        vf_loss: 0.7726086378097534
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.863333333333337
    gpu_util_percent0: 0.3383333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656112140351782
    mean_env_wait_ms: 1.21769094672985
    mean_inference_ms: 4.30112814614607
    mean_raw_obs_processing_ms: 0.37686470004731454
  time_since_restore: 5738.428030252457
  time_this_iter_s: 25.53669285774231
  time_total_s: 5738.428030252457
  timers:
    learn_throughput: 8705.167
    learn_time_ms: 18585.744
    sample_throughput: 23746.666
    sample_time_ms: 6813.251
    update_time_ms: 33.986
  timestamp: 1602759967
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:06:08,321	WARNING util.py:136 -- The `process_trial` operation took 0.6220223903656006 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    224 |          5738.43 | 36241408 |  293.875 |              327.707 |              138.768 |            764.468 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2975.771429172717
    time_step_min: 2753
  date: 2020-10-15_11-06-34
  done: false
  episode_len_mean: 764.4483976785264
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 293.997023604707
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 47556
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.161298049786924e-38
        cur_lr: 5.0e-05
        entropy: 0.0761456706871589
        entropy_coeff: 0.0005000000000000001
        kl: 0.004041388592061897
        model: {}
        policy_loss: -0.009187234177564582
        total_loss: 0.9403306146462759
        vf_explained_var: 0.9970261454582214
        vf_loss: 0.9495559185743332
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.153333333333332
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465582266246018
    mean_env_wait_ms: 1.2176226163557358
    mean_inference_ms: 4.300947109919128
    mean_raw_obs_processing_ms: 0.37685271396218784
  time_since_restore: 5764.2123103141785
  time_this_iter_s: 25.7842800617218
  time_total_s: 5764.2123103141785
  timers:
    learn_throughput: 8708.831
    learn_time_ms: 18577.923
    sample_throughput: 23694.617
    sample_time_ms: 6828.218
    update_time_ms: 36.008
  timestamp: 1602759994
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: cb791_00000
  
2020-10-15 11:06:34,956	WARNING util.py:136 -- The `process_trial` operation took 0.6270804405212402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    225 |          5764.21 | 36403200 |  293.997 |              327.707 |              138.768 |            764.448 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2975.0549998951583
    time_step_min: 2753
  date: 2020-10-15_11-07-01
  done: false
  episode_len_mean: 764.4350094280327
  episode_reward_max: 327.70707070707067
  episode_reward_mean: 294.10033077474924
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 174
  episodes_total: 47730
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.580649024893462e-38
        cur_lr: 5.0e-05
        entropy: 0.07781131751835346
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008867609275815388
        total_loss: .inf
        vf_explained_var: 0.9974204897880554
        vf_loss: 0.6922120799620947
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69677419354839
    gpu_util_percent0: 0.34354838709677427
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655571918875032
    mean_env_wait_ms: 1.2175693536838654
    mean_inference_ms: 4.300803787519205
    mean_raw_obs_processing_ms: 0.37684223070029454
  time_since_restore: 5790.282512187958
  time_this_iter_s: 26.070201873779297
  time_total_s: 5790.282512187958
  timers:
    learn_throughput: 8701.803
    learn_time_ms: 18592.927
    sample_throughput: 23687.695
    sample_time_ms: 6830.213
    update_time_ms: 33.954
  timestamp: 1602760021
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:07:01,906	WARNING util.py:136 -- The `process_trial` operation took 0.6458878517150879 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    226 |          5790.28 | 36564992 |    294.1 |              327.707 |              138.768 |            764.435 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2974.0675608026363
    time_step_min: 2750
  date: 2020-10-15_11-07-27
  done: false
  episode_len_mean: 764.4145807715554
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 294.24937943659927
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 251
  episodes_total: 47981
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.870973537340195e-38
        cur_lr: 5.0e-05
        entropy: 0.08438809836904208
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01008996823899603
        total_loss: .inf
        vf_explained_var: 0.997554361820221
        vf_loss: 0.8385551522175471
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116666666666674
    gpu_util_percent0: 0.292
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655245341726766
    mean_env_wait_ms: 1.2174943034984476
    mean_inference_ms: 4.300601076097116
    mean_raw_obs_processing_ms: 0.3768277445707849
  time_since_restore: 5816.315008878708
  time_this_iter_s: 26.032496690750122
  time_total_s: 5816.315008878708
  timers:
    learn_throughput: 8697.614
    learn_time_ms: 18601.883
    sample_throughput: 23605.127
    sample_time_ms: 6854.104
    update_time_ms: 33.941
  timestamp: 1602760047
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:07:28,801	WARNING util.py:136 -- The `process_trial` operation took 0.634441614151001 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    227 |          5816.32 | 36726784 |  294.249 |              328.768 |              138.768 |            764.415 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2973.2557062451974
    time_step_min: 2750
  date: 2020-10-15_11-07-54
  done: false
  episode_len_mean: 764.3961359674607
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 294.3735927382062
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 207
  episodes_total: 48188
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.030646030601029e-37
        cur_lr: 5.0e-05
        entropy: 0.07661246197919051
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007888570912958434
        total_loss: .inf
        vf_explained_var: 0.9984378218650818
        vf_loss: 0.4478544940551122
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.600000000000005
    gpu_util_percent0: 0.3083333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654966105828157
    mean_env_wait_ms: 1.2174283331788966
    mean_inference_ms: 4.300435321739799
    mean_raw_obs_processing_ms: 0.3768164287404405
  time_since_restore: 5841.940711021423
  time_this_iter_s: 25.625702142715454
  time_total_s: 5841.940711021423
  timers:
    learn_throughput: 8705.402
    learn_time_ms: 18585.241
    sample_throughput: 23636.477
    sample_time_ms: 6845.013
    update_time_ms: 32.207
  timestamp: 1602760074
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:07:55,336	WARNING util.py:136 -- The `process_trial` operation took 0.6852006912231445 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    228 |          5841.94 | 36888576 |  294.374 |              328.768 |              138.768 |            764.396 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2972.5778996378685
    time_step_min: 2750
  date: 2020-10-15_11-08-20
  done: false
  episode_len_mean: 764.3806136795964
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 294.4739684496941
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 176
  episodes_total: 48364
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5459690459015435e-37
        cur_lr: 5.0e-05
        entropy: 0.07723726332187653
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038471327473719916
        model: {}
        policy_loss: -0.010043207956186961
        total_loss: 0.6632312685251236
        vf_explained_var: 0.9975571632385254
        vf_loss: 0.6733130911986033
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.483333333333334
    gpu_util_percent0: 0.29266666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654715794902215
    mean_env_wait_ms: 1.2173742927056144
    mean_inference_ms: 4.3002943766794575
    mean_raw_obs_processing_ms: 0.3768060861094994
  time_since_restore: 5867.436763048172
  time_this_iter_s: 25.496052026748657
  time_total_s: 5867.436763048172
  timers:
    learn_throughput: 8709.932
    learn_time_ms: 18575.576
    sample_throughput: 23673.98
    sample_time_ms: 6834.17
    update_time_ms: 33.635
  timestamp: 1602760100
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: cb791_00000
  
2020-10-15 11:08:21,691	WARNING util.py:136 -- The `process_trial` operation took 0.636310338973999 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    229 |          5867.44 | 37050368 |  294.474 |              328.768 |              138.768 |            764.381 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2971.528136835172
    time_step_min: 2750
  date: 2020-10-15_11-08-46
  done: false
  episode_len_mean: 764.3491763157353
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 294.6336467943319
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 259
  episodes_total: 48623
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.729845229507717e-38
        cur_lr: 5.0e-05
        entropy: 0.08262212388217449
        entropy_coeff: 0.0005000000000000001
        kl: 0.012099679869910082
        model: {}
        policy_loss: -0.008703097947242592
        total_loss: 0.4269913186629613
        vf_explained_var: 0.9987055659294128
        vf_loss: 0.4357357323169708
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.466666666666672
    gpu_util_percent0: 0.3406666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465439795021067
    mean_env_wait_ms: 1.2172960217555517
    mean_inference_ms: 4.300090951427264
    mean_raw_obs_processing_ms: 0.37679146894883464
  time_since_restore: 5892.636901140213
  time_this_iter_s: 25.200138092041016
  time_total_s: 5892.636901140213
  timers:
    learn_throughput: 8739.554
    learn_time_ms: 18512.614
    sample_throughput: 23682.895
    sample_time_ms: 6831.597
    update_time_ms: 34.958
  timestamp: 1602760126
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: cb791_00000
  
2020-10-15 11:08:47,796	WARNING util.py:136 -- The `process_trial` operation took 0.66253662109375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    230 |          5892.64 | 37212160 |  294.634 |              328.768 |              138.768 |            764.349 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2970.9054549927228
    time_step_min: 2750
  date: 2020-10-15_11-09-12
  done: false
  episode_len_mean: 764.3480745596067
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 294.71424610711773
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 197
  episodes_total: 48820
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.729845229507717e-38
        cur_lr: 5.0e-05
        entropy: 0.1062506635983785
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011387352948077023
        total_loss: .inf
        vf_explained_var: 0.9956092834472656
        vf_loss: 1.386370489994685
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.383333333333336
    gpu_util_percent0: 0.35500000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465413838714744
    mean_env_wait_ms: 1.2172344598424318
    mean_inference_ms: 4.299938608821685
    mean_raw_obs_processing_ms: 0.376780958148276
  time_since_restore: 5917.692556142807
  time_this_iter_s: 25.055655002593994
  time_total_s: 5917.692556142807
  timers:
    learn_throughput: 8775.857
    learn_time_ms: 18436.033
    sample_throughput: 23687.601
    sample_time_ms: 6830.24
    update_time_ms: 34.355
  timestamp: 1602760152
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:09:13,907	WARNING util.py:136 -- The `process_trial` operation took 0.6383316516876221 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    231 |          5917.69 | 37373952 |  294.714 |              328.768 |              138.768 |            764.348 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2970.661234325859
    time_step_min: 2750
  date: 2020-10-15_11-09-39
  done: false
  episode_len_mean: 764.3644730129579
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 294.7469584117884
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 185
  episodes_total: 49005
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.159476784426158e-37
        cur_lr: 5.0e-05
        entropy: 0.112020639081796
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012454028338349113
        total_loss: .inf
        vf_explained_var: 0.9916589856147766
        vf_loss: 2.9937411745389304
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.456666666666667
    gpu_util_percent0: 0.3456666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465389243433196
    mean_env_wait_ms: 1.2171788501703862
    mean_inference_ms: 4.29979924339094
    mean_raw_obs_processing_ms: 0.37677090406746816
  time_since_restore: 5943.645859003067
  time_this_iter_s: 25.95330286026001
  time_total_s: 5943.645859003067
  timers:
    learn_throughput: 8760.837
    learn_time_ms: 18467.643
    sample_throughput: 23638.784
    sample_time_ms: 6844.345
    update_time_ms: 33.477
  timestamp: 1602760179
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:09:40,771	WARNING util.py:136 -- The `process_trial` operation took 0.6730446815490723 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    232 |          5943.65 | 37535744 |  294.747 |              328.768 |              138.768 |            764.364 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2969.9109083890366
    time_step_min: 2750
  date: 2020-10-15_11-10-06
  done: false
  episode_len_mean: 764.3539120548946
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 294.86040497549266
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 253
  episodes_total: 49258
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7392151766392367e-37
        cur_lr: 5.0e-05
        entropy: 0.09019534041484197
        entropy_coeff: 0.0005000000000000001
        kl: 0.004634891132203241
        model: {}
        policy_loss: -0.009747870833962224
        total_loss: 1.0614223778247833
        vf_explained_var: 0.9971091151237488
        vf_loss: 1.0712153414885204
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.780645161290323
    gpu_util_percent0: 0.3296774193548388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653584953761176
    mean_env_wait_ms: 1.2171022020776865
    mean_inference_ms: 4.29960209804617
    mean_raw_obs_processing_ms: 0.376756413610785
  time_since_restore: 5969.562641859055
  time_this_iter_s: 25.91678285598755
  time_total_s: 5969.562641859055
  timers:
    learn_throughput: 8745.653
    learn_time_ms: 18499.704
    sample_throughput: 23625.822
    sample_time_ms: 6848.1
    update_time_ms: 32.34
  timestamp: 1602760206
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: cb791_00000
  
2020-10-15 11:10:07,681	WARNING util.py:136 -- The `process_trial` operation took 0.6900665760040283 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    233 |          5969.56 | 37697536 |   294.86 |              328.768 |              138.768 |            764.354 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2969.2522766827215
    time_step_min: 2750
  date: 2020-10-15_11-10-33
  done: false
  episode_len_mean: 764.3329221685236
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 294.9604897783774
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 195
  episodes_total: 49453
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.696075883196183e-38
        cur_lr: 5.0e-05
        entropy: 0.0800873450934887
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037600312692423663
        model: {}
        policy_loss: -0.00907358395246168
        total_loss: 0.7997138947248459
        vf_explained_var: 0.997180163860321
        vf_loss: 0.8088275045156479
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17666666666667
    gpu_util_percent0: 0.339
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465333269204218
    mean_env_wait_ms: 1.217042073615029
    mean_inference_ms: 4.2994549674456755
    mean_raw_obs_processing_ms: 0.37674636307133424
  time_since_restore: 5995.187839508057
  time_this_iter_s: 25.625197649002075
  time_total_s: 5995.187839508057
  timers:
    learn_throughput: 8741.521
    learn_time_ms: 18508.449
    sample_throughput: 23627.2
    sample_time_ms: 6847.701
    update_time_ms: 33.482
  timestamp: 1602760233
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: cb791_00000
  
2020-10-15 11:10:34,190	WARNING util.py:136 -- The `process_trial` operation took 0.6517248153686523 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    234 |          5995.19 | 37859328 |   294.96 |              328.768 |              138.768 |            764.333 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2968.6164714893275
    time_step_min: 2750
  date: 2020-10-15_11-11-00
  done: false
  episode_len_mean: 764.3124144042536
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.0582775308062
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 49652
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3480379415980917e-38
        cur_lr: 5.0e-05
        entropy: 0.08860721873740356
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010932416131254286
        total_loss: .inf
        vf_explained_var: 0.9963204264640808
        vf_loss: 1.1023645003636677
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28666666666667
    gpu_util_percent0: 0.3273333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653064306727037
    mean_env_wait_ms: 1.2169831018459247
    mean_inference_ms: 4.299305085444192
    mean_raw_obs_processing_ms: 0.3767351766828385
  time_since_restore: 6021.081241846085
  time_this_iter_s: 25.893402338027954
  time_total_s: 6021.081241846085
  timers:
    learn_throughput: 8729.81
    learn_time_ms: 18533.278
    sample_throughput: 23672.532
    sample_time_ms: 6834.588
    update_time_ms: 31.856
  timestamp: 1602760260
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:11:01,078	WARNING util.py:136 -- The `process_trial` operation took 0.6668140888214111 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    235 |          6021.08 | 38021120 |  295.058 |              328.768 |              138.768 |            764.312 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2967.7917051060926
    time_step_min: 2750
  date: 2020-10-15_11-11-26
  done: false
  episode_len_mean: 764.2854251417807
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.18979559325425
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 249
  episodes_total: 49901
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.522056912397138e-38
        cur_lr: 5.0e-05
        entropy: 0.08104008374114831
        entropy_coeff: 0.0005000000000000001
        kl: 0.003923490798721711
        model: {}
        policy_loss: -0.009165771557794264
        total_loss: 0.9117594410975774
        vf_explained_var: 0.997338056564331
        vf_loss: 0.9209657609462738
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.019354838709674
    gpu_util_percent0: 0.3319354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652785049360273
    mean_env_wait_ms: 1.2169073496826892
    mean_inference_ms: 4.2991191236095885
    mean_raw_obs_processing_ms: 0.3767217966958562
  time_since_restore: 6046.833250045776
  time_this_iter_s: 25.752008199691772
  time_total_s: 6046.833250045776
  timers:
    learn_throughput: 8741.813
    learn_time_ms: 18507.831
    sample_throughput: 23695.36
    sample_time_ms: 6828.004
    update_time_ms: 31.816
  timestamp: 1602760286
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: cb791_00000
  
2020-10-15 11:11:27,755	WARNING util.py:136 -- The `process_trial` operation took 0.6922719478607178 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    236 |          6046.83 | 38182912 |   295.19 |              328.768 |              138.768 |            764.285 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2967.1249200767265
    time_step_min: 2750
  date: 2020-10-15_11-11-53
  done: false
  episode_len_mean: 764.2633218200332
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.291122941032
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 186
  episodes_total: 50087
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.261028456198569e-38
        cur_lr: 5.0e-05
        entropy: 0.07376684062182903
        entropy_coeff: 0.0005000000000000001
        kl: 0.003515903740966072
        model: {}
        policy_loss: -0.009632612559168289
        total_loss: 0.6488693008820215
        vf_explained_var: 0.9976117014884949
        vf_loss: 0.6585387786229452
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29
    gpu_util_percent0: 0.339
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652560791825223
    mean_env_wait_ms: 1.21684980983024
    mean_inference_ms: 4.298981688134074
    mean_raw_obs_processing_ms: 0.37671249433263326
  time_since_restore: 6072.597918272018
  time_this_iter_s: 25.764668226242065
  time_total_s: 6072.597918272018
  timers:
    learn_throughput: 8748.925
    learn_time_ms: 18492.787
    sample_throughput: 23749.359
    sample_time_ms: 6812.479
    update_time_ms: 31.397
  timestamp: 1602760313
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: cb791_00000
  
2020-10-15 11:11:54,496	WARNING util.py:136 -- The `process_trial` operation took 0.6813957691192627 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    237 |           6072.6 | 38344704 |  295.291 |              328.768 |              138.768 |            764.263 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2966.342274400557
    time_step_min: 2750
  date: 2020-10-15_11-12-20
  done: false
  episode_len_mean: 764.2381397383385
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.40529002596037
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 207
  episodes_total: 50294
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6305142280992846e-38
        cur_lr: 5.0e-05
        entropy: 0.07685546204447746
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009971587472440055
        total_loss: .inf
        vf_explained_var: 0.9974551200866699
        vf_loss: 0.7673762639363607
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.90967741935484
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652315273204433
    mean_env_wait_ms: 1.2167883063210059
    mean_inference_ms: 4.29882983125601
    mean_raw_obs_processing_ms: 0.37670111068639295
  time_since_restore: 6098.508804321289
  time_this_iter_s: 25.91088604927063
  time_total_s: 6098.508804321289
  timers:
    learn_throughput: 8734.134
    learn_time_ms: 18524.104
    sample_throughput: 23769.162
    sample_time_ms: 6806.803
    update_time_ms: 33.545
  timestamp: 1602760340
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:12:21,367	WARNING util.py:136 -- The `process_trial` operation took 0.717613935470581 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    238 |          6098.51 | 38506496 |  295.405 |              328.768 |              138.768 |            764.238 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2965.4454279038455
    time_step_min: 2750
  date: 2020-10-15_11-12-47
  done: false
  episode_len_mean: 764.214459547694
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.53948159699905
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 247
  episodes_total: 50541
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4457713421489273e-38
        cur_lr: 5.0e-05
        entropy: 0.07608568978806336
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009694152569863945
        total_loss: .inf
        vf_explained_var: 0.9980483055114746
        vf_loss: 0.6423189540704092
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.210000000000004
    gpu_util_percent0: 0.37300000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465202607132573
    mean_env_wait_ms: 1.216713783997278
    mean_inference_ms: 4.298652173753717
    mean_raw_obs_processing_ms: 0.37668842177658807
  time_since_restore: 6124.262849569321
  time_this_iter_s: 25.754045248031616
  time_total_s: 6124.262849569321
  timers:
    learn_throughput: 8723.505
    learn_time_ms: 18546.673
    sample_throughput: 23757.481
    sample_time_ms: 6810.15
    update_time_ms: 31.687
  timestamp: 1602760367
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:12:48,136	WARNING util.py:136 -- The `process_trial` operation took 0.6756680011749268 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    239 |          6124.26 | 38668288 |  295.539 |              328.768 |              138.768 |            764.214 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2964.7873209423465
    time_step_min: 2750
  date: 2020-10-15_11-13-13
  done: false
  episode_len_mean: 764.1980244868989
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.63885199663264
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 180
  episodes_total: 50721
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.66865701322339e-38
        cur_lr: 5.0e-05
        entropy: 0.07033161136011283
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037611928419210017
        model: {}
        policy_loss: -0.007581888630132501
        total_loss: 0.40867243707180023
        vf_explained_var: 0.9984171986579895
        vf_loss: 0.4162894959251086
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.264516129032256
    gpu_util_percent0: 0.3058064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651791389678012
    mean_env_wait_ms: 1.2166580345912663
    mean_inference_ms: 4.298520194921281
    mean_raw_obs_processing_ms: 0.37667937037745003
  time_since_restore: 6149.989680528641
  time_this_iter_s: 25.72683095932007
  time_total_s: 6149.989680528641
  timers:
    learn_throughput: 8705.011
    learn_time_ms: 18586.077
    sample_throughput: 23701.102
    sample_time_ms: 6826.349
    update_time_ms: 30.168
  timestamp: 1602760393
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: cb791_00000
  
2020-10-15 11:13:14,928	WARNING util.py:136 -- The `process_trial` operation took 0.6730453968048096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    240 |          6149.99 | 38830080 |  295.639 |              328.768 |              138.768 |            764.198 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2963.9715891229175
    time_step_min: 2750
  date: 2020-10-15_11-13-40
  done: false
  episode_len_mean: 764.173711593207
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.76465747094966
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 214
  episodes_total: 50935
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.834328506611695e-38
        cur_lr: 5.0e-05
        entropy: 0.0764068824549516
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006762156499462435
        total_loss: .inf
        vf_explained_var: 0.9989023804664612
        vf_loss: 0.3236338769396146
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.836666666666666
    gpu_util_percent0: 0.31299999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651543558636396
    mean_env_wait_ms: 1.2165956604637977
    mean_inference_ms: 4.298373233786119
    mean_raw_obs_processing_ms: 0.37666806586261614
  time_since_restore: 6175.789888143539
  time_this_iter_s: 25.80020761489868
  time_total_s: 6175.789888143539
  timers:
    learn_throughput: 8670.781
    learn_time_ms: 18659.449
    sample_throughput: 23727.416
    sample_time_ms: 6818.779
    update_time_ms: 37.301
  timestamp: 1602760420
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:13:41,708	WARNING util.py:136 -- The `process_trial` operation took 0.7438130378723145 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    241 |          6175.79 | 38991872 |  295.765 |              328.768 |              138.768 |            764.174 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2963.0929054714693
    time_step_min: 2750
  date: 2020-10-15_11-14-07
  done: false
  episode_len_mean: 764.1507317740392
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.8945170090019
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 242
  episodes_total: 51177
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.751492759917543e-38
        cur_lr: 5.0e-05
        entropy: 0.07375361646215121
        entropy_coeff: 0.0005000000000000001
        kl: 0.003419696062337607
        model: {}
        policy_loss: -0.0074341544799002195
        total_loss: 0.7301582843065262
        vf_explained_var: 0.9977796077728271
        vf_loss: 0.7376293142636617
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.323333333333334
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465126050759988
    mean_env_wait_ms: 1.2165209952638154
    mean_inference_ms: 4.298195600845874
    mean_raw_obs_processing_ms: 0.3766556286223454
  time_since_restore: 6201.629989624023
  time_this_iter_s: 25.84010148048401
  time_total_s: 6201.629989624023
  timers:
    learn_throughput: 8673.18
    learn_time_ms: 18654.288
    sample_throughput: 23756.877
    sample_time_ms: 6810.323
    update_time_ms: 39.07
  timestamp: 1602760447
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: cb791_00000
  
2020-10-15 11:14:08,594	WARNING util.py:136 -- The `process_trial` operation took 0.7180883884429932 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    242 |          6201.63 | 39153664 |  295.895 |              328.768 |              138.768 |            764.151 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2962.479197926613
    time_step_min: 2750
  date: 2020-10-15_11-14-34
  done: false
  episode_len_mean: 764.1385037775527
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 295.9880216213068
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 179
  episodes_total: 51356
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3757463799587714e-38
        cur_lr: 5.0e-05
        entropy: 0.07057391169170539
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038820116703088083
        model: {}
        policy_loss: -0.0077410436133504845
        total_loss: 0.3109492138028145
        vf_explained_var: 0.9987766146659851
        vf_loss: 0.31872553875048953
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.616129032258062
    gpu_util_percent0: 0.30419354838709683
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651032985133888
    mean_env_wait_ms: 1.2164666892203457
    mean_inference_ms: 4.298069973440789
    mean_raw_obs_processing_ms: 0.3766465137508531
  time_since_restore: 6227.158394098282
  time_this_iter_s: 25.528404474258423
  time_total_s: 6227.158394098282
  timers:
    learn_throughput: 8684.771
    learn_time_ms: 18629.392
    sample_throughput: 23777.067
    sample_time_ms: 6804.54
    update_time_ms: 39.297
  timestamp: 1602760474
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: cb791_00000
  
2020-10-15 11:14:35,314	WARNING util.py:136 -- The `process_trial` operation took 0.7259531021118164 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    243 |          6227.16 | 39315456 |  295.988 |              328.768 |              138.768 |            764.139 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2961.6100366627225
    time_step_min: 2750
  date: 2020-10-15_11-15-00
  done: false
  episode_len_mean: 764.1160108548169
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 296.11849058524757
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 51590
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.878731899793857e-39
        cur_lr: 5.0e-05
        entropy: 0.07632157454888026
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007189355412265286
        total_loss: .inf
        vf_explained_var: 0.9988530278205872
        vf_loss: 0.35429885735114414
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.806666666666672
    gpu_util_percent0: 0.32500000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465077661305647
    mean_env_wait_ms: 1.2163986075310507
    mean_inference_ms: 4.297922067765741
    mean_raw_obs_processing_ms: 0.37663429815568267
  time_since_restore: 6252.821519613266
  time_this_iter_s: 25.66312551498413
  time_total_s: 6252.821519613266
  timers:
    learn_throughput: 8683.98
    learn_time_ms: 18631.089
    sample_throughput: 23767.745
    sample_time_ms: 6807.209
    update_time_ms: 37.756
  timestamp: 1602760500
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:15:01,944	WARNING util.py:136 -- The `process_trial` operation took 0.7258768081665039 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    244 |          6252.82 | 39477248 |  296.118 |              328.768 |              138.768 |            764.116 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2960.8281536708323
    time_step_min: 2750
  date: 2020-10-15_11-15-27
  done: false
  episode_len_mean: 764.1060372114569
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 296.23073376394984
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 222
  episodes_total: 51812
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0318097849690786e-38
        cur_lr: 5.0e-05
        entropy: 0.0748391654342413
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00954398899921216
        total_loss: .inf
        vf_explained_var: 0.9981343746185303
        vf_loss: 0.5885624388853709
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612907
    gpu_util_percent0: 0.32258064516129037
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465051146457071
    mean_env_wait_ms: 1.216329609692623
    mean_inference_ms: 4.297757498333161
    mean_raw_obs_processing_ms: 0.3766238408192432
  time_since_restore: 6278.449369907379
  time_this_iter_s: 25.62785029411316
  time_total_s: 6278.449369907379
  timers:
    learn_throughput: 8691.205
    learn_time_ms: 18615.601
    sample_throughput: 23809.53
    sample_time_ms: 6795.262
    update_time_ms: 37.219
  timestamp: 1602760527
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:15:28,820	WARNING util.py:136 -- The `process_trial` operation took 0.8173537254333496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    245 |          6278.45 | 39639040 |  296.231 |              328.768 |              138.768 |            764.106 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2960.2201497565015
    time_step_min: 2750
  date: 2020-10-15_11-15-54
  done: false
  episode_len_mean: 764.0938834391229
  episode_reward_max: 328.76767676767605
  episode_reward_mean: 296.32061526983614
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 51990
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5477146774536176e-38
        cur_lr: 5.0e-05
        entropy: 0.07325025647878647
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010838372497043261
        total_loss: .inf
        vf_explained_var: 0.9978420734405518
        vf_loss: 0.5886243730783463
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.42903225806452
    gpu_util_percent0: 0.32096774193548383
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465028867087996
    mean_env_wait_ms: 1.216276388358865
    mean_inference_ms: 4.297633055135816
    mean_raw_obs_processing_ms: 0.3766147823051033
  time_since_restore: 6304.536589622498
  time_this_iter_s: 26.087219715118408
  time_total_s: 6304.536589622498
  timers:
    learn_throughput: 8679.952
    learn_time_ms: 18639.734
    sample_throughput: 23755.804
    sample_time_ms: 6810.63
    update_time_ms: 39.027
  timestamp: 1602760554
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:15:56,103	WARNING util.py:136 -- The `process_trial` operation took 0.7305140495300293 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    246 |          6304.54 | 39800832 |  296.321 |              328.768 |              138.768 |            764.094 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2959.372739117106
    time_step_min: 2750
  date: 2020-10-15_11-16-21
  done: false
  episode_len_mean: 764.0725239800119
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 296.44914481492356
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 241
  episodes_total: 52231
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3215720161804262e-38
        cur_lr: 5.0e-05
        entropy: 0.07544809579849243
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007622890057973564
        total_loss: .inf
        vf_explained_var: 0.9980209469795227
        vf_loss: 0.6278948932886124
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58666666666667
    gpu_util_percent0: 0.38266666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146500218703757
    mean_env_wait_ms: 1.2162056704683002
    mean_inference_ms: 4.29748339109464
    mean_raw_obs_processing_ms: 0.3766027523143753
  time_since_restore: 6330.350263595581
  time_this_iter_s: 25.813673973083496
  time_total_s: 6330.350263595581
  timers:
    learn_throughput: 8680.314
    learn_time_ms: 18638.958
    sample_throughput: 23729.033
    sample_time_ms: 6818.314
    update_time_ms: 38.971
  timestamp: 1602760581
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:16:22,908	WARNING util.py:136 -- The `process_trial` operation took 0.6909260749816895 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    247 |          6330.35 | 39962624 |  296.449 |              328.768 |              138.768 |            764.073 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2958.604411540223
    time_step_min: 2750
  date: 2020-10-15_11-16-48
  done: false
  episode_len_mean: 764.0514805422617
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 296.5662075788679
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 52447
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4823580242706394e-38
        cur_lr: 5.0e-05
        entropy: 0.06852923954526584
        entropy_coeff: 0.0005000000000000001
        kl: 0.004509756652017434
        model: {}
        policy_loss: -0.007198471061807747
        total_loss: 0.2968380202849706
        vf_explained_var: 0.9989545941352844
        vf_loss: 0.30407076080640155
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.583870967741934
    gpu_util_percent0: 0.3374193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649783306512146
    mean_env_wait_ms: 1.216138073026983
    mean_inference_ms: 4.297327970791643
    mean_raw_obs_processing_ms: 0.37659233868138386
  time_since_restore: 6356.2885892391205
  time_this_iter_s: 25.93832564353943
  time_total_s: 6356.2885892391205
  timers:
    learn_throughput: 8688.535
    learn_time_ms: 18621.322
    sample_throughput: 23686.626
    sample_time_ms: 6830.521
    update_time_ms: 38.748
  timestamp: 1602760608
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: cb791_00000
  
2020-10-15 11:16:49,879	WARNING util.py:136 -- The `process_trial` operation took 0.7103033065795898 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    248 |          6356.29 | 40124416 |  296.566 |              328.768 |              138.768 |            764.051 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2957.9640996387147
    time_step_min: 2750
  date: 2020-10-15_11-17-15
  done: false
  episode_len_mean: 764.032054570674
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 296.66369503620814
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 182
  episodes_total: 52629
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7411790121353197e-38
        cur_lr: 5.0e-05
        entropy: 0.07312838609019916
        entropy_coeff: 0.0005000000000000001
        kl: 0.004317015875130892
        model: {}
        policy_loss: -0.007663049196708016
        total_loss: 0.29843489204843837
        vf_explained_var: 0.9988184571266174
        vf_loss: 0.30613449215888977
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.55666666666667
    gpu_util_percent0: 0.3746666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649551822271478
    mean_env_wait_ms: 1.2160843995936428
    mean_inference_ms: 4.29720895644983
    mean_raw_obs_processing_ms: 0.3765833852629379
  time_since_restore: 6382.13140463829
  time_this_iter_s: 25.842815399169922
  time_total_s: 6382.13140463829
  timers:
    learn_throughput: 8689.227
    learn_time_ms: 18619.839
    sample_throughput: 23658.5
    sample_time_ms: 6838.642
    update_time_ms: 40.67
  timestamp: 1602760635
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: cb791_00000
  
2020-10-15 11:17:16,678	WARNING util.py:136 -- The `process_trial` operation took 0.7087819576263428 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    249 |          6382.13 | 40286208 |  296.664 |              328.768 |              138.768 |            764.032 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2957.1453636931465
    time_step_min: 2750
  date: 2020-10-15_11-17-42
  done: false
  episode_len_mean: 764.0097594189741
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 296.78907034484644
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 243
  episodes_total: 52872
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.705895060676598e-39
        cur_lr: 5.0e-05
        entropy: 0.07837848986188571
        entropy_coeff: 0.0005000000000000001
        kl: 0.003797521368445208
        model: {}
        policy_loss: -0.008546424874415
        total_loss: 0.5641427760322889
        vf_explained_var: 0.9982908368110657
        vf_loss: 0.5727283830444018
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.40645161290323
    gpu_util_percent0: 0.38838709677419353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649315811497954
    mean_env_wait_ms: 1.2160129712890375
    mean_inference_ms: 4.297057705938142
    mean_raw_obs_processing_ms: 0.3765719335728361
  time_since_restore: 6407.944009542465
  time_this_iter_s: 25.812604904174805
  time_total_s: 6407.944009542465
  timers:
    learn_throughput: 8680.926
    learn_time_ms: 18637.644
    sample_throughput: 23698.951
    sample_time_ms: 6826.969
    update_time_ms: 41.96
  timestamp: 1602760662
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: cb791_00000
  
2020-10-15 11:17:43,644	WARNING util.py:136 -- The `process_trial` operation took 0.7471413612365723 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    250 |          6407.94 | 40448000 |  296.789 |              328.768 |              138.768 |             764.01 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2956.404068397338
    time_step_min: 2750
  date: 2020-10-15_11-18-09
  done: false
  episode_len_mean: 763.9874722128028
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 296.90025438058655
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 210
  episodes_total: 53082
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.352947530338299e-39
        cur_lr: 5.0e-05
        entropy: 0.067605700964729
        entropy_coeff: 0.0005000000000000001
        kl: 0.010952396240706245
        model: {}
        policy_loss: -0.008908896095817909
        total_loss: 0.23168300092220306
        vf_explained_var: 0.9991459846496582
        vf_loss: 0.24062569439411163
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.2
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649061411767034
    mean_env_wait_ms: 1.2159478079405006
    mean_inference_ms: 4.296910861210183
    mean_raw_obs_processing_ms: 0.37656156595444673
  time_since_restore: 6433.913664340973
  time_this_iter_s: 25.96965479850769
  time_total_s: 6433.913664340973
  timers:
    learn_throughput: 8674.02
    learn_time_ms: 18652.482
    sample_throughput: 23673.5
    sample_time_ms: 6834.308
    update_time_ms: 35.847
  timestamp: 1602760689
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: cb791_00000
  
2020-10-15 11:18:10,700	WARNING util.py:136 -- The `process_trial` operation took 0.7405588626861572 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    251 |          6433.91 | 40609792 |    296.9 |              328.768 |              138.768 |            763.987 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2955.771326057145
    time_step_min: 2750
  date: 2020-10-15_11-18-36
  done: false
  episode_len_mean: 763.9696275717075
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 296.9950473347378
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 53272
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.352947530338299e-39
        cur_lr: 5.0e-05
        entropy: 0.0739134382456541
        entropy_coeff: 0.0005000000000000001
        kl: 0.003590337796291957
        model: {}
        policy_loss: -0.009529078815830871
        total_loss: 0.6394646217425665
        vf_explained_var: 0.9976270794868469
        vf_loss: 0.6490306456883749
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.077419354838714
    gpu_util_percent0: 0.3225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648838698985342
    mean_env_wait_ms: 1.2158926026652355
    mean_inference_ms: 4.296792887332947
    mean_raw_obs_processing_ms: 0.37655211133026834
  time_since_restore: 6459.781900167465
  time_this_iter_s: 25.86823582649231
  time_total_s: 6459.781900167465
  timers:
    learn_throughput: 8669.747
    learn_time_ms: 18661.675
    sample_throughput: 23692.867
    sample_time_ms: 6828.722
    update_time_ms: 34.107
  timestamp: 1602760716
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: cb791_00000
  
2020-10-15 11:18:37,575	WARNING util.py:136 -- The `process_trial` operation took 0.7327797412872314 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    252 |          6459.78 | 40771584 |  296.995 |              328.768 |              138.768 |             763.97 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2954.9971391709205
    time_step_min: 2750
  date: 2020-10-15_11-19-03
  done: false
  episode_len_mean: 763.9464499252616
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.11644471622037
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 248
  episodes_total: 53520
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1764737651691496e-39
        cur_lr: 5.0e-05
        entropy: 0.07623437357445557
        entropy_coeff: 0.0005000000000000001
        kl: 0.004315525914231936
        model: {}
        policy_loss: -0.010250157805179091
        total_loss: 0.5029701714714369
        vf_explained_var: 0.9984820485115051
        vf_loss: 0.5132584472497305
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.886666666666667
    gpu_util_percent0: 0.3853333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648566693661216
    mean_env_wait_ms: 1.215817766809811
    mean_inference_ms: 4.296633259451566
    mean_raw_obs_processing_ms: 0.3765405375054937
  time_since_restore: 6485.451947927475
  time_this_iter_s: 25.670047760009766
  time_total_s: 6485.451947927475
  timers:
    learn_throughput: 8667.31
    learn_time_ms: 18666.921
    sample_throughput: 23673.044
    sample_time_ms: 6834.44
    update_time_ms: 33.43
  timestamp: 1602760743
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: cb791_00000
  
2020-10-15 11:19:04,378	WARNING util.py:136 -- The `process_trial` operation took 0.7470498085021973 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    253 |          6485.45 | 40933376 |  297.116 |              328.768 |              138.768 |            763.946 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2954.345821379336
    time_step_min: 2750
  date: 2020-10-15_11-19-29
  done: false
  episode_len_mean: 763.926466481747
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.21553284393707
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 197
  episodes_total: 53717
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0882368825845748e-39
        cur_lr: 5.0e-05
        entropy: 0.07027789577841759
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008162994481002292
        total_loss: .inf
        vf_explained_var: 0.9982782006263733
        vf_loss: 0.47043608625729877
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.796666666666667
    gpu_util_percent0: 0.33366666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648365393568138
    mean_env_wait_ms: 1.2157581171600855
    mean_inference_ms: 4.296507245344498
    mean_raw_obs_processing_ms: 0.376531786367892
  time_since_restore: 6510.974898576736
  time_this_iter_s: 25.522950649261475
  time_total_s: 6510.974898576736
  timers:
    learn_throughput: 8669.808
    learn_time_ms: 18661.544
    sample_throughput: 23701.106
    sample_time_ms: 6826.348
    update_time_ms: 31.654
  timestamp: 1602760769
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:19:30,966	WARNING util.py:136 -- The `process_trial` operation took 0.7192692756652832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    254 |          6510.97 | 41095168 |  297.216 |              328.768 |              138.768 |            763.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2953.697941607736
    time_step_min: 2748
  date: 2020-10-15_11-19-56
  done: false
  episode_len_mean: 763.9048148972475
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.3169998448764
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 53916
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.632355323876862e-39
        cur_lr: 5.0e-05
        entropy: 0.07788402649263541
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008600802044384181
        total_loss: .inf
        vf_explained_var: 0.9986326694488525
        vf_loss: 0.3821653102835019
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.48387096774194
    gpu_util_percent0: 0.3367741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146481496938077
    mean_env_wait_ms: 1.2157001270192702
    mean_inference_ms: 4.29638415454034
    mean_raw_obs_processing_ms: 0.37652228526771
  time_since_restore: 6536.843065023422
  time_this_iter_s: 25.86816644668579
  time_total_s: 6536.843065023422
  timers:
    learn_throughput: 8672.662
    learn_time_ms: 18655.403
    sample_throughput: 23608.059
    sample_time_ms: 6853.253
    update_time_ms: 34.161
  timestamp: 1602760796
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:19:57,832	WARNING util.py:136 -- The `process_trial` operation took 0.6931469440460205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    255 |          6536.84 | 41256960 |  297.317 |              328.768 |              138.768 |            763.905 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2952.965171923212
    time_step_min: 2748
  date: 2020-10-15_11-20-23
  done: false
  episode_len_mean: 763.8860640301318
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.4211383805933
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 246
  episodes_total: 54162
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4485329858152932e-39
        cur_lr: 5.0e-05
        entropy: 0.08927458090086778
        entropy_coeff: 0.0005000000000000001
        kl: 0.004426354290141414
        model: {}
        policy_loss: -0.012866922363173217
        total_loss: 0.9496356248855591
        vf_explained_var: 0.9972410798072815
        vf_loss: 0.9625471780697504
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.173333333333332
    gpu_util_percent0: 0.3363333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647883904285683
    mean_env_wait_ms: 1.215627430291548
    mean_inference_ms: 4.296233527616243
    mean_raw_obs_processing_ms: 0.37651110428777385
  time_since_restore: 6562.556716918945
  time_this_iter_s: 25.71365189552307
  time_total_s: 6562.556716918945
  timers:
    learn_throughput: 8687.261
    learn_time_ms: 18624.051
    sample_throughput: 23641.462
    sample_time_ms: 6843.57
    update_time_ms: 33.729
  timestamp: 1602760823
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: cb791_00000
  
2020-10-15 11:20:24,523	WARNING util.py:136 -- The `process_trial` operation took 0.7213058471679688 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    256 |          6562.56 | 41418752 |  297.421 |              328.768 |              138.768 |            763.886 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2952.4221457109716
    time_step_min: 2748
  date: 2020-10-15_11-20-50
  done: false
  episode_len_mean: 763.8737304974978
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.49537266245005
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 54352
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2242664929076466e-39
        cur_lr: 5.0e-05
        entropy: 0.07836189493536949
        entropy_coeff: 0.0005000000000000001
        kl: 0.004102963527354102
        model: {}
        policy_loss: -0.012294680539829036
        total_loss: 0.9816399961709976
        vf_explained_var: 0.9966129660606384
        vf_loss: 0.9939738710721334
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.832258064516136
    gpu_util_percent0: 0.2909677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647697007520907
    mean_env_wait_ms: 1.2155692567211973
    mean_inference_ms: 4.296114258955096
    mean_raw_obs_processing_ms: 0.3765026313915376
  time_since_restore: 6588.515009641647
  time_this_iter_s: 25.958292722702026
  time_total_s: 6588.515009641647
  timers:
    learn_throughput: 8686.361
    learn_time_ms: 18625.981
    sample_throughput: 23603.291
    sample_time_ms: 6854.637
    update_time_ms: 33.699
  timestamp: 1602760850
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: cb791_00000
  
2020-10-15 11:20:51,683	WARNING util.py:136 -- The `process_trial` operation took 0.7642896175384521 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    257 |          6588.52 | 41580544 |  297.495 |              328.768 |              138.768 |            763.874 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2951.7730432709063
    time_step_min: 2748
  date: 2020-10-15_11-21-17
  done: false
  episode_len_mean: 763.8506672043405
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.5962643518548
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 204
  episodes_total: 54556
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.121332464538233e-40
        cur_lr: 5.0e-05
        entropy: 0.07590627359847228
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007971510911981264
        total_loss: .inf
        vf_explained_var: 0.99843430519104
        vf_loss: 0.4551419640580813
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.143333333333334
    gpu_util_percent0: 0.35500000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647467897214164
    mean_env_wait_ms: 1.2155099167055776
    mean_inference_ms: 4.295986975344596
    mean_raw_obs_processing_ms: 0.3764924847016818
  time_since_restore: 6614.242600679398
  time_this_iter_s: 25.727591037750244
  time_total_s: 6614.242600679398
  timers:
    learn_throughput: 8688.768
    learn_time_ms: 18620.822
    sample_throughput: 23630.178
    sample_time_ms: 6846.838
    update_time_ms: 32.016
  timestamp: 1602760877
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:21:18,389	WARNING util.py:136 -- The `process_trial` operation took 0.7218761444091797 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    258 |          6614.24 | 41742336 |  297.596 |              328.768 |              138.768 |            763.851 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2951.080743527006
    time_step_min: 2748
  date: 2020-10-15_11-21-44
  done: false
  episode_len_mean: 763.825654593559
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.7021828539937
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 249
  episodes_total: 54805
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.181998696807349e-40
        cur_lr: 5.0e-05
        entropy: 0.08642653686304887
        entropy_coeff: 0.0005000000000000001
        kl: 0.00500023605612417
        model: {}
        policy_loss: -0.009260633175775487
        total_loss: 1.578952819108963
        vf_explained_var: 0.9955456256866455
        vf_loss: 1.588256706794103
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77741935483871
    gpu_util_percent0: 0.26322580645161286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647238539520582
    mean_env_wait_ms: 1.2154357235934727
    mean_inference_ms: 4.29584404379258
    mean_raw_obs_processing_ms: 0.376481706050741
  time_since_restore: 6639.984618425369
  time_this_iter_s: 25.74201774597168
  time_total_s: 6639.984618425369
  timers:
    learn_throughput: 8689.535
    learn_time_ms: 18619.178
    sample_throughput: 23668.603
    sample_time_ms: 6835.722
    update_time_ms: 32.132
  timestamp: 1602760904
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: cb791_00000
  
2020-10-15 11:21:45,269	WARNING util.py:136 -- The `process_trial` operation took 0.7155168056488037 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    259 |          6639.98 | 41904128 |  297.702 |              328.768 |              138.768 |            763.826 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2950.543532066681
    time_step_min: 2746
  date: 2020-10-15_11-22-11
  done: false
  episode_len_mean: 763.813555931402
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.7852902972657
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 182
  episodes_total: 54987
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.181998696807349e-40
        cur_lr: 5.0e-05
        entropy: 0.08070773258805275
        entropy_coeff: 0.0005000000000000001
        kl: 0.005144810226435463
        model: {}
        policy_loss: -0.007251753156500247
        total_loss: 0.5784070442120234
        vf_explained_var: 0.9978243708610535
        vf_loss: 0.585699145992597
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.253333333333334
    gpu_util_percent0: 0.2846666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647034547833876
    mean_env_wait_ms: 1.2153813274542447
    mean_inference_ms: 4.295730174622953
    mean_raw_obs_processing_ms: 0.37647385521379456
  time_since_restore: 6665.774880647659
  time_this_iter_s: 25.79026222229004
  time_total_s: 6665.774880647659
  timers:
    learn_throughput: 8694.723
    learn_time_ms: 18608.068
    sample_throughput: 23636.259
    sample_time_ms: 6845.077
    update_time_ms: 31.037
  timestamp: 1602760931
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: cb791_00000
  
2020-10-15 11:22:12,056	WARNING util.py:136 -- The `process_trial` operation took 0.7395267486572266 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    260 |          6665.77 | 42065920 |  297.785 |              328.768 |              138.768 |            763.814 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2949.875054387237
    time_step_min: 2746
  date: 2020-10-15_11-22-37
  done: false
  episode_len_mean: 763.7986738890197
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 297.88719035131095
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 212
  episodes_total: 55199
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.181998696807349e-40
        cur_lr: 5.0e-05
        entropy: 0.08874880708754063
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007837751905147647
        total_loss: .inf
        vf_explained_var: 0.998152494430542
        vf_loss: 0.5493695313731829
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.774193548387096
    gpu_util_percent0: 0.30870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646843047741656
    mean_env_wait_ms: 1.215320148891997
    mean_inference_ms: 4.2956106129799645
    mean_raw_obs_processing_ms: 0.37646349464382123
  time_since_restore: 6691.480546236038
  time_this_iter_s: 25.705665588378906
  time_total_s: 6691.480546236038
  timers:
    learn_throughput: 8705.178
    learn_time_ms: 18585.721
    sample_throughput: 23649.294
    sample_time_ms: 6841.304
    update_time_ms: 30.788
  timestamp: 1602760957
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:22:38,926	WARNING util.py:136 -- The `process_trial` operation took 0.7354221343994141 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    261 |          6691.48 | 42227712 |  297.887 |              328.768 |              138.768 |            763.799 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2949.109615002798
    time_step_min: 2746
  date: 2020-10-15_11-23-04
  done: false
  episode_len_mean: 763.7819523105227
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 298.0015307652477
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 243
  episodes_total: 55442
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.377299804521102e-39
        cur_lr: 5.0e-05
        entropy: 0.08253948080042998
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007293641382905965
        total_loss: .inf
        vf_explained_var: 0.998725414276123
        vf_loss: 0.40339099367459613
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.483333333333334
    gpu_util_percent0: 0.296
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646580017378538
    mean_env_wait_ms: 1.2152480287887502
    mean_inference_ms: 4.295465490361636
    mean_raw_obs_processing_ms: 0.3764535108385225
  time_since_restore: 6717.448409795761
  time_this_iter_s: 25.9678635597229
  time_total_s: 6717.448409795761
  timers:
    learn_throughput: 8706.745
    learn_time_ms: 18582.376
    sample_throughput: 23606.702
    sample_time_ms: 6853.647
    update_time_ms: 30.348
  timestamp: 1602760984
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:23:05,972	WARNING util.py:136 -- The `process_trial` operation took 0.7273459434509277 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    262 |          6717.45 | 42389504 |  298.002 |              328.768 |              138.768 |            763.782 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2948.6086283822683
    time_step_min: 2746
  date: 2020-10-15_11-23-31
  done: false
  episode_len_mean: 763.7680455926505
  episode_reward_max: 328.76767676767645
  episode_reward_mean: 298.0798181916243
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 55623
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.065949706781654e-39
        cur_lr: 5.0e-05
        entropy: 0.0765521830568711
        entropy_coeff: 0.0005000000000000001
        kl: 0.004041717465346058
        model: {}
        policy_loss: -0.010155390618213763
        total_loss: 0.6841204961140951
        vf_explained_var: 0.9974865317344666
        vf_loss: 0.6943141569693884
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.68387096774193
    gpu_util_percent0: 0.3061290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646376086964086
    mean_env_wait_ms: 1.2151943796393991
    mean_inference_ms: 4.2953549893006535
    mean_raw_obs_processing_ms: 0.37644534112021516
  time_since_restore: 6743.406202554703
  time_this_iter_s: 25.95779275894165
  time_total_s: 6743.406202554703
  timers:
    learn_throughput: 8692.349
    learn_time_ms: 18613.15
    sample_throughput: 23613.207
    sample_time_ms: 6851.759
    update_time_ms: 32.158
  timestamp: 1602761011
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: cb791_00000
  
2020-10-15 11:23:33,024	WARNING util.py:136 -- The `process_trial` operation took 0.8001930713653564 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    263 |          6743.41 | 42551296 |   298.08 |              328.768 |              138.768 |            763.768 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2947.930400501747
    time_step_min: 2746
  date: 2020-10-15_11-23-58
  done: false
  episode_len_mean: 763.7522562853663
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.1817425743719
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 221
  episodes_total: 55844
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.032974853390827e-39
        cur_lr: 5.0e-05
        entropy: 0.08270146821935971
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009026173249973604
        total_loss: .inf
        vf_explained_var: 0.9979791045188904
        vf_loss: 0.616348554690679
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.22666666666667
    gpu_util_percent0: 0.2966666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646187244551215
    mean_env_wait_ms: 1.2151311084903795
    mean_inference_ms: 4.295240592858246
    mean_raw_obs_processing_ms: 0.37643535280887863
  time_since_restore: 6768.99623632431
  time_this_iter_s: 25.590033769607544
  time_total_s: 6768.99623632431
  timers:
    learn_throughput: 8689.633
    learn_time_ms: 18618.968
    sample_throughput: 23614.635
    sample_time_ms: 6851.345
    update_time_ms: 32.329
  timestamp: 1602761038
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:23:59,706	WARNING util.py:136 -- The `process_trial` operation took 0.7430410385131836 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    264 |             6769 | 42713088 |  298.182 |              328.768 |              138.768 |            763.752 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2947.211503114014
    time_step_min: 2746
  date: 2020-10-15_11-24-25
  done: false
  episode_len_mean: 763.7299022754833
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.2906996349109
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 232
  episodes_total: 56076
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5494622800862402e-39
        cur_lr: 5.0e-05
        entropy: 0.07322607189416885
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008959981159326466
        total_loss: .inf
        vf_explained_var: 0.9989588260650635
        vf_loss: 0.32276992748181027
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.22258064516129
    gpu_util_percent0: 0.36999999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645936984972213
    mean_env_wait_ms: 1.2150620215707442
    mean_inference_ms: 4.29509789599784
    mean_raw_obs_processing_ms: 0.37642558671747645
  time_since_restore: 6794.725270271301
  time_this_iter_s: 25.729033946990967
  time_total_s: 6794.725270271301
  timers:
    learn_throughput: 8688.31
    learn_time_ms: 18621.804
    sample_throughput: 23682.828
    sample_time_ms: 6831.617
    update_time_ms: 33.127
  timestamp: 1602761065
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:24:26,474	WARNING util.py:136 -- The `process_trial` operation took 0.7426400184631348 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    265 |          6794.73 | 42874880 |  298.291 |              328.768 |              138.768 |             763.73 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2946.6988509018465
    time_step_min: 2746
  date: 2020-10-15_11-24-52
  done: false
  episode_len_mean: 763.7177595676983
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.36943137760795
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 56257
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3241934201293603e-39
        cur_lr: 5.0e-05
        entropy: 0.07481773570179939
        entropy_coeff: 0.0005000000000000001
        kl: 0.004735726824340721
        model: {}
        policy_loss: -0.01090862385899527
        total_loss: 0.6061863203843435
        vf_explained_var: 0.9977076053619385
        vf_loss: 0.6171323408683141
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.083333333333332
    gpu_util_percent0: 0.3693333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645722828880872
    mean_env_wait_ms: 1.2150085110640394
    mean_inference_ms: 4.294988964908301
    mean_raw_obs_processing_ms: 0.37641739305604327
  time_since_restore: 6820.297427654266
  time_this_iter_s: 25.572157382965088
  time_total_s: 6820.297427654266
  timers:
    learn_throughput: 8689.55
    learn_time_ms: 18619.145
    sample_throughput: 23715.791
    sample_time_ms: 6822.121
    update_time_ms: 33.318
  timestamp: 1602761092
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: cb791_00000
  
2020-10-15 11:24:53,061	WARNING util.py:136 -- The `process_trial` operation took 0.7521915435791016 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    266 |           6820.3 | 43036672 |  298.369 |              328.768 |              138.768 |            763.718 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2946.0266789491398
    time_step_min: 2746
  date: 2020-10-15_11-25-18
  done: false
  episode_len_mean: 763.7034591417646
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.4747451501274
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 56488
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1620967100646801e-39
        cur_lr: 5.0e-05
        entropy: 0.07532897777855396
        entropy_coeff: 0.0005000000000000001
        kl: 0.005004640435799956
        model: {}
        policy_loss: -0.007906585963307103
        total_loss: 0.4448355908195178
        vf_explained_var: 0.9985678195953369
        vf_loss: 0.4527798468867938
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.080645161290324
    gpu_util_percent0: 0.36032258064516126
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645536270715878
    mean_env_wait_ms: 1.214942408673184
    mean_inference_ms: 4.294875028230378
    mean_raw_obs_processing_ms: 0.37640762435887193
  time_since_restore: 6846.200524330139
  time_this_iter_s: 25.903096675872803
  time_total_s: 6846.200524330139
  timers:
    learn_throughput: 8685.115
    learn_time_ms: 18628.653
    sample_throughput: 23793.419
    sample_time_ms: 6799.864
    update_time_ms: 32.419
  timestamp: 1602761118
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: cb791_00000
  
2020-10-15 11:25:20,065	WARNING util.py:136 -- The `process_trial` operation took 0.7442712783813477 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    267 |           6846.2 | 43198464 |  298.475 |              328.768 |              138.768 |            763.703 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2945.3647992942215
    time_step_min: 2746
  date: 2020-10-15_11-25-45
  done: false
  episode_len_mean: 763.6864971611948
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.57805440945384
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 226
  episodes_total: 56714
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1620967100646801e-39
        cur_lr: 5.0e-05
        entropy: 0.07215368188917637
        entropy_coeff: 0.0005000000000000001
        kl: 0.003956345938301335
        model: {}
        policy_loss: -0.008628045484025884
        total_loss: 0.5510932728648186
        vf_explained_var: 0.9981803894042969
        vf_loss: 0.5597573866446813
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94
    gpu_util_percent0: 0.3313333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645287767141563
    mean_env_wait_ms: 1.2148744725617922
    mean_inference_ms: 4.294735411193345
    mean_raw_obs_processing_ms: 0.3763979654963558
  time_since_restore: 6871.882381916046
  time_this_iter_s: 25.681857585906982
  time_total_s: 6871.882381916046
  timers:
    learn_throughput: 8689.581
    learn_time_ms: 18619.08
    sample_throughput: 23784.541
    sample_time_ms: 6802.402
    update_time_ms: 34.375
  timestamp: 1602761145
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: cb791_00000
  
2020-10-15 11:25:46,754	WARNING util.py:136 -- The `process_trial` operation took 0.7395734786987305 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    268 |          6871.88 | 43360256 |  298.578 |              328.768 |              138.768 |            763.686 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2944.838668824316
    time_step_min: 2746
  date: 2020-10-15_11-26-12
  done: false
  episode_len_mean: 763.6739203037388
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.65934360745473
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 177
  episodes_total: 56891
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.810483550323401e-40
        cur_lr: 5.0e-05
        entropy: 0.06929263348380725
        entropy_coeff: 0.0005000000000000001
        kl: 0.004582066011304657
        model: {}
        policy_loss: -0.009892430711867442
        total_loss: 0.42375336835781735
        vf_explained_var: 0.9983647465705872
        vf_loss: 0.4336804449558258
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.693548387096776
    gpu_util_percent0: 0.27
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645091996961634
    mean_env_wait_ms: 1.2148228278456878
    mean_inference_ms: 4.294634716082672
    mean_raw_obs_processing_ms: 0.37639027037750267
  time_since_restore: 6898.079126358032
  time_this_iter_s: 26.196744441986084
  time_total_s: 6898.079126358032
  timers:
    learn_throughput: 8681.325
    learn_time_ms: 18636.787
    sample_throughput: 23688.781
    sample_time_ms: 6829.9
    update_time_ms: 35.03
  timestamp: 1602761172
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: cb791_00000
  
2020-10-15 11:26:13,974	WARNING util.py:136 -- The `process_trial` operation took 0.7597470283508301 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    269 |          6898.08 | 43522048 |  298.659 |              328.768 |              138.768 |            763.674 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2944.0881925347253
    time_step_min: 2746
  date: 2020-10-15_11-26-39
  done: false
  episode_len_mean: 763.6593033432522
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.77102903708874
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 239
  episodes_total: 57130
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9052417751617003e-40
        cur_lr: 5.0e-05
        entropy: 0.07845454973479112
        entropy_coeff: 0.0005000000000000001
        kl: 0.004694547038525343
        model: {}
        policy_loss: -0.007104375554869573
        total_loss: 0.4253923570116361
        vf_explained_var: 0.9987325072288513
        vf_loss: 0.43253597617149353
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.293548387096777
    gpu_util_percent0: 0.33290322580645154
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644880832463542
    mean_env_wait_ms: 1.2147543274263661
    mean_inference_ms: 4.294512407923175
    mean_raw_obs_processing_ms: 0.37638016334168045
  time_since_restore: 6924.019311666489
  time_this_iter_s: 25.94018530845642
  time_total_s: 6924.019311666489
  timers:
    learn_throughput: 8686.901
    learn_time_ms: 18624.824
    sample_throughput: 23604.068
    sample_time_ms: 6854.412
    update_time_ms: 36.464
  timestamp: 1602761199
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: cb791_00000
  
2020-10-15 11:26:41,107	WARNING util.py:136 -- The `process_trial` operation took 0.7496449947357178 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    270 |          6924.02 | 43683840 |  298.771 |              328.768 |              138.768 |            763.659 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2943.3948874541966
    time_step_min: 2746
  date: 2020-10-15_11-27-06
  done: false
  episode_len_mean: 763.6372909728155
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.8737656429681
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 219
  episodes_total: 57349
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4526208875808502e-40
        cur_lr: 5.0e-05
        entropy: 0.06860757060348988
        entropy_coeff: 0.0005000000000000001
        kl: 0.00443505864435186
        model: {}
        policy_loss: -0.007393998025994127
        total_loss: 0.24724265684684119
        vf_explained_var: 0.9990975260734558
        vf_loss: 0.25467095896601677
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.783333333333335
    gpu_util_percent0: 0.3306666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644658815824182
    mean_env_wait_ms: 1.214688568526508
    mean_inference_ms: 4.29438501574354
    mean_raw_obs_processing_ms: 0.3763710264194897
  time_since_restore: 6949.749157905579
  time_this_iter_s: 25.729846239089966
  time_total_s: 6949.749157905579
  timers:
    learn_throughput: 8690.358
    learn_time_ms: 18617.416
    sample_throughput: 23576.172
    sample_time_ms: 6862.522
    update_time_ms: 36.557
  timestamp: 1602761226
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: cb791_00000
  
2020-10-15 11:27:07,931	WARNING util.py:136 -- The `process_trial` operation took 0.7355091571807861 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    271 |          6949.75 | 43845632 |  298.874 |              328.768 |              138.768 |            763.637 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2942.83598573789
    time_step_min: 2746
  date: 2020-10-15_11-27-34
  done: false
  episode_len_mean: 763.6244481523969
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 298.95626880969445
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 185
  episodes_total: 57534
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.263104437904251e-41
        cur_lr: 5.0e-05
        entropy: 0.07340251592298348
        entropy_coeff: 0.0005000000000000001
        kl: 0.003951294599877049
        model: {}
        policy_loss: -0.009418351449615633
        total_loss: 0.5010652989149094
        vf_explained_var: 0.9981892704963684
        vf_loss: 0.510520356396834
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65806451612903
    gpu_util_percent0: 0.3161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464447111736747
    mean_env_wait_ms: 1.2146364511654753
    mean_inference_ms: 4.294291783590359
    mean_raw_obs_processing_ms: 0.37636345485418593
  time_since_restore: 6975.904631853104
  time_this_iter_s: 26.155473947525024
  time_total_s: 6975.904631853104
  timers:
    learn_throughput: 8691.046
    learn_time_ms: 18615.941
    sample_throughput: 23516.531
    sample_time_ms: 6879.926
    update_time_ms: 38.136
  timestamp: 1602761254
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: cb791_00000
  
2020-10-15 11:27:35,111	WARNING util.py:136 -- The `process_trial` operation took 0.7461004257202148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    272 |           6975.9 | 44007424 |  298.956 |              328.768 |              138.768 |            763.624 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2942.164368851323
    time_step_min: 2746
  date: 2020-10-15_11-28-00
  done: false
  episode_len_mean: 763.606525313717
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 299.0591732994155
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 241
  episodes_total: 57775
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6315522189521254e-41
        cur_lr: 5.0e-05
        entropy: 0.07737080566585064
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010833719104994088
        total_loss: .inf
        vf_explained_var: 0.9981641173362732
        vf_loss: 0.6296995505690575
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.23225806451613
    gpu_util_percent0: 0.27741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644257773301517
    mean_env_wait_ms: 1.2145659330430103
    mean_inference_ms: 4.294166565069164
    mean_raw_obs_processing_ms: 0.37635350602298034
  time_since_restore: 7001.771580219269
  time_this_iter_s: 25.86694836616516
  time_total_s: 7001.771580219269
  timers:
    learn_throughput: 8692.002
    learn_time_ms: 18613.893
    sample_throughput: 23540.002
    sample_time_ms: 6873.067
    update_time_ms: 37.168
  timestamp: 1602761280
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:28:02,185	WARNING util.py:136 -- The `process_trial` operation took 0.7504639625549316 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    273 |          7001.77 | 44169216 |  299.059 |              328.768 |              138.768 |            763.607 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2941.599409754582
    time_step_min: 2746
  date: 2020-10-15_11-28-27
  done: false
  episode_len_mean: 763.5940566737379
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 299.1488439525381
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 206
  episodes_total: 57981
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.447328328428189e-41
        cur_lr: 5.0e-05
        entropy: 0.06818708466986816
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045184008389090495
        model: {}
        policy_loss: -0.007188711839262396
        total_loss: 0.3352556253472964
        vf_explained_var: 0.9987686276435852
        vf_loss: 0.3424784317612648
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.233333333333334
    gpu_util_percent0: 0.33100000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644060796436734
    mean_env_wait_ms: 1.214504341615738
    mean_inference_ms: 4.2940503384660875
    mean_raw_obs_processing_ms: 0.3763450719898623
  time_since_restore: 7027.476446390152
  time_this_iter_s: 25.70486617088318
  time_total_s: 7027.476446390152
  timers:
    learn_throughput: 8692.351
    learn_time_ms: 18613.146
    sample_throughput: 23517.874
    sample_time_ms: 6879.533
    update_time_ms: 39.303
  timestamp: 1602761307
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: cb791_00000
  
2020-10-15 11:28:28,953	WARNING util.py:136 -- The `process_trial` operation took 0.7909815311431885 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    274 |          7027.48 | 44331008 |  299.149 |              328.768 |              138.768 |            763.594 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2941.0546305215357
    time_step_min: 2746
  date: 2020-10-15_11-28-54
  done: false
  episode_len_mean: 763.5805930382467
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 299.2348148437532
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 194
  episodes_total: 58175
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7236641642140946e-41
        cur_lr: 5.0e-05
        entropy: 0.07087160398562749
        entropy_coeff: 0.0005000000000000001
        kl: 0.003477622017574807
        model: {}
        policy_loss: -0.008034988190047443
        total_loss: 0.33778785665829975
        vf_explained_var: 0.9987411499023438
        vf_loss: 0.34585829079151154
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.096774193548388
    gpu_util_percent0: 0.34096774193548385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643871140937445
    mean_env_wait_ms: 1.2144500099646094
    mean_inference_ms: 4.293955488090201
    mean_raw_obs_processing_ms: 0.3763377378958023
  time_since_restore: 7053.25664973259
  time_this_iter_s: 25.780203342437744
  time_total_s: 7053.25664973259
  timers:
    learn_throughput: 8692.635
    learn_time_ms: 18612.538
    sample_throughput: 23515.556
    sample_time_ms: 6880.212
    update_time_ms: 36.873
  timestamp: 1602761334
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: cb791_00000
  
2020-10-15 11:28:55,759	WARNING util.py:136 -- The `process_trial` operation took 0.7548315525054932 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    275 |          7053.26 | 44492800 |  299.235 |              328.768 |              138.768 |            763.581 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2940.3561493662214
    time_step_min: 2746
  date: 2020-10-15_11-29-21
  done: false
  episode_len_mean: 763.5674010167925
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 299.34240935519614
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 244
  episodes_total: 58419
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3618320821070473e-41
        cur_lr: 5.0e-05
        entropy: 0.07170034262041251
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009507794410941036
        total_loss: .inf
        vf_explained_var: 0.9986686110496521
        vf_loss: 0.4280104065934817
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.74193548387097
    gpu_util_percent0: 0.28548387096774197
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643657497904425
    mean_env_wait_ms: 1.214378397607995
    mean_inference_ms: 4.29383006962047
    mean_raw_obs_processing_ms: 0.3763272828718082
  time_since_restore: 7079.129723548889
  time_this_iter_s: 25.87307381629944
  time_total_s: 7079.129723548889
  timers:
    learn_throughput: 8680.651
    learn_time_ms: 18638.233
    sample_throughput: 23501.099
    sample_time_ms: 6884.444
    update_time_ms: 36.179
  timestamp: 1602761361
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:29:22,877	WARNING util.py:136 -- The `process_trial` operation took 0.7897164821624756 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    276 |          7079.13 | 44654592 |  299.342 |              328.768 |              138.768 |            763.567 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2939.757379176127
    time_step_min: 2746
  date: 2020-10-15_11-29-48
  done: false
  episode_len_mean: 763.5536201719667
  episode_reward_max: 328.7676767676765
  episode_reward_mean: 299.43215817930894
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 197
  episodes_total: 58616
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0427481231605705e-41
        cur_lr: 5.0e-05
        entropy: 0.06384637672454119
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006535389703155185
        total_loss: .inf
        vf_explained_var: 0.998887300491333
        vf_loss: 0.2974754919608434
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.330000000000002
    gpu_util_percent0: 0.3436666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643485027483383
    mean_env_wait_ms: 1.214319794710854
    mean_inference_ms: 4.293723732168084
    mean_raw_obs_processing_ms: 0.3763197805367655
  time_since_restore: 7104.853107452393
  time_this_iter_s: 25.723383903503418
  time_total_s: 7104.853107452393
  timers:
    learn_throughput: 8687.682
    learn_time_ms: 18623.15
    sample_throughput: 23485.159
    sample_time_ms: 6889.117
    update_time_ms: 35.946
  timestamp: 1602761388
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:29:49,677	WARNING util.py:136 -- The `process_trial` operation took 0.8012452125549316 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    277 |          7104.85 | 44816384 |  299.432 |              328.768 |              138.768 |            763.554 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2939.1805049851973
    time_step_min: 2746
  date: 2020-10-15_11-30-15
  done: false
  episode_len_mean: 763.539370547328
  episode_reward_max: 328.7676767676768
  episode_reward_mean: 299.51882485954866
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 197
  episodes_total: 58813
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.064122184740856e-41
        cur_lr: 5.0e-05
        entropy: 0.07233512339492638
        entropy_coeff: 0.0005000000000000001
        kl: 0.006205440343668063
        model: {}
        policy_loss: -0.006526612948315839
        total_loss: 0.35190559426943463
        vf_explained_var: 0.9986814856529236
        vf_loss: 0.3584683711330096
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96774193548387
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643299132896523
    mean_env_wait_ms: 1.214264657422202
    mean_inference_ms: 4.2936263206369025
    mean_raw_obs_processing_ms: 0.3763122896764291
  time_since_restore: 7130.825808286667
  time_this_iter_s: 25.972700834274292
  time_total_s: 7130.825808286667
  timers:
    learn_throughput: 8670.766
    learn_time_ms: 18659.482
    sample_throughput: 23511.307
    sample_time_ms: 6881.455
    update_time_ms: 35.622
  timestamp: 1602761415
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: cb791_00000
  
2020-10-15 11:30:16,721	WARNING util.py:136 -- The `process_trial` operation took 0.7896175384521484 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    278 |          7130.83 | 44978176 |  299.519 |              328.768 |              138.768 |            763.539 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2938.4455645161293
    time_step_min: 2746
  date: 2020-10-15_11-30-42
  done: false
  episode_len_mean: 763.516059123309
  episode_reward_max: 328.7676767676768
  episode_reward_mean: 299.63010717711614
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 59063
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.064122184740856e-41
        cur_lr: 5.0e-05
        entropy: 0.07053930188218753
        entropy_coeff: 0.0005000000000000001
        kl: 0.004462655323247115
        model: {}
        policy_loss: -0.0076561655635790276
        total_loss: 0.5245793436964353
        vf_explained_var: 0.9983269572257996
        vf_loss: 0.5322707742452621
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.193333333333335
    gpu_util_percent0: 0.31933333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643079593498007
    mean_env_wait_ms: 1.2141922080008198
    mean_inference_ms: 4.293504035656761
    mean_raw_obs_processing_ms: 0.376302171230832
  time_since_restore: 7156.376531600952
  time_this_iter_s: 25.55072331428528
  time_total_s: 7156.376531600952
  timers:
    learn_throughput: 8688.005
    learn_time_ms: 18622.458
    sample_throughput: 23602.362
    sample_time_ms: 6854.907
    update_time_ms: 33.279
  timestamp: 1602761442
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: cb791_00000
  
2020-10-15 11:30:43,436	WARNING util.py:136 -- The `process_trial` operation took 0.7983310222625732 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    279 |          7156.38 | 45139968 |   299.63 |              328.768 |              138.768 |            763.516 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2937.93058956648
    time_step_min: 2746
  date: 2020-10-15_11-31-09
  done: false
  episode_len_mean: 763.4978059812327
  episode_reward_max: 328.7676767676768
  episode_reward_mean: 299.7098134862428
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 189
  episodes_total: 59252
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.532061092370428e-41
        cur_lr: 5.0e-05
        entropy: 0.06751483057936032
        entropy_coeff: 0.0005000000000000001
        kl: 0.003643976136421164
        model: {}
        policy_loss: -0.008687249493353496
        total_loss: 0.4906865780552228
        vf_explained_var: 0.9980722069740295
        vf_loss: 0.4994075944026311
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.532258064516128
    gpu_util_percent0: 0.3583870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464290627430543
    mean_env_wait_ms: 1.2141362432968412
    mean_inference_ms: 4.293408667359824
    mean_raw_obs_processing_ms: 0.3762953369136825
  time_since_restore: 7182.215963602066
  time_this_iter_s: 25.83943200111389
  time_total_s: 7182.215963602066
  timers:
    learn_throughput: 8684.032
    learn_time_ms: 18630.978
    sample_throughput: 23702.645
    sample_time_ms: 6825.905
    update_time_ms: 33.71
  timestamp: 1602761469
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: cb791_00000
  
2020-10-15 11:31:10,380	WARNING util.py:136 -- The `process_trial` operation took 0.8096716403961182 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    280 |          7182.22 | 45301760 |   299.71 |              328.768 |              138.768 |            763.498 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2937.3268819737796
    time_step_min: 2746
  date: 2020-10-15_11-31-36
  done: false
  episode_len_mean: 763.4742507316089
  episode_reward_max: 328.7676767676768
  episode_reward_mean: 299.7986005570182
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 206
  episodes_total: 59458
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.66030546185214e-42
        cur_lr: 5.0e-05
        entropy: 0.07036675016085307
        entropy_coeff: 0.0005000000000000001
        kl: 0.00394921643116201
        model: {}
        policy_loss: -0.009160030521646453
        total_loss: 0.34553734461466473
        vf_explained_var: 0.9988163113594055
        vf_loss: 0.35473255316416424
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.956666666666667
    gpu_util_percent0: 0.36000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642746859505515
    mean_env_wait_ms: 1.214077908532524
    mean_inference_ms: 4.293309481088927
    mean_raw_obs_processing_ms: 0.3762866055525392
  time_since_restore: 7208.139324903488
  time_this_iter_s: 25.92336130142212
  time_total_s: 7208.139324903488
  timers:
    learn_throughput: 8678.429
    learn_time_ms: 18643.007
    sample_throughput: 23682.282
    sample_time_ms: 6831.774
    update_time_ms: 33.699
  timestamp: 1602761496
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: cb791_00000
  
2020-10-15 11:31:37,448	WARNING util.py:136 -- The `process_trial` operation took 0.7713892459869385 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    281 |          7208.14 | 45463552 |  299.799 |              328.768 |              138.768 |            763.474 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2936.5963262775067
    time_step_min: 2746
  date: 2020-10-15_11-32-03
  done: false
  episode_len_mean: 763.4474592168291
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 299.9095395721864
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 248
  episodes_total: 59706
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.83015273092607e-42
        cur_lr: 5.0e-05
        entropy: 0.07199750219782193
        entropy_coeff: 0.0005000000000000001
        kl: 0.006578337633982301
        model: {}
        policy_loss: -0.006896220923711856
        total_loss: 0.41448502242565155
        vf_explained_var: 0.9986414909362793
        vf_loss: 0.4214172462622325
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.84193548387097
    gpu_util_percent0: 0.3196774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642503345769245
    mean_env_wait_ms: 1.2140065133799056
    mean_inference_ms: 4.293187045983982
    mean_raw_obs_processing_ms: 0.37627781281200484
  time_since_restore: 7234.082690954208
  time_this_iter_s: 25.943366050720215
  time_total_s: 7234.082690954208
  timers:
    learn_throughput: 8675.811
    learn_time_ms: 18648.632
    sample_throughput: 23777.19
    sample_time_ms: 6804.505
    update_time_ms: 33.935
  timestamp: 1602761523
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: cb791_00000
  
2020-10-15 11:32:04,467	WARNING util.py:136 -- The `process_trial` operation took 0.7780520915985107 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    282 |          7234.08 | 45625344 |   299.91 |              328.768 |              138.768 |            763.447 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2936.06737067237
    time_step_min: 2746
  date: 2020-10-15_11-32-30
  done: false
  episode_len_mean: 763.4317464558252
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 299.99099583677184
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 59887
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.83015273092607e-42
        cur_lr: 5.0e-05
        entropy: 0.06653563678264618
        entropy_coeff: 0.0005000000000000001
        kl: 0.004583308628449838
        model: {}
        policy_loss: -0.004597411374561489
        total_loss: 0.2796569342414538
        vf_explained_var: 0.9988612532615662
        vf_loss: 0.28428761288523674
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870963
    gpu_util_percent0: 0.38903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642343310375522
    mean_env_wait_ms: 1.2139531264872825
    mean_inference_ms: 4.2930958749693175
    mean_raw_obs_processing_ms: 0.37627088809913883
  time_since_restore: 7260.028479576111
  time_this_iter_s: 25.945788621902466
  time_total_s: 7260.028479576111
  timers:
    learn_throughput: 8679.418
    learn_time_ms: 18640.882
    sample_throughput: 23761.111
    sample_time_ms: 6809.109
    update_time_ms: 33.525
  timestamp: 1602761550
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: cb791_00000
  
2020-10-15 11:32:31,501	WARNING util.py:136 -- The `process_trial` operation took 0.7995657920837402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    283 |          7260.03 | 45787136 |  299.991 |              328.768 |              138.768 |            763.432 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2935.4507058666845
    time_step_min: 2746
  date: 2020-10-15_11-32-57
  done: false
  episode_len_mean: 763.4120152394896
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.0874191866254
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 220
  episodes_total: 60107
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.915076365463035e-42
        cur_lr: 5.0e-05
        entropy: 0.07720796515544255
        entropy_coeff: 0.0005000000000000001
        kl: 0.004859715312098463
        model: {}
        policy_loss: -0.008481157943606377
        total_loss: 0.3256527930498123
        vf_explained_var: 0.9988664984703064
        vf_loss: 0.3341725518306096
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02333333333333
    gpu_util_percent0: 0.3296666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642172113975915
    mean_env_wait_ms: 1.2138917110813228
    mean_inference_ms: 4.292998987026131
    mean_raw_obs_processing_ms: 0.3762622999974217
  time_since_restore: 7285.575392246246
  time_this_iter_s: 25.546912670135498
  time_total_s: 7285.575392246246
  timers:
    learn_throughput: 8683.061
    learn_time_ms: 18633.061
    sample_throughput: 23780.077
    sample_time_ms: 6803.679
    update_time_ms: 33.145
  timestamp: 1602761577
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: cb791_00000
  
2020-10-15 11:32:58,135	WARNING util.py:136 -- The `process_trial` operation took 0.8079538345336914 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    284 |          7285.58 | 45948928 |  300.087 |              328.768 |              138.768 |            763.412 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2934.7993466328376
    time_step_min: 2746
  date: 2020-10-15_11-33-23
  done: false
  episode_len_mean: 763.3969706009082
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.1808489589138
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 235
  episodes_total: 60342
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.575381827315175e-43
        cur_lr: 5.0e-05
        entropy: 0.07204409812887509
        entropy_coeff: 0.0005000000000000001
        kl: 0.003634159977082163
        model: {}
        policy_loss: -0.009808233260021856
        total_loss: 0.414315365254879
        vf_explained_var: 0.9986381530761719
        vf_loss: 0.42415960133075714
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.863333333333333
    gpu_util_percent0: 0.3353333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641958362302723
    mean_env_wait_ms: 1.2138233137323908
    mean_inference_ms: 4.292880891447068
    mean_raw_obs_processing_ms: 0.37625380771947176
  time_since_restore: 7311.106329917908
  time_this_iter_s: 25.530937671661377
  time_total_s: 7311.106329917908
  timers:
    learn_throughput: 8686.133
    learn_time_ms: 18626.469
    sample_throughput: 23816.645
    sample_time_ms: 6793.232
    update_time_ms: 32.566
  timestamp: 1602761603
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: cb791_00000
  
2020-10-15 11:33:24,745	WARNING util.py:136 -- The `process_trial` operation took 0.793360710144043 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    285 |          7311.11 | 46110720 |  300.181 |              328.768 |              138.768 |            763.397 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2934.2830806368943
    time_step_min: 2746
  date: 2020-10-15_11-33-50
  done: false
  episode_len_mean: 763.3814441506939
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.25697406984574
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 60520
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.787690913657587e-43
        cur_lr: 5.0e-05
        entropy: 0.07235482645531495
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054230052046477795
        model: {}
        policy_loss: -0.0078039182553766295
        total_loss: 0.2341267131268978
        vf_explained_var: 0.999030351638794
        vf_loss: 0.24196681131919226
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.345161290322583
    gpu_util_percent0: 0.2819354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641784063907273
    mean_env_wait_ms: 1.213771255668602
    mean_inference_ms: 4.292792421549978
    mean_raw_obs_processing_ms: 0.3762470728618644
  time_since_restore: 7336.9251828193665
  time_this_iter_s: 25.81885290145874
  time_total_s: 7336.9251828193665
  timers:
    learn_throughput: 8692.326
    learn_time_ms: 18613.199
    sample_throughput: 23793.382
    sample_time_ms: 6799.874
    update_time_ms: 33.828
  timestamp: 1602761630
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: cb791_00000
  
2020-10-15 11:33:51,847	WARNING util.py:136 -- The `process_trial` operation took 0.8281364440917969 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    286 |          7336.93 | 46272512 |  300.257 |              328.768 |              138.768 |            763.381 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2933.688458434221
    time_step_min: 2746
  date: 2020-10-15_11-34-17
  done: false
  episode_len_mean: 763.3688230452675
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.344579124579
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 230
  episodes_total: 60750
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.787690913657587e-43
        cur_lr: 5.0e-05
        entropy: 0.08480767471094926
        entropy_coeff: 0.0005000000000000001
        kl: 0.00438870006473735
        model: {}
        policy_loss: -0.009024240100795092
        total_loss: 0.8722346474726995
        vf_explained_var: 0.9973358511924744
        vf_loss: 0.881301278869311
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.923333333333332
    gpu_util_percent0: 0.3713333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641609602213065
    mean_env_wait_ms: 1.2137069458923233
    mean_inference_ms: 4.292693733574133
    mean_raw_obs_processing_ms: 0.37623797276138393
  time_since_restore: 7362.564334154129
  time_this_iter_s: 25.639151334762573
  time_total_s: 7362.564334154129
  timers:
    learn_throughput: 8699.517
    learn_time_ms: 18597.814
    sample_throughput: 23804.675
    sample_time_ms: 6796.648
    update_time_ms: 41.456
  timestamp: 1602761657
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: cb791_00000
  
2020-10-15 11:34:18,580	WARNING util.py:136 -- The `process_trial` operation took 0.8083441257476807 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    287 |          7362.56 | 46434304 |  300.345 |              328.768 |              138.768 |            763.369 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2933.1717811546164
    time_step_min: 2746
  date: 2020-10-15_11-34-44
  done: false
  episode_len_mean: 763.3687783918526
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.4249534060117
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 227
  episodes_total: 60977
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3938454568287937e-43
        cur_lr: 5.0e-05
        entropy: 0.07925443537533283
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010389683399504671
        total_loss: .inf
        vf_explained_var: 0.9972968697547913
        vf_loss: 0.9006663908561071
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.88
    gpu_util_percent0: 0.372
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464140777193867
    mean_env_wait_ms: 1.213640900926005
    mean_inference_ms: 4.292578525730123
    mean_raw_obs_processing_ms: 0.37623014219450807
  time_since_restore: 7388.028525829315
  time_this_iter_s: 25.464191675186157
  time_total_s: 7388.028525829315
  timers:
    learn_throughput: 8722.477
    learn_time_ms: 18548.859
    sample_throughput: 23806.978
    sample_time_ms: 6795.991
    update_time_ms: 41.002
  timestamp: 1602761684
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:34:45,161	WARNING util.py:136 -- The `process_trial` operation took 0.8246233463287354 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    288 |          7388.03 | 46596096 |  300.425 |              328.768 |              138.768 |            763.369 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2932.68692322796
    time_step_min: 2746
  date: 2020-10-15_11-35-10
  done: false
  episode_len_mean: 763.3630120186411
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.49844780897007
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 61155
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.590768185243191e-43
        cur_lr: 5.0e-05
        entropy: 0.0719764909396569
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048462659275780124
        model: {}
        policy_loss: -0.00877534601992617
        total_loss: 0.44379519174496335
        vf_explained_var: 0.9983288645744324
        vf_loss: 0.4526065265138944
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.463333333333335
    gpu_util_percent0: 0.32366666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464123898448456
    mean_env_wait_ms: 1.2135900414835634
    mean_inference_ms: 4.292497213213992
    mean_raw_obs_processing_ms: 0.3762238247683065
  time_since_restore: 7413.615791320801
  time_this_iter_s: 25.587265491485596
  time_total_s: 7413.615791320801
  timers:
    learn_throughput: 8719.737
    learn_time_ms: 18554.688
    sample_throughput: 23816.185
    sample_time_ms: 6793.364
    update_time_ms: 40.676
  timestamp: 1602761710
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: cb791_00000
  
2020-10-15 11:35:11,981	WARNING util.py:136 -- The `process_trial` operation took 0.8265628814697266 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    289 |          7413.62 | 46757888 |  300.498 |              328.768 |              138.768 |            763.363 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2932.0416306704265
    time_step_min: 2746
  date: 2020-10-15_11-35-37
  done: false
  episode_len_mean: 763.3585228383397
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.5922683866091
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 61388
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7953840926215954e-43
        cur_lr: 5.0e-05
        entropy: 0.0812924262136221
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007744997603973995
        total_loss: .inf
        vf_explained_var: 0.9981916546821594
        vf_loss: 0.5869274238745371
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.18064516129032
    gpu_util_percent0: 0.2958064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641033350056726
    mean_env_wait_ms: 1.2135241362610198
    mean_inference_ms: 4.292392004554064
    mean_raw_obs_processing_ms: 0.3762144491989024
  time_since_restore: 7439.448356628418
  time_this_iter_s: 25.832565307617188
  time_total_s: 7439.448356628418
  timers:
    learn_throughput: 8717.239
    learn_time_ms: 18560.005
    sample_throughput: 23841.352
    sample_time_ms: 6786.192
    update_time_ms: 40.774
  timestamp: 1602761737
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:35:38,951	WARNING util.py:136 -- The `process_trial` operation took 0.8408536911010742 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    290 |          7439.45 | 46919680 |  300.592 |              328.768 |              138.768 |            763.359 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2931.4817533659234
    time_step_min: 2746
  date: 2020-10-15_11-36-04
  done: false
  episode_len_mean: 763.3545413231188
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.6783041739867
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 224
  episodes_total: 61612
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6930761389323933e-43
        cur_lr: 5.0e-05
        entropy: 0.07548688910901546
        entropy_coeff: 0.0005000000000000001
        kl: 0.005068026909915109
        model: {}
        policy_loss: -0.01135059882896409
        total_loss: 0.4246595899264018
        vf_explained_var: 0.9985715746879578
        vf_loss: 0.4360479265451431
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89666666666667
    gpu_util_percent0: 0.301
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640866168695973
    mean_env_wait_ms: 1.2134597661118047
    mean_inference_ms: 4.292286601664801
    mean_raw_obs_processing_ms: 0.3762073248843332
  time_since_restore: 7465.385724544525
  time_this_iter_s: 25.937367916107178
  time_total_s: 7465.385724544525
  timers:
    learn_throughput: 8709.814
    learn_time_ms: 18575.827
    sample_throughput: 23894.413
    sample_time_ms: 6771.123
    update_time_ms: 40.805
  timestamp: 1602761764
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: cb791_00000
  
2020-10-15 11:36:06,101	WARNING util.py:136 -- The `process_trial` operation took 0.8099579811096191 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    291 |          7465.39 | 47081472 |  300.678 |              328.768 |              138.768 |            763.355 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2930.9790941330784
    time_step_min: 2746
  date: 2020-10-15_11-36-31
  done: false
  episode_len_mean: 763.3470513982393
  episode_reward_max: 328.7676767676769
  episode_reward_mean: 300.7536723069639
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 180
  episodes_total: 61792
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6930761389323933e-43
        cur_lr: 5.0e-05
        entropy: 0.07069008921583493
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007767485658405349
        total_loss: .inf
        vf_explained_var: 0.9986122250556946
        vf_loss: 0.3537807563940684
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.322580645161295
    gpu_util_percent0: 0.3254838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640716011152555
    mean_env_wait_ms: 1.2134091184559572
    mean_inference_ms: 4.292211234442072
    mean_raw_obs_processing_ms: 0.37620126052641134
  time_since_restore: 7491.180378198624
  time_this_iter_s: 25.79465365409851
  time_total_s: 7491.180378198624
  timers:
    learn_throughput: 8720.504
    learn_time_ms: 18553.056
    sample_throughput: 23894.294
    sample_time_ms: 6771.156
    update_time_ms: 39.174
  timestamp: 1602761791
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:36:33,048	WARNING util.py:136 -- The `process_trial` operation took 0.812382698059082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    292 |          7491.18 | 47243264 |  300.754 |              328.768 |              138.768 |            763.347 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2930.364623326343
    time_step_min: 2746
  date: 2020-10-15_11-36-58
  done: false
  episode_len_mean: 763.3386641732093
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 300.8449382831848
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 237
  episodes_total: 62029
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.03961420839859e-43
        cur_lr: 5.0e-05
        entropy: 0.07483759957055251
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009314892250889292
        total_loss: .inf
        vf_explained_var: 0.9981407523155212
        vf_loss: 0.6094729502995809
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.616666666666664
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640505865972178
    mean_env_wait_ms: 1.2133426629944435
    mean_inference_ms: 4.292102813423788
    mean_raw_obs_processing_ms: 0.3761918616290696
  time_since_restore: 7516.905264377594
  time_this_iter_s: 25.724886178970337
  time_total_s: 7516.905264377594
  timers:
    learn_throughput: 8730.185
    learn_time_ms: 18532.482
    sample_throughput: 23870.647
    sample_time_ms: 6777.864
    update_time_ms: 40.43
  timestamp: 1602761818
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:36:59,886	WARNING util.py:136 -- The `process_trial` operation took 0.819831371307373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    293 |          7516.91 | 47405056 |  300.845 |              328.768 |              138.768 |            763.339 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2929.7778099279835
    time_step_min: 2746
  date: 2020-10-15_11-37-25
  done: false
  episode_len_mean: 763.3318232203961
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 300.932989833756
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 218
  episodes_total: 62247
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.059421312597884e-43
        cur_lr: 5.0e-05
        entropy: 0.07010493737955888
        entropy_coeff: 0.0005000000000000001
        kl: 0.004499604692682624
        model: {}
        policy_loss: -0.008024982448356846
        total_loss: 0.305780661602815
        vf_explained_var: 0.9989109039306641
        vf_loss: 0.3138406922419866
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.773333333333333
    gpu_util_percent0: 0.3486666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640332493502464
    mean_env_wait_ms: 1.2132793579129475
    mean_inference_ms: 4.292002832515093
    mean_raw_obs_processing_ms: 0.37618480888493316
  time_since_restore: 7542.488507509232
  time_this_iter_s: 25.583243131637573
  time_total_s: 7542.488507509232
  timers:
    learn_throughput: 8728.669
    learn_time_ms: 18535.701
    sample_throughput: 23867.77
    sample_time_ms: 6778.681
    update_time_ms: 38.872
  timestamp: 1602761845
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: cb791_00000
  
2020-10-15 11:37:26,596	WARNING util.py:136 -- The `process_trial` operation took 0.8277764320373535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    294 |          7542.49 | 47566848 |  300.933 |              328.768 |              138.768 |            763.332 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2929.2843863191974
    time_step_min: 2746
  date: 2020-10-15_11-37-52
  done: false
  episode_len_mean: 763.3205035798376
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.00906620381875
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 186
  episodes_total: 62433
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.029710656298942e-43
        cur_lr: 5.0e-05
        entropy: 0.07020291313529015
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0066739102961340295
        total_loss: .inf
        vf_explained_var: 0.9986224174499512
        vf_loss: 0.37363456437985104
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.529032258064515
    gpu_util_percent0: 0.3635483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640179571663858
    mean_env_wait_ms: 1.2132278123763969
    mean_inference_ms: 4.29192459367337
    mean_raw_obs_processing_ms: 0.3761787100379396
  time_since_restore: 7568.623522281647
  time_this_iter_s: 26.13501477241516
  time_total_s: 7568.623522281647
  timers:
    learn_throughput: 8707.412
    learn_time_ms: 18580.951
    sample_throughput: 23821.354
    sample_time_ms: 6791.889
    update_time_ms: 38.871
  timestamp: 1602761872
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:37:53,901	WARNING util.py:136 -- The `process_trial` operation took 0.8555445671081543 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    295 |          7568.62 | 47728640 |  301.009 |              328.768 |              138.768 |            763.321 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2928.679636607485
    time_step_min: 2746
  date: 2020-10-15_11-38-19
  done: false
  episode_len_mean: 763.3075585198895
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.10341225598665
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 238
  episodes_total: 62671
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5445659844484126e-43
        cur_lr: 5.0e-05
        entropy: 0.07324327776829402
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008803500337914253
        total_loss: .inf
        vf_explained_var: 0.9983174204826355
        vf_loss: 0.5224509686231613
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79032258064516
    gpu_util_percent0: 0.33806451612903227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640011939753367
    mean_env_wait_ms: 1.2131610706504297
    mean_inference_ms: 4.291824747508831
    mean_raw_obs_processing_ms: 0.3761702355834641
  time_since_restore: 7594.518788814545
  time_this_iter_s: 25.89526653289795
  time_total_s: 7594.518788814545
  timers:
    learn_throughput: 8699.743
    learn_time_ms: 18597.331
    sample_throughput: 23849.186
    sample_time_ms: 6783.963
    update_time_ms: 37.048
  timestamp: 1602761899
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:38:20,970	WARNING util.py:136 -- The `process_trial` operation took 0.8232297897338867 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    296 |          7594.52 | 47890432 |  301.103 |              328.768 |              138.768 |            763.308 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2928.1135987524663
    time_step_min: 2746
  date: 2020-10-15_11-38-46
  done: false
  episode_len_mean: 763.2940699394113
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.18788572717284
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 212
  episodes_total: 62883
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.8168489766726205e-43
        cur_lr: 5.0e-05
        entropy: 0.06746729711691539
        entropy_coeff: 0.0005000000000000001
        kl: 0.00511969206854701
        model: {}
        policy_loss: -0.008524221334179552
        total_loss: 0.1774152380724748
        vf_explained_var: 0.9993212223052979
        vf_loss: 0.18597319722175598
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.516666666666666
    gpu_util_percent0: 0.331
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463983270166834
    mean_env_wait_ms: 1.2130998995457964
    mean_inference_ms: 4.291726988668974
    mean_raw_obs_processing_ms: 0.37616328208572036
  time_since_restore: 7620.136889457703
  time_this_iter_s: 25.61810064315796
  time_total_s: 7620.136889457703
  timers:
    learn_throughput: 8693.805
    learn_time_ms: 18610.033
    sample_throughput: 23873.297
    sample_time_ms: 6777.112
    update_time_ms: 29.25
  timestamp: 1602761926
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: cb791_00000
  
2020-10-15 11:38:47,686	WARNING util.py:136 -- The `process_trial` operation took 0.8067893981933594 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    297 |          7620.14 | 48052224 |  301.188 |              328.768 |              138.768 |            763.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2927.59872449789
    time_step_min: 2746
  date: 2020-10-15_11-39-13
  done: false
  episode_len_mean: 763.2775038447513
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.26396413839524
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 63073
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.8168489766726205e-43
        cur_lr: 5.0e-05
        entropy: 0.07567522364358108
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056622622068971395
        model: {}
        policy_loss: -0.007780098705552518
        total_loss: 0.3067936959366004
        vf_explained_var: 0.998784065246582
        vf_loss: 0.31461162368456524
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.033333333333335
    gpu_util_percent0: 0.3843333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639675443586508
    mean_env_wait_ms: 1.213047672800109
    mean_inference_ms: 4.29164573131712
    mean_raw_obs_processing_ms: 0.3761567080675272
  time_since_restore: 7645.708956718445
  time_this_iter_s: 25.572067260742188
  time_total_s: 7645.708956718445
  timers:
    learn_throughput: 8690.712
    learn_time_ms: 18616.657
    sample_throughput: 23862.509
    sample_time_ms: 6780.176
    update_time_ms: 27.509
  timestamp: 1602761953
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: cb791_00000
  
2020-10-15 11:39:14,526	WARNING util.py:136 -- The `process_trial` operation took 0.8748598098754883 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    298 |          7645.71 | 48214016 |  301.264 |              328.768 |              138.768 |            763.278 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2926.9660224094064
    time_step_min: 2746
  date: 2020-10-15_11-39-40
  done: false
  episode_len_mean: 763.2579916608756
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.3608888812312
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 243
  episodes_total: 63316
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.8168489766726205e-43
        cur_lr: 5.0e-05
        entropy: 0.07405542892714341
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008788440648155907
        total_loss: .inf
        vf_explained_var: 0.9989457726478577
        vf_loss: 0.34065699328978855
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16774193548387
    gpu_util_percent0: 0.362258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639493019390631
    mean_env_wait_ms: 1.2129794943661456
    mean_inference_ms: 4.29154561175728
    mean_raw_obs_processing_ms: 0.3761483392974908
  time_since_restore: 7671.353079557419
  time_this_iter_s: 25.644122838974
  time_total_s: 7671.353079557419
  timers:
    learn_throughput: 8695.72
    learn_time_ms: 18605.936
    sample_throughput: 23837.292
    sample_time_ms: 6787.348
    update_time_ms: 35.349
  timestamp: 1602761980
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:39:41,299	WARNING util.py:136 -- The `process_trial` operation took 0.8248317241668701 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    299 |          7671.35 | 48375808 |  301.361 |              328.768 |              138.768 |            763.258 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2926.4735573514818
    time_step_min: 2746
  date: 2020-10-15_11-40-06
  done: false
  episode_len_mean: 763.2449776434285
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.43259584318514
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 200
  episodes_total: 63516
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.022527346500893e-42
        cur_lr: 5.0e-05
        entropy: 0.07874068059027195
        entropy_coeff: 0.0005000000000000001
        kl: 0.005912370320099096
        model: {}
        policy_loss: -0.010610315424855798
        total_loss: 0.558735673626264
        vf_explained_var: 0.9979423880577087
        vf_loss: 0.5693853398164114
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.85666666666667
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463933374626595
    mean_env_wait_ms: 1.212922384678405
    mean_inference_ms: 4.29145806992216
    mean_raw_obs_processing_ms: 0.37614205909716414
  time_since_restore: 7696.9300673007965
  time_this_iter_s: 25.576987743377686
  time_total_s: 7696.9300673007965
  timers:
    learn_throughput: 8701.602
    learn_time_ms: 18593.357
    sample_throughput: 23848.874
    sample_time_ms: 6784.052
    update_time_ms: 33.164
  timestamp: 1602762006
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: cb791_00000
  
2020-10-15 11:40:08,104	WARNING util.py:136 -- The `process_trial` operation took 0.832136869430542 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    300 |          7696.93 | 48537600 |  301.433 |              328.768 |              138.768 |            763.245 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2926.0737506674623
    time_step_min: 2746
  date: 2020-10-15_11-40-34
  done: false
  episode_len_mean: 763.2419443441684
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.4881552961535
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 197
  episodes_total: 63713
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.022527346500893e-42
        cur_lr: 5.0e-05
        entropy: 0.0870880422492822
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012727110263464661
        total_loss: .inf
        vf_explained_var: 0.9974377155303955
        vf_loss: 0.7960474739472071
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.99677419354839
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463918422691373
    mean_env_wait_ms: 1.2128689784767035
    mean_inference_ms: 4.291379515865291
    mean_raw_obs_processing_ms: 0.3761350879441071
  time_since_restore: 7722.96781206131
  time_this_iter_s: 26.037744760513306
  time_total_s: 7722.96781206131
  timers:
    learn_throughput: 8705.211
    learn_time_ms: 18585.65
    sample_throughput: 23790.022
    sample_time_ms: 6800.834
    update_time_ms: 33.733
  timestamp: 1602762034
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:40:35,431	WARNING util.py:136 -- The `process_trial` operation took 0.879004955291748 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    301 |          7722.97 | 48699392 |  301.488 |              328.768 |              138.768 |            763.242 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2925.556692482203
    time_step_min: 2746
  date: 2020-10-15_11-41-01
  done: false
  episode_len_mean: 763.2260061919504
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.5701526949766
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 241
  episodes_total: 63954
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5337910197513392e-42
        cur_lr: 5.0e-05
        entropy: 0.07846434973180294
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010466307847915838
        total_loss: .inf
        vf_explained_var: 0.9976531863212585
        vf_loss: 0.7656258593002955
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.867741935483874
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638990629193663
    mean_env_wait_ms: 1.2128007876943727
    mean_inference_ms: 4.291274284880112
    mean_raw_obs_processing_ms: 0.3761271831666109
  time_since_restore: 7748.991187810898
  time_this_iter_s: 26.023375749588013
  time_total_s: 7748.991187810898
  timers:
    learn_throughput: 8696.586
    learn_time_ms: 18604.083
    sample_throughput: 23761.943
    sample_time_ms: 6808.871
    update_time_ms: 35.869
  timestamp: 1602762061
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:41:02,676	WARNING util.py:136 -- The `process_trial` operation took 0.8561711311340332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    302 |          7748.99 | 48861184 |   301.57 |              328.768 |              138.768 |            763.226 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2925.132023085322
    time_step_min: 2746
  date: 2020-10-15_11-41-28
  done: false
  episode_len_mean: 763.2138458900373
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.6347353250032
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 195
  episodes_total: 64149
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3006865296270096e-42
        cur_lr: 5.0e-05
        entropy: 0.07128727560242017
        entropy_coeff: 0.0005000000000000001
        kl: 0.003599448381767919
        model: {}
        policy_loss: -0.009485388970157752
        total_loss: 0.973455806573232
        vf_explained_var: 0.9965738654136658
        vf_loss: 0.9829768389463425
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.738709677419358
    gpu_util_percent0: 0.30258064516129035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638845800191227
    mean_env_wait_ms: 1.2127460323598527
    mean_inference_ms: 4.29119490772562
    mean_raw_obs_processing_ms: 0.37612122883138543
  time_since_restore: 7774.841058254242
  time_this_iter_s: 25.849870443344116
  time_total_s: 7774.841058254242
  timers:
    learn_throughput: 8692.274
    learn_time_ms: 18613.311
    sample_throughput: 23781.667
    sample_time_ms: 6803.224
    update_time_ms: 34.712
  timestamp: 1602762088
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: cb791_00000
  
2020-10-15 11:41:29,679	WARNING util.py:136 -- The `process_trial` operation took 0.8340263366699219 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    303 |          7774.84 | 49022976 |  301.635 |              328.768 |              138.768 |            763.214 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2924.5964334022606
    time_step_min: 2746
  date: 2020-10-15_11-41-55
  done: false
  episode_len_mean: 763.1934025296
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.71463210369006
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 209
  episodes_total: 64358
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1503432648135048e-42
        cur_lr: 5.0e-05
        entropy: 0.07093589690824349
        entropy_coeff: 0.0005000000000000001
        kl: 0.004899081502420207
        model: {}
        policy_loss: -0.008196324847328166
        total_loss: 0.3444829210639
        vf_explained_var: 0.9987449049949646
        vf_loss: 0.3527147173881531
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.88666666666667
    gpu_util_percent0: 0.352
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638688892138804
    mean_env_wait_ms: 1.2126894655257443
    mean_inference_ms: 4.291114604841979
    mean_raw_obs_processing_ms: 0.3761141894386431
  time_since_restore: 7800.3008444309235
  time_this_iter_s: 25.45978617668152
  time_total_s: 7800.3008444309235
  timers:
    learn_throughput: 8695.724
    learn_time_ms: 18605.927
    sample_throughput: 23799.877
    sample_time_ms: 6798.018
    update_time_ms: 34.603
  timestamp: 1602762115
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: cb791_00000
  
2020-10-15 11:41:56,284	WARNING util.py:136 -- The `process_trial` operation took 0.8386998176574707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    304 |           7800.3 | 49184768 |  301.715 |              328.768 |              138.768 |            763.193 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2924.0141272693477
    time_step_min: 2746
  date: 2020-10-15_11-42-22
  done: false
  episode_len_mean: 763.1704156668472
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.80134575885006
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 237
  episodes_total: 64595
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.751716324067524e-43
        cur_lr: 5.0e-05
        entropy: 0.06825490730504195
        entropy_coeff: 0.0005000000000000001
        kl: 0.004300725219460825
        model: {}
        policy_loss: -0.00922158818381528
        total_loss: 0.48798858374357224
        vf_explained_var: 0.9983968138694763
        vf_loss: 0.49724429845809937
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.477419354838712
    gpu_util_percent0: 0.31419354838709684
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463850163478096
    mean_env_wait_ms: 1.212622752144365
    mean_inference_ms: 4.291014107249589
    mean_raw_obs_processing_ms: 0.37610647898635385
  time_since_restore: 7826.041253328323
  time_this_iter_s: 25.740408897399902
  time_total_s: 7826.041253328323
  timers:
    learn_throughput: 8714.706
    learn_time_ms: 18565.401
    sample_throughput: 23825.275
    sample_time_ms: 6790.772
    update_time_ms: 34.627
  timestamp: 1602762142
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: cb791_00000
  
2020-10-15 11:42:23,250	WARNING util.py:136 -- The `process_trial` operation took 0.8490591049194336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    305 |          7826.04 | 49346560 |  301.801 |              328.768 |              138.768 |             763.17 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2923.6041177560855
    time_step_min: 2746
  date: 2020-10-15_11-42-49
  done: false
  episode_len_mean: 763.1541608137937
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.86774884981185
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 188
  episodes_total: 64783
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.875858162033762e-43
        cur_lr: 5.0e-05
        entropy: 0.06946391426026821
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010510722933152769
        total_loss: .inf
        vf_explained_var: 0.9986698627471924
        vf_loss: 0.34862810373306274
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.54516129032258
    gpu_util_percent0: 0.2829032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638357564644044
    mean_env_wait_ms: 1.2125700351029187
    mean_inference_ms: 4.29093398410703
    mean_raw_obs_processing_ms: 0.3761005969653185
  time_since_restore: 7852.07733798027
  time_this_iter_s: 26.03608465194702
  time_total_s: 7852.07733798027
  timers:
    learn_throughput: 8717.482
    learn_time_ms: 18559.487
    sample_throughput: 23765.174
    sample_time_ms: 6807.945
    update_time_ms: 36.342
  timestamp: 1602762169
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:42:50,618	WARNING util.py:136 -- The `process_trial` operation took 0.8487627506256104 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    306 |          7852.08 | 49508352 |  301.868 |              328.768 |              138.768 |            763.154 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2923.0863488740783
    time_step_min: 2746
  date: 2020-10-15_11-43-16
  done: false
  episode_len_mean: 763.1323067930101
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 301.9507581972816
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 225
  episodes_total: 65008
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.313787243050643e-43
        cur_lr: 5.0e-05
        entropy: 0.0730263361086448
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008205605370070165
        total_loss: .inf
        vf_explained_var: 0.9983727931976318
        vf_loss: 0.5143369634946188
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.923333333333336
    gpu_util_percent0: 0.40833333333333344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638203154654636
    mean_env_wait_ms: 1.2125098015356834
    mean_inference_ms: 4.290852592484958
    mean_raw_obs_processing_ms: 0.37609316109001506
  time_since_restore: 7878.074340581894
  time_this_iter_s: 25.997002601623535
  time_total_s: 7878.074340581894
  timers:
    learn_throughput: 8706.781
    learn_time_ms: 18582.298
    sample_throughput: 23727.815
    sample_time_ms: 6818.664
    update_time_ms: 37.891
  timestamp: 1602762196
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:43:17,875	WARNING util.py:136 -- The `process_trial` operation took 0.8574538230895996 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    307 |          7878.07 | 49670144 |  301.951 |              328.768 |              138.768 |            763.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2922.581757388917
    time_step_min: 2746
  date: 2020-10-15_11-43-43
  done: false
  episode_len_mean: 763.1136300928906
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.027456576247
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 230
  episodes_total: 65238
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.4706808645759646e-43
        cur_lr: 5.0e-05
        entropy: 0.07554179057478905
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011270057982377088
        total_loss: .inf
        vf_explained_var: 0.9974391460418701
        vf_loss: 0.7935339957475662
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.141935483870967
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463803341921185
    mean_env_wait_ms: 1.2124464390063632
    mean_inference_ms: 4.290756374161962
    mean_raw_obs_processing_ms: 0.37608646539642654
  time_since_restore: 7903.9725461006165
  time_this_iter_s: 25.898205518722534
  time_total_s: 7903.9725461006165
  timers:
    learn_throughput: 8693.25
    learn_time_ms: 18611.221
    sample_throughput: 23729.888
    sample_time_ms: 6818.068
    update_time_ms: 41.738
  timestamp: 1602762223
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:43:44,947	WARNING util.py:136 -- The `process_trial` operation took 0.8719034194946289 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    308 |          7903.97 | 49831936 |  302.027 |              328.768 |              138.768 |            763.114 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2922.2136094584052
    time_step_min: 2746
  date: 2020-10-15_11-44-10
  done: false
  episode_len_mean: 763.1019718740446
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.0774050501961
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 182
  episodes_total: 65420
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.706021296863945e-43
        cur_lr: 5.0e-05
        entropy: 0.08627836778759956
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012601413201385489
        total_loss: .inf
        vf_explained_var: 0.9965766072273254
        vf_loss: 0.9678879429896673
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.783870967741937
    gpu_util_percent0: 0.3358064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637887692103768
    mean_env_wait_ms: 1.21239548424569
    mean_inference_ms: 4.290682609335702
    mean_raw_obs_processing_ms: 0.3760806148706252
  time_since_restore: 7930.011920928955
  time_this_iter_s: 26.039374828338623
  time_total_s: 7930.011920928955
  timers:
    learn_throughput: 8670.229
    learn_time_ms: 18660.637
    sample_throughput: 23743.318
    sample_time_ms: 6814.212
    update_time_ms: 35.791
  timestamp: 1602762250
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:44:12,190	WARNING util.py:136 -- The `process_trial` operation took 0.8958358764648438 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    309 |          7930.01 | 49993728 |  302.077 |              328.768 |              138.768 |            763.102 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2921.9189831748354
    time_step_min: 2746
  date: 2020-10-15_11-44-38
  done: false
  episode_len_mean: 763.0907470870459
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.1217429030998
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 235
  episodes_total: 65655
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4559031945295913e-42
        cur_lr: 5.0e-05
        entropy: 0.09321262997885545
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01378149853068559
        total_loss: .inf
        vf_explained_var: 0.9928681254386902
        vf_loss: 2.5911226073900857
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.44516129032258
    gpu_util_percent0: 0.317741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463770882688373
    mean_env_wait_ms: 1.212331630858118
    mean_inference_ms: 4.290589165950226
    mean_raw_obs_processing_ms: 0.3760723466446093
  time_since_restore: 7955.913537502289
  time_this_iter_s: 25.90161657333374
  time_total_s: 7955.913537502289
  timers:
    learn_throughput: 8665.198
    learn_time_ms: 18671.472
    sample_throughput: 23705.388
    sample_time_ms: 6825.115
    update_time_ms: 35.889
  timestamp: 1602762278
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:44:39,263	WARNING util.py:136 -- The `process_trial` operation took 0.8537335395812988 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    310 |          7955.91 | 50155520 |  302.122 |              328.768 |              138.768 |            763.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2921.50156449359
    time_step_min: 2746
  date: 2020-10-15_11-45-04
  done: false
  episode_len_mean: 763.0729563567362
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.18895213999565
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 220
  episodes_total: 65875
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.183854791794387e-42
        cur_lr: 5.0e-05
        entropy: 0.0760619839032491
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010026817714485029
        total_loss: .inf
        vf_explained_var: 0.9974761009216309
        vf_loss: 0.7721571624279022
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.3
    gpu_util_percent0: 0.2996666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463755799255426
    mean_env_wait_ms: 1.2122724324182326
    mean_inference_ms: 4.29050489438336
    mean_raw_obs_processing_ms: 0.37606659638196666
  time_since_restore: 7981.515456676483
  time_this_iter_s: 25.601919174194336
  time_total_s: 7981.515456676483
  timers:
    learn_throughput: 8674.006
    learn_time_ms: 18652.512
    sample_throughput: 23786.914
    sample_time_ms: 6801.723
    update_time_ms: 33.475
  timestamp: 1602762304
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:45:06,187	WARNING util.py:136 -- The `process_trial` operation took 0.916111946105957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    311 |          7981.52 | 50317312 |  302.189 |              328.768 |              138.768 |            763.073 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2921.0950000757334
    time_step_min: 2746
  date: 2020-10-15_11-45-32
  done: false
  episode_len_mean: 763.0577202543143
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.25097707318395
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 185
  episodes_total: 66060
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.27578218769158e-42
        cur_lr: 5.0e-05
        entropy: 0.07246144178013007
        entropy_coeff: 0.0005000000000000001
        kl: 0.004129683385447909
        model: {}
        policy_loss: -0.009010189431137405
        total_loss: 0.5461960236231486
        vf_explained_var: 0.997896134853363
        vf_loss: 0.5552424490451813
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.735483870967744
    gpu_util_percent0: 0.2974193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637412064918126
    mean_env_wait_ms: 1.2122219422186102
    mean_inference_ms: 4.290434585299137
    mean_raw_obs_processing_ms: 0.3760612064331957
  time_since_restore: 8007.443670749664
  time_this_iter_s: 25.928214073181152
  time_total_s: 8007.443670749664
  timers:
    learn_throughput: 8676.165
    learn_time_ms: 18647.871
    sample_throughput: 23803.809
    sample_time_ms: 6796.895
    update_time_ms: 33.12
  timestamp: 1602762332
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: cb791_00000
  
2020-10-15 11:45:33,405	WARNING util.py:136 -- The `process_trial` operation took 0.8759744167327881 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    312 |          8007.44 | 50479104 |  302.251 |              328.768 |              138.768 |            763.058 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2920.51854199683
    time_step_min: 2746
  date: 2020-10-15_11-45-59
  done: false
  episode_len_mean: 763.0361872869339
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.3370888722503
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 66294
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.63789109384579e-42
        cur_lr: 5.0e-05
        entropy: 0.07234183202187221
        entropy_coeff: 0.0005000000000000001
        kl: 0.006878962623886764
        model: {}
        policy_loss: -0.011858578926573196
        total_loss: 0.24153058975934982
        vf_explained_var: 0.9991645812988281
        vf_loss: 0.25342534606655437
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.806451612903228
    gpu_util_percent0: 0.2816129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637245163507576
    mean_env_wait_ms: 1.2121588082853565
    mean_inference_ms: 4.290344488196168
    mean_raw_obs_processing_ms: 0.3760530847995784
  time_since_restore: 8033.118682384491
  time_this_iter_s: 25.67501163482666
  time_total_s: 8033.118682384491
  timers:
    learn_throughput: 8684.098
    learn_time_ms: 18630.835
    sample_throughput: 23808.432
    sample_time_ms: 6795.576
    update_time_ms: 32.879
  timestamp: 1602762359
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: cb791_00000
  
2020-10-15 11:46:00,262	WARNING util.py:136 -- The `process_trial` operation took 0.8608291149139404 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    313 |          8033.12 | 50640896 |  302.337 |              328.768 |              138.768 |            763.036 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2920.1976049705886
    time_step_min: 2746
  date: 2020-10-15_11-46-26
  done: false
  episode_len_mean: 763.0296496767403
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.3691515971623
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 66510
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.63789109384579e-42
        cur_lr: 5.0e-05
        entropy: 0.10553259775042534
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012572684907354414
        total_loss: .inf
        vf_explained_var: 0.9926254153251648
        vf_loss: 2.667700946331024
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.00967741935484
    gpu_util_percent0: 0.27419354838709675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637093599195214
    mean_env_wait_ms: 1.2120996760401113
    mean_inference_ms: 4.290258811219063
    mean_raw_obs_processing_ms: 0.3760472135975153
  time_since_restore: 8059.069551944733
  time_this_iter_s: 25.9508695602417
  time_total_s: 8059.069551944733
  timers:
    learn_throughput: 8665.287
    learn_time_ms: 18671.28
    sample_throughput: 23787.035
    sample_time_ms: 6801.688
    update_time_ms: 34.435
  timestamp: 1602762386
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:46:27,580	WARNING util.py:136 -- The `process_trial` operation took 0.8768637180328369 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    314 |          8059.07 | 50802688 |  302.369 |              328.768 |              138.768 |             763.03 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2920.2777527752773
    time_step_min: 2746
  date: 2020-10-15_11-46-53
  done: false
  episode_len_mean: 763.0437487818408
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.3535966874246
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 189
  episodes_total: 66699
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4568366407686853e-42
        cur_lr: 5.0e-05
        entropy: 0.11792958403627078
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011629965396423358
        total_loss: .inf
        vf_explained_var: 0.9882053732872009
        vf_loss: 4.696357409159343
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.770967741935486
    gpu_util_percent0: 0.2887096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636953691137705
    mean_env_wait_ms: 1.2120496577339055
    mean_inference_ms: 4.290188234212002
    mean_raw_obs_processing_ms: 0.37604180899878736
  time_since_restore: 8085.200965881348
  time_this_iter_s: 26.13141393661499
  time_total_s: 8085.200965881348
  timers:
    learn_throughput: 8652.33
    learn_time_ms: 18699.24
    sample_throughput: 23759.448
    sample_time_ms: 6809.586
    update_time_ms: 36.288
  timestamp: 1602762413
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:46:54,986	WARNING util.py:136 -- The `process_trial` operation took 0.9518840312957764 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    315 |           8085.2 | 50964480 |  302.354 |              328.768 |              138.768 |            763.044 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2920.0680649405012
    time_step_min: 2746
  date: 2020-10-15_11-47-20
  done: false
  episode_len_mean: 763.0411767342487
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.3988699956188
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 232
  episodes_total: 66931
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6852549611530275e-42
        cur_lr: 5.0e-05
        entropy: 0.08917479030787945
        entropy_coeff: 0.0005000000000000001
        kl: 0.005094811281499763
        model: {}
        policy_loss: -0.011261997991823591
        total_loss: 1.501033882300059
        vf_explained_var: 0.9957756996154785
        vf_loss: 1.5123404264450073
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.570000000000004
    gpu_util_percent0: 0.334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636816315572676
    mean_env_wait_ms: 1.211988284353626
    mean_inference_ms: 4.290106923397993
    mean_raw_obs_processing_ms: 0.37603479943948265
  time_since_restore: 8110.738846063614
  time_this_iter_s: 25.537880182266235
  time_total_s: 8110.738846063614
  timers:
    learn_throughput: 8670.012
    learn_time_ms: 18661.104
    sample_throughput: 23797.024
    sample_time_ms: 6798.833
    update_time_ms: 34.281
  timestamp: 1602762440
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: cb791_00000
  
2020-10-15 11:47:21,833	WARNING util.py:136 -- The `process_trial` operation took 0.8932106494903564 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    316 |          8110.74 | 51126272 |  302.399 |              328.768 |              138.768 |            763.041 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2919.665936456843
    time_step_min: 2746
  date: 2020-10-15_11-47-47
  done: false
  episode_len_mean: 763.0281339826937
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.465058971828
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 212
  episodes_total: 67143
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6852549611530275e-42
        cur_lr: 5.0e-05
        entropy: 0.0758395300557216
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007537841263304775
        total_loss: .inf
        vf_explained_var: 0.9972102046012878
        vf_loss: 0.8080111791690191
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69677419354839
    gpu_util_percent0: 0.3206451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636644727051423
    mean_env_wait_ms: 1.2119304678971692
    mean_inference_ms: 4.29002002688978
    mean_raw_obs_processing_ms: 0.37602860431495466
  time_since_restore: 8136.902886390686
  time_this_iter_s: 26.164040327072144
  time_total_s: 8136.902886390686
  timers:
    learn_throughput: 8668.729
    learn_time_ms: 18663.867
    sample_throughput: 23744.981
    sample_time_ms: 6813.735
    update_time_ms: 34.151
  timestamp: 1602762467
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:47:49,327	WARNING util.py:136 -- The `process_trial` operation took 0.9140439033508301 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    317 |           8136.9 | 51288064 |  302.465 |              328.768 |              138.768 |            763.028 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2919.237930368367
    time_step_min: 2746
  date: 2020-10-15_11-48-15
  done: false
  episode_len_mean: 763.0139301413806
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.53097732103004
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 67336
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.527882441729541e-42
        cur_lr: 5.0e-05
        entropy: 0.07254663482308388
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009033093335650241
        total_loss: .inf
        vf_explained_var: 0.9984230399131775
        vf_loss: 0.42026430120070773
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.990322580645167
    gpu_util_percent0: 0.37419354838709684
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463651570354693
    mean_env_wait_ms: 1.2118795021299742
    mean_inference_ms: 4.2899488892008275
    mean_raw_obs_processing_ms: 0.3760229079001127
  time_since_restore: 8162.673620462418
  time_this_iter_s: 25.770734071731567
  time_total_s: 8162.673620462418
  timers:
    learn_throughput: 8678.045
    learn_time_ms: 18643.831
    sample_throughput: 23721.723
    sample_time_ms: 6820.415
    update_time_ms: 30.603
  timestamp: 1602762495
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:48:16,319	WARNING util.py:136 -- The `process_trial` operation took 0.8917403221130371 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    318 |          8162.67 | 51449856 |  302.531 |              328.768 |              138.768 |            763.014 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2918.6867947996566
    time_step_min: 2746
  date: 2020-10-15_11-48-42
  done: false
  episode_len_mean: 762.994435647374
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.6150579836814
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 237
  episodes_total: 67573
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.291823662594313e-42
        cur_lr: 5.0e-05
        entropy: 0.06789879066248734
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0074246267904527485
        total_loss: .inf
        vf_explained_var: 0.9986758828163147
        vf_loss: 0.402530459066232
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02903225806451
    gpu_util_percent0: 0.2645161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636367461629263
    mean_env_wait_ms: 1.2118175399443212
    mean_inference_ms: 4.289866133063459
    mean_raw_obs_processing_ms: 0.376016122422351
  time_since_restore: 8188.703552007675
  time_this_iter_s: 26.02993154525757
  time_total_s: 8188.703552007675
  timers:
    learn_throughput: 8683.357
    learn_time_ms: 18632.424
    sample_throughput: 23693.114
    sample_time_ms: 6828.651
    update_time_ms: 31.03
  timestamp: 1602762522
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:48:43,582	WARNING util.py:136 -- The `process_trial` operation took 0.9058184623718262 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    319 |           8188.7 | 51611648 |  302.615 |              328.768 |              138.768 |            762.994 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2918.192315073144
    time_step_min: 2746
  date: 2020-10-15_11-49-09
  done: false
  episode_len_mean: 762.9781948009796
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.6866867309904
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 209
  episodes_total: 67782
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2437735493891467e-41
        cur_lr: 5.0e-05
        entropy: 0.06573309749364853
        entropy_coeff: 0.0005000000000000001
        kl: 0.003936013963539153
        model: {}
        policy_loss: -0.007945787457477612
        total_loss: 0.5961089457074801
        vf_explained_var: 0.9978835582733154
        vf_loss: 0.6040875886877378
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25483870967742
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636214472249184
    mean_env_wait_ms: 1.211761545045309
    mean_inference_ms: 4.289789572861308
    mean_raw_obs_processing_ms: 0.3760108303227936
  time_since_restore: 8214.549340248108
  time_this_iter_s: 25.84578824043274
  time_total_s: 8214.549340248108
  timers:
    learn_throughput: 8687.269
    learn_time_ms: 18624.034
    sample_throughput: 23682.754
    sample_time_ms: 6831.638
    update_time_ms: 31.167
  timestamp: 1602762549
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: cb791_00000
  
2020-10-15 11:49:10,663	WARNING util.py:136 -- The `process_trial` operation took 0.9139370918273926 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    320 |          8214.55 | 51773440 |  302.687 |              328.768 |              138.768 |            762.978 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2917.752307216768
    time_step_min: 2746
  date: 2020-10-15_11-49-36
  done: false
  episode_len_mean: 762.9652240430728
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.7537881388243
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 196
  episodes_total: 67978
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.218867746945733e-42
        cur_lr: 5.0e-05
        entropy: 0.07275867151717345
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007768089271849021
        total_loss: .inf
        vf_explained_var: 0.998301088809967
        vf_loss: 0.46719616154829663
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.836666666666662
    gpu_util_percent0: 0.34766666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636074108240946
    mean_env_wait_ms: 1.2117106830799802
    mean_inference_ms: 4.289719313325142
    mean_raw_obs_processing_ms: 0.3760046546826837
  time_since_restore: 8240.284176111221
  time_this_iter_s: 25.734835863113403
  time_total_s: 8240.284176111221
  timers:
    learn_throughput: 8683.367
    learn_time_ms: 18632.405
    sample_throughput: 23669.859
    sample_time_ms: 6835.36
    update_time_ms: 31.097
  timestamp: 1602762576
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:49:37,723	WARNING util.py:136 -- The `process_trial` operation took 0.9075672626495361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    321 |          8240.28 | 51935232 |  302.754 |              328.768 |              138.768 |            762.965 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2917.418115144848
    time_step_min: 2746
  date: 2020-10-15_11-50-03
  done: false
  episode_len_mean: 762.9779077608702
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.7859268203185
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 236
  episodes_total: 68214
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.328301620418603e-42
        cur_lr: 5.0e-05
        entropy: 0.11431884268919627
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012345805473159999
        total_loss: .inf
        vf_explained_var: 0.9918860793113708
        vf_loss: 3.2345633506774902
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.609677419354846
    gpu_util_percent0: 0.33612903225806456
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463590961910094
    mean_env_wait_ms: 1.211647733353869
    mean_inference_ms: 4.289631469661672
    mean_raw_obs_processing_ms: 0.37599805922843665
  time_since_restore: 8266.282269716263
  time_this_iter_s: 25.998093605041504
  time_total_s: 8266.282269716263
  timers:
    learn_throughput: 8682.637
    learn_time_ms: 18633.97
    sample_throughput: 23648.376
    sample_time_ms: 6841.569
    update_time_ms: 30.565
  timestamp: 1602762603
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:50:04,966	WARNING util.py:136 -- The `process_trial` operation took 0.921013355255127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    322 |          8266.28 | 52097024 |  302.786 |              328.768 |              138.768 |            762.978 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2917.497616122616
    time_step_min: 2746
  date: 2020-10-15_11-50-30
  done: false
  episode_len_mean: 762.9926478111526
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.7851082630735
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 68415
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.39924524306279e-41
        cur_lr: 5.0e-05
        entropy: 0.1062534898519516
        entropy_coeff: 0.0005000000000000001
        kl: 0.005585477881443997
        model: {}
        policy_loss: -0.01434286559621493
        total_loss: 3.346762716770172
        vf_explained_var: 0.9911521077156067
        vf_loss: 3.3611587484677634
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10967741935484
    gpu_util_percent0: 0.31999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635785053969602
    mean_env_wait_ms: 1.2115947940323557
    mean_inference_ms: 4.289560690183078
    mean_raw_obs_processing_ms: 0.3759930070224446
  time_since_restore: 8292.238604307175
  time_this_iter_s: 25.956334590911865
  time_total_s: 8292.238604307175
  timers:
    learn_throughput: 8671.626
    learn_time_ms: 18657.631
    sample_throughput: 23607.152
    sample_time_ms: 6853.516
    update_time_ms: 31.458
  timestamp: 1602762630
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: cb791_00000
  
2020-10-15 11:50:32,155	WARNING util.py:136 -- The `process_trial` operation took 0.9045946598052979 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    323 |          8292.24 | 52258816 |  302.785 |              328.768 |              138.768 |            762.993 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2917.3978883509303
    time_step_min: 2746
  date: 2020-10-15_11-50-58
  done: false
  episode_len_mean: 762.996210520179
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.80970907718785
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 196
  episodes_total: 68611
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.39924524306279e-41
        cur_lr: 5.0e-05
        entropy: 0.09101049477855365
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049938377148161335
        model: {}
        policy_loss: -0.011510156550987935
        total_loss: 1.8518372575441997
        vf_explained_var: 0.9945216178894043
        vf_loss: 1.8633928994337718
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.938709677419357
    gpu_util_percent0: 0.2916129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635646501345834
    mean_env_wait_ms: 1.2115450116139685
    mean_inference_ms: 4.2894940291807595
    mean_raw_obs_processing_ms: 0.37598719999342745
  time_since_restore: 8318.168824911118
  time_this_iter_s: 25.93022060394287
  time_total_s: 8318.168824911118
  timers:
    learn_throughput: 8681.859
    learn_time_ms: 18635.64
    sample_throughput: 23570.963
    sample_time_ms: 6864.039
    update_time_ms: 29.813
  timestamp: 1602762658
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: cb791_00000
  
2020-10-15 11:50:59,369	WARNING util.py:136 -- The `process_trial` operation took 0.9420475959777832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    324 |          8318.17 | 52420608 |   302.81 |              328.768 |              138.768 |            762.996 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2916.9326881157904
    time_step_min: 2746
  date: 2020-10-15_11-51-25
  done: false
  episode_len_mean: 762.9771251797307
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.8843808218561
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 242
  episodes_total: 68853
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.99622621531395e-42
        cur_lr: 5.0e-05
        entropy: 0.06917376319567363
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031674818795484803
        model: {}
        policy_loss: -0.008605119490918392
        total_loss: 0.7881099830071131
        vf_explained_var: 0.9975513815879822
        vf_loss: 0.7967496911684672
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.14193548387097
    gpu_util_percent0: 0.28967741935483876
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635485280731406
    mean_env_wait_ms: 1.211480486625959
    mean_inference_ms: 4.289402405951004
    mean_raw_obs_processing_ms: 0.37598025607840735
  time_since_restore: 8344.195081949234
  time_this_iter_s: 26.026257038116455
  time_total_s: 8344.195081949234
  timers:
    learn_throughput: 8684.47
    learn_time_ms: 18630.037
    sample_throughput: 23586.397
    sample_time_ms: 6859.547
    update_time_ms: 28.58
  timestamp: 1602762685
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: cb791_00000
  
2020-10-15 11:51:26,736	WARNING util.py:136 -- The `process_trial` operation took 0.9393196105957031 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    325 |           8344.2 | 52582400 |  302.884 |              328.768 |              138.768 |            762.977 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2916.513049022591
    time_step_min: 2746
  date: 2020-10-15_11-51-52
  done: false
  episode_len_mean: 762.9613167651489
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 302.94918159699165
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 195
  episodes_total: 69048
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.498113107656975e-42
        cur_lr: 5.0e-05
        entropy: 0.06395160344739755
        entropy_coeff: 0.0005000000000000001
        kl: 0.004037508217152208
        model: {}
        policy_loss: -0.008743886573938653
        total_loss: 0.3061472177505493
        vf_explained_var: 0.9988009333610535
        vf_loss: 0.3149230753382047
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.083333333333332
    gpu_util_percent0: 0.31033333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635356800312435
    mean_env_wait_ms: 1.2114297538525953
    mean_inference_ms: 4.289337558823992
    mean_raw_obs_processing_ms: 0.3759755946629242
  time_since_restore: 8370.111559867859
  time_this_iter_s: 25.916477918624878
  time_total_s: 8370.111559867859
  timers:
    learn_throughput: 8663.981
    learn_time_ms: 18674.093
    sample_throughput: 23613.403
    sample_time_ms: 6851.702
    update_time_ms: 29.764
  timestamp: 1602762712
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: cb791_00000
  
2020-10-15 11:51:54,032	WARNING util.py:136 -- The `process_trial` operation took 0.9549221992492676 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    326 |          8370.11 | 52744192 |  302.949 |              328.768 |              138.768 |            762.961 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2916.023850798879
    time_step_min: 2746
  date: 2020-10-15_11-52-19
  done: false
  episode_len_mean: 762.9449762492601
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.0208289854843
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 213
  episodes_total: 69261
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7490565538284875e-42
        cur_lr: 5.0e-05
        entropy: 0.06562380741039912
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010868868315204358
        total_loss: .inf
        vf_explained_var: 0.99867182970047
        vf_loss: 0.39044930537541706
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.648387096774197
    gpu_util_percent0: 0.3680645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635238967150216
    mean_env_wait_ms: 1.2113755629600784
    mean_inference_ms: 4.289270677312194
    mean_raw_obs_processing_ms: 0.37596934664430975
  time_since_restore: 8395.814534187317
  time_this_iter_s: 25.702974319458008
  time_total_s: 8395.814534187317
  timers:
    learn_throughput: 8682.747
    learn_time_ms: 18633.734
    sample_throughput: 23659.378
    sample_time_ms: 6838.388
    update_time_ms: 35.74
  timestamp: 1602762739
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:52:21,019	WARNING util.py:136 -- The `process_trial` operation took 0.9529008865356445 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    327 |          8395.81 | 52905984 |  303.021 |              328.768 |              138.768 |            762.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2915.5391404506513
    time_step_min: 2746
  date: 2020-10-15_11-52-46
  done: false
  episode_len_mean: 762.9288139983308
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.0925716136236
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 69494
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.623584830742732e-42
        cur_lr: 5.0e-05
        entropy: 0.06332199772198994
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009792679748594916
        total_loss: .inf
        vf_explained_var: 0.9984169602394104
        vf_loss: 0.5140553067127863
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25
    gpu_util_percent0: 0.3413333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635061873438288
    mean_env_wait_ms: 1.211314782444527
    mean_inference_ms: 4.289187127287846
    mean_raw_obs_processing_ms: 0.3759632191428122
  time_since_restore: 8421.330665826797
  time_this_iter_s: 25.51613163948059
  time_total_s: 8421.330665826797
  timers:
    learn_throughput: 8693.062
    learn_time_ms: 18611.625
    sample_throughput: 23670.787
    sample_time_ms: 6835.092
    update_time_ms: 35.617
  timestamp: 1602762766
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:52:47,890	WARNING util.py:136 -- The `process_trial` operation took 0.9332010746002197 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    328 |          8421.33 | 53067776 |  303.093 |              328.768 |              138.768 |            762.929 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2915.1211699164346
    time_step_min: 2746
  date: 2020-10-15_11-53-13
  done: false
  episode_len_mean: 762.9141709119609
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.1575719888124
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 191
  episodes_total: 69685
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.935377246114098e-42
        cur_lr: 5.0e-05
        entropy: 0.06136918719857931
        entropy_coeff: 0.0005000000000000001
        kl: 0.004071943927556276
        model: {}
        policy_loss: -0.008099759668160308
        total_loss: 0.3071156069636345
        vf_explained_var: 0.9987518191337585
        vf_loss: 0.3152460306882858
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.70322580645162
    gpu_util_percent0: 0.3064516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634944433905495
    mean_env_wait_ms: 1.2112653411471486
    mean_inference_ms: 4.28912126270816
    mean_raw_obs_processing_ms: 0.37595845646981335
  time_since_restore: 8447.140485048294
  time_this_iter_s: 25.809819221496582
  time_total_s: 8447.140485048294
  timers:
    learn_throughput: 8702.311
    learn_time_ms: 18591.843
    sample_throughput: 23672.832
    sample_time_ms: 6834.501
    update_time_ms: 33.075
  timestamp: 1602762793
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: cb791_00000
  
2020-10-15 11:53:14,934	WARNING util.py:136 -- The `process_trial` operation took 0.9009709358215332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    329 |          8447.14 | 53229568 |  303.158 |              328.768 |              138.768 |            762.914 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2914.629981106149
    time_step_min: 2746
  date: 2020-10-15_11-53-40
  done: false
  episode_len_mean: 762.8996323476817
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.22735010722636
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 218
  episodes_total: 69903
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.967688623057049e-42
        cur_lr: 5.0e-05
        entropy: 0.0643474276488026
        entropy_coeff: 0.0005000000000000001
        kl: 0.004861408961005509
        model: {}
        policy_loss: -0.010934298887150362
        total_loss: 0.336390420794487
        vf_explained_var: 0.9988219141960144
        vf_loss: 0.3473568856716156
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06129032258065
    gpu_util_percent0: 0.34903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634814458191014
    mean_env_wait_ms: 1.2112099457793306
    mean_inference_ms: 4.289053276468293
    mean_raw_obs_processing_ms: 0.3759522733392337
  time_since_restore: 8473.020020484924
  time_this_iter_s: 25.87953543663025
  time_total_s: 8473.020020484924
  timers:
    learn_throughput: 8699.082
    learn_time_ms: 18598.744
    sample_throughput: 23666.701
    sample_time_ms: 6836.272
    update_time_ms: 35.605
  timestamp: 1602762820
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: cb791_00000
  
2020-10-15 11:53:42,072	WARNING util.py:136 -- The `process_trial` operation took 0.9183082580566406 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    330 |          8473.02 | 53391360 |  303.227 |              328.768 |              138.768 |              762.9 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2914.1274555959767
    time_step_min: 2746
  date: 2020-10-15_11-54-08
  done: false
  episode_len_mean: 762.8845780933641
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.30289981688713
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 70134
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.838443115285245e-43
        cur_lr: 5.0e-05
        entropy: 0.0615044329315424
        entropy_coeff: 0.0005000000000000001
        kl: 0.004994722975728412
        model: {}
        policy_loss: -0.008044289652995454
        total_loss: 0.23722197487950325
        vf_explained_var: 0.9991980195045471
        vf_loss: 0.2452970196803411
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.274193548387103
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634659001568134
    mean_env_wait_ms: 1.2111502298676586
    mean_inference_ms: 4.288974341182248
    mean_raw_obs_processing_ms: 0.37594655005117306
  time_since_restore: 8498.97104382515
  time_this_iter_s: 25.95102334022522
  time_total_s: 8498.97104382515
  timers:
    learn_throughput: 8693.609
    learn_time_ms: 18610.453
    sample_throughput: 23640.548
    sample_time_ms: 6843.835
    update_time_ms: 37.61
  timestamp: 1602762848
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: cb791_00000
  
2020-10-15 11:54:09,338	WARNING util.py:136 -- The `process_trial` operation took 0.9140024185180664 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    331 |          8498.97 | 53553152 |  303.303 |              328.768 |              138.768 |            762.885 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2913.7039113299093
    time_step_min: 2746
  date: 2020-10-15_11-54-35
  done: false
  episode_len_mean: 762.8693438753164
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.3679031721037
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 188
  episodes_total: 70322
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.919221557642622e-43
        cur_lr: 5.0e-05
        entropy: 0.062040843069553375
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006292439162886391
        total_loss: .inf
        vf_explained_var: 0.9989210963249207
        vf_loss: 0.30987070749203366
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116129032258065
    gpu_util_percent0: 0.28709677419354834
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634526909929926
    mean_env_wait_ms: 1.2111017428261104
    mean_inference_ms: 4.288905422494112
    mean_raw_obs_processing_ms: 0.37594142832410055
  time_since_restore: 8524.989003896713
  time_this_iter_s: 26.01796007156372
  time_total_s: 8524.989003896713
  timers:
    learn_throughput: 8692.142
    learn_time_ms: 18613.594
    sample_throughput: 23648.257
    sample_time_ms: 6841.604
    update_time_ms: 37.781
  timestamp: 1602762875
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:54:36,721	WARNING util.py:136 -- The `process_trial` operation took 0.9441823959350586 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    332 |          8524.99 | 53714944 |  303.368 |              328.768 |              138.768 |            762.869 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2913.2278477422283
    time_step_min: 2746
  date: 2020-10-15_11-55-02
  done: false
  episode_len_mean: 762.8513415826849
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.43863046848105
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 229
  episodes_total: 70551
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.378832336463933e-43
        cur_lr: 5.0e-05
        entropy: 0.07061944591502349
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037392291123978794
        model: {}
        policy_loss: -0.010029158177834082
        total_loss: 0.8280335664749146
        vf_explained_var: 0.997319221496582
        vf_loss: 0.8380980243285497
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89354838709678
    gpu_util_percent0: 0.3538709677419356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634395308277823
    mean_env_wait_ms: 1.2110435518563263
    mean_inference_ms: 4.288834828720445
    mean_raw_obs_processing_ms: 0.375935074324
  time_since_restore: 8550.851609706879
  time_this_iter_s: 25.862605810165405
  time_total_s: 8550.851609706879
  timers:
    learn_throughput: 8702.303
    learn_time_ms: 18591.86
    sample_throughput: 23612.319
    sample_time_ms: 6852.017
    update_time_ms: 38.602
  timestamp: 1602762902
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: cb791_00000
  
2020-10-15 11:55:03,910	WARNING util.py:136 -- The `process_trial` operation took 0.9890291690826416 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    333 |          8550.85 | 53876736 |  303.439 |              328.768 |              138.768 |            762.851 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2912.7724888668977
    time_step_min: 2746
  date: 2020-10-15_11-55-29
  done: false
  episode_len_mean: 762.8364794981208
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.5073025162181
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 223
  episodes_total: 70774
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6894161682319665e-43
        cur_lr: 5.0e-05
        entropy: 0.06865913296739261
        entropy_coeff: 0.0005000000000000001
        kl: 0.004029958120857676
        model: {}
        policy_loss: -0.011268424340717806
        total_loss: 0.4561513885855675
        vf_explained_var: 0.9984280467033386
        vf_loss: 0.46745414038499195
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16774193548387
    gpu_util_percent0: 0.2912903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634246252920483
    mean_env_wait_ms: 1.2109862550654065
    mean_inference_ms: 4.288757483618179
    mean_raw_obs_processing_ms: 0.37592967297741897
  time_since_restore: 8576.67750287056
  time_this_iter_s: 25.82589316368103
  time_total_s: 8576.67750287056
  timers:
    learn_throughput: 8702.567
    learn_time_ms: 18591.297
    sample_throughput: 23621.972
    sample_time_ms: 6849.216
    update_time_ms: 40.262
  timestamp: 1602762929
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: cb791_00000
  
2020-10-15 11:55:31,039	WARNING util.py:136 -- The `process_trial` operation took 0.9539499282836914 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    334 |          8576.68 | 54038528 |  303.507 |              328.768 |              138.768 |            762.836 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2912.4205324158934
    time_step_min: 2746
  date: 2020-10-15_11-55-56
  done: false
  episode_len_mean: 762.8243683149899
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.5613256904951
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 187
  episodes_total: 70961
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8447080841159832e-43
        cur_lr: 5.0e-05
        entropy: 0.0700186441342036
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010340267627422387
        total_loss: .inf
        vf_explained_var: 0.9966973662376404
        vf_loss: 0.8846477568149567
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.283870967741937
    gpu_util_percent0: 0.2906451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463411152637358
    mean_env_wait_ms: 1.2109384656977433
    mean_inference_ms: 4.288694252147328
    mean_raw_obs_processing_ms: 0.37592452549198835
  time_since_restore: 8602.553108930588
  time_this_iter_s: 25.875606060028076
  time_total_s: 8602.553108930588
  timers:
    learn_throughput: 8708.763
    learn_time_ms: 18578.069
    sample_throughput: 23636.764
    sample_time_ms: 6844.93
    update_time_ms: 40.786
  timestamp: 1602762956
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:55:58,261	WARNING util.py:136 -- The `process_trial` operation took 0.9906723499298096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    335 |          8602.55 | 54200320 |  303.561 |              328.768 |              138.768 |            762.824 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2911.970078983613
    time_step_min: 2746
  date: 2020-10-15_11-56-23
  done: false
  episode_len_mean: 762.8125377494979
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.6290033621792
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 232
  episodes_total: 71193
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7670621261739743e-43
        cur_lr: 5.0e-05
        entropy: 0.07258581990996997
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037479647047196827
        model: {}
        policy_loss: -0.01160572263567398
        total_loss: 0.6439986477295557
        vf_explained_var: 0.9978951811790466
        vf_loss: 0.6556406815846761
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.61666666666667
    gpu_util_percent0: 0.322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633989956880902
    mean_env_wait_ms: 1.2108800302291363
    mean_inference_ms: 4.288624609071767
    mean_raw_obs_processing_ms: 0.37591851807422255
  time_since_restore: 8628.15276670456
  time_this_iter_s: 25.599657773971558
  time_total_s: 8628.15276670456
  timers:
    learn_throughput: 8724.027
    learn_time_ms: 18545.563
    sample_throughput: 23633.042
    sample_time_ms: 6846.008
    update_time_ms: 39.389
  timestamp: 1602762983
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: cb791_00000
  
2020-10-15 11:56:25,225	WARNING util.py:136 -- The `process_trial` operation took 0.9335761070251465 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    336 |          8628.15 | 54362112 |  303.629 |              328.768 |              138.768 |            762.813 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2911.5172201204987
    time_step_min: 2746
  date: 2020-10-15_11-56-51
  done: false
  episode_len_mean: 762.7978966236749
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.69707621100287
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 71409
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3835310630869871e-43
        cur_lr: 5.0e-05
        entropy: 0.06814419167737167
        entropy_coeff: 0.0005000000000000001
        kl: 0.004475093291451533
        model: {}
        policy_loss: -0.007937712963515272
        total_loss: 0.3048054799437523
        vf_explained_var: 0.9988961815834045
        vf_loss: 0.3127772609392802
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096777
    gpu_util_percent0: 0.31806451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463384755948903
    mean_env_wait_ms: 1.2108254635477398
    mean_inference_ms: 4.288553148075127
    mean_raw_obs_processing_ms: 0.37591354376509073
  time_since_restore: 8654.030025959015
  time_this_iter_s: 25.877259254455566
  time_total_s: 8654.030025959015
  timers:
    learn_throughput: 8713.705
    learn_time_ms: 18567.531
    sample_throughput: 23634.392
    sample_time_ms: 6845.617
    update_time_ms: 34.109
  timestamp: 1602763011
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: cb791_00000
  
2020-10-15 11:56:52,421	WARNING util.py:136 -- The `process_trial` operation took 0.9653329849243164 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    337 |          8654.03 | 54523904 |  303.697 |              328.768 |              138.768 |            762.798 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2911.144116825042
    time_step_min: 2746
  date: 2020-10-15_11-57-18
  done: false
  episode_len_mean: 762.7886702328245
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.7526668238268
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 71599
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.917655315434936e-44
        cur_lr: 5.0e-05
        entropy: 0.0700108961512645
        entropy_coeff: 0.0005000000000000001
        kl: 0.004025433755790194
        model: {}
        policy_loss: -0.011625094960133234
        total_loss: 0.4274258886774381
        vf_explained_var: 0.9983403086662292
        vf_loss: 0.4390859752893448
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.180645161290318
    gpu_util_percent0: 0.2951612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633745605157158
    mean_env_wait_ms: 1.2107770478713344
    mean_inference_ms: 4.2884921803538925
    mean_raw_obs_processing_ms: 0.3759085274103737
  time_since_restore: 8679.835048913956
  time_this_iter_s: 25.805022954940796
  time_total_s: 8679.835048913956
  timers:
    learn_throughput: 8706.623
    learn_time_ms: 18582.636
    sample_throughput: 23618.625
    sample_time_ms: 6850.187
    update_time_ms: 34.231
  timestamp: 1602763038
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: cb791_00000
  
2020-10-15 11:57:19,521	WARNING util.py:136 -- The `process_trial` operation took 0.951420783996582 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    338 |          8679.84 | 54685696 |  303.753 |              328.768 |              138.768 |            762.789 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2910.686900201964
    time_step_min: 2746
  date: 2020-10-15_11-57-45
  done: false
  episode_len_mean: 762.7784753737784
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.8187203774807
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 235
  episodes_total: 71834
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.458827657717468e-44
        cur_lr: 5.0e-05
        entropy: 0.07383952662348747
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045124044409021735
        model: {}
        policy_loss: -0.009838801740746325
        total_loss: 0.9299543152252833
        vf_explained_var: 0.9972420334815979
        vf_loss: 0.939830000201861
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06129032258064
    gpu_util_percent0: 0.34258064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463358002789866
    mean_env_wait_ms: 1.2107179995455015
    mean_inference_ms: 4.2884183930992235
    mean_raw_obs_processing_ms: 0.37590252889657655
  time_since_restore: 8705.873311758041
  time_this_iter_s: 26.038262844085693
  time_total_s: 8705.873311758041
  timers:
    learn_throughput: 8697.11
    learn_time_ms: 18602.961
    sample_throughput: 23617.689
    sample_time_ms: 6850.459
    update_time_ms: 34.749
  timestamp: 1602763065
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: cb791_00000
  
2020-10-15 11:57:47,001	WARNING util.py:136 -- The `process_trial` operation took 0.9447863101959229 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    339 |          8705.87 | 54847488 |  303.819 |              328.768 |              138.768 |            762.778 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2910.2778240702164
    time_step_min: 2746
  date: 2020-10-15_11-58-12
  done: false
  episode_len_mean: 762.7673259768201
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.87892037173725
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 211
  episodes_total: 72045
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.729413828858734e-44
        cur_lr: 5.0e-05
        entropy: 0.07001438674827416
        entropy_coeff: 0.0005000000000000001
        kl: 0.004174699482973665
        model: {}
        policy_loss: -0.01037671046772933
        total_loss: 0.3571045845746994
        vf_explained_var: 0.9987242817878723
        vf_loss: 0.36751629660526913
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05806451612904
    gpu_util_percent0: 0.315483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633466142038096
    mean_env_wait_ms: 1.210664223118209
    mean_inference_ms: 4.288349903304019
    mean_raw_obs_processing_ms: 0.37589752204094445
  time_since_restore: 8731.863772630692
  time_this_iter_s: 25.990460872650146
  time_total_s: 8731.863772630692
  timers:
    learn_throughput: 8694.203
    learn_time_ms: 18609.18
    sample_throughput: 23598.825
    sample_time_ms: 6855.935
    update_time_ms: 33.345
  timestamp: 1602763092
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: cb791_00000
  
2020-10-15 11:58:14,384	WARNING util.py:136 -- The `process_trial` operation took 0.9329497814178467 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    340 |          8731.86 | 55009280 |  303.879 |              328.768 |              138.768 |            762.767 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2909.9123244411203
    time_step_min: 2746
  date: 2020-10-15_11-58-40
  done: false
  episode_len_mean: 762.75686974819
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 303.9366282115979
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 192
  episodes_total: 72237
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.64706914429367e-45
        cur_lr: 5.0e-05
        entropy: 0.06827904842793941
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007403415782998006
        total_loss: .inf
        vf_explained_var: 0.9976740479469299
        vf_loss: 0.6270856459935507
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612907
    gpu_util_percent0: 0.33225806451612905
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463335230735102
    mean_env_wait_ms: 1.21061703107874
    mean_inference_ms: 4.2882939849734045
    mean_raw_obs_processing_ms: 0.3758926020887024
  time_since_restore: 8757.686029195786
  time_this_iter_s: 25.822256565093994
  time_total_s: 8757.686029195786
  timers:
    learn_throughput: 8700.166
    learn_time_ms: 18596.428
    sample_throughput: 23601.861
    sample_time_ms: 6855.053
    update_time_ms: 32.605
  timestamp: 1602763120
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:58:41,523	WARNING util.py:136 -- The `process_trial` operation took 0.9676265716552734 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    341 |          8757.69 | 55171072 |  303.937 |              328.768 |              138.768 |            762.757 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2909.4074738745703
    time_step_min: 2746
  date: 2020-10-15_11-59-07
  done: false
  episode_len_mean: 762.7423356052872
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.0122650941656
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 241
  episodes_total: 72478
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2970603716440505e-44
        cur_lr: 5.0e-05
        entropy: 0.06839862403770287
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007854011550080031
        total_loss: .inf
        vf_explained_var: 0.9989078640937805
        vf_loss: 0.34104523186882335
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.948387096774198
    gpu_util_percent0: 0.29903225806451617
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633194321090442
    mean_env_wait_ms: 1.2105558118440265
    mean_inference_ms: 4.288215963446026
    mean_raw_obs_processing_ms: 0.3758860879381673
  time_since_restore: 8783.741505384445
  time_this_iter_s: 26.055476188659668
  time_total_s: 8783.741505384445
  timers:
    learn_throughput: 8710.673
    learn_time_ms: 18573.995
    sample_throughput: 23520.272
    sample_time_ms: 6878.832
    update_time_ms: 32.932
  timestamp: 1602763147
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 11:59:08,989	WARNING util.py:136 -- The `process_trial` operation took 0.9467813968658447 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    342 |          8783.74 | 55332864 |  304.012 |              328.768 |              138.768 |            762.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2908.9733346181906
    time_step_min: 2746
  date: 2020-10-15_11-59-34
  done: false
  episode_len_mean: 762.731411667584
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.07552270086654
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 202
  episodes_total: 72680
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.945590557466076e-44
        cur_lr: 5.0e-05
        entropy: 0.06657517204682033
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050688750731448335
        model: {}
        policy_loss: -0.01049167473684065
        total_loss: 0.2251389647523562
        vf_explained_var: 0.9991205334663391
        vf_loss: 0.2356639268497626
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.80645161290322
    gpu_util_percent0: 0.34354838709677415
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633081698552686
    mean_env_wait_ms: 1.2105056120745612
    mean_inference_ms: 4.288156833892776
    mean_raw_obs_processing_ms: 0.3758820233678664
  time_since_restore: 8809.590632677078
  time_this_iter_s: 25.849127292633057
  time_total_s: 8809.590632677078
  timers:
    learn_throughput: 8712.304
    learn_time_ms: 18570.519
    sample_throughput: 23546.595
    sample_time_ms: 6871.142
    update_time_ms: 33.016
  timestamp: 1602763174
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: cb791_00000
  
2020-10-15 11:59:36,211	WARNING util.py:136 -- The `process_trial` operation took 1.026545763015747 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    343 |          8809.59 | 55494656 |  304.076 |              328.768 |              138.768 |            762.731 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2908.67333434012
    time_step_min: 2746
  date: 2020-10-15_12-00-02
  done: false
  episode_len_mean: 762.7336233156783
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.11875956524534
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 198
  episodes_total: 72878
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.945590557466076e-44
        cur_lr: 5.0e-05
        entropy: 0.08621939892570178
        entropy_coeff: 0.0005000000000000001
        kl: 0.005510358217482765
        model: {}
        policy_loss: -0.013044081832049415
        total_loss: 0.9271038820346197
        vf_explained_var: 0.9970248341560364
        vf_loss: 0.9401910503705343
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94838709677419
    gpu_util_percent0: 0.28774193548387095
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632954266807618
    mean_env_wait_ms: 1.2104568126795778
    mean_inference_ms: 4.288099378301337
    mean_raw_obs_processing_ms: 0.37587667672980013
  time_since_restore: 8835.608973741531
  time_this_iter_s: 26.018341064453125
  time_total_s: 8835.608973741531
  timers:
    learn_throughput: 8706.9
    learn_time_ms: 18582.045
    sample_throughput: 23566.344
    sample_time_ms: 6865.384
    update_time_ms: 33.549
  timestamp: 1602763202
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: cb791_00000
  
2020-10-15 12:00:03,591	WARNING util.py:136 -- The `process_trial` operation took 0.9927520751953125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    344 |          8835.61 | 55656448 |  304.119 |              328.768 |              138.768 |            762.734 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2908.3673896681494
    time_step_min: 2746
  date: 2020-10-15_12-00-29
  done: false
  episode_len_mean: 762.7354952539869
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.1709946249705
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 236
  episodes_total: 73114
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.945590557466076e-44
        cur_lr: 5.0e-05
        entropy: 0.07761044551928838
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011320474790409207
        total_loss: .inf
        vf_explained_var: 0.9969576001167297
        vf_loss: 1.0453643550475438
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.016129032258068
    gpu_util_percent0: 0.32967741935483874
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632820058229518
    mean_env_wait_ms: 1.2103972959095621
    mean_inference_ms: 4.28802439678674
    mean_raw_obs_processing_ms: 0.37587081619604346
  time_since_restore: 8861.450466156006
  time_this_iter_s: 25.841492414474487
  time_total_s: 8861.450466156006
  timers:
    learn_throughput: 8706.383
    learn_time_ms: 18583.148
    sample_throughput: 23551.643
    sample_time_ms: 6869.669
    update_time_ms: 32.819
  timestamp: 1602763229
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:00:30,791	WARNING util.py:136 -- The `process_trial` operation took 1.0038080215454102 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    345 |          8861.45 | 55818240 |  304.171 |              328.768 |              138.768 |            762.735 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2908.077229614466
    time_step_min: 2746
  date: 2020-10-15_12-00-56
  done: false
  episode_len_mean: 762.7366123796273
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.210431923788
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 200
  episodes_total: 73314
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.918385836199114e-44
        cur_lr: 5.0e-05
        entropy: 0.08558133120338123
        entropy_coeff: 0.0005000000000000001
        kl: 0.005396293825469911
        model: {}
        policy_loss: -0.013991864706137372
        total_loss: 0.8910327802101771
        vf_explained_var: 0.9970316886901855
        vf_loss: 0.9050674537817637
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.080645161290327
    gpu_util_percent0: 0.3070967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632701365567652
    mean_env_wait_ms: 1.2103482596254436
    mean_inference_ms: 4.287967855016791
    mean_raw_obs_processing_ms: 0.3758665522481541
  time_since_restore: 8887.414614200592
  time_this_iter_s: 25.96414804458618
  time_total_s: 8887.414614200592
  timers:
    learn_throughput: 8695.952
    learn_time_ms: 18605.439
    sample_throughput: 23512.908
    sample_time_ms: 6880.986
    update_time_ms: 33.011
  timestamp: 1602763256
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: cb791_00000
  
2020-10-15 12:00:58,209	WARNING util.py:136 -- The `process_trial` operation took 0.9843511581420898 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    346 |          8887.41 | 55980032 |   304.21 |              328.768 |              138.768 |            762.737 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2908.0003947135606
    time_step_min: 2746
  date: 2020-10-15_12-01-23
  done: false
  episode_len_mean: 762.7602911168549
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.22482545493
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 196
  episodes_total: 73510
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.918385836199114e-44
        cur_lr: 5.0e-05
        entropy: 0.09570368751883507
        entropy_coeff: 0.0005000000000000001
        kl: 0.004869019767890374
        model: {}
        policy_loss: -0.015415742037779031
        total_loss: 2.1073656380176544
        vf_explained_var: 0.9942100048065186
        vf_loss: 2.122829178969065
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.690322580645155
    gpu_util_percent0: 0.3183870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632578338450836
    mean_env_wait_ms: 1.2102997230636696
    mean_inference_ms: 4.287908463854141
    mean_raw_obs_processing_ms: 0.3758611027788393
  time_since_restore: 8913.103849887848
  time_this_iter_s: 25.68923568725586
  time_total_s: 8913.103849887848
  timers:
    learn_throughput: 8698.758
    learn_time_ms: 18599.436
    sample_throughput: 23549.712
    sample_time_ms: 6870.233
    update_time_ms: 30.728
  timestamp: 1602763283
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: cb791_00000
  
2020-10-15 12:01:25,263	WARNING util.py:136 -- The `process_trial` operation took 0.9546172618865967 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    347 |           8913.1 | 56141824 |  304.225 |              328.768 |              138.768 |             762.76 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2907.731127211549
    time_step_min: 2746
  date: 2020-10-15_12-01-51
  done: false
  episode_len_mean: 762.7600721424407
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.27767278578864
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 73743
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.459192918099557e-44
        cur_lr: 5.0e-05
        entropy: 0.06894108094274998
        entropy_coeff: 0.0005000000000000001
        kl: 0.005075545089008908
        model: {}
        policy_loss: -0.00977887746800358
        total_loss: 0.7057152638832728
        vf_explained_var: 0.9978468418121338
        vf_loss: 0.7155286073684692
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.854838709677427
    gpu_util_percent0: 0.28193548387096773
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463243947104465
    mean_env_wait_ms: 1.2102418740885186
    mean_inference_ms: 4.287839705388106
    mean_raw_obs_processing_ms: 0.3758558564859602
  time_since_restore: 8938.94693827629
  time_this_iter_s: 25.843088388442993
  time_total_s: 8938.94693827629
  timers:
    learn_throughput: 8696.863
    learn_time_ms: 18603.49
    sample_throughput: 23551.996
    sample_time_ms: 6869.566
    update_time_ms: 30.689
  timestamp: 1602763311
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: cb791_00000
  
2020-10-15 12:01:52,433	WARNING util.py:136 -- The `process_trial` operation took 0.9667210578918457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    348 |          8938.95 | 56303616 |  304.278 |              328.768 |              138.768 |             762.76 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2907.3279258557704
    time_step_min: 2746
  date: 2020-10-15_12-02-18
  done: false
  episode_len_mean: 762.749503035876
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.3394713337104
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 206
  episodes_total: 73949
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.459192918099557e-44
        cur_lr: 5.0e-05
        entropy: 0.06342075113207102
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046050337453683214
        model: {}
        policy_loss: -0.00901176871654267
        total_loss: 0.22969535614053407
        vf_explained_var: 0.9991278052330017
        vf_loss: 0.23873882989088693
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.338709677419356
    gpu_util_percent0: 0.29
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632330591183984
    mean_env_wait_ms: 1.2101917448091806
    mean_inference_ms: 4.287780453765895
    mean_raw_obs_processing_ms: 0.37585140412744594
  time_since_restore: 8965.172308444977
  time_this_iter_s: 26.225370168685913
  time_total_s: 8965.172308444977
  timers:
    learn_throughput: 8699.951
    learn_time_ms: 18596.887
    sample_throughput: 23467.116
    sample_time_ms: 6894.413
    update_time_ms: 30.54
  timestamp: 1602763338
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: cb791_00000
  
2020-10-15 12:02:20,018	WARNING util.py:136 -- The `process_trial` operation took 0.9894309043884277 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    349 |          8965.17 | 56465408 |  304.339 |              328.768 |              138.768 |             762.75 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2906.9224718343116
    time_step_min: 2746
  date: 2020-10-15_12-02-45
  done: false
  episode_len_mean: 762.7390161016264
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.4013621120991
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 205
  episodes_total: 74154
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.295964590497785e-45
        cur_lr: 5.0e-05
        entropy: 0.06770333399375279
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006484849203843623
        total_loss: .inf
        vf_explained_var: 0.9990413188934326
        vf_loss: 0.26893555497129756
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.07096774193549
    gpu_util_percent0: 0.2996774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632213687617476
    mean_env_wait_ms: 1.210141617798495
    mean_inference_ms: 4.287723129328878
    mean_raw_obs_processing_ms: 0.37584607915640844
  time_since_restore: 8991.024416923523
  time_this_iter_s: 25.852108478546143
  time_total_s: 8991.024416923523
  timers:
    learn_throughput: 8705.23
    learn_time_ms: 18585.608
    sample_throughput: 23505.713
    sample_time_ms: 6883.093
    update_time_ms: 29.075
  timestamp: 1602763365
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:02:47,200	WARNING util.py:136 -- The `process_trial` operation took 0.977574348449707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    350 |          8991.02 | 56627200 |  304.401 |              328.768 |              138.768 |            762.739 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2906.478735709482
    time_step_min: 2746
  date: 2020-10-15_12-03-13
  done: false
  episode_len_mean: 762.7290325182487
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.46734236665526
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 235
  episodes_total: 74389
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0943946885746677e-44
        cur_lr: 5.0e-05
        entropy: 0.0673687035838763
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010370220814365894
        total_loss: .inf
        vf_explained_var: 0.9988407492637634
        vf_loss: 0.3626883129278819
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.883870967741935
    gpu_util_percent0: 0.2854838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632079852809415
    mean_env_wait_ms: 1.2100839788625235
    mean_inference_ms: 4.287656065332428
    mean_raw_obs_processing_ms: 0.3758407631067043
  time_since_restore: 9016.840253353119
  time_this_iter_s: 25.815836429595947
  time_total_s: 9016.840253353119
  timers:
    learn_throughput: 8711.182
    learn_time_ms: 18572.91
    sample_throughput: 23496.232
    sample_time_ms: 6885.87
    update_time_ms: 27.962
  timestamp: 1602763393
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:03:14,412	WARNING util.py:136 -- The `process_trial` operation took 0.9831943511962891 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    351 |          9016.84 | 56788992 |  304.467 |              328.768 |              138.768 |            762.729 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2906.13676119443
    time_step_min: 2746
  date: 2020-10-15_12-03-40
  done: false
  episode_len_mean: 762.7212442180063
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.52041769169864
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 196
  episodes_total: 74585
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6415920328620018e-44
        cur_lr: 5.0e-05
        entropy: 0.06815042657156785
        entropy_coeff: 0.0005000000000000001
        kl: 0.004271164691696565
        model: {}
        policy_loss: -0.01097593258600682
        total_loss: 0.4795456329981486
        vf_explained_var: 0.998236358165741
        vf_loss: 0.49055564155181247
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.186666666666667
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631952557603978
    mean_env_wait_ms: 1.210035637871287
    mean_inference_ms: 4.2875947123577065
    mean_raw_obs_processing_ms: 0.3758362578437081
  time_since_restore: 9042.558306455612
  time_this_iter_s: 25.718053102493286
  time_total_s: 9042.558306455612
  timers:
    learn_throughput: 8709.963
    learn_time_ms: 18575.509
    sample_throughput: 23618.296
    sample_time_ms: 6850.282
    update_time_ms: 27.614
  timestamp: 1602763420
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: cb791_00000
  
2020-10-15 12:03:41,630	WARNING util.py:136 -- The `process_trial` operation took 1.0481550693511963 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    352 |          9042.56 | 56950784 |   304.52 |              328.768 |              138.768 |            762.721 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2905.77398341359
    time_step_min: 2746
  date: 2020-10-15_12-04-07
  done: false
  episode_len_mean: 762.7169213492159
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.57522888614193
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 214
  episodes_total: 74799
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.207960164310009e-45
        cur_lr: 5.0e-05
        entropy: 0.07075904433925946
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010161362666015824
        total_loss: .inf
        vf_explained_var: 0.9983585476875305
        vf_loss: 0.4882360075910886
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.745161290322585
    gpu_util_percent0: 0.357741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631860302021774
    mean_env_wait_ms: 1.2099848808432487
    mean_inference_ms: 4.287544404354722
    mean_raw_obs_processing_ms: 0.3758311207644591
  time_since_restore: 9068.319984436035
  time_this_iter_s: 25.761677980422974
  time_total_s: 9068.319984436035
  timers:
    learn_throughput: 8707.818
    learn_time_ms: 18580.085
    sample_throughput: 23649.758
    sample_time_ms: 6841.169
    update_time_ms: 28.888
  timestamp: 1602763447
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:04:08,757	WARNING util.py:136 -- The `process_trial` operation took 1.0065586566925049 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    353 |          9068.32 | 57112576 |  304.575 |              328.768 |              138.768 |            762.717 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2905.341009707702
    time_step_min: 2746
  date: 2020-10-15_12-04-34
  done: false
  episode_len_mean: 762.70605483067
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.63953660096564
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 232
  episodes_total: 75031
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2311940246465011e-44
        cur_lr: 5.0e-05
        entropy: 0.06731745352347691
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008199563870827356
        total_loss: .inf
        vf_explained_var: 0.9987933039665222
        vf_loss: 0.3723234112064044
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.651612903225804
    gpu_util_percent0: 0.3316129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631703311924538
    mean_env_wait_ms: 1.2099278646594194
    mean_inference_ms: 4.287473446857379
    mean_raw_obs_processing_ms: 0.3758259416201581
  time_since_restore: 9094.54067659378
  time_this_iter_s: 26.22069215774536
  time_total_s: 9094.54067659378
  timers:
    learn_throughput: 8710.512
    learn_time_ms: 18574.339
    sample_throughput: 23527.797
    sample_time_ms: 6876.632
    update_time_ms: 29.221
  timestamp: 1602763474
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:04:36,486	WARNING util.py:136 -- The `process_trial` operation took 1.0374705791473389 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    354 |          9094.54 | 57274368 |   304.64 |              328.768 |              138.768 |            762.706 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2904.997818626799
    time_step_min: 2746
  date: 2020-10-15_12-05-02
  done: false
  episode_len_mean: 762.6963082118026
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.6930026659489
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 75221
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.846791036969752e-44
        cur_lr: 5.0e-05
        entropy: 0.06721238854030769
        entropy_coeff: 0.0005000000000000001
        kl: 0.004371711208174626
        model: {}
        policy_loss: -0.009506488063683113
        total_loss: 0.29992035528024036
        vf_explained_var: 0.9988048076629639
        vf_loss: 0.30946044872204465
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.503225806451617
    gpu_util_percent0: 0.27903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463160147843103
    mean_env_wait_ms: 1.2098809729620545
    mean_inference_ms: 4.287414380431474
    mean_raw_obs_processing_ms: 0.37582146485483053
  time_since_restore: 9120.623035430908
  time_this_iter_s: 26.082358837127686
  time_total_s: 9120.623035430908
  timers:
    learn_throughput: 8700.078
    learn_time_ms: 18596.614
    sample_throughput: 23525.841
    sample_time_ms: 6877.204
    update_time_ms: 29.153
  timestamp: 1602763502
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: cb791_00000
  
2020-10-15 12:05:04,015	WARNING util.py:136 -- The `process_trial` operation took 0.9816665649414062 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    355 |          9120.62 | 57436160 |  304.693 |              328.768 |              138.768 |            762.696 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2904.5717203331387
    time_step_min: 2746
  date: 2020-10-15_12-05-30
  done: false
  episode_len_mean: 762.6809114165661
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.7560047005852
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 222
  episodes_total: 75443
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.23395518484876e-45
        cur_lr: 5.0e-05
        entropy: 0.06647048766414325
        entropy_coeff: 0.0005000000000000001
        kl: 0.003912462561856955
        model: {}
        policy_loss: -0.008827410638332367
        total_loss: 0.3551873291532199
        vf_explained_var: 0.9987463355064392
        vf_loss: 0.36404797186454135
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.456249999999997
    gpu_util_percent0: 0.2878125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631479947486234
    mean_env_wait_ms: 1.2098283091994984
    mean_inference_ms: 4.287359942730364
    mean_raw_obs_processing_ms: 0.37581609317617626
  time_since_restore: 9146.755295753479
  time_this_iter_s: 26.1322603225708
  time_total_s: 9146.755295753479
  timers:
    learn_throughput: 8694.366
    learn_time_ms: 18608.832
    sample_throughput: 23554.103
    sample_time_ms: 6868.952
    update_time_ms: 29.68
  timestamp: 1602763530
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: cb791_00000
  
2020-10-15 12:05:31,599	WARNING util.py:136 -- The `process_trial` operation took 1.0104377269744873 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    356 |          9146.76 | 57597952 |  304.756 |              328.768 |              138.768 |            762.681 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2904.1555752423014
    time_step_min: 2746
  date: 2020-10-15_12-05-57
  done: false
  episode_len_mean: 762.6673230427658
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.8193742948327
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 225
  episodes_total: 75668
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.61697759242438e-45
        cur_lr: 5.0e-05
        entropy: 0.06391402209798495
        entropy_coeff: 0.0005000000000000001
        kl: 0.004031292007615169
        model: {}
        policy_loss: -0.00804075320290091
        total_loss: 0.5399792740742365
        vf_explained_var: 0.9981496334075928
        vf_loss: 0.5480519781510035
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73225806451613
    gpu_util_percent0: 0.3219354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631341714529025
    mean_env_wait_ms: 1.2097740577555585
    mean_inference_ms: 4.28729432935498
    mean_raw_obs_processing_ms: 0.3758116071964167
  time_since_restore: 9172.864647626877
  time_this_iter_s: 26.109351873397827
  time_total_s: 9172.864647626877
  timers:
    learn_throughput: 8681.056
    learn_time_ms: 18637.364
    sample_throughput: 23550.844
    sample_time_ms: 6869.902
    update_time_ms: 31.285
  timestamp: 1602763557
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: cb791_00000
  
2020-10-15 12:05:59,067	WARNING util.py:136 -- The `process_trial` operation took 0.9901647567749023 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    357 |          9172.86 | 57759744 |  304.819 |              328.768 |              138.768 |            762.667 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2903.802793495034
    time_step_min: 2746
  date: 2020-10-15_12-06-25
  done: false
  episode_len_mean: 762.6565556698041
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.8726619992536
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 75858
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.30848879621219e-45
        cur_lr: 5.0e-05
        entropy: 0.062068283247450985
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008644593729210706
        total_loss: .inf
        vf_explained_var: 0.9988253116607666
        vf_loss: 0.3056554893652598
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06129032258065
    gpu_util_percent0: 0.30483870967741933
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631247560900643
    mean_env_wait_ms: 1.2097272502055827
    mean_inference_ms: 4.287239133935255
    mean_raw_obs_processing_ms: 0.37580680885090034
  time_since_restore: 9198.95613360405
  time_this_iter_s: 26.09148597717285
  time_total_s: 9198.95613360405
  timers:
    learn_throughput: 8669.62
    learn_time_ms: 18661.949
    sample_throughput: 23561.488
    sample_time_ms: 6866.799
    update_time_ms: 32.952
  timestamp: 1602763585
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:06:26,533	WARNING util.py:136 -- The `process_trial` operation took 1.0063698291778564 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    358 |          9198.96 | 57921536 |  304.873 |              328.768 |              138.768 |            762.657 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2903.3752416268885
    time_step_min: 2746
  date: 2020-10-15_12-06-52
  done: false
  episode_len_mean: 762.6423257892385
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.9382899255148
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 228
  episodes_total: 76086
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.462733194318285e-45
        cur_lr: 5.0e-05
        entropy: 0.06548627155522506
        entropy_coeff: 0.0005000000000000001
        kl: 0.00394569643928359
        model: {}
        policy_loss: -0.008201323148872083
        total_loss: 0.3187115242083867
        vf_explained_var: 0.9988863468170166
        vf_loss: 0.32694560289382935
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.129032258064516
    gpu_util_percent0: 0.32387096774193547
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631118944443494
    mean_env_wait_ms: 1.2096734801923223
    mean_inference_ms: 4.287180868262567
    mean_raw_obs_processing_ms: 0.37580177562648254
  time_since_restore: 9225.074249744415
  time_this_iter_s: 26.1181161403656
  time_total_s: 9225.074249744415
  timers:
    learn_throughput: 8662.197
    learn_time_ms: 18677.941
    sample_throughput: 23661.132
    sample_time_ms: 6837.881
    update_time_ms: 33.347
  timestamp: 1602763612
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: cb791_00000
  
2020-10-15 12:06:54,165	WARNING util.py:136 -- The `process_trial` operation took 1.0388596057891846 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    359 |          9225.07 | 58083328 |  304.938 |              328.768 |              138.768 |            762.642 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2902.993771635372
    time_step_min: 2746
  date: 2020-10-15_12-07-19
  done: false
  episode_len_mean: 762.6315610133285
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 304.9981319823133
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 217
  episodes_total: 76303
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7313665971591425e-45
        cur_lr: 5.0e-05
        entropy: 0.06488336126009624
        entropy_coeff: 0.0005000000000000001
        kl: 0.004734738768699269
        model: {}
        policy_loss: -0.00967734218769086
        total_loss: 0.4030301918586095
        vf_explained_var: 0.9986050724983215
        vf_loss: 0.4127399722735087
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02258064516129
    gpu_util_percent0: 0.3041935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463099334925049
    mean_env_wait_ms: 1.2096213562680367
    mean_inference_ms: 4.287120403614287
    mean_raw_obs_processing_ms: 0.375797359596293
  time_since_restore: 9250.892237663269
  time_this_iter_s: 25.81798791885376
  time_total_s: 9250.892237663269
  timers:
    learn_throughput: 8662.003
    learn_time_ms: 18678.359
    sample_throughput: 23666.566
    sample_time_ms: 6836.311
    update_time_ms: 36.05
  timestamp: 1602763639
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: cb791_00000
  
2020-10-15 12:07:21,498	WARNING util.py:136 -- The `process_trial` operation took 1.026341199874878 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    360 |          9250.89 | 58245120 |  304.998 |              328.768 |              138.768 |            762.632 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2902.6490184025006
    time_step_min: 2746
  date: 2020-10-15_12-07-47
  done: false
  episode_len_mean: 762.621548839155
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.0478268884197
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 76496
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.656832985795712e-46
        cur_lr: 5.0e-05
        entropy: 0.07133735219637553
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010883684978277111
        total_loss: .inf
        vf_explained_var: 0.998360812664032
        vf_loss: 0.45892872164646786
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.735483870967744
    gpu_util_percent0: 0.3019354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630925890942414
    mean_env_wait_ms: 1.2095748535235802
    mean_inference_ms: 4.28707036411859
    mean_raw_obs_processing_ms: 0.375792990224744
  time_since_restore: 9276.946058273315
  time_this_iter_s: 26.053820610046387
  time_total_s: 9276.946058273315
  timers:
    learn_throughput: 8646.55
    learn_time_ms: 18711.741
    sample_throughput: 23675.359
    sample_time_ms: 6833.772
    update_time_ms: 36.466
  timestamp: 1602763667
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:07:49,030	WARNING util.py:136 -- The `process_trial` operation took 0.9992530345916748 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    361 |          9276.95 | 58406912 |  305.048 |              328.768 |              138.768 |            762.622 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2902.261774984352
    time_step_min: 2746
  date: 2020-10-15_12-08-14
  done: false
  episode_len_mean: 762.611544827766
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.10561596256315
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 76727
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2985249478693566e-45
        cur_lr: 5.0e-05
        entropy: 0.07515035507579644
        entropy_coeff: 0.0005000000000000001
        kl: 0.006415666042206188
        model: {}
        policy_loss: -0.00885423234043022
        total_loss: 0.5417986462513605
        vf_explained_var: 0.9982650279998779
        vf_loss: 0.5506904522577921
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.922580645161286
    gpu_util_percent0: 0.3832258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630781032497978
    mean_env_wait_ms: 1.2095201281720875
    mean_inference_ms: 4.287005247106323
    mean_raw_obs_processing_ms: 0.37578745781884026
  time_since_restore: 9302.593888759613
  time_this_iter_s: 25.647830486297607
  time_total_s: 9302.593888759613
  timers:
    learn_throughput: 8652.898
    learn_time_ms: 18698.013
    sample_throughput: 23650.845
    sample_time_ms: 6840.855
    update_time_ms: 34.806
  timestamp: 1602763694
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: cb791_00000
  
2020-10-15 12:08:16,062	WARNING util.py:136 -- The `process_trial` operation took 1.0214335918426514 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    362 |          9302.59 | 58568704 |  305.106 |              328.768 |              138.768 |            762.612 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2901.9481677979766
    time_step_min: 2746
  date: 2020-10-15_12-08-41
  done: false
  episode_len_mean: 762.6061917573205
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.1533272444488
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 214
  episodes_total: 76941
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2985249478693566e-45
        cur_lr: 5.0e-05
        entropy: 0.07111688951651256
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010008877240276584
        total_loss: .inf
        vf_explained_var: 0.9982573390007019
        vf_loss: 0.5514033635457357
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.793548387096777
    gpu_util_percent0: 0.31806451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630669992887851
    mean_env_wait_ms: 1.2094692452202054
    mean_inference_ms: 4.286950417617855
    mean_raw_obs_processing_ms: 0.375783722687796
  time_since_restore: 9328.384769678116
  time_this_iter_s: 25.790880918502808
  time_total_s: 9328.384769678116
  timers:
    learn_throughput: 8655.428
    learn_time_ms: 18692.548
    sample_throughput: 23641.442
    sample_time_ms: 6843.576
    update_time_ms: 31.141
  timestamp: 1602763721
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:08:43,346	WARNING util.py:136 -- The `process_trial` operation took 1.0953114032745361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    363 |          9328.38 | 58730496 |  305.153 |              328.768 |              138.768 |            762.606 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2901.6198975290226
    time_step_min: 2746
  date: 2020-10-15_12-09-09
  done: false
  episode_len_mean: 762.5972463505069
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.2013192049621
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 77134
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9477874218040347e-45
        cur_lr: 5.0e-05
        entropy: 0.06605415356655915
        entropy_coeff: 0.0005000000000000001
        kl: 0.004037562621912609
        model: {}
        policy_loss: -0.009201822135461649
        total_loss: 0.4245661422610283
        vf_explained_var: 0.9984129071235657
        vf_loss: 0.43380099286635715
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.980645161290326
    gpu_util_percent0: 0.3103225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630569971299182
    mean_env_wait_ms: 1.209422543884023
    mean_inference_ms: 4.286898343817181
    mean_raw_obs_processing_ms: 0.3757790612972214
  time_since_restore: 9354.241778850555
  time_this_iter_s: 25.857009172439575
  time_total_s: 9354.241778850555
  timers:
    learn_throughput: 8660.278
    learn_time_ms: 18682.08
    sample_throughput: 23768.923
    sample_time_ms: 6806.871
    update_time_ms: 29.452
  timestamp: 1602763749
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: cb791_00000
  
2020-10-15 12:09:10,697	WARNING util.py:136 -- The `process_trial` operation took 1.062136173248291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    364 |          9354.24 | 58892288 |  305.201 |              328.768 |              138.768 |            762.597 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2901.2106971511334
    time_step_min: 2746
  date: 2020-10-15_12-09-36
  done: false
  episode_len_mean: 762.5838331092958
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.26192856075994
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 77368
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.738937109020174e-46
        cur_lr: 5.0e-05
        entropy: 0.06413978524506092
        entropy_coeff: 0.0005000000000000001
        kl: 0.004576902138069272
        model: {}
        policy_loss: -0.009310711571743013
        total_loss: 0.34890464196602505
        vf_explained_var: 0.9988422393798828
        vf_loss: 0.35824742416540784
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.303225806451614
    gpu_util_percent0: 0.33032258064516123
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630421541846295
    mean_env_wait_ms: 1.2093675992745008
    mean_inference_ms: 4.286836298017383
    mean_raw_obs_processing_ms: 0.3757735519424275
  time_since_restore: 9380.250782728195
  time_this_iter_s: 26.00900387763977
  time_total_s: 9380.250782728195
  timers:
    learn_throughput: 8670.838
    learn_time_ms: 18659.328
    sample_throughput: 23750.557
    sample_time_ms: 6812.135
    update_time_ms: 29.297
  timestamp: 1602763776
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: cb791_00000
  
2020-10-15 12:09:38,165	WARNING util.py:136 -- The `process_trial` operation took 1.036388635635376 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    365 |          9380.25 | 59054080 |  305.262 |              328.768 |              138.768 |            762.584 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2900.852113130167
    time_step_min: 2746
  date: 2020-10-15_12-10-04
  done: false
  episode_len_mean: 762.571347546985
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.3184014732906
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 210
  episodes_total: 77578
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.869468554510087e-46
        cur_lr: 5.0e-05
        entropy: 0.05819589893023173
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042392665054649115
        model: {}
        policy_loss: -0.007046663760168788
        total_loss: 0.3849010691046715
        vf_explained_var: 0.9987024664878845
        vf_loss: 0.39197683582703274
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.187096774193556
    gpu_util_percent0: 0.25516129032258067
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630333657478484
    mean_env_wait_ms: 1.2093179240276049
    mean_inference_ms: 4.286783560424831
    mean_raw_obs_processing_ms: 0.3757697137097055
  time_since_restore: 9406.10963511467
  time_this_iter_s: 25.85885238647461
  time_total_s: 9406.10963511467
  timers:
    learn_throughput: 8680.884
    learn_time_ms: 18637.733
    sample_throughput: 23733.817
    sample_time_ms: 6816.94
    update_time_ms: 28.642
  timestamp: 1602763804
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: cb791_00000
  
2020-10-15 12:10:05,638	WARNING util.py:136 -- The `process_trial` operation took 1.0691547393798828 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    366 |          9406.11 | 59215872 |  305.318 |              328.768 |              138.768 |            762.571 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2900.487997838839
    time_step_min: 2746
  date: 2020-10-15_12-10-31
  done: false
  episode_len_mean: 762.557287045966
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.3726308407117
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 197
  episodes_total: 77775
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4347342772550434e-46
        cur_lr: 5.0e-05
        entropy: 0.06317174217353265
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008516689013655801
        total_loss: .nan
        vf_explained_var: 0.9992242455482483
        vf_loss: 0.20658443619807562
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.90645161290323
    gpu_util_percent0: 0.35258064516129034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630236652454073
    mean_env_wait_ms: 1.2092714648710376
    mean_inference_ms: 4.286734469930922
    mean_raw_obs_processing_ms: 0.37576514196208366
  time_since_restore: 9432.009567260742
  time_this_iter_s: 25.899932146072388
  time_total_s: 9432.009567260742
  timers:
    learn_throughput: 8691.965
    learn_time_ms: 18613.972
    sample_throughput: 23693.989
    sample_time_ms: 6828.399
    update_time_ms: 28.861
  timestamp: 1602763831
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:10:33,126	WARNING util.py:136 -- The `process_trial` operation took 1.050767183303833 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    367 |          9432.01 | 59377664 |  305.373 |              328.768 |              138.768 |            762.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2900.053170206217
    time_step_min: 2746
  date: 2020-10-15_12-10-59
  done: false
  episode_len_mean: 762.538511824649
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.44030382657553
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 240
  episodes_total: 78015
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.652101415882566e-46
        cur_lr: 5.0e-05
        entropy: 0.062440150727828346
        entropy_coeff: 0.0005000000000000001
        kl: 0.005081891121032338
        model: {}
        policy_loss: -0.0069952768778118
        total_loss: 0.18444154659907022
        vf_explained_var: 0.9993569254875183
        vf_loss: 0.1914680413901806
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.400000000000002
    gpu_util_percent0: 0.3561290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630080534607337
    mean_env_wait_ms: 1.2092149256131612
    mean_inference_ms: 4.286669847295582
    mean_raw_obs_processing_ms: 0.37575999088788375
  time_since_restore: 9458.15214896202
  time_this_iter_s: 26.142581701278687
  time_total_s: 9458.15214896202
  timers:
    learn_throughput: 8689.731
    learn_time_ms: 18618.758
    sample_throughput: 23702.193
    sample_time_ms: 6826.035
    update_time_ms: 28.884
  timestamp: 1602763859
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: cb791_00000
  
2020-10-15 12:11:00,664	WARNING util.py:136 -- The `process_trial` operation took 1.01822829246521 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    368 |          9458.15 | 59539456 |   305.44 |              328.768 |              138.768 |            762.539 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2899.667191117592
    time_step_min: 2746
  date: 2020-10-15_12-11-26
  done: false
  episode_len_mean: 762.5226040707784
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.4976955811566
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 78216
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.652101415882566e-46
        cur_lr: 5.0e-05
        entropy: 0.0641391125197212
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005486166730406694
        total_loss: .nan
        vf_explained_var: 0.9991929531097412
        vf_loss: 0.21208480869730315
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.919354838709676
    gpu_util_percent0: 0.3254838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462999192399078
    mean_env_wait_ms: 1.2091671362469474
    mean_inference_ms: 4.28661864680448
    mean_raw_obs_processing_ms: 0.3757560018103797
  time_since_restore: 9484.023054122925
  time_this_iter_s: 25.87090516090393
  time_total_s: 9484.023054122925
  timers:
    learn_throughput: 8703.172
    learn_time_ms: 18590.003
    sample_throughput: 23716.055
    sample_time_ms: 6822.045
    update_time_ms: 28.353
  timestamp: 1602763886
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:11:27,990	WARNING util.py:136 -- The `process_trial` operation took 1.0331547260284424 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    369 |          9484.02 | 59701248 |  305.498 |              328.768 |              138.768 |            762.523 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2899.2943953253975
    time_step_min: 2746
  date: 2020-10-15_12-11-53
  done: false
  episode_len_mean: 762.5115914307574
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.5516411243265
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 204
  episodes_total: 78420
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.478152123823849e-46
        cur_lr: 5.0e-05
        entropy: 0.07040976608792941
        entropy_coeff: 0.0005000000000000001
        kl: 0.004972028856476148
        model: {}
        policy_loss: -0.0071309504710370675
        total_loss: 0.47460587571064633
        vf_explained_var: 0.99826580286026
        vf_loss: 0.4817720452944438
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.222580645161294
    gpu_util_percent0: 0.3090322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462989686838073
    mean_env_wait_ms: 1.2091209680402744
    mean_inference_ms: 4.2865747165884756
    mean_raw_obs_processing_ms: 0.3757514672419465
  time_since_restore: 9509.825659275055
  time_this_iter_s: 25.802605152130127
  time_total_s: 9509.825659275055
  timers:
    learn_throughput: 8702.986
    learn_time_ms: 18590.402
    sample_throughput: 23711.959
    sample_time_ms: 6823.224
    update_time_ms: 27.63
  timestamp: 1602763913
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: cb791_00000
  
2020-10-15 12:11:55,486	WARNING util.py:136 -- The `process_trial` operation took 1.0947797298431396 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    370 |          9509.83 | 59863040 |  305.552 |              328.768 |              138.768 |            762.512 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2898.898631287048
    time_step_min: 2746
  date: 2020-10-15_12-12-21
  done: false
  episode_len_mean: 762.4963192758064
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.61160073135443
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 78653
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7390760619119245e-46
        cur_lr: 5.0e-05
        entropy: 0.06866457127034664
        entropy_coeff: 0.0005000000000000001
        kl: 0.005003662663511932
        model: {}
        policy_loss: -0.009263621604380509
        total_loss: 0.5076224183042845
        vf_explained_var: 0.9983111023902893
        vf_loss: 0.5169203753272692
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.651612903225807
    gpu_util_percent0: 0.28935483870967743
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629761064289631
    mean_env_wait_ms: 1.2090660265129638
    mean_inference_ms: 4.286513071458203
    mean_raw_obs_processing_ms: 0.37574677710904236
  time_since_restore: 9535.671426534653
  time_this_iter_s: 25.84576725959778
  time_total_s: 9535.671426534653
  timers:
    learn_throughput: 8714.642
    learn_time_ms: 18565.537
    sample_throughput: 23728.831
    sample_time_ms: 6818.372
    update_time_ms: 27.5
  timestamp: 1602763941
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: cb791_00000
  
2020-10-15 12:12:22,772	WARNING util.py:136 -- The `process_trial` operation took 1.0231573581695557 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    371 |          9535.67 | 60024832 |  305.612 |              328.768 |              138.768 |            762.496 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2898.525528459411
    time_step_min: 2746
  date: 2020-10-15_12-12-48
  done: false
  episode_len_mean: 762.4841921042953
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.66720096863514
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 200
  episodes_total: 78853
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7390760619119245e-46
        cur_lr: 5.0e-05
        entropy: 0.06683955527842045
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0061761010632229345
        total_loss: .nan
        vf_explained_var: 0.999389111995697
        vf_loss: 0.15936804686983427
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.296774193548384
    gpu_util_percent0: 0.33483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146296588519183
    mean_env_wait_ms: 1.209018630639308
    mean_inference_ms: 4.286459616687838
    mean_raw_obs_processing_ms: 0.3757427192409634
  time_since_restore: 9561.410011768341
  time_this_iter_s: 25.738585233688354
  time_total_s: 9561.410011768341
  timers:
    learn_throughput: 8708.482
    learn_time_ms: 18578.668
    sample_throughput: 23754.738
    sample_time_ms: 6810.936
    update_time_ms: 28.962
  timestamp: 1602763968
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:12:50,098	WARNING util.py:136 -- The `process_trial` operation took 1.0429844856262207 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    372 |          9561.41 | 60186624 |  305.667 |              328.768 |              138.768 |            762.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2898.1728291954546
    time_step_min: 2746
  date: 2020-10-15_12-13-16
  done: false
  episode_len_mean: 762.4769619932966
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.7158724409719
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 212
  episodes_total: 79065
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1086140928678876e-46
        cur_lr: 5.0e-05
        entropy: 0.07935183060665925
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053243753500282764
        model: {}
        policy_loss: -0.011488187956274487
        total_loss: 0.5813831190268198
        vf_explained_var: 0.998049259185791
        vf_loss: 0.5929109702507654
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.16129032258064
    gpu_util_percent0: 0.3138709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629556305348027
    mean_env_wait_ms: 1.208970907132274
    mean_inference_ms: 4.286418090691863
    mean_raw_obs_processing_ms: 0.3757384180497808
  time_since_restore: 9587.55818271637
  time_this_iter_s: 26.148170948028564
  time_total_s: 9587.55818271637
  timers:
    learn_throughput: 8692.35
    learn_time_ms: 18613.148
    sample_throughput: 23759.766
    sample_time_ms: 6809.495
    update_time_ms: 31.415
  timestamp: 1602763996
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: cb791_00000
  
2020-10-15 12:13:17,700	WARNING util.py:136 -- The `process_trial` operation took 1.0679404735565186 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    373 |          9587.56 | 60348416 |  305.716 |              328.768 |              138.768 |            762.477 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2897.8245076144694
    time_step_min: 2746
  date: 2020-10-15_12-13-43
  done: false
  episode_len_mean: 762.467753732849
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.77003234524403
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 79296
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1086140928678876e-46
        cur_lr: 5.0e-05
        entropy: 0.0746439794699351
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010360082921882471
        total_loss: .nan
        vf_explained_var: 0.9978477358818054
        vf_loss: 0.6635805716117223
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.067741935483863
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629427101470993
    mean_env_wait_ms: 1.2089171705228807
    mean_inference_ms: 4.286354106793335
    mean_raw_obs_processing_ms: 0.37573361472999606
  time_since_restore: 9613.70351266861
  time_this_iter_s: 26.14532995223999
  time_total_s: 9613.70351266861
  timers:
    learn_throughput: 8679.295
    learn_time_ms: 18641.145
    sample_throughput: 23754.78
    sample_time_ms: 6810.924
    update_time_ms: 32.612
  timestamp: 1602764023
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:13:45,284	WARNING util.py:136 -- The `process_trial` operation took 1.0549061298370361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    374 |           9613.7 | 60510208 |   305.77 |              328.768 |              138.768 |            762.468 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2897.496022655758
    time_step_min: 2746
  date: 2020-10-15_12-14-11
  done: false
  episode_len_mean: 762.4583149869793
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.8184522323206
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 79489
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.162921139301832e-46
        cur_lr: 5.0e-05
        entropy: 0.07177605604132016
        entropy_coeff: 0.0005000000000000001
        kl: 0.004688677377998829
        model: {}
        policy_loss: -0.009814971021720945
        total_loss: 0.4282446801662445
        vf_explained_var: 0.9983475208282471
        vf_loss: 0.4380955348412196
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.193548387096776
    gpu_util_percent0: 0.30709677419354836
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462933395784233
    mean_env_wait_ms: 1.2088711764823843
    mean_inference_ms: 4.28630464934256
    mean_raw_obs_processing_ms: 0.3757294715509724
  time_since_restore: 9639.840722799301
  time_this_iter_s: 26.13721013069153
  time_total_s: 9639.840722799301
  timers:
    learn_throughput: 8671.271
    learn_time_ms: 18658.395
    sample_throughput: 23774.802
    sample_time_ms: 6805.188
    update_time_ms: 33.68
  timestamp: 1602764051
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: cb791_00000
  
2020-10-15 12:14:12,887	WARNING util.py:136 -- The `process_trial` operation took 1.0865347385406494 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    375 |          9639.84 | 60672000 |  305.818 |              328.768 |              138.768 |            762.458 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2897.134099809218
    time_step_min: 2746
  date: 2020-10-15_12-14-38
  done: false
  episode_len_mean: 762.4441670534807
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.8733783114733
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 222
  episodes_total: 79711
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.081460569650916e-46
        cur_lr: 5.0e-05
        entropy: 0.07293577057619889
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009677944957123449
        total_loss: .nan
        vf_explained_var: 0.9981253743171692
        vf_loss: 0.5698269158601761
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.780645161290327
    gpu_util_percent0: 0.2793548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146292163621291
    mean_env_wait_ms: 1.2088212563402236
    mean_inference_ms: 4.286255900281165
    mean_raw_obs_processing_ms: 0.3757248203341166
  time_since_restore: 9665.56643819809
  time_this_iter_s: 25.725715398788452
  time_total_s: 9665.56643819809
  timers:
    learn_throughput: 8680.457
    learn_time_ms: 18638.651
    sample_throughput: 23785.483
    sample_time_ms: 6802.132
    update_time_ms: 33.607
  timestamp: 1602764078
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:14:40,065	WARNING util.py:136 -- The `process_trial` operation took 1.0703754425048828 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    376 |          9665.57 | 60833792 |  305.873 |              328.768 |              138.768 |            762.444 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2896.7597221353026
    time_step_min: 2746
  date: 2020-10-15_12-15-05
  done: false
  episode_len_mean: 762.4290039282408
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.9302279178301
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 223
  episodes_total: 79934
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.622190854476372e-46
        cur_lr: 5.0e-05
        entropy: 0.0671902975688378
        entropy_coeff: 0.0005000000000000001
        kl: 0.00488072008981059
        model: {}
        policy_loss: -0.006213476920189957
        total_loss: 0.33239157249530155
        vf_explained_var: 0.9988444447517395
        vf_loss: 0.3386386459072431
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.676666666666666
    gpu_util_percent0: 0.3236666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629099942257248
    mean_env_wait_ms: 1.2087699037894046
    mean_inference_ms: 4.286198976353973
    mean_raw_obs_processing_ms: 0.3757207219361202
  time_since_restore: 9691.322182655334
  time_this_iter_s: 25.755744457244873
  time_total_s: 9691.322182655334
  timers:
    learn_throughput: 8683.732
    learn_time_ms: 18631.62
    sample_throughput: 23818.703
    sample_time_ms: 6792.645
    update_time_ms: 34.057
  timestamp: 1602764105
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: cb791_00000
  
2020-10-15 12:15:07,358	WARNING util.py:136 -- The `process_trial` operation took 1.056800127029419 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    377 |          9691.32 | 60995584 |   305.93 |              328.768 |              138.768 |            762.429 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2896.4284144544613
    time_step_min: 2746
  date: 2020-10-15_12-15-33
  done: false
  episode_len_mean: 762.4163494539781
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 305.98016861280496
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 191
  episodes_total: 80125
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.311095427238186e-46
        cur_lr: 5.0e-05
        entropy: 0.06956584006547928
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007110475184163079
        total_loss: .nan
        vf_explained_var: 0.9990882277488708
        vf_loss: 0.23926805208126703
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.26774193548387
    gpu_util_percent0: 0.28516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629014098188142
    mean_env_wait_ms: 1.2087244703242292
    mean_inference_ms: 4.2861518282051065
    mean_raw_obs_processing_ms: 0.375716507163577
  time_since_restore: 9717.257111787796
  time_this_iter_s: 25.934929132461548
  time_total_s: 9717.257111787796
  timers:
    learn_throughput: 8690.381
    learn_time_ms: 18617.365
    sample_throughput: 23801.796
    sample_time_ms: 6797.47
    update_time_ms: 32.903
  timestamp: 1602764133
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:15:34,842	WARNING util.py:136 -- The `process_trial` operation took 1.073087215423584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    378 |          9717.26 | 61157376 |   305.98 |              328.768 |              138.768 |            762.416 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2896.0380492299264
    time_step_min: 2746
  date: 2020-10-15_12-16-00
  done: false
  episode_len_mean: 762.4046617551894
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.0383280512827
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 80356
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4666431408572795e-46
        cur_lr: 5.0e-05
        entropy: 0.07504426874220371
        entropy_coeff: 0.0005000000000000001
        kl: 0.0073127787715444965
        model: {}
        policy_loss: -0.00916151870721175
        total_loss: 0.34044645726680756
        vf_explained_var: 0.9988306164741516
        vf_loss: 0.3496454904476802
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.096774193548388
    gpu_util_percent0: 0.3261290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462889941173487
    mean_env_wait_ms: 1.2086728125130166
    mean_inference_ms: 4.286099832268292
    mean_raw_obs_processing_ms: 0.3757117721805668
  time_since_restore: 9743.063313961029
  time_this_iter_s: 25.806202173233032
  time_total_s: 9743.063313961029
  timers:
    learn_throughput: 8696.232
    learn_time_ms: 18604.838
    sample_throughput: 23786.712
    sample_time_ms: 6801.781
    update_time_ms: 41.377
  timestamp: 1602764160
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: cb791_00000
  
2020-10-15 12:16:02,101	WARNING util.py:136 -- The `process_trial` operation took 1.0743143558502197 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    379 |          9743.06 | 61319168 |  306.038 |              328.768 |              138.768 |            762.405 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2895.66395142364
    time_step_min: 2746
  date: 2020-10-15_12-16-27
  done: false
  episode_len_mean: 762.39390855384
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.09563790614266
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 80572
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4666431408572795e-46
        cur_lr: 5.0e-05
        entropy: 0.07352524623274803
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009124476591144534
        total_loss: .nan
        vf_explained_var: 0.9993723034858704
        vf_loss: 0.17200967172781625
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.7483870967742
    gpu_util_percent0: 0.3412903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628794100885884
    mean_env_wait_ms: 1.2086229920731475
    mean_inference_ms: 4.286049062828853
    mean_raw_obs_processing_ms: 0.3757081241327831
  time_since_restore: 9768.959874391556
  time_this_iter_s: 25.896560430526733
  time_total_s: 9768.959874391556
  timers:
    learn_throughput: 8695.326
    learn_time_ms: 18606.779
    sample_throughput: 23790.201
    sample_time_ms: 6800.783
    update_time_ms: 39.371
  timestamp: 1602764187
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:16:29,458	WARNING util.py:136 -- The `process_trial` operation took 1.0656497478485107 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    380 |          9768.96 | 61480960 |  306.096 |              328.768 |              138.768 |            762.394 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2895.3605485153666
    time_step_min: 2746
  date: 2020-10-15_12-16-55
  done: false
  episode_len_mean: 762.3848649184063
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.1423014784948
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 194
  episodes_total: 80766
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.199964711285919e-46
        cur_lr: 5.0e-05
        entropy: 0.0759756335367759
        entropy_coeff: 0.0005000000000000001
        kl: 0.004500900860875845
        model: {}
        policy_loss: -0.009353399043902755
        total_loss: 0.40103856722513836
        vf_explained_var: 0.998494565486908
        vf_loss: 0.4104299495617549
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.490322580645163
    gpu_util_percent0: 0.3461290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462870730041851
    mean_env_wait_ms: 1.2085780957021897
    mean_inference_ms: 4.285999333214499
    mean_raw_obs_processing_ms: 0.3757038835016631
  time_since_restore: 9795.146077632904
  time_this_iter_s: 26.186203241348267
  time_total_s: 9795.146077632904
  timers:
    learn_throughput: 8683.581
    learn_time_ms: 18631.945
    sample_throughput: 23765.0
    sample_time_ms: 6807.995
    update_time_ms: 40.493
  timestamp: 1602764215
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: cb791_00000
  
2020-10-15 12:16:57,155	WARNING util.py:136 -- The `process_trial` operation took 1.127004861831665 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    381 |          9795.15 | 61642752 |  306.142 |              328.768 |              138.768 |            762.385 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2895.035956915932
    time_step_min: 2746
  date: 2020-10-15_12-17-23
  done: false
  episode_len_mean: 762.3777917700655
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.19433042974634
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 80997
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5999823556429596e-46
        cur_lr: 5.0e-05
        entropy: 0.0791272483766079
        entropy_coeff: 0.0005000000000000001
        kl: 0.004205916930610935
        model: {}
        policy_loss: -0.011460133425619764
        total_loss: 0.48291995624701184
        vf_explained_var: 0.998439610004425
        vf_loss: 0.4944196591774623
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.435483870967737
    gpu_util_percent0: 0.33580645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628573272252846
    mean_env_wait_ms: 1.2085255025116666
    mean_inference_ms: 4.285942313606733
    mean_raw_obs_processing_ms: 0.3756987526852841
  time_since_restore: 9821.000180244446
  time_this_iter_s: 25.854102611541748
  time_total_s: 9821.000180244446
  timers:
    learn_throughput: 8691.289
    learn_time_ms: 18615.421
    sample_throughput: 23695.588
    sample_time_ms: 6827.938
    update_time_ms: 39.258
  timestamp: 1602764243
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: cb791_00000
  
2020-10-15 12:17:24,527	WARNING util.py:136 -- The `process_trial` operation took 1.1240696907043457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    382 |             9821 | 61804544 |  306.194 |              328.768 |              138.768 |            762.378 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2894.6892286464044
    time_step_min: 2746
  date: 2020-10-15_12-17-50
  done: false
  episode_len_mean: 762.3692123928677
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.2490358963488
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 211
  episodes_total: 81208
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2999911778214798e-46
        cur_lr: 5.0e-05
        entropy: 0.07524318744738896
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0061040801932297955
        total_loss: .nan
        vf_explained_var: 0.9991684556007385
        vf_loss: 0.23204670349756876
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53225806451613
    gpu_util_percent0: 0.2822580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628481152179934
    mean_env_wait_ms: 1.208477948729623
    mean_inference_ms: 4.2858983515622695
    mean_raw_obs_processing_ms: 0.3756954855897099
  time_since_restore: 9846.937115907669
  time_this_iter_s: 25.936935663223267
  time_total_s: 9846.937115907669
  timers:
    learn_throughput: 8699.8
    learn_time_ms: 18597.21
    sample_throughput: 23707.336
    sample_time_ms: 6824.554
    update_time_ms: 38.17
  timestamp: 1602764270
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:17:51,952	WARNING util.py:136 -- The `process_trial` operation took 1.0616605281829834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    383 |          9846.94 | 61966336 |  306.249 |              328.768 |              138.768 |            762.369 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2894.3740060222453
    time_step_min: 2746
  date: 2020-10-15_12-18-17
  done: false
  episode_len_mean: 762.3631639722864
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.2962975785071
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 196
  episodes_total: 81404
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9499867667322197e-46
        cur_lr: 5.0e-05
        entropy: 0.077145724867781
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009576152098209908
        total_loss: .nan
        vf_explained_var: 0.9986498951911926
        vf_loss: 0.36572322249412537
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.00645161290323
    gpu_util_percent0: 0.33483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462840605670408
    mean_env_wait_ms: 1.2084335080270205
    mean_inference_ms: 4.285851962869577
    mean_raw_obs_processing_ms: 0.37569136539280973
  time_since_restore: 9872.849731683731
  time_this_iter_s: 25.91261577606201
  time_total_s: 9872.849731683731
  timers:
    learn_throughput: 8712.781
    learn_time_ms: 18569.502
    sample_throughput: 23690.118
    sample_time_ms: 6829.514
    update_time_ms: 36.22
  timestamp: 1602764297
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:18:19,368	WARNING util.py:136 -- The `process_trial` operation took 1.0747723579406738 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    384 |          9872.85 | 62128128 |  306.296 |              328.768 |              138.768 |            762.363 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2894.0187261786587
    time_step_min: 2746
  date: 2020-10-15_12-18-45
  done: false
  episode_len_mean: 762.357538340928
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.34982449810457
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 232
  episodes_total: 81636
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.92498015009833e-46
        cur_lr: 5.0e-05
        entropy: 0.07753084972500801
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011679587153291019
        total_loss: .nan
        vf_explained_var: 0.998344361782074
        vf_loss: 0.5356478914618492
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.2
    gpu_util_percent0: 0.32225806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628254896048534
    mean_env_wait_ms: 1.2083812700656418
    mean_inference_ms: 4.285795949375796
    mean_raw_obs_processing_ms: 0.37568655701499815
  time_since_restore: 9898.73006105423
  time_this_iter_s: 25.880329370498657
  time_total_s: 9898.73006105423
  timers:
    learn_throughput: 8723.816
    learn_time_ms: 18546.013
    sample_throughput: 23707.355
    sample_time_ms: 6824.549
    update_time_ms: 36.245
  timestamp: 1602764325
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:18:46,786	WARNING util.py:136 -- The `process_trial` operation took 1.0738239288330078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    385 |          9898.73 | 62289920 |   306.35 |              328.768 |              138.768 |            762.358 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2893.6975990807073
    time_step_min: 2746
  date: 2020-10-15_12-19-12
  done: false
  episode_len_mean: 762.3521462347722
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.39601597529764
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 205
  episodes_total: 81841
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.387470225147495e-46
        cur_lr: 5.0e-05
        entropy: 0.0745590856919686
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00936261925380677
        total_loss: .nan
        vf_explained_var: 0.998302698135376
        vf_loss: 0.4998508443435033
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.086666666666666
    gpu_util_percent0: 0.3086666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628175697790702
    mean_env_wait_ms: 1.2083349534331118
    mean_inference_ms: 4.285754019599284
    mean_raw_obs_processing_ms: 0.3756832131941439
  time_since_restore: 9924.413328886032
  time_this_iter_s: 25.683267831802368
  time_total_s: 9924.413328886032
  timers:
    learn_throughput: 8717.804
    learn_time_ms: 18558.803
    sample_throughput: 23736.676
    sample_time_ms: 6816.119
    update_time_ms: 36.28
  timestamp: 1602764352
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:19:14,057	WARNING util.py:136 -- The `process_trial` operation took 1.1091985702514648 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    386 |          9924.41 | 62451712 |  306.396 |              328.768 |              138.768 |            762.352 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2893.401985293221
    time_step_min: 2746
  date: 2020-10-15_12-19-39
  done: false
  episode_len_mean: 762.3459081933644
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.44252438822286
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 82042
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.581205337721242e-46
        cur_lr: 5.0e-05
        entropy: 0.07436320123573144
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009173754505657902
        total_loss: .nan
        vf_explained_var: 0.9988076686859131
        vf_loss: 0.34515172491470975
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.364516129032257
    gpu_util_percent0: 0.335483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462808356561046
    mean_env_wait_ms: 1.2082897868421638
    mean_inference_ms: 4.285707419929898
    mean_raw_obs_processing_ms: 0.375678815397606
  time_since_restore: 9950.233837604523
  time_this_iter_s: 25.8205087184906
  time_total_s: 9950.233837604523
  timers:
    learn_throughput: 8715.223
    learn_time_ms: 18564.299
    sample_throughput: 23728.228
    sample_time_ms: 6818.545
    update_time_ms: 34.266
  timestamp: 1602764379
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:19:41,459	WARNING util.py:136 -- The `process_trial` operation took 1.0963430404663086 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    387 |          9950.23 | 62613504 |  306.443 |              328.768 |              138.768 |            762.346 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2893.0084509782464
    time_step_min: 2746
  date: 2020-10-15_12-20-07
  done: false
  episode_len_mean: 762.334658110309
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.5022657848078
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 236
  episodes_total: 82278
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.871808006581863e-46
        cur_lr: 5.0e-05
        entropy: 0.07677416317164898
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007946528450095988
        total_loss: .inf
        vf_explained_var: 0.9989142417907715
        vf_loss: 0.3337307075659434
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.793548387096777
    gpu_util_percent0: 0.32354838709677425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462793479537869
    mean_env_wait_ms: 1.2082365975006586
    mean_inference_ms: 4.285649757930328
    mean_raw_obs_processing_ms: 0.37567446179718245
  time_since_restore: 9976.040270090103
  time_this_iter_s: 25.806432485580444
  time_total_s: 9976.040270090103
  timers:
    learn_throughput: 8719.716
    learn_time_ms: 18554.733
    sample_throughput: 23746.243
    sample_time_ms: 6813.373
    update_time_ms: 34.903
  timestamp: 1602764407
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:20:08,857	WARNING util.py:136 -- The `process_trial` operation took 1.0835423469543457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    388 |          9976.04 | 62775296 |  306.502 |              328.768 |              138.768 |            762.335 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2892.7743692382337
    time_step_min: 2746
  date: 2020-10-15_12-20-34
  done: false
  episode_len_mean: 762.3413111216188
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.53215455761546
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 82479
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4807712009872797e-45
        cur_lr: 5.0e-05
        entropy: 0.09831805775562923
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01560020144097507
        total_loss: .inf
        vf_explained_var: 0.9959954619407654
        vf_loss: 1.2752392888069153
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.738709677419358
    gpu_util_percent0: 0.3790322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627870241262195
    mean_env_wait_ms: 1.2081907937555156
    mean_inference_ms: 4.285605878956602
    mean_raw_obs_processing_ms: 0.37567075934492017
  time_since_restore: 10001.877286434174
  time_this_iter_s: 25.837016344070435
  time_total_s: 10001.877286434174
  timers:
    learn_throughput: 8714.629
    learn_time_ms: 18565.564
    sample_throughput: 23742.844
    sample_time_ms: 6814.348
    update_time_ms: 26.679
  timestamp: 1602764434
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:20:36,312	WARNING util.py:136 -- The `process_trial` operation took 1.1335740089416504 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    389 |          10001.9 | 62937088 |  306.532 |              328.768 |              138.768 |            762.341 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2892.7991335600277
    time_step_min: 2746
  date: 2020-10-15_12-21-02
  done: false
  episode_len_mean: 762.3695873046591
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.53134384632017
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 197
  episodes_total: 82676
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.221156801480919e-45
        cur_lr: 5.0e-05
        entropy: 0.1086118792494138
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01352236681850627
        total_loss: .inf
        vf_explained_var: 0.9948596358299255
        vf_loss: 1.9487033287684123
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.348387096774196
    gpu_util_percent0: 0.3264516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462778734302351
    mean_env_wait_ms: 1.2081470322155832
    mean_inference_ms: 4.28556633382141
    mean_raw_obs_processing_ms: 0.37566658589537305
  time_since_restore: 10027.75690817833
  time_this_iter_s: 25.879621744155884
  time_total_s: 10027.75690817833
  timers:
    learn_throughput: 8713.756
    learn_time_ms: 18567.424
    sample_throughput: 23734.303
    sample_time_ms: 6816.8
    update_time_ms: 28.992
  timestamp: 1602764462
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:21:03,884	WARNING util.py:136 -- The `process_trial` operation took 1.1763932704925537 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    390 |          10027.8 | 63098880 |  306.531 |              328.768 |              138.768 |             762.37 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2892.601882466514
    time_step_min: 2746
  date: 2020-10-15_12-21-30
  done: false
  episode_len_mean: 762.3733129093343
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.57085966590347
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 82909
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3317352022213795e-45
        cur_lr: 5.0e-05
        entropy: 0.0842429952075084
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011202224772811556
        total_loss: .inf
        vf_explained_var: 0.9977743029594421
        vf_loss: 0.7420231352249781
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.19375
    gpu_util_percent0: 0.3096875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462765533549195
    mean_env_wait_ms: 1.2080957406487787
    mean_inference_ms: 4.285514670147311
    mean_raw_obs_processing_ms: 0.37566245435219864
  time_since_restore: 10054.006557226181
  time_this_iter_s: 26.249649047851562
  time_total_s: 10054.006557226181
  timers:
    learn_throughput: 8715.049
    learn_time_ms: 18564.669
    sample_throughput: 23706.403
    sample_time_ms: 6824.823
    update_time_ms: 27.606
  timestamp: 1602764490
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:21:31,715	WARNING util.py:136 -- The `process_trial` operation took 1.1392567157745361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    391 |            10054 | 63260672 |  306.571 |              328.768 |              138.768 |            762.373 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2892.29924766777
    time_step_min: 2746
  date: 2020-10-15_12-21-58
  done: false
  episode_len_mean: 762.3660995740789
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.61681934730984
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 205
  episodes_total: 83114
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.997602803332068e-45
        cur_lr: 5.0e-05
        entropy: 0.07367175817489624
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012196212958466882
        total_loss: .inf
        vf_explained_var: 0.9988099932670593
        vf_loss: 0.33608217785755795
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.36451612903226
    gpu_util_percent0: 0.29258064516129034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627569761535753
    mean_env_wait_ms: 1.208049243536023
    mean_inference_ms: 4.285466977999165
    mean_raw_obs_processing_ms: 0.3756588857915969
  time_since_restore: 10080.302074432373
  time_this_iter_s: 26.295517206192017
  time_total_s: 10080.302074432373
  timers:
    learn_throughput: 8695.753
    learn_time_ms: 18605.864
    sample_throughput: 23674.92
    sample_time_ms: 6833.899
    update_time_ms: 29.596
  timestamp: 1602764518
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:21:59,620	WARNING util.py:136 -- The `process_trial` operation took 1.0877869129180908 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    392 |          10080.3 | 63422464 |  306.617 |              328.768 |              138.768 |            762.366 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2892.001176795514
    time_step_min: 2746
  date: 2020-10-15_12-22-25
  done: false
  episode_len_mean: 762.3589106534159
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.6612931853461
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 202
  episodes_total: 83316
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.496404204998103e-45
        cur_lr: 5.0e-05
        entropy: 0.07566639470557372
        entropy_coeff: 0.0005000000000000001
        kl: 0.005354722496122122
        model: {}
        policy_loss: -0.011208058958194064
        total_loss: 0.4942755401134491
        vf_explained_var: 0.9982976913452148
        vf_loss: 0.5055214290817579
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.741935483870968
    gpu_util_percent0: 0.30677419354838703
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627484937990828
    mean_env_wait_ms: 1.2080056136089963
    mean_inference_ms: 4.285431330786194
    mean_raw_obs_processing_ms: 0.37565505152604833
  time_since_restore: 10106.281360387802
  time_this_iter_s: 25.979285955429077
  time_total_s: 10106.281360387802
  timers:
    learn_throughput: 8692.538
    learn_time_ms: 18612.745
    sample_throughput: 23681.553
    sample_time_ms: 6831.985
    update_time_ms: 36.409
  timestamp: 1602764545
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: cb791_00000
  
2020-10-15 12:22:27,132	WARNING util.py:136 -- The `process_trial` operation took 1.1383495330810547 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    393 |          10106.3 | 63584256 |  306.661 |              328.768 |              138.768 |            762.359 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2891.647923077844
    time_step_min: 2746
  date: 2020-10-15_12-22-53
  done: false
  episode_len_mean: 762.3514099004213
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.7146729406279
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 236
  episodes_total: 83552
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.496404204998103e-45
        cur_lr: 5.0e-05
        entropy: 0.07296063440541427
        entropy_coeff: 0.0005000000000000001
        kl: 0.005397937260568142
        model: {}
        policy_loss: -0.010422029571297267
        total_loss: 0.5439854214588801
        vf_explained_var: 0.9982850551605225
        vf_loss: 0.5544439281026522
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.92258064516129
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627349892042163
    mean_env_wait_ms: 1.2079537439811818
    mean_inference_ms: 4.285378362093557
    mean_raw_obs_processing_ms: 0.3756509824387446
  time_since_restore: 10132.357427597046
  time_this_iter_s: 26.076067209243774
  time_total_s: 10132.357427597046
  timers:
    learn_throughput: 8679.443
    learn_time_ms: 18640.828
    sample_throughput: 23695.735
    sample_time_ms: 6827.896
    update_time_ms: 36.523
  timestamp: 1602764573
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: cb791_00000
  
2020-10-15 12:22:54,821	WARNING util.py:136 -- The `process_trial` operation took 1.1208736896514893 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    394 |          10132.4 | 63746048 |  306.715 |              328.768 |              138.768 |            762.351 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2891.3483013188074
    time_step_min: 2746
  date: 2020-10-15_12-23-20
  done: false
  episode_len_mean: 762.3457152750415
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.76002819324077
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 83751
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.496404204998103e-45
        cur_lr: 5.0e-05
        entropy: 0.07164729572832584
        entropy_coeff: 0.0005000000000000001
        kl: 0.004336326460664471
        model: {}
        policy_loss: -0.007151017809519544
        total_loss: 0.38157307108243305
        vf_explained_var: 0.9986712336540222
        vf_loss: 0.38875990857680637
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.838709677419356
    gpu_util_percent0: 0.3467741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627263911770333
    mean_env_wait_ms: 1.2079080022581457
    mean_inference_ms: 4.285329966114667
    mean_raw_obs_processing_ms: 0.3756468935883121
  time_since_restore: 10158.356402873993
  time_this_iter_s: 25.99897527694702
  time_total_s: 10158.356402873993
  timers:
    learn_throughput: 8680.538
    learn_time_ms: 18638.477
    sample_throughput: 23614.441
    sample_time_ms: 6851.401
    update_time_ms: 37.029
  timestamp: 1602764600
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: cb791_00000
  
2020-10-15 12:23:22,442	WARNING util.py:136 -- The `process_trial` operation took 1.1599907875061035 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    395 |          10158.4 | 63907840 |   306.76 |              328.768 |              138.768 |            762.346 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2891.031005719733
    time_step_min: 2746
  date: 2020-10-15_12-23-48
  done: false
  episode_len_mean: 762.3447039626484
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.80472551477425
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 208
  episodes_total: 83959
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7482021024990514e-45
        cur_lr: 5.0e-05
        entropy: 0.08067366356650989
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009760816563357366
        total_loss: .inf
        vf_explained_var: 0.9980411529541016
        vf_loss: 0.6085672279198965
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.870967741935488
    gpu_util_percent0: 0.34580645161290324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462718323389442
    mean_env_wait_ms: 1.2078628909571105
    mean_inference_ms: 4.285291092030459
    mean_raw_obs_processing_ms: 0.3756428425602545
  time_since_restore: 10184.20174074173
  time_this_iter_s: 25.845337867736816
  time_total_s: 10184.20174074173
  timers:
    learn_throughput: 8681.743
    learn_time_ms: 18635.89
    sample_throughput: 23580.077
    sample_time_ms: 6861.386
    update_time_ms: 44.593
  timestamp: 1602764628
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:23:49,782	WARNING util.py:136 -- The `process_trial` operation took 1.1013143062591553 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    396 |          10184.2 | 64069632 |  306.805 |              328.768 |              138.768 |            762.345 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2890.827302221113
    time_step_min: 2746
  date: 2020-10-15_12-24-15
  done: false
  episode_len_mean: 762.361651581023
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.8294811128892
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 227
  episodes_total: 84186
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.622303153748576e-45
        cur_lr: 5.0e-05
        entropy: 0.09893319569528103
        entropy_coeff: 0.0005000000000000001
        kl: 0.005944653685825567
        model: {}
        policy_loss: -0.012813713188127926
        total_loss: 2.057167112827301
        vf_explained_var: 0.9947555661201477
        vf_loss: 2.0700302521387735
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925806451612907
    gpu_util_percent0: 0.28967741935483865
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627069332020723
    mean_env_wait_ms: 1.2078138691433316
    mean_inference_ms: 4.285243327003969
    mean_raw_obs_processing_ms: 0.37563953120895094
  time_since_restore: 10209.950160503387
  time_this_iter_s: 25.748419761657715
  time_total_s: 10209.950160503387
  timers:
    learn_throughput: 8690.712
    learn_time_ms: 18616.657
    sample_throughput: 23571.829
    sample_time_ms: 6863.786
    update_time_ms: 44.331
  timestamp: 1602764655
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: cb791_00000
  
2020-10-15 12:24:17,082	WARNING util.py:136 -- The `process_trial` operation took 1.149867296218872 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    397 |            10210 | 64231424 |  306.829 |              328.768 |              138.768 |            762.362 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2890.6810803760923
    time_step_min: 2746
  date: 2020-10-15_12-24-43
  done: false
  episode_len_mean: 762.370218061152
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.85844579954545
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 194
  episodes_total: 84380
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.622303153748576e-45
        cur_lr: 5.0e-05
        entropy: 0.08484440731505553
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012089209278504617
        total_loss: .inf
        vf_explained_var: 0.9968830943107605
        vf_loss: 0.9665664434432983
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.45483870967742
    gpu_util_percent0: 0.2974193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462696893866901
    mean_env_wait_ms: 1.2077696641601579
    mean_inference_ms: 4.2851966929457435
    mean_raw_obs_processing_ms: 0.3756353537848226
  time_since_restore: 10235.902372598648
  time_this_iter_s: 25.95221209526062
  time_total_s: 10235.902372598648
  timers:
    learn_throughput: 8694.18
    learn_time_ms: 18609.23
    sample_throughput: 23502.213
    sample_time_ms: 6884.118
    update_time_ms: 44.658
  timestamp: 1602764683
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:24:44,595	WARNING util.py:136 -- The `process_trial` operation took 1.1607749462127686 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    398 |          10235.9 | 64393216 |  306.858 |              328.768 |              138.768 |             762.37 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2890.363746023487
    time_step_min: 2746
  date: 2020-10-15_12-25-10
  done: false
  episode_len_mean: 762.367490957233
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.90455513789385
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 218
  episodes_total: 84598
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.433454730622866e-45
        cur_lr: 5.0e-05
        entropy: 0.08119836449623108
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009923421132043586
        total_loss: .inf
        vf_explained_var: 0.9983837604522705
        vf_loss: 0.5013881276051203
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.483870967741936
    gpu_util_percent0: 0.3354838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626906051689978
    mean_env_wait_ms: 1.207722723556271
    mean_inference_ms: 4.285160594134515
    mean_raw_obs_processing_ms: 0.375631369075876
  time_since_restore: 10261.976846456528
  time_this_iter_s: 26.07447385787964
  time_total_s: 10261.976846456528
  timers:
    learn_throughput: 8683.194
    learn_time_ms: 18632.774
    sample_throughput: 23505.559
    sample_time_ms: 6883.138
    update_time_ms: 44.454
  timestamp: 1602764710
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:25:12,337	WARNING util.py:136 -- The `process_trial` operation took 1.1592090129852295 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    399 |            10262 | 64555008 |  306.905 |              328.768 |              138.768 |            762.367 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2890.0553753612076
    time_step_min: 2746
  date: 2020-10-15_12-25-38
  done: false
  episode_len_mean: 762.3604640196171
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.95308169881383
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 226
  episodes_total: 84824
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.26501820959343e-44
        cur_lr: 5.0e-05
        entropy: 0.07813943984607856
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051932048906261725
        model: {}
        policy_loss: -0.012225643527926877
        total_loss: 0.4573753699660301
        vf_explained_var: 0.9984560012817383
        vf_loss: 0.4696400960286458
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.92903225806452
    gpu_util_percent0: 0.33096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626776453254503
    mean_env_wait_ms: 1.2076741746030317
    mean_inference_ms: 4.285109754194409
    mean_raw_obs_processing_ms: 0.37562781335233697
  time_since_restore: 10287.829294681549
  time_this_iter_s: 25.852448225021362
  time_total_s: 10287.829294681549
  timers:
    learn_throughput: 8684.797
    learn_time_ms: 18629.335
    sample_throughput: 23506.657
    sample_time_ms: 6882.816
    update_time_ms: 43.518
  timestamp: 1602764738
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: cb791_00000
  
2020-10-15 12:25:39,863	WARNING util.py:136 -- The `process_trial` operation took 1.1675450801849365 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    400 |          10287.8 | 64716800 |  306.953 |              328.768 |              138.768 |             762.36 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2889.793132259317
    time_step_min: 2746
  date: 2020-10-15_12-26-05
  done: false
  episode_len_mean: 762.3580855368401
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 306.99419241820664
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 192
  episodes_total: 85016
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.26501820959343e-44
        cur_lr: 5.0e-05
        entropy: 0.07587504014372826
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011311206491276002
        total_loss: .inf
        vf_explained_var: 0.9987655282020569
        vf_loss: 0.3412531564633052
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.15806451612903
    gpu_util_percent0: 0.3054838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626704854517547
    mean_env_wait_ms: 1.207630380782503
    mean_inference_ms: 4.28506491306818
    mean_raw_obs_processing_ms: 0.3756240657998349
  time_since_restore: 10313.731085538864
  time_this_iter_s: 25.901790857315063
  time_total_s: 10313.731085538864
  timers:
    learn_throughput: 8692.011
    learn_time_ms: 18613.873
    sample_throughput: 23543.672
    sample_time_ms: 6871.995
    update_time_ms: 43.168
  timestamp: 1602764765
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:26:07,444	WARNING util.py:136 -- The `process_trial` operation took 1.1704611778259277 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    401 |          10313.7 | 64878592 |  306.994 |              328.768 |              138.768 |            762.358 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2889.471179239193
    time_step_min: 2746
  date: 2020-10-15_12-26-33
  done: false
  episode_len_mean: 762.3511344705413
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.0440786001215
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 222
  episodes_total: 85238
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8975273143901446e-44
        cur_lr: 5.0e-05
        entropy: 0.07684870064258575
        entropy_coeff: 0.0005000000000000001
        kl: 0.004649578748891751
        model: {}
        policy_loss: -0.012008041056105867
        total_loss: 0.3763616035381953
        vf_explained_var: 0.9986848831176758
        vf_loss: 0.38840806980927783
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.18064516129032
    gpu_util_percent0: 0.3487096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462660805558355
    mean_env_wait_ms: 1.207583289251394
    mean_inference_ms: 4.285027236046634
    mean_raw_obs_processing_ms: 0.3756200157529446
  time_since_restore: 10339.486563682556
  time_this_iter_s: 25.755478143692017
  time_total_s: 10339.486563682556
  timers:
    learn_throughput: 8705.975
    learn_time_ms: 18584.019
    sample_throughput: 23613.757
    sample_time_ms: 6851.599
    update_time_ms: 40.936
  timestamp: 1602764793
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: cb791_00000
  
2020-10-15 12:26:34,749	WARNING util.py:136 -- The `process_trial` operation took 1.1593949794769287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    402 |          10339.5 | 65040384 |  307.044 |              328.768 |              138.768 |            762.351 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2889.1378381858835
    time_step_min: 2746
  date: 2020-10-15_12-27-00
  done: false
  episode_len_mean: 762.3449413747104
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.09466106689285
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 220
  episodes_total: 85458
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.487636571950723e-45
        cur_lr: 5.0e-05
        entropy: 0.07071438804268837
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007848317755209186
        total_loss: .inf
        vf_explained_var: 0.9986402988433838
        vf_loss: 0.4052441318829854
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.622580645161293
    gpu_util_percent0: 0.2870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626488919028632
    mean_env_wait_ms: 1.2075358204984927
    mean_inference_ms: 4.284978901767973
    mean_raw_obs_processing_ms: 0.3756164508690091
  time_since_restore: 10365.316440343857
  time_this_iter_s: 25.82987666130066
  time_total_s: 10365.316440343857
  timers:
    learn_throughput: 8718.703
    learn_time_ms: 18556.889
    sample_throughput: 23581.834
    sample_time_ms: 6860.874
    update_time_ms: 34.34
  timestamp: 1602764820
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:27:02,173	WARNING util.py:136 -- The `process_trial` operation took 1.177032470703125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    403 |          10365.3 | 65202176 |  307.095 |              328.768 |              138.768 |            762.345 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2888.872913527467
    time_step_min: 2746
  date: 2020-10-15_12-27-28
  done: false
  episode_len_mean: 762.3425102159953
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.13503334571624
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 192
  episodes_total: 85650
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4231454857926084e-44
        cur_lr: 5.0e-05
        entropy: 0.07184633612632751
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009467845016539892
        total_loss: .inf
        vf_explained_var: 0.9984790682792664
        vf_loss: 0.4189435963829358
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.912903225806446
    gpu_util_percent0: 0.2825806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626417367006364
    mean_env_wait_ms: 1.2074928968434284
    mean_inference_ms: 4.2849387462054604
    mean_raw_obs_processing_ms: 0.3756129387985411
  time_since_restore: 10391.39414525032
  time_this_iter_s: 26.077704906463623
  time_total_s: 10391.39414525032
  timers:
    learn_throughput: 8720.961
    learn_time_ms: 18552.083
    sample_throughput: 23565.004
    sample_time_ms: 6865.774
    update_time_ms: 34.168
  timestamp: 1602764848
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:27:29,945	WARNING util.py:136 -- The `process_trial` operation took 1.1904447078704834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    404 |          10391.4 | 65363968 |  307.135 |              328.768 |              138.768 |            762.343 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2888.559071262975
    time_step_min: 2746
  date: 2020-10-15_12-27-55
  done: false
  episode_len_mean: 762.3356583911686
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.18557530213855
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 226
  episodes_total: 85876
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1347182286889126e-44
        cur_lr: 5.0e-05
        entropy: 0.0722438624749581
        entropy_coeff: 0.0005000000000000001
        kl: 0.006212375902881225
        model: {}
        policy_loss: -0.00829869338000814
        total_loss: 0.21146421134471893
        vf_explained_var: 0.9992805123329163
        vf_loss: 0.21979902436335882
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.984375
    gpu_util_percent0: 0.3390625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626323262412552
    mean_env_wait_ms: 1.2074450492808602
    mean_inference_ms: 4.28490145403443
    mean_raw_obs_processing_ms: 0.3756089296201971
  time_since_restore: 10417.319878339767
  time_this_iter_s: 25.92573308944702
  time_total_s: 10417.319878339767
  timers:
    learn_throughput: 8715.161
    learn_time_ms: 18564.431
    sample_throughput: 23638.684
    sample_time_ms: 6844.374
    update_time_ms: 34.161
  timestamp: 1602764875
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: cb791_00000
  
2020-10-15 12:27:57,625	WARNING util.py:136 -- The `process_trial` operation took 1.143390417098999 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    405 |          10417.3 | 65525760 |  307.186 |              328.768 |              138.768 |            762.336 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2888.25190857435
    time_step_min: 2746
  date: 2020-10-15_12-28-23
  done: false
  episode_len_mean: 762.3327138841785
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.22953524184663
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 222
  episodes_total: 86098
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1347182286889126e-44
        cur_lr: 5.0e-05
        entropy: 0.07334172974030177
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012206496602933234
        total_loss: .inf
        vf_explained_var: 0.9979698657989502
        vf_loss: 0.6466275354226431
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.306451612903224
    gpu_util_percent0: 0.27032258064516124
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626214244801497
    mean_env_wait_ms: 1.2073971177064093
    mean_inference_ms: 4.284853383542934
    mean_raw_obs_processing_ms: 0.3756055152498117
  time_since_restore: 10443.2886095047
  time_this_iter_s: 25.96873116493225
  time_total_s: 10443.2886095047
  timers:
    learn_throughput: 8710.875
    learn_time_ms: 18573.565
    sample_throughput: 23643.579
    sample_time_ms: 6842.957
    update_time_ms: 27.601
  timestamp: 1602764903
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:28:25,180	WARNING util.py:136 -- The `process_trial` operation took 1.1684844493865967 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    406 |          10443.3 | 65687552 |   307.23 |              328.768 |              138.768 |            762.333 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2888.0713076548477
    time_step_min: 2746
  date: 2020-10-15_12-28-50
  done: false
  episode_len_mean: 762.335191516486
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.2570960810514
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 187
  episodes_total: 86285
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2020773430333693e-44
        cur_lr: 5.0e-05
        entropy: 0.0764562717328469
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052596050857876735
        model: {}
        policy_loss: -0.012118542188545689
        total_loss: 0.6767774571975073
        vf_explained_var: 0.9977302551269531
        vf_loss: 0.6889342268308004
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.096774193548388
    gpu_util_percent0: 0.32096774193548383
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626134431614152
    mean_env_wait_ms: 1.2073558671076072
    mean_inference_ms: 4.284811821431187
    mean_raw_obs_processing_ms: 0.3756017916905487
  time_since_restore: 10468.9343957901
  time_this_iter_s: 25.64578628540039
  time_total_s: 10468.9343957901
  timers:
    learn_throughput: 8707.742
    learn_time_ms: 18580.247
    sample_throughput: 23672.645
    sample_time_ms: 6834.555
    update_time_ms: 27.503
  timestamp: 1602764930
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: cb791_00000
  
2020-10-15 12:28:52,558	WARNING util.py:136 -- The `process_trial` operation took 1.1505992412567139 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    407 |          10468.9 | 65849344 |  307.257 |              328.768 |              138.768 |            762.335 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2887.8119340849958
    time_step_min: 2746
  date: 2020-10-15_12-29-18
  done: false
  episode_len_mean: 762.3354254802691
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.29775527660246
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 229
  episodes_total: 86514
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2020773430333693e-44
        cur_lr: 5.0e-05
        entropy: 0.07525912423928578
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011514960458346954
        total_loss: .inf
        vf_explained_var: 0.9981492161750793
        vf_loss: 0.6247005561987559
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.11612903225807
    gpu_util_percent0: 0.37677419354838715
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146260225701323
    mean_env_wait_ms: 1.2073067264295259
    mean_inference_ms: 4.284769296537659
    mean_raw_obs_processing_ms: 0.3755974831810884
  time_since_restore: 10494.933893442154
  time_this_iter_s: 25.999497652053833
  time_total_s: 10494.933893442154
  timers:
    learn_throughput: 8699.032
    learn_time_ms: 18598.852
    sample_throughput: 23758.51
    sample_time_ms: 6809.855
    update_time_ms: 27.635
  timestamp: 1602764958
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:29:20,099	WARNING util.py:136 -- The `process_trial` operation took 1.1305696964263916 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    408 |          10494.9 | 66011136 |  307.298 |              328.768 |              138.768 |            762.335 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2887.508656785282
    time_step_min: 2746
  date: 2020-10-15_12-29-46
  done: false
  episode_len_mean: 762.3295478128531
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.34345286051627
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 220
  episodes_total: 86734
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.803116014550054e-44
        cur_lr: 5.0e-05
        entropy: 0.06601444818079472
        entropy_coeff: 0.0005000000000000001
        kl: 0.003978466944924246
        model: {}
        policy_loss: -0.008540013145344952
        total_loss: 0.3494286860028903
        vf_explained_var: 0.998812198638916
        vf_loss: 0.3580017040173213
    num_steps_sampled: 66172928
    num_steps_trained: 66172928
  iterations_since_restore: 409
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.477419354838712
    gpu_util_percent0: 0.2980645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625928975419636
    mean_env_wait_ms: 1.2072598201376832
    mean_inference_ms: 4.284725996986137
    mean_raw_obs_processing_ms: 0.37559429506323394
  time_since_restore: 10521.0163667202
  time_this_iter_s: 26.082473278045654
  time_total_s: 10521.0163667202
  timers:
    learn_throughput: 8697.769
    learn_time_ms: 18601.552
    sample_throughput: 23775.275
    sample_time_ms: 6805.053
    update_time_ms: 28.693
  timestamp: 1602764986
  timesteps_since_restore: 0
  timesteps_total: 66172928
  training_iteration: 409
  trial_id: cb791_00000
  
2020-10-15 12:29:47,844	WARNING util.py:136 -- The `process_trial` operation took 1.1393566131591797 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    409 |            10521 | 66172928 |  307.343 |              328.768 |              138.768 |             762.33 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2887.2242708836866
    time_step_min: 2746
  date: 2020-10-15_12-30-13
  done: false
  episode_len_mean: 762.3229565717572
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.3852345717745
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 191
  episodes_total: 86925
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.401558007275027e-44
        cur_lr: 5.0e-05
        entropy: 0.06655303513010342
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007406430891326939
        total_loss: .inf
        vf_explained_var: 0.9991776943206787
        vf_loss: 0.22697941089669862
    num_steps_sampled: 66334720
    num_steps_trained: 66334720
  iterations_since_restore: 410
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.138709677419353
    gpu_util_percent0: 0.34774193548387106
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625849158309853
    mean_env_wait_ms: 1.2072184661692158
    mean_inference_ms: 4.28468712555045
    mean_raw_obs_processing_ms: 0.37559047836581017
  time_since_restore: 10546.964350938797
  time_this_iter_s: 25.947984218597412
  time_total_s: 10546.964350938797
  timers:
    learn_throughput: 8693.624
    learn_time_ms: 18610.42
    sample_throughput: 23773.866
    sample_time_ms: 6805.456
    update_time_ms: 27.825
  timestamp: 1602765013
  timesteps_since_restore: 0
  timesteps_total: 66334720
  training_iteration: 410
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:30:15,487	WARNING util.py:136 -- The `process_trial` operation took 1.1735105514526367 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    410 |            10547 | 66334720 |  307.385 |              328.768 |              138.768 |            762.323 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2886.873754534184
    time_step_min: 2746
  date: 2020-10-15_12-30-41
  done: false
  episode_len_mean: 762.3157363318226
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.4362122747756
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 230
  episodes_total: 87155
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6023370109125403e-44
        cur_lr: 5.0e-05
        entropy: 0.06945406086742878
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01062692350630338
        total_loss: .inf
        vf_explained_var: 0.9990739226341248
        vf_loss: 0.2879049281279246
    num_steps_sampled: 66496512
    num_steps_trained: 66496512
  iterations_since_restore: 411
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.29677419354839
    gpu_util_percent0: 0.32451612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625738723121448
    mean_env_wait_ms: 1.2071692713870374
    mean_inference_ms: 4.284641178991615
    mean_raw_obs_processing_ms: 0.3755864132294955
  time_since_restore: 10572.793045282364
  time_this_iter_s: 25.828694343566895
  time_total_s: 10572.793045282364
  timers:
    learn_throughput: 8697.4
    learn_time_ms: 18602.341
    sample_throughput: 23771.969
    sample_time_ms: 6805.999
    update_time_ms: 27.969
  timestamp: 1602765041
  timesteps_since_restore: 0
  timesteps_total: 66496512
  training_iteration: 411
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:30:42,975	WARNING util.py:136 -- The `process_trial` operation took 1.1497578620910645 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    411 |          10572.8 | 66496512 |  307.436 |              328.768 |              138.768 |            762.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2886.570306416892
    time_step_min: 2746
  date: 2020-10-15_12-31-08
  done: false
  episode_len_mean: 762.31037758524
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.4821885171198
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 87371
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.403505516368809e-44
        cur_lr: 5.0e-05
        entropy: 0.06780586019158363
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008971169678261504
        total_loss: .inf
        vf_explained_var: 0.9990864396095276
        vf_loss: 0.25914231191078824
    num_steps_sampled: 66658304
    num_steps_trained: 66658304
  iterations_since_restore: 412
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.019354838709678
    gpu_util_percent0: 0.3474193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625650268794
    mean_env_wait_ms: 1.2071226907819013
    mean_inference_ms: 4.284599256277646
    mean_raw_obs_processing_ms: 0.37558301511731057
  time_since_restore: 10598.760903835297
  time_this_iter_s: 25.96785855293274
  time_total_s: 10598.760903835297
  timers:
    learn_throughput: 8696.573
    learn_time_ms: 18604.11
    sample_throughput: 23713.74
    sample_time_ms: 6822.711
    update_time_ms: 27.943
  timestamp: 1602765068
  timesteps_since_restore: 0
  timesteps_total: 66658304
  training_iteration: 412
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:31:10,665	WARNING util.py:136 -- The `process_trial` operation took 1.2123980522155762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    412 |          10598.8 | 66658304 |  307.482 |              328.768 |              138.768 |             762.31 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2886.298257640674
    time_step_min: 2746
  date: 2020-10-15_12-31-37
  done: false
  episode_len_mean: 762.3032867388424
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.52416552810536
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 87564
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.105258274553214e-44
        cur_lr: 5.0e-05
        entropy: 0.07025131459037463
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009245724184438586
        total_loss: .inf
        vf_explained_var: 0.9991000294685364
        vf_loss: 0.2676999270915985
    num_steps_sampled: 66820096
    num_steps_trained: 66820096
  iterations_since_restore: 413
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.049999999999997
    gpu_util_percent0: 0.39156250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625587262472267
    mean_env_wait_ms: 1.207081556950579
    mean_inference_ms: 4.284564933846881
    mean_raw_obs_processing_ms: 0.37557964547792516
  time_since_restore: 10625.173118829727
  time_this_iter_s: 26.412214994430542
  time_total_s: 10625.173118829727
  timers:
    learn_throughput: 8669.605
    learn_time_ms: 18661.98
    sample_throughput: 23708.818
    sample_time_ms: 6824.128
    update_time_ms: 26.656
  timestamp: 1602765097
  timesteps_since_restore: 0
  timesteps_total: 66820096
  training_iteration: 413
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:31:38,674	WARNING util.py:136 -- The `process_trial` operation took 1.1624805927276611 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    413 |          10625.2 | 66820096 |  307.524 |              328.768 |              138.768 |            762.303 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2885.979078353617
    time_step_min: 2746
  date: 2020-10-15_12-32-04
  done: false
  episode_len_mean: 762.2942080984111
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.5757092538228
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 87795
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2157887411829823e-43
        cur_lr: 5.0e-05
        entropy: 0.0706899327536424
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01001466196612455
        total_loss: .inf
        vf_explained_var: 0.9994226098060608
        vf_loss: 0.17471979930996895
    num_steps_sampled: 66981888
    num_steps_trained: 66981888
  iterations_since_restore: 414
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.664516129032258
    gpu_util_percent0: 0.2709677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625470380514471
    mean_env_wait_ms: 1.2070333009907792
    mean_inference_ms: 4.284521102572394
    mean_raw_obs_processing_ms: 0.37557581551211267
  time_since_restore: 10651.066718578339
  time_this_iter_s: 25.89359974861145
  time_total_s: 10651.066718578339
  timers:
    learn_throughput: 8681.633
    learn_time_ms: 18636.126
    sample_throughput: 23731.908
    sample_time_ms: 6817.488
    update_time_ms: 28.299
  timestamp: 1602765124
  timesteps_since_restore: 0
  timesteps_total: 66981888
  training_iteration: 414
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:32:06,174	WARNING util.py:136 -- The `process_trial` operation took 1.1837236881256104 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    414 |          10651.1 | 66981888 |  307.576 |              328.768 |              138.768 |            762.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2885.6726650600217
    time_step_min: 2746
  date: 2020-10-15_12-32-32
  done: false
  episode_len_mean: 762.2868180940153
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.6207294346305
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 212
  episodes_total: 88007
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.823683111774473e-43
        cur_lr: 5.0e-05
        entropy: 0.06235715591659149
        entropy_coeff: 0.0005000000000000001
        kl: 0.005519280559383333
        model: {}
        policy_loss: -0.00760101400859033
        total_loss: 0.12966084169844785
        vf_explained_var: 0.9995315670967102
        vf_loss: 0.1372930289556583
    num_steps_sampled: 67143680
    num_steps_trained: 67143680
  iterations_since_restore: 415
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.596774193548388
    gpu_util_percent0: 0.327741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625387211037277
    mean_env_wait_ms: 1.2069869235324004
    mean_inference_ms: 4.284479908948687
    mean_raw_obs_processing_ms: 0.37557227118708164
  time_since_restore: 10677.186678886414
  time_this_iter_s: 26.11996030807495
  time_total_s: 10677.186678886414
  timers:
    learn_throughput: 8682.457
    learn_time_ms: 18634.357
    sample_throughput: 23657.284
    sample_time_ms: 6838.993
    update_time_ms: 26.244
  timestamp: 1602765152
  timesteps_since_restore: 0
  timesteps_total: 67143680
  training_iteration: 415
  trial_id: cb791_00000
  
2020-10-15 12:32:33,999	WARNING util.py:136 -- The `process_trial` operation took 1.1648714542388916 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    415 |          10677.2 | 67143680 |  307.621 |              328.768 |              138.768 |            762.287 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2885.371044572984
    time_step_min: 2746
  date: 2020-10-15_12-32-59
  done: false
  episode_len_mean: 762.2780328537905
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.6660163516606
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 202
  episodes_total: 88209
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.823683111774473e-43
        cur_lr: 5.0e-05
        entropy: 0.06260811040798824
        entropy_coeff: 0.0005000000000000001
        kl: 0.005968688715559741
        model: {}
        policy_loss: -0.006404271235320873
        total_loss: 0.20761217921972275
        vf_explained_var: 0.999232828617096
        vf_loss: 0.2140477548042933
    num_steps_sampled: 67305472
    num_steps_trained: 67305472
  iterations_since_restore: 416
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53548387096774
    gpu_util_percent0: 0.3325806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625320055384525
    mean_env_wait_ms: 1.206944995770193
    mean_inference_ms: 4.284446722390082
    mean_raw_obs_processing_ms: 0.37556852639150623
  time_since_restore: 10703.077426671982
  time_this_iter_s: 25.890747785568237
  time_total_s: 10703.077426671982
  timers:
    learn_throughput: 8680.76
    learn_time_ms: 18637.999
    sample_throughput: 23662.328
    sample_time_ms: 6837.535
    update_time_ms: 25.439
  timestamp: 1602765179
  timesteps_since_restore: 0
  timesteps_total: 67305472
  training_iteration: 416
  trial_id: cb791_00000
  
2020-10-15 12:33:01,621	WARNING util.py:136 -- The `process_trial` operation took 1.2149181365966797 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    416 |          10703.1 | 67305472 |  307.666 |              328.768 |              138.768 |            762.278 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2885.05081677904
    time_step_min: 2746
  date: 2020-10-15_12-33-27
  done: false
  episode_len_mean: 762.2687623678407
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.7161768644777
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 226
  episodes_total: 88435
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.823683111774473e-43
        cur_lr: 5.0e-05
        entropy: 0.0710162880520026
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009995837569780027
        total_loss: .inf
        vf_explained_var: 0.9994451999664307
        vf_loss: 0.1657672462364038
    num_steps_sampled: 67467264
    num_steps_trained: 67467264
  iterations_since_restore: 417
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.799999999999997
    gpu_util_percent0: 0.2890625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462519038501702
    mean_env_wait_ms: 1.2068971030579907
    mean_inference_ms: 4.2843973845679875
    mean_raw_obs_processing_ms: 0.37556498791627707
  time_since_restore: 10729.367727518082
  time_this_iter_s: 26.290300846099854
  time_total_s: 10729.367727518082
  timers:
    learn_throughput: 8661.431
    learn_time_ms: 18679.593
    sample_throughput: 23644.196
    sample_time_ms: 6842.779
    update_time_ms: 27.961
  timestamp: 1602765207
  timesteps_since_restore: 0
  timesteps_total: 67467264
  training_iteration: 417
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:33:29,519	WARNING util.py:136 -- The `process_trial` operation took 1.1694602966308594 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    417 |          10729.4 | 67467264 |  307.716 |              328.768 |              138.768 |            762.269 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2884.839100697501
    time_step_min: 2746
  date: 2020-10-15_12-33-55
  done: false
  episode_len_mean: 762.2737333739466
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.7406501471887
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 206
  episodes_total: 88641
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7355246676617107e-43
        cur_lr: 5.0e-05
        entropy: 0.08704975061118603
        entropy_coeff: 0.0005000000000000001
        kl: 0.005355683853849769
        model: {}
        policy_loss: -0.011683466417404512
        total_loss: 1.6165152589480083
        vf_explained_var: 0.9951724410057068
        vf_loss: 1.6282422443230946
    num_steps_sampled: 67629056
    num_steps_trained: 67629056
  iterations_since_restore: 418
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.253125
    gpu_util_percent0: 0.32156250000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462512299716643
    mean_env_wait_ms: 1.2068526856883632
    mean_inference_ms: 4.284361918637505
    mean_raw_obs_processing_ms: 0.3755617052305397
  time_since_restore: 10755.41182231903
  time_this_iter_s: 26.044094800949097
  time_total_s: 10755.41182231903
  timers:
    learn_throughput: 8661.614
    learn_time_ms: 18679.197
    sample_throughput: 23595.194
    sample_time_ms: 6856.99
    update_time_ms: 27.592
  timestamp: 1602765235
  timesteps_since_restore: 0
  timesteps_total: 67629056
  training_iteration: 418
  trial_id: cb791_00000
  
2020-10-15 12:33:57,396	WARNING util.py:136 -- The `process_trial` operation took 1.2259325981140137 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    418 |          10755.4 | 67629056 |  307.741 |              328.768 |              138.768 |            762.274 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2884.671812541522
    time_step_min: 2746
  date: 2020-10-15_12-34-23
  done: false
  episode_len_mean: 762.2760669908158
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.7672176928658
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 207
  episodes_total: 88848
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7355246676617107e-43
        cur_lr: 5.0e-05
        entropy: 0.08622600448628266
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010536226405141255
        total_loss: .inf
        vf_explained_var: 0.9967954158782959
        vf_loss: 1.0434202949206035
    num_steps_sampled: 67790848
    num_steps_trained: 67790848
  iterations_since_restore: 419
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.180645161290325
    gpu_util_percent0: 0.3551612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625055622130484
    mean_env_wait_ms: 1.206810565543494
    mean_inference_ms: 4.284330760446074
    mean_raw_obs_processing_ms: 0.37555811923581534
  time_since_restore: 10781.463560342789
  time_this_iter_s: 26.051738023757935
  time_total_s: 10781.463560342789
  timers:
    learn_throughput: 8667.807
    learn_time_ms: 18665.853
    sample_throughput: 23597.696
    sample_time_ms: 6856.263
    update_time_ms: 28.181
  timestamp: 1602765263
  timesteps_since_restore: 0
  timesteps_total: 67790848
  training_iteration: 419
  trial_id: cb791_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 12:34:25,055	WARNING util.py:136 -- The `process_trial` operation took 1.18636155128479 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | RUNNING  | 172.17.0.4:53595 |    419 |          10781.5 | 67790848 |  307.767 |              328.768 |              138.768 |            762.276 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_cb791_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 2884.5042849280603
    time_step_min: 2746
  date: 2020-10-15_12-34-51
  done: true
  episode_len_mean: 762.2801329261721
  episode_reward_max: 328.76767676767696
  episode_reward_mean: 307.78815934629193
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 224
  episodes_total: 89072
  experiment_id: b226f155d3634b5780dddd5255106f90
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.103287001492566e-43
        cur_lr: 5.0e-05
        entropy: 0.09609198570251465
        entropy_coeff: 0.0005000000000000001
        kl: 0.005715881823562086
        model: {}
        policy_loss: -0.014468325146784386
        total_loss: 1.800827145576477
        vf_explained_var: 0.9950910210609436
        vf_loss: 1.8153435289859772
    num_steps_sampled: 67952640
    num_steps_trained: 67952640
  iterations_since_restore: 420
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.416129032258063
    gpu_util_percent0: 0.2764516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 53595
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624922617182926
    mean_env_wait_ms: 1.2067632840928249
    mean_inference_ms: 4.284284328546395
    mean_raw_obs_processing_ms: 0.3755545910303431
  time_since_restore: 10807.540640830994
  time_this_iter_s: 26.077080488204956
  time_total_s: 10807.540640830994
  timers:
    learn_throughput: 8663.613
    learn_time_ms: 18674.887
    sample_throughput: 23587.096
    sample_time_ms: 6859.344
    update_time_ms: 28.585
  timestamp: 1602765291
  timesteps_since_restore: 0
  timesteps_total: 67952640
  training_iteration: 420
  trial_id: cb791_00000
  
2020-10-15 12:34:53,054	WARNING util.py:136 -- The `process_trial` operation took 1.3785111904144287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | TERMINATED |       |    420 |          10807.5 | 67952640 |  307.788 |              328.768 |              138.768 |             762.28 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_cb791_00000 | TERMINATED |       |    420 |          10807.5 | 67952640 |  307.788 |              328.768 |              138.768 |             762.28 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


